(0.0, 1.0) <class 'float'> 0.5
(3, 100) <class 'int'> 7
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0, 8) <class 'int'> 4
(0.0, 1.0) <class 'float'> 0.5
(0.005, 8.0) <class 'float'> 4.0
(3, 25) <class 'int'> 8
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.01
(0.0, 1.0) <class 'float'> 0.05
(0.0, 1.0) <class 'float'> 0.1
(0.0, 1.0) <class 'float'> 0.5
params:  [0.6490142459033698, 0.01, 0.01, 0.01, 0.5725886814698102, 0.16028107392892293, 0.3609746921562613, 0.6627680130757894, 0.3591576842195144, 0.7302304187458726, 0.9734511492175708, 0.1570316401879731, 0.4297539875829992, 0.9569089569224076, 0.20430656143020776, 0.01, 0.01, 0.5942741997785822]
Training loss = 0.04603796402613322
step = 0, Training Accuracy: 0.27666666666666667
Validation Accuracy: 0.33
Training loss = 0.042811973094940184
step = 1, Training Accuracy: 0.35
Training loss = 0.039402780731519066
step = 2, Training Accuracy: 0.38
Training loss = 0.03417291939258575
step = 3, Training Accuracy: 0.48333333333333334
Training loss = 0.03234585622946421
step = 4, Training Accuracy: 0.5166666666666667
Training loss = 0.030811251600583393
step = 5, Training Accuracy: 0.5633333333333334
Training loss = 0.03180532455444336
step = 6, Training Accuracy: 0.57
Training loss = 0.030920472542444864
step = 7, Training Accuracy: 0.5033333333333333
Training loss = 0.030714842279752096
step = 8, Training Accuracy: 0.57
Training loss = 0.030006034175554912
step = 9, Training Accuracy: 0.5833333333333334
Training loss = 0.0296871421734492
step = 10, Training Accuracy: 0.55
Training loss = 0.029136495590209963
step = 11, Training Accuracy: 0.6166666666666667
Training loss = 0.030747533837954203
step = 12, Training Accuracy: 0.57
Training loss = 0.02726531744003296
step = 13, Training Accuracy: 0.63
Training loss = 0.0275265896320343
step = 14, Training Accuracy: 0.5866666666666667
Validation Accuracy: 0.53125
params:  [0.22759277734363675, 0.01, 0.4959508325786198, 0.99, 0.31948801633118096, 0.21249187506201694, 0.3198083930243585, 0.6127094055037016, 0.15470192677330918, 0.5332767769129598, 0.33637248720779855, 0.01, 0.5202584614063771, 0.4322671098540393, 0.4496946306764662, 0.01, 0.3467634736309567, 0.13374690500869335]
Training loss = 0.0358107183376948
step = 0, Training Accuracy: 0.47
Validation Accuracy: 0.48
Training loss = 0.03216777284940084
step = 1, Training Accuracy: 0.53
Training loss = 0.032985967993736265
step = 2, Training Accuracy: 0.5033333333333333
Training loss = 0.03359534740447998
step = 3, Training Accuracy: 0.5266666666666666
Training loss = 0.03170238494873047
step = 4, Training Accuracy: 0.5166666666666667
Training loss = 0.028810818195343018
step = 5, Training Accuracy: 0.6
Training loss = 0.029670669436454772
step = 6, Training Accuracy: 0.5866666666666667
Training loss = 0.028826722304026286
step = 7, Training Accuracy: 0.61
Training loss = 0.03218865990638733
step = 8, Training Accuracy: 0.5266666666666666
Training loss = 0.029601672093073527
step = 9, Training Accuracy: 0.6066666666666667
Training loss = 0.03039000391960144
step = 10, Training Accuracy: 0.53
Training loss = 0.02942953089872996
step = 11, Training Accuracy: 0.6
Training loss = 0.029611469904581706
step = 12, Training Accuracy: 0.59
Training loss = 0.0281391042470932
step = 13, Training Accuracy: 0.59
Training loss = 0.029826664725939433
step = 14, Training Accuracy: 0.5866666666666667
Validation Accuracy: 0.54
params:  [0.5626590785014266, 0.01, 0.5972251908184385, 0.01, 0.6030854868705384, 0.6171366678656747, 0.3618083687120638, 0.2840467374815874, 0.05644340288977179, 0.4096688913232134, 0.4649928198488812, 0.27868321162971843, 0.7215399739986231, 0.5590583707607371, 0.01, 0.01, 0.01, 0.6835028866522603]
Training loss = 0.03378105243047078
step = 0, Training Accuracy: 0.4766666666666667
Validation Accuracy: 0.535
Training loss = 0.03333891729513804
step = 1, Training Accuracy: 0.49333333333333335
Training loss = 0.03260416030883789
step = 2, Training Accuracy: 0.51
Training loss = 0.03218088825543722
step = 3, Training Accuracy: 0.5
Training loss = 0.03309569497903188
step = 4, Training Accuracy: 0.5133333333333333
Training loss = 0.031892716487248736
step = 5, Training Accuracy: 0.5133333333333333
Training loss = 0.030366917451222736
step = 6, Training Accuracy: 0.5233333333333333
Training loss = 0.031388169328371684
step = 7, Training Accuracy: 0.5333333333333333
Training loss = 0.030862138470013935
step = 8, Training Accuracy: 0.54
Training loss = 0.0322233921289444
step = 9, Training Accuracy: 0.5266666666666666
Training loss = 0.03315916796525319
step = 10, Training Accuracy: 0.4766666666666667
Training loss = 0.030602724552154542
step = 11, Training Accuracy: 0.52
Training loss = 0.030846354961395265
step = 12, Training Accuracy: 0.5133333333333333
Training loss = 0.02994855443636576
step = 13, Training Accuracy: 0.5466666666666666
Training loss = 0.03012517313162486
step = 14, Training Accuracy: 0.55
Validation Accuracy: 0.53125
params:  [0.8092998567487852, 0.01, 0.6084908075142903, 0.8010598693676072, 0.47839696352589983, 0.7068720085712469, 0.7437577467182594, 0.1411380127757988, 0.16809950779819155, 0.4443023070008549, 0.3559350332117664, 0.519936265409435, 0.5993790294210692, 0.4072362872446356, 0.01, 0.3293840357348595, 0.2084186816525242, 0.9614109699397908]
Training loss = 0.03632204035917918
step = 0, Training Accuracy: 0.52
Validation Accuracy: 0.5525
Training loss = 0.03421685338020325
step = 1, Training Accuracy: 0.49
Training loss = 0.033287692666053775
step = 2, Training Accuracy: 0.49666666666666665
Training loss = 0.03343942900498708
step = 3, Training Accuracy: 0.49666666666666665
Training loss = 0.032079036037127175
step = 4, Training Accuracy: 0.5266666666666666
Training loss = 0.03221737821896871
step = 5, Training Accuracy: 0.55
Training loss = 0.03152140518029531
step = 6, Training Accuracy: 0.5533333333333333
Training loss = 0.030378608107566832
step = 7, Training Accuracy: 0.5566666666666666
Training loss = 0.03105631947517395
step = 8, Training Accuracy: 0.5533333333333333
Training loss = 0.031744856437047324
step = 9, Training Accuracy: 0.52
Training loss = 0.031201775074005126
step = 10, Training Accuracy: 0.5733333333333334
Training loss = 0.031407474279403685
step = 11, Training Accuracy: 0.5366666666666666
Training loss = 0.030292149186134338
step = 12, Training Accuracy: 0.53
Training loss = 0.028836667935053507
step = 13, Training Accuracy: 0.6
Training loss = 0.031184059182802835
step = 14, Training Accuracy: 0.5466666666666666
Validation Accuracy: 0.5525
params:  [0.48925218826701455, 0.1398624462999672, 0.7746206353106222, 0.34947288692463907, 0.25745191913204374, 0.1445189345179058, 0.9433682134224548, 0.6071337714535239, 0.43409843364874645, 0.01, 0.527215537526004, 0.13757052213296705, 0.5261141204714513, 0.7465707513125671, 0.01, 0.5193930967442019, 0.01, 0.6539802299340068]
Training loss = 0.03085186958312988
step = 0, Training Accuracy: 0.56
Validation Accuracy: 0.5575
Training loss = 0.030936147570610046
step = 1, Training Accuracy: 0.5533333333333333
Training loss = 0.031016770799954733
step = 2, Training Accuracy: 0.56
Training loss = 0.02875237802664439
step = 3, Training Accuracy: 0.5933333333333334
Training loss = 0.027643757263819378
step = 4, Training Accuracy: 0.59
Training loss = 0.027789644797643027
step = 5, Training Accuracy: 0.6433333333333333
Training loss = 0.027692455450693765
step = 6, Training Accuracy: 0.62
Training loss = 0.02834332267443339
step = 7, Training Accuracy: 0.58
Training loss = 0.02725198447704315
step = 8, Training Accuracy: 0.6
Training loss = 0.02706956704457601
step = 9, Training Accuracy: 0.6033333333333334
Training loss = 0.026298725605010988
step = 10, Training Accuracy: 0.6266666666666667
Training loss = 0.02601787785689036
step = 11, Training Accuracy: 0.6266666666666667
Training loss = 0.027674393852551778
step = 12, Training Accuracy: 0.5966666666666667
Training loss = 0.027328266104062398
step = 13, Training Accuracy: 0.6466666666666666
Training loss = 0.02793625235557556
step = 14, Training Accuracy: 0.6233333333333333
Validation Accuracy: 0.54875
params:  [0.5291232648044121, 0.16245237044642336, 0.45161428650019725, 0.25931681923351435, 0.3971856450419692, 0.17380640317039228, 0.07538877738487576, 0.42962385998745595, 0.5015340369927382, 0.5783165816539668, 0.5885233876847262, 0.01, 0.3823675540603527, 0.40170135602066953, 0.01, 0.34059349715986675, 0.665855770363159, 0.5523733438495517]
Training loss = 0.03010623017946879
step = 0, Training Accuracy: 0.5366666666666666
Validation Accuracy: 0.555
Training loss = 0.027941978375116985
step = 1, Training Accuracy: 0.5966666666666667
Training loss = 0.029083774089813233
step = 2, Training Accuracy: 0.5833333333333334
Training loss = 0.027502831419308982
step = 3, Training Accuracy: 0.6033333333333334
Training loss = 0.026241171757380166
step = 4, Training Accuracy: 0.68
Training loss = 0.02692253311475118
step = 5, Training Accuracy: 0.6466666666666666
Training loss = 0.024420009354750315
step = 6, Training Accuracy: 0.6333333333333333
Training loss = 0.025183629393577576
step = 7, Training Accuracy: 0.65
Training loss = 0.025407172044118246
step = 8, Training Accuracy: 0.66
Training loss = 0.025734625856081644
step = 9, Training Accuracy: 0.6666666666666666
Training loss = 0.02519250551859538
step = 10, Training Accuracy: 0.6566666666666666
Training loss = 0.025700260202089945
step = 11, Training Accuracy: 0.65
Training loss = 0.025091195503870647
step = 12, Training Accuracy: 0.6966666666666667
Training loss = 0.023310007055600483
step = 13, Training Accuracy: 0.69
Training loss = 0.022819600105285644
step = 14, Training Accuracy: 0.6833333333333333
Validation Accuracy: 0.52875
params:  [0.5772651172168293, 0.01, 0.9208382932808297, 0.22718376356157832, 0.737309584112914, 0.5255799098060322, 0.8428468443545061, 0.1493965887141404, 0.489586469088427, 0.5904642027000837, 0.4419790151310166, 0.9662453610183132, 0.5180690629823079, 0.49204583736523494, 0.01, 0.02766622527014984, 0.2760571281400811, 0.99]
Training loss = 0.03178843935330709
step = 0, Training Accuracy: 0.5533333333333333
Validation Accuracy: 0.57625
Training loss = 0.03017967363198598
step = 1, Training Accuracy: 0.5366666666666666
Training loss = 0.02845862289269765
step = 2, Training Accuracy: 0.61
Training loss = 0.026909178694089254
step = 3, Training Accuracy: 0.6233333333333333
Training loss = 0.0283968981107076
step = 4, Training Accuracy: 0.58
Training loss = 0.02806278705596924
step = 5, Training Accuracy: 0.6
Training loss = 0.026369685133298237
step = 6, Training Accuracy: 0.6266666666666667
Training loss = 0.02667718450228373
step = 7, Training Accuracy: 0.6066666666666667
Training loss = 0.02711460053920746
step = 8, Training Accuracy: 0.6166666666666667
Training loss = 0.028198822140693663
step = 9, Training Accuracy: 0.63
Training loss = 0.024073660572369895
step = 10, Training Accuracy: 0.68
Training loss = 0.026817599336306255
step = 11, Training Accuracy: 0.6266666666666667
Training loss = 0.025723705490430196
step = 12, Training Accuracy: 0.67
Training loss = 0.024702436725298562
step = 13, Training Accuracy: 0.6633333333333333
Training loss = 0.027848238547643026
step = 14, Training Accuracy: 0.6266666666666667
Validation Accuracy: 0.5775
params:  [0.2028391024607935, 0.4333799396867903, 0.5682379803812388, 0.13074070506981345, 0.7440551652109009, 0.20338154513829731, 0.26502401229912886, 0.9649803215052618, 0.22417272972985908, 0.6420777291905545, 0.1809961904475219, 0.24784161971453547, 0.03480097068016019, 0.3489573037651402, 0.039895409526292364, 0.01, 0.01, 0.5553901575596912]
Training loss = 0.03218200782934825
step = 0, Training Accuracy: 0.5833333333333334
Validation Accuracy: 0.5825
Training loss = 0.028765133420626324
step = 1, Training Accuracy: 0.63
Training loss = 0.028779932260513306
step = 2, Training Accuracy: 0.67
Training loss = 0.0267107888062795
step = 3, Training Accuracy: 0.6433333333333333
Training loss = 0.026635331710179646
step = 4, Training Accuracy: 0.6333333333333333
Training loss = 0.026354689796765644
step = 5, Training Accuracy: 0.6333333333333333
Training loss = 0.026165152589480083
step = 6, Training Accuracy: 0.6433333333333333
Training loss = 0.02723174591859182
step = 7, Training Accuracy: 0.6466666666666666
Training loss = 0.0273406587044398
step = 8, Training Accuracy: 0.6166666666666667
Training loss = 0.02675510088602702
step = 9, Training Accuracy: 0.68
Training loss = 0.02583867947260539
step = 10, Training Accuracy: 0.64
Training loss = 0.026529796520868936
step = 11, Training Accuracy: 0.64
Training loss = 0.023033629059791564
step = 12, Training Accuracy: 0.7133333333333334
Training loss = 0.023453378876050313
step = 13, Training Accuracy: 0.6933333333333334
Training loss = 0.025237369338671368
step = 14, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.61375
[[0.6490142459033698, 0.01, 0.01, 0.01, 0.5725886814698102, 0.16028107392892293, 0.3609746921562613, 0.6627680130757894, 0.3591576842195144, 0.7302304187458726, 0.9734511492175708, 0.1570316401879731, 0.4297539875829992, 0.9569089569224076, 0.20430656143020776, 0.01, 0.01, 0.5942741997785822], [0.22759277734363675, 0.01, 0.4959508325786198, 0.99, 0.31948801633118096, 0.21249187506201694, 0.3198083930243585, 0.6127094055037016, 0.15470192677330918, 0.5332767769129598, 0.33637248720779855, 0.01, 0.5202584614063771, 0.4322671098540393, 0.4496946306764662, 0.01, 0.3467634736309567, 0.13374690500869335], [0.5626590785014266, 0.01, 0.5972251908184385, 0.01, 0.6030854868705384, 0.6171366678656747, 0.3618083687120638, 0.2840467374815874, 0.05644340288977179, 0.4096688913232134, 0.4649928198488812, 0.27868321162971843, 0.7215399739986231, 0.5590583707607371, 0.01, 0.01, 0.01, 0.6835028866522603], [0.8092998567487852, 0.01, 0.6084908075142903, 0.8010598693676072, 0.47839696352589983, 0.7068720085712469, 0.7437577467182594, 0.1411380127757988, 0.16809950779819155, 0.4443023070008549, 0.3559350332117664, 0.519936265409435, 0.5993790294210692, 0.4072362872446356, 0.01, 0.3293840357348595, 0.2084186816525242, 0.9614109699397908], [0.48925218826701455, 0.1398624462999672, 0.7746206353106222, 0.34947288692463907, 0.25745191913204374, 0.1445189345179058, 0.9433682134224548, 0.6071337714535239, 0.43409843364874645, 0.01, 0.527215537526004, 0.13757052213296705, 0.5261141204714513, 0.7465707513125671, 0.01, 0.5193930967442019, 0.01, 0.6539802299340068], [0.5291232648044121, 0.16245237044642336, 0.45161428650019725, 0.25931681923351435, 0.3971856450419692, 0.17380640317039228, 0.07538877738487576, 0.42962385998745595, 0.5015340369927382, 0.5783165816539668, 0.5885233876847262, 0.01, 0.3823675540603527, 0.40170135602066953, 0.01, 0.34059349715986675, 0.665855770363159, 0.5523733438495517], [0.5772651172168293, 0.01, 0.9208382932808297, 0.22718376356157832, 0.737309584112914, 0.5255799098060322, 0.8428468443545061, 0.1493965887141404, 0.489586469088427, 0.5904642027000837, 0.4419790151310166, 0.9662453610183132, 0.5180690629823079, 0.49204583736523494, 0.01, 0.02766622527014984, 0.2760571281400811, 0.99], [0.2028391024607935, 0.4333799396867903, 0.5682379803812388, 0.13074070506981345, 0.7440551652109009, 0.20338154513829731, 0.26502401229912886, 0.9649803215052618, 0.22417272972985908, 0.6420777291905545, 0.1809961904475219, 0.24784161971453547, 0.03480097068016019, 0.3489573037651402, 0.039895409526292364, 0.01, 0.01, 0.5553901575596912]]
gen	nevals	avg     	std      	min    	max    
0  	8     	0.552969	0.0274142	0.52875	0.61375
params:  [0.2527118591157992, 0.18576947776659733, 0.6788368444026867, 0.2575381530166783, 0.6646178303990872, 0.6094661758524441, 0.15794807331059169, 0.4036244770980164, 0.5949868056072448, 0.8883188988778965, 0.19295292681594695, 0.3207157166095878, 0.2785266349570545, 0.32686536935637217, 0.08068352301036749, 0.01, 0.01, 0.8593936289836066]
Training loss = 0.029914746284484862
step = 0, Training Accuracy: 0.5966666666666667
Validation Accuracy: 0.64375
Training loss = 0.026934128403663635
step = 1, Training Accuracy: 0.6233333333333333
Training loss = 0.025523614088694253
step = 2, Training Accuracy: 0.6466666666666666
Training loss = 0.024289698998133342
step = 3, Training Accuracy: 0.6833333333333333
Training loss = 0.024989232619603476
step = 4, Training Accuracy: 0.67
Training loss = 0.023847521146138508
step = 5, Training Accuracy: 0.6733333333333333
Training loss = 0.025641695062319437
step = 6, Training Accuracy: 0.6666666666666666
Training loss = 0.024744027455647785
step = 7, Training Accuracy: 0.6833333333333333
Training loss = 0.025073134104410807
step = 8, Training Accuracy: 0.6566666666666666
Training loss = 0.023948963085810345
step = 9, Training Accuracy: 0.6833333333333333
Training loss = 0.02170207550128301
step = 10, Training Accuracy: 0.7166666666666667
Training loss = 0.025219504634539286
step = 11, Training Accuracy: 0.6533333333333333
Training loss = 0.0230442809065183
step = 12, Training Accuracy: 0.72
Training loss = 0.025606282552083335
step = 13, Training Accuracy: 0.6533333333333333
Training loss = 0.02447031021118164
step = 14, Training Accuracy: 0.68
Validation Accuracy: 0.60375
params:  [0.4870181506307103, 0.01, 0.8403731806704337, 0.01, 0.7057843847792684, 0.31402613531812834, 0.8301471434365932, 0.6471614542914136, 0.12943552313881002, 0.8389813804855252, 0.42073812960105356, 0.7819641023349703, 0.3373539746951151, 0.4674132625096403, 0.06529932714639815, 0.01, 0.14234843601694677, 0.979560035428049]
Training loss = 0.027507124940554302
step = 0, Training Accuracy: 0.6366666666666667
Validation Accuracy: 0.6525
Training loss = 0.026514142354329428
step = 1, Training Accuracy: 0.6533333333333333
Training loss = 0.02477593223253886
step = 2, Training Accuracy: 0.6666666666666666
Training loss = 0.02825150271256765
step = 3, Training Accuracy: 0.6166666666666667
Training loss = 0.02642288009325663
step = 4, Training Accuracy: 0.6133333333333333
Training loss = 0.02385330537954966
step = 5, Training Accuracy: 0.6466666666666666
Training loss = 0.025105064709981282
step = 6, Training Accuracy: 0.66
Training loss = 0.02505130926767985
step = 7, Training Accuracy: 0.6533333333333333
Training loss = 0.023605916301409402
step = 8, Training Accuracy: 0.6966666666666667
Training loss = 0.023384370605150858
step = 9, Training Accuracy: 0.69
Training loss = 0.021207005381584168
step = 10, Training Accuracy: 0.7166666666666667
Training loss = 0.0226074743270874
step = 11, Training Accuracy: 0.7
Training loss = 0.022114214301109315
step = 12, Training Accuracy: 0.6966666666666667
Training loss = 0.022410358190536498
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.022612195809682212
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.66375
params:  [0.9057216036590037, 0.8707660362975531, 0.3576621717194899, 0.13682745752720915, 0.9461950123485705, 0.23999651645327827, 0.7105873519087081, 0.99, 0.41382545816468025, 0.31099614582497204, 0.21788925825332567, 0.816177159421283, 0.5761705308647589, 0.6409429925434347, 0.20353115562790192, 0.24978934499770417, 0.01, 0.3236746856557943]
Training loss = 0.029581403533617656
step = 0, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.61
Training loss = 0.02741263826688131
step = 1, Training Accuracy: 0.63
Training loss = 0.02629935403664907
step = 2, Training Accuracy: 0.62
Training loss = 0.027980082233746848
step = 3, Training Accuracy: 0.6466666666666666
Training loss = 0.029284175237019858
step = 4, Training Accuracy: 0.61
Training loss = 0.02761504371960958
step = 5, Training Accuracy: 0.6566666666666666
Training loss = 0.025415940682093303
step = 6, Training Accuracy: 0.6533333333333333
Training loss = 0.024974148472150168
step = 7, Training Accuracy: 0.6566666666666666
Training loss = 0.025759124557177226
step = 8, Training Accuracy: 0.6533333333333333
Training loss = 0.026152294774850208
step = 9, Training Accuracy: 0.6733333333333333
Training loss = 0.023962908387184144
step = 10, Training Accuracy: 0.6933333333333334
Training loss = 0.0228519997994105
step = 11, Training Accuracy: 0.7033333333333334
Training loss = 0.024801121950149538
step = 12, Training Accuracy: 0.6866666666666666
Training loss = 0.02558451771736145
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.023039393226305643
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.66875
params:  [0.42574784681153227, 0.35981361710918613, 0.7345735000763088, 0.3212981295350035, 0.7179078962924732, 0.7731903804586826, 0.38693663226611485, 0.5835217839513791, 0.7109528957755435, 0.7439183941571931, 0.15257663151694184, 0.7569602242953709, 0.32991427178542554, 0.28143467890062446, 0.29449446593402206, 0.35430606008521554, 0.5117666178845296, 0.7733995902589652]
Training loss = 0.029616622328758238
step = 0, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.66
Training loss = 0.027237460414568582
step = 1, Training Accuracy: 0.6233333333333333
Training loss = 0.024982192317644755
step = 2, Training Accuracy: 0.6266666666666667
Training loss = 0.02574288288752238
step = 3, Training Accuracy: 0.6566666666666666
Training loss = 0.02740939497947693
step = 4, Training Accuracy: 0.6233333333333333
Training loss = 0.02563426156838735
step = 5, Training Accuracy: 0.6466666666666666
Training loss = 0.026523382663726808
step = 6, Training Accuracy: 0.63
Training loss = 0.02538396974404653
step = 7, Training Accuracy: 0.6333333333333333
Training loss = 0.02508370320002238
step = 8, Training Accuracy: 0.65
Training loss = 0.025324768821398416
step = 9, Training Accuracy: 0.6533333333333333
Training loss = 0.02583116849263509
step = 10, Training Accuracy: 0.6233333333333333
Training loss = 0.02490577280521393
step = 11, Training Accuracy: 0.6166666666666667
Training loss = 0.02369224190711975
step = 12, Training Accuracy: 0.6766666666666666
Training loss = 0.024651028513908387
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.026215227643648784
step = 14, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.6925
params:  [0.9517548710039496, 0.6362795089331157, 0.99, 0.3237650792838843, 0.5552505312114266, 0.6391883001402359, 0.680614208578144, 0.5338085376233894, 0.7765772588506616, 0.99, 0.29953956516059516, 0.7015876535991992, 0.3084888623617549, 0.3373846854433625, 0.01, 0.4484044412960806, 0.24018711922392005, 0.0588888645698834]
Training loss = 0.027173051436742146
step = 0, Training Accuracy: 0.65
Validation Accuracy: 0.69125
Training loss = 0.025347727139790853
step = 1, Training Accuracy: 0.6433333333333333
Training loss = 0.024819334745407106
step = 2, Training Accuracy: 0.6533333333333333
Training loss = 0.02532988985379537
step = 3, Training Accuracy: 0.66
Training loss = 0.024971061547597248
step = 4, Training Accuracy: 0.6866666666666666
Training loss = 0.022134964962800343
step = 5, Training Accuracy: 0.6966666666666667
Training loss = 0.024336697260538737
step = 6, Training Accuracy: 0.6666666666666666
Training loss = 0.022351003289222716
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.02468431572119395
step = 8, Training Accuracy: 0.7066666666666667
Training loss = 0.022155543466409047
step = 9, Training Accuracy: 0.6966666666666667
Training loss = 0.023946195244789123
step = 10, Training Accuracy: 0.6766666666666666
Training loss = 0.0230353581905365
step = 11, Training Accuracy: 0.6866666666666666
Training loss = 0.023812375466028848
step = 12, Training Accuracy: 0.6866666666666666
Training loss = 0.021759926080703734
step = 13, Training Accuracy: 0.72
Training loss = 0.02327234129110972
step = 14, Training Accuracy: 0.68
Validation Accuracy: 0.70125
params:  [0.33001004454746263, 0.6019386730157827, 0.5427133471479202, 0.4663079513652586, 0.5230613498055849, 0.6353032999489934, 0.3978031109958317, 0.45687213309727503, 0.2061100456982739, 0.7016262808071232, 0.01, 0.35278219414362105, 0.01, 0.07618478358069003, 0.01, 0.17314202539017293, 0.13114044303716413, 0.7050357857983277]
Training loss = 0.02792901953061422
step = 0, Training Accuracy: 0.61
Validation Accuracy: 0.66625
Training loss = 0.02632472276687622
step = 1, Training Accuracy: 0.6366666666666667
Training loss = 0.025272391637166342
step = 2, Training Accuracy: 0.6266666666666667
Training loss = 0.02288348356882731
step = 3, Training Accuracy: 0.7166666666666667
Training loss = 0.024367334047953288
step = 4, Training Accuracy: 0.6766666666666666
Training loss = 0.023897483944892883
step = 5, Training Accuracy: 0.65
Training loss = 0.023526620666186014
step = 6, Training Accuracy: 0.6966666666666667
Training loss = 0.02453286647796631
step = 7, Training Accuracy: 0.6866666666666666
Training loss = 0.02158467431863149
step = 8, Training Accuracy: 0.7066666666666667
Training loss = 0.024033737778663637
step = 9, Training Accuracy: 0.69
Training loss = 0.02374017188946406
step = 10, Training Accuracy: 0.6766666666666666
Training loss = 0.021332530081272127
step = 11, Training Accuracy: 0.6966666666666667
Training loss = 0.024199721415837604
step = 12, Training Accuracy: 0.68
Training loss = 0.024230910340944926
step = 13, Training Accuracy: 0.6833333333333333
Training loss = 0.02290866692860921
step = 14, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.7075
params:  [0.6440079625217932, 0.26959266305415874, 0.43672455157323764, 0.1852821013566481, 0.6991347357151376, 0.6231122555229595, 0.01, 0.5966597737538134, 0.2161984272571978, 0.23796508351250767, 0.2238916022328583, 0.4117954628288382, 0.32523335262152525, 0.35535667525658315, 0.4993406654442177, 0.09308301887054765, 0.031712090309046884, 0.4879111160540341]
Training loss = 0.026922135949134826
step = 0, Training Accuracy: 0.5966666666666667
Validation Accuracy: 0.69625
Training loss = 0.02587210714817047
step = 1, Training Accuracy: 0.6666666666666666
Training loss = 0.02507356603940328
step = 2, Training Accuracy: 0.6733333333333333
Training loss = 0.024073885480562846
step = 3, Training Accuracy: 0.6833333333333333
Training loss = 0.023203465541203817
step = 4, Training Accuracy: 0.6733333333333333
Training loss = 0.024040348132451376
step = 5, Training Accuracy: 0.6666666666666666
Training loss = 0.023372396429379782
step = 6, Training Accuracy: 0.68
Training loss = 0.02351716955502828
step = 7, Training Accuracy: 0.6966666666666667
Training loss = 0.023904859224955242
step = 8, Training Accuracy: 0.6766666666666666
Training loss = 0.024787779053052267
step = 9, Training Accuracy: 0.6566666666666666
Training loss = 0.024617170294125874
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.02482551078001658
step = 11, Training Accuracy: 0.6366666666666667
Training loss = 0.02433108945687612
step = 12, Training Accuracy: 0.6833333333333333
Training loss = 0.024187792539596558
step = 13, Training Accuracy: 0.6566666666666666
Training loss = 0.02288013180096944
step = 14, Training Accuracy: 0.6833333333333333
Validation Accuracy: 0.725
params:  [0.6842847341888261, 0.10265341058375221, 0.9752952370492287, 0.01, 0.8291686825720256, 0.2915972035434517, 0.648572803470205, 0.5768600155302801, 0.4537161633447674, 0.4822222743403863, 0.6747761078599415, 0.5021429534454886, 0.4300197263325207, 0.22304758747764467, 0.201788069314835, 0.01, 0.06131597448663676, 0.8990623676974682]
Training loss = 0.024071801702181497
step = 0, Training Accuracy: 0.67
Validation Accuracy: 0.72375
Training loss = 0.023226100504398346
step = 1, Training Accuracy: 0.6766666666666666
Training loss = 0.022290425101915996
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.022366604208946227
step = 3, Training Accuracy: 0.7033333333333334
Training loss = 0.021293036142985024
step = 4, Training Accuracy: 0.71
Training loss = 0.02192617247502009
step = 5, Training Accuracy: 0.73
Training loss = 0.019715936680634816
step = 6, Training Accuracy: 0.73
Training loss = 0.02036389470100403
step = 7, Training Accuracy: 0.7033333333333334
Training loss = 0.02075740208228429
step = 8, Training Accuracy: 0.7333333333333333
Training loss = 0.019967162509759266
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.02060239772001902
step = 10, Training Accuracy: 0.7366666666666667
Training loss = 0.020359431008497873
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.018570903539657593
step = 12, Training Accuracy: 0.75
Training loss = 0.021375815868377685
step = 13, Training Accuracy: 0.73
Training loss = 0.01876537412405014
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.70875
[[0.2527118591157992, 0.18576947776659733, 0.6788368444026867, 0.2575381530166783, 0.6646178303990872, 0.6094661758524441, 0.15794807331059169, 0.4036244770980164, 0.5949868056072448, 0.8883188988778965, 0.19295292681594695, 0.3207157166095878, 0.2785266349570545, 0.32686536935637217, 0.08068352301036749, 0.01, 0.01, 0.8593936289836066], [0.4870181506307103, 0.01, 0.8403731806704337, 0.01, 0.7057843847792684, 0.31402613531812834, 0.8301471434365932, 0.6471614542914136, 0.12943552313881002, 0.8389813804855252, 0.42073812960105356, 0.7819641023349703, 0.3373539746951151, 0.4674132625096403, 0.06529932714639815, 0.01, 0.14234843601694677, 0.979560035428049], [0.9057216036590037, 0.8707660362975531, 0.3576621717194899, 0.13682745752720915, 0.9461950123485705, 0.23999651645327827, 0.7105873519087081, 0.99, 0.41382545816468025, 0.31099614582497204, 0.21788925825332567, 0.816177159421283, 0.5761705308647589, 0.6409429925434347, 0.20353115562790192, 0.24978934499770417, 0.01, 0.3236746856557943], [0.42574784681153227, 0.35981361710918613, 0.7345735000763088, 0.3212981295350035, 0.7179078962924732, 0.7731903804586826, 0.38693663226611485, 0.5835217839513791, 0.7109528957755435, 0.7439183941571931, 0.15257663151694184, 0.7569602242953709, 0.32991427178542554, 0.28143467890062446, 0.29449446593402206, 0.35430606008521554, 0.5117666178845296, 0.7733995902589652], [0.9517548710039496, 0.6362795089331157, 0.99, 0.3237650792838843, 0.5552505312114266, 0.6391883001402359, 0.680614208578144, 0.5338085376233894, 0.7765772588506616, 0.99, 0.29953956516059516, 0.7015876535991992, 0.3084888623617549, 0.3373846854433625, 0.01, 0.4484044412960806, 0.24018711922392005, 0.0588888645698834], [0.33001004454746263, 0.6019386730157827, 0.5427133471479202, 0.4663079513652586, 0.5230613498055849, 0.6353032999489934, 0.3978031109958317, 0.45687213309727503, 0.2061100456982739, 0.7016262808071232, 0.01, 0.35278219414362105, 0.01, 0.07618478358069003, 0.01, 0.17314202539017293, 0.13114044303716413, 0.7050357857983277], [0.6440079625217932, 0.26959266305415874, 0.43672455157323764, 0.1852821013566481, 0.6991347357151376, 0.6231122555229595, 0.01, 0.5966597737538134, 0.2161984272571978, 0.23796508351250767, 0.2238916022328583, 0.4117954628288382, 0.32523335262152525, 0.35535667525658315, 0.4993406654442177, 0.09308301887054765, 0.031712090309046884, 0.4879111160540341], [0.6842847341888261, 0.10265341058375221, 0.9752952370492287, 0.01, 0.8291686825720256, 0.2915972035434517, 0.648572803470205, 0.5768600155302801, 0.4537161633447674, 0.4822222743403863, 0.6747761078599415, 0.5021429534454886, 0.4300197263325207, 0.22304758747764467, 0.201788069314835, 0.01, 0.06131597448663676, 0.8990623676974682]]
1  	8     	0.683906	0.0358787	0.60375	0.725  
params:  [0.5186055773314363, 0.7433671294580155, 0.6550669957182447, 0.188923521044451, 0.8742249154134852, 0.767904237265789, 0.3190508136433558, 0.01, 0.21368708518909363, 0.13431821570302915, 0.5273394271455817, 0.21239535879612753, 0.3449506064013801, 0.15794368093385736, 0.53929501699627, 0.2331438311393055, 0.01, 0.12866098297469608]
Training loss = 0.030750040213267008
step = 0, Training Accuracy: 0.5566666666666666
Validation Accuracy: 0.72875
Training loss = 0.02569192906220754
step = 1, Training Accuracy: 0.63
Training loss = 0.028026909828186036
step = 2, Training Accuracy: 0.5933333333333334
Training loss = 0.027714675466219585
step = 3, Training Accuracy: 0.6133333333333333
Training loss = 0.02728368600209554
step = 4, Training Accuracy: 0.6
Training loss = 0.026504071553548177
step = 5, Training Accuracy: 0.6266666666666667
Training loss = 0.025858777562777203
step = 6, Training Accuracy: 0.6533333333333333
Training loss = 0.024269495606422425
step = 7, Training Accuracy: 0.67
Training loss = 0.0257819002866745
step = 8, Training Accuracy: 0.69
Training loss = 0.02598691125710805
step = 9, Training Accuracy: 0.62
Training loss = 0.026586955388387044
step = 10, Training Accuracy: 0.6266666666666667
Training loss = 0.025469204982121785
step = 11, Training Accuracy: 0.6633333333333333
Training loss = 0.026444253325462342
step = 12, Training Accuracy: 0.6533333333333333
Training loss = 0.026097487807273865
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.026664900581041973
step = 14, Training Accuracy: 0.6233333333333333
Validation Accuracy: 0.695
params:  [0.6319744405167024, 0.3407537165150726, 0.7603052997696376, 0.01, 0.4605114374084842, 0.99, 0.09803684449110589, 0.99, 0.44218848312606124, 0.2908291651048159, 0.3908457624759782, 0.35387122854200115, 0.3421663268518262, 0.5300892333117332, 0.3562691987840995, 0.031031555369809616, 0.01, 0.34082701853787395]
Training loss = 0.026195990641911825
step = 0, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.6825
Training loss = 0.024321553707122804
step = 1, Training Accuracy: 0.64
Training loss = 0.02446939965089162
step = 2, Training Accuracy: 0.6733333333333333
Training loss = 0.023097317119439444
step = 3, Training Accuracy: 0.73
Training loss = 0.022950980265935263
step = 4, Training Accuracy: 0.6866666666666666
Training loss = 0.02488486369450887
step = 5, Training Accuracy: 0.6866666666666666
Training loss = 0.024197211066881816
step = 6, Training Accuracy: 0.6666666666666666
Training loss = 0.021698774298032124
step = 7, Training Accuracy: 0.76
Training loss = 0.02470428705215454
step = 8, Training Accuracy: 0.6533333333333333
Training loss = 0.02188043862581253
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.022011119226614636
step = 10, Training Accuracy: 0.72
Training loss = 0.02203435758749644
step = 11, Training Accuracy: 0.72
Training loss = 0.023161711196104686
step = 12, Training Accuracy: 0.7233333333333334
Training loss = 0.021846326092878978
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.02251909554004669
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.695
params:  [0.7303994435641324, 0.3193840053713262, 0.99, 0.32220858820282516, 0.8670300925468037, 0.37878203704221125, 0.26522701606519905, 0.6428932244592045, 0.16324812360535954, 0.38733454120635463, 0.647650580726275, 0.597188707197085, 0.01, 0.22370776286528982, 0.8392307279277298, 0.01, 0.01, 0.4883935559751993]
Training loss = 0.02297025998433431
step = 0, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.7125
Training loss = 0.02241429517666499
step = 1, Training Accuracy: 0.73
Training loss = 0.022732835014661154
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.02350237121184667
step = 3, Training Accuracy: 0.7
Training loss = 0.021137048999468486
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.02122165381908417
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.021516855359077453
step = 6, Training Accuracy: 0.71
Training loss = 0.02064790338277817
step = 7, Training Accuracy: 0.71
Training loss = 0.02073298513889313
step = 8, Training Accuracy: 0.7033333333333334
Training loss = 0.018313846389452618
step = 9, Training Accuracy: 0.75
Training loss = 0.021741576294104257
step = 10, Training Accuracy: 0.7233333333333334
Training loss = 0.01970629781484604
step = 11, Training Accuracy: 0.73
Training loss = 0.02330189347267151
step = 12, Training Accuracy: 0.7166666666666667
Training loss = 0.019427828590075174
step = 13, Training Accuracy: 0.7333333333333333
Training loss = 0.02211251050233841
step = 14, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.725
params:  [0.29141356733015106, 0.3027558347314705, 0.4989697627692252, 0.01, 0.99, 0.18456860958794935, 0.14982393232366023, 0.33861062375694406, 0.2806036780130331, 0.1288305519891953, 0.5685915752643101, 0.951126439884555, 0.99, 0.199860954106371, 0.364058124764509, 0.05288754994581041, 0.4996149118383487, 0.1619369393464284]
Training loss = 0.02495954910914103
step = 0, Training Accuracy: 0.66
Validation Accuracy: 0.70625
Training loss = 0.02117153565088908
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.02244503955046336
step = 2, Training Accuracy: 0.6833333333333333
Training loss = 0.020792451898256938
step = 3, Training Accuracy: 0.7066666666666667
Training loss = 0.01993285189072291
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.020949279765288036
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.020432284971078237
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.020569666425387066
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.021068353255589804
step = 8, Training Accuracy: 0.73
Training loss = 0.018021156489849092
step = 9, Training Accuracy: 0.76
Training loss = 0.017752361098925272
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.01923983037471771
step = 11, Training Accuracy: 0.77
Training loss = 0.01802792022625605
step = 12, Training Accuracy: 0.76
Training loss = 0.01917313734690348
step = 13, Training Accuracy: 0.73
Training loss = 0.019850666026274364
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7
params:  [0.5721920852462673, 0.1897572014034405, 0.3510329887631424, 0.5084527994130055, 0.5056203314680515, 0.057646942318726635, 0.2954865186817738, 0.9507731925432757, 0.6200696287166163, 0.64112585302879, 0.42794185249276984, 0.3794620336201443, 0.39540016350334495, 0.585597905146028, 0.369020919656388, 0.01, 0.01, 0.517756274189737]
Training loss = 0.0204358638326327
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.71125
Training loss = 0.018995510439078014
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.018468086222807566
step = 2, Training Accuracy: 0.78
Training loss = 0.01759679873784383
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.015600430369377137
step = 4, Training Accuracy: 0.79
Training loss = 0.018398519853750864
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.014853881051143011
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.014757226705551147
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.01417324274778366
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.014735390891631445
step = 9, Training Accuracy: 0.81
Training loss = 0.013164589802424112
step = 10, Training Accuracy: 0.8333333333333334
Training loss = 0.01363656019171079
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.014376823703447977
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.013227454622586569
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.01367720901966095
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.6975
params:  [0.7182343489229696, 0.2779568218831104, 0.22021387814995913, 0.29135402285285167, 0.6705855486976471, 0.6812758470011803, 0.13754657548308488, 0.887690884781791, 0.5449850617293124, 0.4420143989220523, 0.4276095530715469, 0.6928923586275837, 0.025357193074032558, 0.06615114718775872, 0.369740800680644, 0.01, 0.01, 0.3161213448073599]
Training loss = 0.0304065469900767
step = 0, Training Accuracy: 0.6166666666666667
Validation Accuracy: 0.7075
Training loss = 0.02155232012271881
step = 1, Training Accuracy: 0.71
Training loss = 0.022560011943181357
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.02195111960172653
step = 3, Training Accuracy: 0.74
Training loss = 0.022761251231034597
step = 4, Training Accuracy: 0.7033333333333334
Training loss = 0.02227773070335388
step = 5, Training Accuracy: 0.68
Training loss = 0.020735259552796682
step = 6, Training Accuracy: 0.7
Training loss = 0.02103379269440969
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.021256738702456156
step = 8, Training Accuracy: 0.68
Training loss = 0.02153458187977473
step = 9, Training Accuracy: 0.7033333333333334
Training loss = 0.018113298416137694
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.01875242163737615
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.01720418095588684
step = 12, Training Accuracy: 0.77
Training loss = 0.017987741231918333
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.01844495048125585
step = 14, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.73625
params:  [0.8709231016225322, 0.16385921681837082, 0.3236690918101169, 0.18476326985478672, 0.8679734432160227, 0.8831856439498464, 0.6289064044332993, 0.9480332464929029, 0.4217467711542978, 0.3669426852923011, 0.25897958724961423, 0.7479882546282982, 0.01, 0.01, 0.22102587076975813, 0.01, 0.21629269662954992, 0.6953071791750509]
Training loss = 0.027151500980059307
step = 0, Training Accuracy: 0.6166666666666667
Validation Accuracy: 0.6525
Training loss = 0.02361495276292165
step = 1, Training Accuracy: 0.6533333333333333
Training loss = 0.023764002124468487
step = 2, Training Accuracy: 0.67
Training loss = 0.02145221839348475
step = 3, Training Accuracy: 0.71
Training loss = 0.02252034346262614
step = 4, Training Accuracy: 0.6666666666666666
Training loss = 0.01965065022309621
step = 5, Training Accuracy: 0.73
Training loss = 0.020904106100400287
step = 6, Training Accuracy: 0.7266666666666667
Training loss = 0.020285879174868265
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.020990991393725077
step = 8, Training Accuracy: 0.7166666666666667
Training loss = 0.020363950232664744
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.019578970472017922
step = 10, Training Accuracy: 0.74
Training loss = 0.01974402864774068
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.019938443501790363
step = 12, Training Accuracy: 0.7133333333333334
Training loss = 0.019154919385910033
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.02188715696334839
step = 14, Training Accuracy: 0.67
Validation Accuracy: 0.72625
params:  [0.8256755269372071, 0.27510867599075506, 0.36330997193470793, 0.01, 0.99, 0.2843711341282288, 0.1335049181921499, 0.26389594573745906, 0.10316160734428681, 0.1396815879178161, 0.20889542558635582, 0.23810479686709518, 0.01, 0.19608942178090233, 0.4152117136086925, 0.3432638510642113, 0.42915073443456686, 0.9492701155225478]
Training loss = 0.03090612272421519
step = 0, Training Accuracy: 0.6
Validation Accuracy: 0.7525
Training loss = 0.028288942178090415
step = 1, Training Accuracy: 0.6066666666666667
Training loss = 0.026013480226198833
step = 2, Training Accuracy: 0.6333333333333333
Training loss = 0.02635064701239268
step = 3, Training Accuracy: 0.57
Training loss = 0.024421855409940085
step = 4, Training Accuracy: 0.6833333333333333
Training loss = 0.025320993463198344
step = 5, Training Accuracy: 0.6333333333333333
Training loss = 0.023751670718193053
step = 6, Training Accuracy: 0.6733333333333333
Training loss = 0.024178956548372904
step = 7, Training Accuracy: 0.66
Training loss = 0.02283000648021698
step = 8, Training Accuracy: 0.67
Training loss = 0.023228594462076823
step = 9, Training Accuracy: 0.6566666666666666
Training loss = 0.02266602079073588
step = 10, Training Accuracy: 0.6633333333333333
Training loss = 0.024737619558970133
step = 11, Training Accuracy: 0.66
Training loss = 0.024308767517407737
step = 12, Training Accuracy: 0.6366666666666667
Training loss = 0.024341880281766256
step = 13, Training Accuracy: 0.67
Training loss = 0.0229299000898997
step = 14, Training Accuracy: 0.6266666666666667
Validation Accuracy: 0.7075
[[0.5186055773314363, 0.7433671294580155, 0.6550669957182447, 0.188923521044451, 0.8742249154134852, 0.767904237265789, 0.3190508136433558, 0.01, 0.21368708518909363, 0.13431821570302915, 0.5273394271455817, 0.21239535879612753, 0.3449506064013801, 0.15794368093385736, 0.53929501699627, 0.2331438311393055, 0.01, 0.12866098297469608], [0.6319744405167024, 0.3407537165150726, 0.7603052997696376, 0.01, 0.4605114374084842, 0.99, 0.09803684449110589, 0.99, 0.44218848312606124, 0.2908291651048159, 0.3908457624759782, 0.35387122854200115, 0.3421663268518262, 0.5300892333117332, 0.3562691987840995, 0.031031555369809616, 0.01, 0.34082701853787395], [0.7303994435641324, 0.3193840053713262, 0.99, 0.32220858820282516, 0.8670300925468037, 0.37878203704221125, 0.26522701606519905, 0.6428932244592045, 0.16324812360535954, 0.38733454120635463, 0.647650580726275, 0.597188707197085, 0.01, 0.22370776286528982, 0.8392307279277298, 0.01, 0.01, 0.4883935559751993], [0.29141356733015106, 0.3027558347314705, 0.4989697627692252, 0.01, 0.99, 0.18456860958794935, 0.14982393232366023, 0.33861062375694406, 0.2806036780130331, 0.1288305519891953, 0.5685915752643101, 0.951126439884555, 0.99, 0.199860954106371, 0.364058124764509, 0.05288754994581041, 0.4996149118383487, 0.1619369393464284], [0.5721920852462673, 0.1897572014034405, 0.3510329887631424, 0.5084527994130055, 0.5056203314680515, 0.057646942318726635, 0.2954865186817738, 0.9507731925432757, 0.6200696287166163, 0.64112585302879, 0.42794185249276984, 0.3794620336201443, 0.39540016350334495, 0.585597905146028, 0.369020919656388, 0.01, 0.01, 0.517756274189737], [0.7182343489229696, 0.2779568218831104, 0.22021387814995913, 0.29135402285285167, 0.6705855486976471, 0.6812758470011803, 0.13754657548308488, 0.887690884781791, 0.5449850617293124, 0.4420143989220523, 0.4276095530715469, 0.6928923586275837, 0.025357193074032558, 0.06615114718775872, 0.369740800680644, 0.01, 0.01, 0.3161213448073599], [0.8709231016225322, 0.16385921681837082, 0.3236690918101169, 0.18476326985478672, 0.8679734432160227, 0.8831856439498464, 0.6289064044332993, 0.9480332464929029, 0.4217467711542978, 0.3669426852923011, 0.25897958724961423, 0.7479882546282982, 0.01, 0.01, 0.22102587076975813, 0.01, 0.21629269662954992, 0.6953071791750509], [0.8256755269372071, 0.27510867599075506, 0.36330997193470793, 0.01, 0.99, 0.2843711341282288, 0.1335049181921499, 0.26389594573745906, 0.10316160734428681, 0.1396815879178161, 0.20889542558635582, 0.23810479686709518, 0.01, 0.19608942178090233, 0.4152117136086925, 0.3432638510642113, 0.42915073443456686, 0.9492701155225478]]
2  	8     	0.710312	0.0153698	0.695  	0.73625
params:  [0.6890255490240389, 0.3917404364969219, 0.2650484648832353, 0.01, 0.5624325412093983, 0.99, 0.6130908896393101, 0.753470640507238, 0.5357237123709919, 0.5474251565642074, 0.8731405754236152, 0.4439673259020336, 0.17675532997934834, 0.4297097877656507, 0.3785005834023018, 0.01, 0.04183046433624421, 0.19930285383908458]
Training loss = 0.02299927095572154
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.6975
Training loss = 0.02312808096408844
step = 1, Training Accuracy: 0.68
Training loss = 0.02535268485546112
step = 2, Training Accuracy: 0.6266666666666667
Training loss = 0.025059400002161662
step = 3, Training Accuracy: 0.6766666666666666
Training loss = 0.02201336681842804
step = 4, Training Accuracy: 0.6833333333333333
Training loss = 0.023385009070237477
step = 5, Training Accuracy: 0.6933333333333334
Training loss = 0.024433202942212424
step = 6, Training Accuracy: 0.6666666666666666
Training loss = 0.02469196101029714
step = 7, Training Accuracy: 0.67
Training loss = 0.022444565693537394
step = 8, Training Accuracy: 0.6866666666666666
Training loss = 0.023430001934369404
step = 9, Training Accuracy: 0.6733333333333333
Training loss = 0.02129694660504659
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.021946249802907308
step = 11, Training Accuracy: 0.6733333333333333
Training loss = 0.02350244015455246
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.023249114950497946
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.021427700519561766
step = 14, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.715
params:  [0.4587443622884647, 0.01, 0.2963602909265572, 0.3171322276094889, 0.99, 0.99, 0.16766445388589346, 0.7682056397945187, 0.5099165900751998, 0.08871266458758631, 0.5842591042127645, 0.7264553749264846, 0.18072636051302415, 0.01, 0.5905093328631085, 0.2528133541908793, 0.461825537408044, 0.6117742233501717]
Training loss = 0.022131110827128094
step = 0, Training Accuracy: 0.7
Validation Accuracy: 0.73625
Training loss = 0.022743170062700907
step = 1, Training Accuracy: 0.6766666666666666
Training loss = 0.021889605522155763
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.021363106767336527
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.02193110207716624
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.01990666429201762
step = 5, Training Accuracy: 0.72
Training loss = 0.02057403971751531
step = 6, Training Accuracy: 0.72
Training loss = 0.02088256279627482
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.018798303107420603
step = 8, Training Accuracy: 0.76
Training loss = 0.018227388461430866
step = 9, Training Accuracy: 0.73
Training loss = 0.018658101161321005
step = 10, Training Accuracy: 0.72
Training loss = 0.019045199751853942
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.01899812380472819
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01800110956033071
step = 13, Training Accuracy: 0.74
Training loss = 0.017698834935824078
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.74625
params:  [0.6873253601368715, 0.1819205012835215, 0.5695663324053375, 0.16217286600392902, 0.8171205525922394, 0.37725046677679563, 0.12209366562184437, 0.684231840626919, 0.3592180772136172, 0.3616531577495542, 0.8087232574469052, 0.6948174156789697, 0.01, 0.01, 0.4502743139250118, 0.01, 0.3045399298244223, 0.8587807606471689]
Training loss = 0.022488366961479187
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.73875
Training loss = 0.021219258705774943
step = 1, Training Accuracy: 0.71
Training loss = 0.020557891031106314
step = 2, Training Accuracy: 0.72
Training loss = 0.019101179937521615
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.022429822484652202
step = 4, Training Accuracy: 0.71
Training loss = 0.018414565324783326
step = 5, Training Accuracy: 0.75
Training loss = 0.020152119398117067
step = 6, Training Accuracy: 0.7
Training loss = 0.017943529685338338
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.017887311081091564
step = 8, Training Accuracy: 0.75
Training loss = 0.020487785140673318
step = 9, Training Accuracy: 0.74
Training loss = 0.01855487753947576
step = 10, Training Accuracy: 0.76
Training loss = 0.018971474766731264
step = 11, Training Accuracy: 0.72
Training loss = 0.01749232719341914
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017205427785714468
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.018688077628612517
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.715
params:  [0.6654963422460012, 0.13670253718254474, 0.690590404111789, 0.6925846939988052, 0.8085571109187725, 0.8636507677020913, 0.47860891793067367, 0.7716808012308557, 0.3742623882623306, 0.46726223748597107, 0.4632300780275958, 0.6310960376358875, 0.0433381697890587, 0.01, 0.8413998950173696, 0.01, 0.06057169887832519, 0.7206128106633642]
Training loss = 0.026900466680526734
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.69125
Training loss = 0.022515077193578086
step = 1, Training Accuracy: 0.68
Training loss = 0.022568124731381735
step = 2, Training Accuracy: 0.6666666666666666
Training loss = 0.023532134890556337
step = 3, Training Accuracy: 0.6733333333333333
Training loss = 0.020893697142601014
step = 4, Training Accuracy: 0.72
Training loss = 0.022230899333953856
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.022424583633740742
step = 6, Training Accuracy: 0.6666666666666666
Training loss = 0.023495522737503053
step = 7, Training Accuracy: 0.6766666666666666
Training loss = 0.021542250712712607
step = 8, Training Accuracy: 0.7
Training loss = 0.020054641366004943
step = 9, Training Accuracy: 0.72
Training loss = 0.01987201323111852
step = 10, Training Accuracy: 0.76
Training loss = 0.023214065432548524
step = 11, Training Accuracy: 0.6933333333333334
Training loss = 0.01910869240760803
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.020720490018526713
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.018965855538845063
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.74375
params:  [0.99, 0.400821896926181, 0.01, 0.062049798773587905, 0.7760320404755806, 0.6321567509547683, 0.6485131095399703, 0.4564499927295465, 0.5624179259939377, 0.7709448603528148, 0.6827193750256175, 0.6184455737379682, 0.01, 0.2807842738802163, 0.35523082149910123, 0.14571662329637097, 0.6319405859745141, 0.528882505260618]
Training loss = 0.025603694717089336
step = 0, Training Accuracy: 0.66
Validation Accuracy: 0.71625
Training loss = 0.024848666389783225
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.023560849527517955
step = 2, Training Accuracy: 0.69
Training loss = 0.025464583039283752
step = 3, Training Accuracy: 0.6666666666666666
Training loss = 0.023270806670188902
step = 4, Training Accuracy: 0.7
Training loss = 0.023194137215614318
step = 5, Training Accuracy: 0.6733333333333333
Training loss = 0.022407472729682923
step = 6, Training Accuracy: 0.69
Training loss = 0.02282988468805949
step = 7, Training Accuracy: 0.7133333333333334
Training loss = 0.020881590843200685
step = 8, Training Accuracy: 0.7033333333333334
Training loss = 0.021504347721735637
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.021372934579849245
step = 10, Training Accuracy: 0.71
Training loss = 0.023523156245549518
step = 11, Training Accuracy: 0.6933333333333334
Training loss = 0.021593372523784637
step = 12, Training Accuracy: 0.72
Training loss = 0.020623007814089458
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.022570945421854654
step = 14, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.72875
params:  [0.6862729988668821, 0.23570785250818363, 0.4603091324489978, 0.31000149421687895, 0.8405414693835123, 0.9240313979514253, 0.03536526263728823, 0.6933236518729566, 0.06001046736254417, 0.14636229084203828, 0.18893281206378035, 0.5645982014674986, 0.01, 0.01, 0.5872710218287095, 0.01, 0.01, 0.5626541621043551]
Training loss = 0.04093619267145793
step = 0, Training Accuracy: 0.38
Validation Accuracy: 0.7025
Training loss = 0.035573788086573285
step = 1, Training Accuracy: 0.4066666666666667
Training loss = 0.034707971413930255
step = 2, Training Accuracy: 0.42
Training loss = 0.03447281757990519
step = 3, Training Accuracy: 0.4
Training loss = 0.03562743107477824
step = 4, Training Accuracy: 0.37333333333333335
Training loss = 0.03489166299502055
step = 5, Training Accuracy: 0.3933333333333333
Training loss = 0.03310630917549133
step = 6, Training Accuracy: 0.42333333333333334
Training loss = 0.03566804011662801
step = 7, Training Accuracy: 0.3933333333333333
Training loss = 0.03394318163394928
step = 8, Training Accuracy: 0.46
Training loss = 0.03339769045511882
step = 9, Training Accuracy: 0.45
Training loss = 0.03473341981569926
step = 10, Training Accuracy: 0.38333333333333336
Training loss = 0.032931029597918195
step = 11, Training Accuracy: 0.42333333333333334
Training loss = 0.03353375017642975
step = 12, Training Accuracy: 0.41
Training loss = 0.032984185020128884
step = 13, Training Accuracy: 0.4866666666666667
Training loss = 0.0345136284828186
step = 14, Training Accuracy: 0.4
Validation Accuracy: 0.70375
params:  [0.9457652036817917, 0.22646106656688558, 0.3584862414750838, 0.8749139453148874, 0.7773775845486547, 0.6020738741464651, 0.01, 0.4298251126513746, 0.01, 0.3794966958082817, 0.6527073257955891, 0.704740689949738, 0.0656780537958006, 0.01, 0.5380079275519791, 0.01, 0.14834410280806556, 0.4003391742078008]
Training loss = 0.02859058658281962
step = 0, Training Accuracy: 0.5733333333333334
Validation Accuracy: 0.71875
Training loss = 0.027911876042683918
step = 1, Training Accuracy: 0.57
Training loss = 0.027011005679766338
step = 2, Training Accuracy: 0.5366666666666666
Training loss = 0.028152212500572205
step = 3, Training Accuracy: 0.6066666666666667
Training loss = 0.025855239828427634
step = 4, Training Accuracy: 0.62
Training loss = 0.027040311296780903
step = 5, Training Accuracy: 0.5966666666666667
Training loss = 0.02793566445509593
step = 6, Training Accuracy: 0.6066666666666667
Training loss = 0.025813137690226237
step = 7, Training Accuracy: 0.5666666666666667
Training loss = 0.02706929584344228
step = 8, Training Accuracy: 0.58
Training loss = 0.024895315368970234
step = 9, Training Accuracy: 0.6233333333333333
Training loss = 0.027105910976727803
step = 10, Training Accuracy: 0.59
Training loss = 0.027083600958188375
step = 11, Training Accuracy: 0.5633333333333334
Training loss = 0.02685239811738332
step = 12, Training Accuracy: 0.5766666666666667
Training loss = 0.026448149879773456
step = 13, Training Accuracy: 0.6133333333333333
Training loss = 0.025160631537437438
step = 14, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.72625
params:  [0.893483516652816, 0.01, 0.13088339175224686, 0.14501554064776528, 0.44148988471646167, 0.49858940186539985, 0.01, 0.99, 0.8397498081058397, 0.5935319422953433, 0.7203651898152916, 0.6681428776399532, 0.1583377746172415, 0.2684949968304886, 0.7132888379501496, 0.01, 0.01, 0.8813271413692936]
Training loss = 0.021047937174638112
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.7225
Training loss = 0.01989253560702006
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.018799133896827698
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.01859056293964386
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.01763951708873113
step = 4, Training Accuracy: 0.77
Training loss = 0.016938528418540953
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.01604580064614614
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.015877085824807485
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016385866006215413
step = 8, Training Accuracy: 0.78
Training loss = 0.015390504896640778
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015643039147059123
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01563566694657008
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.014483851393063863
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.015149949491024018
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.013785768747329712
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7625
[[0.6890255490240389, 0.3917404364969219, 0.2650484648832353, 0.01, 0.5624325412093983, 0.99, 0.6130908896393101, 0.753470640507238, 0.5357237123709919, 0.5474251565642074, 0.8731405754236152, 0.4439673259020336, 0.17675532997934834, 0.4297097877656507, 0.3785005834023018, 0.01, 0.04183046433624421, 0.19930285383908458], [0.4587443622884647, 0.01, 0.2963602909265572, 0.3171322276094889, 0.99, 0.99, 0.16766445388589346, 0.7682056397945187, 0.5099165900751998, 0.08871266458758631, 0.5842591042127645, 0.7264553749264846, 0.18072636051302415, 0.01, 0.5905093328631085, 0.2528133541908793, 0.461825537408044, 0.6117742233501717], [0.6873253601368715, 0.1819205012835215, 0.5695663324053375, 0.16217286600392902, 0.8171205525922394, 0.37725046677679563, 0.12209366562184437, 0.684231840626919, 0.3592180772136172, 0.3616531577495542, 0.8087232574469052, 0.6948174156789697, 0.01, 0.01, 0.4502743139250118, 0.01, 0.3045399298244223, 0.8587807606471689], [0.6654963422460012, 0.13670253718254474, 0.690590404111789, 0.6925846939988052, 0.8085571109187725, 0.8636507677020913, 0.47860891793067367, 0.7716808012308557, 0.3742623882623306, 0.46726223748597107, 0.4632300780275958, 0.6310960376358875, 0.0433381697890587, 0.01, 0.8413998950173696, 0.01, 0.06057169887832519, 0.7206128106633642], [0.99, 0.400821896926181, 0.01, 0.062049798773587905, 0.7760320404755806, 0.6321567509547683, 0.6485131095399703, 0.4564499927295465, 0.5624179259939377, 0.7709448603528148, 0.6827193750256175, 0.6184455737379682, 0.01, 0.2807842738802163, 0.35523082149910123, 0.14571662329637097, 0.6319405859745141, 0.528882505260618], [0.6862729988668821, 0.23570785250818363, 0.4603091324489978, 0.31000149421687895, 0.8405414693835123, 0.9240313979514253, 0.03536526263728823, 0.6933236518729566, 0.06001046736254417, 0.14636229084203828, 0.18893281206378035, 0.5645982014674986, 0.01, 0.01, 0.5872710218287095, 0.01, 0.01, 0.5626541621043551], [0.9457652036817917, 0.22646106656688558, 0.3584862414750838, 0.8749139453148874, 0.7773775845486547, 0.6020738741464651, 0.01, 0.4298251126513746, 0.01, 0.3794966958082817, 0.6527073257955891, 0.704740689949738, 0.0656780537958006, 0.01, 0.5380079275519791, 0.01, 0.14834410280806556, 0.4003391742078008], [0.893483516652816, 0.01, 0.13088339175224686, 0.14501554064776528, 0.44148988471646167, 0.49858940186539985, 0.01, 0.99, 0.8397498081058397, 0.5935319422953433, 0.7203651898152916, 0.6681428776399532, 0.1583377746172415, 0.2684949968304886, 0.7132888379501496, 0.01, 0.01, 0.8813271413692936]]
3  	8     	0.730156	0.0182478	0.70375	0.7625 
params:  [0.6554169512707358, 0.04851761017465711, 0.34086797930277646, 0.5256806295610943, 0.8975223091021686, 0.99, 0.05392374073634655, 0.849812494112581, 0.8759134484087799, 0.1633744641530694, 0.5593000700044074, 0.43342391409538655, 0.3303117210461405, 0.01, 0.29404102509202573, 0.01, 0.6723278298695613, 0.99]
Training loss = 0.024714905619621277
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.7625
Training loss = 0.02171625663836797
step = 1, Training Accuracy: 0.6933333333333334
Training loss = 0.019638391037782033
step = 2, Training Accuracy: 0.74
Training loss = 0.019698732594648997
step = 3, Training Accuracy: 0.78
Training loss = 0.02051750401655833
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01811497817436854
step = 5, Training Accuracy: 0.79
Training loss = 0.01900898774464925
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.017546530465284982
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.0183096577723821
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01815942645072937
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.018248182137807212
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01672689914703369
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.018609680036703744
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.019034692843755086
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.018511263430118562
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.72625
params:  [0.4904314538598838, 0.4964823357181862, 0.23804660528517682, 0.4314588646900515, 0.8138377738501571, 0.42426074911858624, 0.1992190661375486, 0.99, 0.7665361826025723, 0.2217482928140758, 0.7587686580331936, 0.7857024848938926, 0.2532185070245379, 0.41649073912808154, 0.571761147174777, 0.0652059590296713, 0.01, 0.8250595361889469]
Training loss = 0.026843317349751792
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.73
Training loss = 0.02291365186373393
step = 1, Training Accuracy: 0.6933333333333334
Training loss = 0.021368415355682374
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.022667387823263805
step = 3, Training Accuracy: 0.68
Training loss = 0.021760329206784567
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.021904715100924173
step = 5, Training Accuracy: 0.7033333333333334
Training loss = 0.0213459845383962
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.02324453721443812
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.02056898295879364
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.020917653838793435
step = 9, Training Accuracy: 0.74
Training loss = 0.021792437334855398
step = 10, Training Accuracy: 0.7066666666666667
Training loss = 0.020800141096115114
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.0201325590411822
step = 12, Training Accuracy: 0.74
Training loss = 0.01966155966122945
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.02168522000312805
step = 14, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.72375
params:  [0.7532533458774183, 0.16371792174517244, 0.4755055038448164, 0.45888334288613575, 0.7779118170862779, 0.5684897668022142, 0.08650908469020627, 0.7848799526567481, 0.6056856286939699, 0.23891798465418082, 0.5361639739940879, 0.5272753222810791, 0.48236527086187164, 0.01, 0.6115607880627449, 0.08261959000757672, 0.01, 0.99]
Training loss = 0.020230962932109832
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.73
Training loss = 0.019950664937496185
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.020217973788579306
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01877781460682551
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017919867436091107
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017070737381776172
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.018353002270062764
step = 6, Training Accuracy: 0.77
Training loss = 0.017594800889492036
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.018260965049266817
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01593519111474355
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.01610668867826462
step = 10, Training Accuracy: 0.8
Training loss = 0.016356731553872426
step = 11, Training Accuracy: 0.81
Training loss = 0.01593993792931239
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014374523361523946
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.017005704641342163
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.76375
params:  [0.99, 0.013926749141582036, 0.3308015372523703, 0.35504358672154873, 0.99, 0.7943187929823528, 0.32917373112258186, 0.9898934541831087, 0.6367537214727047, 0.3202272094403538, 0.44487914085093416, 0.4453138469972491, 0.01, 0.07988040690562867, 0.9321156544551825, 0.17878615735263764, 0.30221537670036036, 0.8731286979680338]
Training loss = 0.021397897998491924
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.73625
Training loss = 0.020588352580865225
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.019613300859928132
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.01953630119562149
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.018399393558502196
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.018728194038073222
step = 5, Training Accuracy: 0.76
Training loss = 0.01638412654399872
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.018633363644282024
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01819684435923894
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.01898571600516637
step = 9, Training Accuracy: 0.73
Training loss = 0.019393357435862225
step = 10, Training Accuracy: 0.73
Training loss = 0.017682022253672283
step = 11, Training Accuracy: 0.8
Training loss = 0.016587358514467875
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.018161910672982534
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.016388036608695984
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7475
params:  [0.6020362357298495, 0.01, 0.3684655933471602, 0.2876211959742367, 0.4085448655054039, 0.7145708967646712, 0.5170319876879959, 0.7421704193293429, 0.9032559367249267, 0.6651168046717215, 0.7584205812264431, 0.7702196965959514, 0.5974820299968643, 0.08398108364812851, 0.4452123303440009, 0.2110894096169066, 0.05466239060301119, 0.9493141945620016]
Training loss = 0.025828649401664735
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.7575
Training loss = 0.020530128677686055
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.02199276735385259
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.020698045790195466
step = 3, Training Accuracy: 0.74
Training loss = 0.02080175777276357
step = 4, Training Accuracy: 0.7166666666666667
Training loss = 0.017927456895510355
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.020202473104000092
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.01803039252758026
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.018413666486740112
step = 8, Training Accuracy: 0.78
Training loss = 0.01707644095023473
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.017537298997243246
step = 10, Training Accuracy: 0.78
Training loss = 0.01586940387884776
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.017433468202749887
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.016944647431373597
step = 13, Training Accuracy: 0.79
Training loss = 0.017694046497344972
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7525
params:  [0.9809107475070586, 0.11682721702242446, 0.5422634650922884, 0.306785030413657, 0.4538169867335622, 0.6905331780157395, 0.096332019692663, 0.6068988961434747, 0.8619632676580533, 0.6203690906995979, 0.9266464298295578, 0.5995258016886579, 0.7575908080738872, 0.24606799862769618, 0.48650872769228987, 0.151212241902193, 0.18369505942269065, 0.8532535915389955]
Training loss = 0.02139726241429647
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7575
Training loss = 0.020927517612775167
step = 1, Training Accuracy: 0.6833333333333333
Training loss = 0.018604586323102315
step = 2, Training Accuracy: 0.77
Training loss = 0.01725291758775711
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01755944679180781
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.017080645362536114
step = 5, Training Accuracy: 0.77
Training loss = 0.017408259411652884
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016713723242282867
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.019581044415632885
step = 8, Training Accuracy: 0.78
Training loss = 0.015675906042257944
step = 9, Training Accuracy: 0.75
Training loss = 0.01530005137125651
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.01766250004371007
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.017418110072612764
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.018463746706644694
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.016927946408589682
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.73375
params:  [0.3817915628373865, 0.010707516437798222, 0.415790003067972, 0.3419812404558649, 0.8448994336927141, 0.663654048460311, 0.01, 0.36692559012434767, 0.5805254067051441, 0.6336564037287414, 0.6791802187170304, 0.13224970232067934, 0.5127696439091296, 0.14983589849260126, 0.6643481900162426, 0.01, 0.013836765627570663, 0.7546205119536844]
Training loss = 0.02039049208164215
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.74125
Training loss = 0.019082201321919758
step = 1, Training Accuracy: 0.76
Training loss = 0.020560853083928427
step = 2, Training Accuracy: 0.73
Training loss = 0.018247265815734864
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.017434951762358347
step = 4, Training Accuracy: 0.75
Training loss = 0.01766594409942627
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01718263268470764
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.017864966491858165
step = 7, Training Accuracy: 0.8
Training loss = 0.017806691428025562
step = 8, Training Accuracy: 0.7333333333333333
Training loss = 0.018286572794119518
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.018273010750611624
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.016942250033219655
step = 11, Training Accuracy: 0.79
Training loss = 0.016102777719497682
step = 12, Training Accuracy: 0.79
Training loss = 0.015670476655165355
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.015188275376955667
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.7475
params:  [0.99, 0.01, 0.17507505980721882, 0.380954913987391, 0.5566927395187854, 0.6665679738868041, 0.03408813144715303, 0.970394246567103, 0.5262728712212416, 0.6336637592907466, 0.741368831686196, 0.684650446398168, 0.01, 0.01, 0.9041096121493388, 0.1939783339708292, 0.3400585188337778, 0.9080968708662599]
Training loss = 0.019202487965424855
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.7475
Training loss = 0.018715726633866628
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.015342491269111634
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01755827873945236
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.017044781744480132
step = 4, Training Accuracy: 0.78
Training loss = 0.015147116482257843
step = 5, Training Accuracy: 0.78
Training loss = 0.015521097779273987
step = 6, Training Accuracy: 0.79
Training loss = 0.015008746385574341
step = 7, Training Accuracy: 0.82
Training loss = 0.01617459863424301
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015562062958876292
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.015925926466782887
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.015600950022538503
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.014028721849123637
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.014696731120347977
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.016372201442718507
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7475
[[0.6554169512707358, 0.04851761017465711, 0.34086797930277646, 0.5256806295610943, 0.8975223091021686, 0.99, 0.05392374073634655, 0.849812494112581, 0.8759134484087799, 0.1633744641530694, 0.5593000700044074, 0.43342391409538655, 0.3303117210461405, 0.01, 0.29404102509202573, 0.01, 0.6723278298695613, 0.99], [0.4904314538598838, 0.4964823357181862, 0.23804660528517682, 0.4314588646900515, 0.8138377738501571, 0.42426074911858624, 0.1992190661375486, 0.99, 0.7665361826025723, 0.2217482928140758, 0.7587686580331936, 0.7857024848938926, 0.2532185070245379, 0.41649073912808154, 0.571761147174777, 0.0652059590296713, 0.01, 0.8250595361889469], [0.7532533458774183, 0.16371792174517244, 0.4755055038448164, 0.45888334288613575, 0.7779118170862779, 0.5684897668022142, 0.08650908469020627, 0.7848799526567481, 0.6056856286939699, 0.23891798465418082, 0.5361639739940879, 0.5272753222810791, 0.48236527086187164, 0.01, 0.6115607880627449, 0.08261959000757672, 0.01, 0.99], [0.99, 0.013926749141582036, 0.3308015372523703, 0.35504358672154873, 0.99, 0.7943187929823528, 0.32917373112258186, 0.9898934541831087, 0.6367537214727047, 0.3202272094403538, 0.44487914085093416, 0.4453138469972491, 0.01, 0.07988040690562867, 0.9321156544551825, 0.17878615735263764, 0.30221537670036036, 0.8731286979680338], [0.6020362357298495, 0.01, 0.3684655933471602, 0.2876211959742367, 0.4085448655054039, 0.7145708967646712, 0.5170319876879959, 0.7421704193293429, 0.9032559367249267, 0.6651168046717215, 0.7584205812264431, 0.7702196965959514, 0.5974820299968643, 0.08398108364812851, 0.4452123303440009, 0.2110894096169066, 0.05466239060301119, 0.9493141945620016], [0.9809107475070586, 0.11682721702242446, 0.5422634650922884, 0.306785030413657, 0.4538169867335622, 0.6905331780157395, 0.096332019692663, 0.6068988961434747, 0.8619632676580533, 0.6203690906995979, 0.9266464298295578, 0.5995258016886579, 0.7575908080738872, 0.24606799862769618, 0.48650872769228987, 0.151212241902193, 0.18369505942269065, 0.8532535915389955], [0.3817915628373865, 0.010707516437798222, 0.415790003067972, 0.3419812404558649, 0.8448994336927141, 0.663654048460311, 0.01, 0.36692559012434767, 0.5805254067051441, 0.6336564037287414, 0.6791802187170304, 0.13224970232067934, 0.5127696439091296, 0.14983589849260126, 0.6643481900162426, 0.01, 0.013836765627570663, 0.7546205119536844], [0.99, 0.01, 0.17507505980721882, 0.380954913987391, 0.5566927395187854, 0.6665679738868041, 0.03408813144715303, 0.970394246567103, 0.5262728712212416, 0.6336637592907466, 0.741368831686196, 0.684650446398168, 0.01, 0.01, 0.9041096121493388, 0.1939783339708292, 0.3400585188337778, 0.9080968708662599]]
4  	8     	0.742813	0.0128353	0.72375	0.76375
params:  [0.9743471390937155, 0.01, 0.3473094547256132, 0.5025186312089236, 0.7979607901203007, 0.8598431164062815, 0.5468604303871273, 0.7938915812346098, 0.5591469414097966, 0.4084017050048083, 0.905523633603884, 0.4410927675169186, 0.3254415821240188, 0.05539087071014883, 0.5025040552229842, 0.023038428771768427, 0.01, 0.99]
Training loss = 0.026221566001574197
step = 0, Training Accuracy: 0.7
Validation Accuracy: 0.76
Training loss = 0.0253554833928744
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.021721469561258953
step = 2, Training Accuracy: 0.73
Training loss = 0.020916588306427
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.020158773958683013
step = 4, Training Accuracy: 0.7
Training loss = 0.020564655264218648
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.01934063067038854
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.02032419780890147
step = 7, Training Accuracy: 0.72
Training loss = 0.0197001384695371
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.019880429804325105
step = 9, Training Accuracy: 0.7166666666666667
Training loss = 0.01848280519247055
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.019115796486536663
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.018666107455889383
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.018637596070766448
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.018528437018394472
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.7325
params:  [0.4415328947695141, 0.07021323292696627, 0.24779983642833084, 0.15614827676729787, 0.4505357400171212, 0.7574540524960829, 0.19458438328503522, 0.7138440666455375, 0.08219752945546877, 0.421416424524278, 0.5544397981503654, 0.20278567483439058, 0.46715804496891356, 0.01, 0.8010541766273226, 0.01, 0.01, 0.8291487362060425]
Training loss = 0.02654342432816823
step = 0, Training Accuracy: 0.59
Validation Accuracy: 0.7325
Training loss = 0.024738008975982665
step = 1, Training Accuracy: 0.6166666666666667
Training loss = 0.027134008010228475
step = 2, Training Accuracy: 0.6133333333333333
Training loss = 0.025520487030347188
step = 3, Training Accuracy: 0.6266666666666667
Training loss = 0.025598878463109334
step = 4, Training Accuracy: 0.5766666666666667
Training loss = 0.026176793773969014
step = 5, Training Accuracy: 0.5933333333333334
Training loss = 0.024849346081415813
step = 6, Training Accuracy: 0.6233333333333333
Training loss = 0.024597589174906412
step = 7, Training Accuracy: 0.6133333333333333
Training loss = 0.024684798220793405
step = 8, Training Accuracy: 0.5766666666666667
Training loss = 0.02341358721256256
step = 9, Training Accuracy: 0.64
Training loss = 0.02372527301311493
step = 10, Training Accuracy: 0.6066666666666667
Training loss = 0.025095047156016032
step = 11, Training Accuracy: 0.5633333333333334
Training loss = 0.02401388645172119
step = 12, Training Accuracy: 0.6233333333333333
Training loss = 0.02474219838778178
step = 13, Training Accuracy: 0.62
Training loss = 0.02314262052377065
step = 14, Training Accuracy: 0.63
Validation Accuracy: 0.7725
params:  [0.45655261374958894, 0.28959177914162637, 0.4102865644519106, 0.38394828430270245, 0.748876545882403, 0.7216078375948091, 0.01, 0.6627161286835748, 0.7304662697482691, 0.29038985892476954, 0.6654354545405743, 0.22314453872073975, 0.45754972631262436, 0.01, 0.45120092221136754, 0.21690134927957871, 0.5524976013319487, 0.7573283901057044]
Training loss = 0.023577566544214886
step = 0, Training Accuracy: 0.6933333333333334
Validation Accuracy: 0.755
Training loss = 0.02189827730258306
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.02228715201218923
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.020218248665332793
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.020104277531305947
step = 4, Training Accuracy: 0.7166666666666667
Training loss = 0.01969559441010157
step = 5, Training Accuracy: 0.7266666666666667
Training loss = 0.020399033029874166
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.01861551731824875
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.019194019337495167
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01992186556259791
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.01860211451848348
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.018144992391268412
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.018985339105129242
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.019466604590415954
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.019145468473434447
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.75625
params:  [0.5826713516912992, 0.01, 0.4989362398331357, 0.22124389977586245, 0.4169308004837283, 0.7409013522075509, 0.5870522448234213, 0.8740165031180577, 0.37575971848472095, 0.848582443424093, 0.7477306966759937, 0.5674264566314041, 0.23553848214295478, 0.15446830333108638, 0.99, 0.19516276055425097, 0.01, 0.6365661967084713]
Training loss = 0.021337152222792307
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7725
Training loss = 0.02172095815340678
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.020011354784170786
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.018326805134614307
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.017922303875287374
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.01966931978861491
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.018711353838443755
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.02056117425362269
step = 7, Training Accuracy: 0.7133333333333334
Training loss = 0.017234083314736685
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.018255746165911357
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.01872857838869095
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.017434695959091185
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.019012086788813273
step = 12, Training Accuracy: 0.75
Training loss = 0.01768399029970169
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.01741345425446828
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7625
params:  [0.9287161021450052, 0.08653743133908304, 0.17718140933503276, 0.7190722885819331, 0.5986206196367085, 0.6389642074782694, 0.24293320898617707, 0.5855916024989213, 0.8124990500859415, 0.5030379335034116, 0.7529475962486503, 0.8608090334548619, 0.21061591200387936, 0.3643222054646696, 0.9144858503467419, 0.09847417115558195, 0.1218209832121081, 0.92191924692096]
Training loss = 0.020077821513017017
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.765
Training loss = 0.021864075660705567
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.019849278132120767
step = 2, Training Accuracy: 0.73
Training loss = 0.019952233731746673
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.019773473143577577
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.01855732798576355
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.02038071850935618
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.019318248530228933
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01865408589442571
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01879981338977814
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018304693102836608
step = 10, Training Accuracy: 0.75
Training loss = 0.015801442364851634
step = 11, Training Accuracy: 0.81
Training loss = 0.017701278030872344
step = 12, Training Accuracy: 0.78
Training loss = 0.016785769164562224
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.018757329682509104
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.74875
params:  [0.5318248331647102, 0.4864880843434498, 0.26770429522452355, 0.47526269075738437, 0.780159790228954, 0.7662607486522847, 0.16838094273462273, 0.674688711160368, 0.7337400860401571, 0.24337344294062924, 0.6574205722082318, 0.3921136902953661, 0.4767922685372501, 0.01, 0.5926925529751637, 0.03138338257311053, 0.1789182482342722, 0.5095512702028804]
Training loss = 0.022913997968037923
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.75125
Training loss = 0.022484690149625144
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.019406698346138
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.023180773456891377
step = 3, Training Accuracy: 0.7033333333333334
Training loss = 0.02114395727713903
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.020905848840872446
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.02049455185731252
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.022138544420401255
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.0203341011206309
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.01943288137515386
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.019502399663130443
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.020215625464916228
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.019794159531593324
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.018222007155418395
step = 13, Training Accuracy: 0.75
Training loss = 0.01641304502884547
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7275
params:  [0.5502155160140576, 0.17509625908417498, 0.01, 0.6722434796459023, 0.7735124746507598, 0.9050727294233578, 0.11102961335586559, 0.6095801989754537, 0.823865114847545, 0.2745602557476807, 0.3345444777352301, 0.2921804911207247, 0.3124102933453182, 0.01, 0.5085612338776794, 0.21532717727567258, 0.038893645956846926, 0.6081920315953486]
Training loss = 0.0216515576839447
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.73875
Training loss = 0.020267601410547894
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.01912293056646983
step = 2, Training Accuracy: 0.75
Training loss = 0.017309462825457256
step = 3, Training Accuracy: 0.77
Training loss = 0.015351082483927409
step = 4, Training Accuracy: 0.78
Training loss = 0.01746134509642919
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01614887495835622
step = 6, Training Accuracy: 0.8
Training loss = 0.01632994661728541
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.014740665058294931
step = 8, Training Accuracy: 0.81
Training loss = 0.016145555476347606
step = 9, Training Accuracy: 0.78
Training loss = 0.014565800527731577
step = 10, Training Accuracy: 0.83
Training loss = 0.016411950488885243
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.015654678046703337
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01532644639412562
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.014356236259142558
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.76
params:  [0.39891800311679365, 0.07802590702331712, 0.7453929448266465, 0.666198504803189, 0.40228009369770473, 0.99, 0.12754028473078854, 0.3665074587874475, 0.7500366777076941, 0.4989771752115162, 0.9005501123193693, 0.8383454951880137, 0.471757675853862, 0.13163709859400205, 0.7951696359218832, 0.0897375283602321, 0.065582593774635, 0.99]
Training loss = 0.0259168275197347
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.74
Training loss = 0.022100907862186433
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.021747943262259165
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.020832371711730958
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.01963738739490509
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01920059899489085
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01971680372953415
step = 6, Training Accuracy: 0.7266666666666667
Training loss = 0.018557446797688802
step = 7, Training Accuracy: 0.76
Training loss = 0.022429864009221395
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.019885009328524272
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.01920475552479426
step = 10, Training Accuracy: 0.77
Training loss = 0.01933132698138555
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.018512041767438252
step = 12, Training Accuracy: 0.76
Training loss = 0.01932384153207143
step = 13, Training Accuracy: 0.73
Training loss = 0.018321277101834614
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.755
[[0.9743471390937155, 0.01, 0.3473094547256132, 0.5025186312089236, 0.7979607901203007, 0.8598431164062815, 0.5468604303871273, 0.7938915812346098, 0.5591469414097966, 0.4084017050048083, 0.905523633603884, 0.4410927675169186, 0.3254415821240188, 0.05539087071014883, 0.5025040552229842, 0.023038428771768427, 0.01, 0.99], [0.4415328947695141, 0.07021323292696627, 0.24779983642833084, 0.15614827676729787, 0.4505357400171212, 0.7574540524960829, 0.19458438328503522, 0.7138440666455375, 0.08219752945546877, 0.421416424524278, 0.5544397981503654, 0.20278567483439058, 0.46715804496891356, 0.01, 0.8010541766273226, 0.01, 0.01, 0.8291487362060425], [0.45655261374958894, 0.28959177914162637, 0.4102865644519106, 0.38394828430270245, 0.748876545882403, 0.7216078375948091, 0.01, 0.6627161286835748, 0.7304662697482691, 0.29038985892476954, 0.6654354545405743, 0.22314453872073975, 0.45754972631262436, 0.01, 0.45120092221136754, 0.21690134927957871, 0.5524976013319487, 0.7573283901057044], [0.5826713516912992, 0.01, 0.4989362398331357, 0.22124389977586245, 0.4169308004837283, 0.7409013522075509, 0.5870522448234213, 0.8740165031180577, 0.37575971848472095, 0.848582443424093, 0.7477306966759937, 0.5674264566314041, 0.23553848214295478, 0.15446830333108638, 0.99, 0.19516276055425097, 0.01, 0.6365661967084713], [0.9287161021450052, 0.08653743133908304, 0.17718140933503276, 0.7190722885819331, 0.5986206196367085, 0.6389642074782694, 0.24293320898617707, 0.5855916024989213, 0.8124990500859415, 0.5030379335034116, 0.7529475962486503, 0.8608090334548619, 0.21061591200387936, 0.3643222054646696, 0.9144858503467419, 0.09847417115558195, 0.1218209832121081, 0.92191924692096], [0.5318248331647102, 0.4864880843434498, 0.26770429522452355, 0.47526269075738437, 0.780159790228954, 0.7662607486522847, 0.16838094273462273, 0.674688711160368, 0.7337400860401571, 0.24337344294062924, 0.6574205722082318, 0.3921136902953661, 0.4767922685372501, 0.01, 0.5926925529751637, 0.03138338257311053, 0.1789182482342722, 0.5095512702028804], [0.5502155160140576, 0.17509625908417498, 0.01, 0.6722434796459023, 0.7735124746507598, 0.9050727294233578, 0.11102961335586559, 0.6095801989754537, 0.823865114847545, 0.2745602557476807, 0.3345444777352301, 0.2921804911207247, 0.3124102933453182, 0.01, 0.5085612338776794, 0.21532717727567258, 0.038893645956846926, 0.6081920315953486], [0.39891800311679365, 0.07802590702331712, 0.7453929448266465, 0.666198504803189, 0.40228009369770473, 0.99, 0.12754028473078854, 0.3665074587874475, 0.7500366777076941, 0.4989771752115162, 0.9005501123193693, 0.8383454951880137, 0.471757675853862, 0.13163709859400205, 0.7951696359218832, 0.0897375283602321, 0.065582593774635, 0.99]]
5  	8     	0.751875	0.0141973	0.7275 	0.7725 
params:  [0.6998889158202064, 0.07109134979656023, 0.2945658253707681, 0.09283312001562202, 0.4829021965610328, 0.8148653237778251, 0.18465627764673953, 0.799313737143105, 0.48780829769102124, 0.24734720529460164, 0.7683470174131818, 0.35224949204539674, 0.01, 0.4614979426575929, 0.8927122891467596, 0.01, 0.07074487187895914, 0.6235397770125558]
Training loss = 0.022321667472521463
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.76375
Training loss = 0.02245826452970505
step = 1, Training Accuracy: 0.7033333333333334
Training loss = 0.020765365759531657
step = 2, Training Accuracy: 0.6766666666666666
Training loss = 0.019224631786346435
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.020203672150770822
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.020138791104157765
step = 5, Training Accuracy: 0.72
Training loss = 0.02128672003746033
step = 6, Training Accuracy: 0.7066666666666667
Training loss = 0.0209489972392718
step = 7, Training Accuracy: 0.7333333333333333
Training loss = 0.021020129024982452
step = 8, Training Accuracy: 0.7166666666666667
Training loss = 0.01925570140282313
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.01945819060007731
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.019872263967990876
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.018133504291375478
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.018188333809375762
step = 13, Training Accuracy: 0.77
Training loss = 0.020444880425930022
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.78125
params:  [0.84908141515184, 0.13455296078691914, 0.29606292809355594, 0.9392753579280604, 0.4919835528080426, 0.6081373773908303, 0.17586582879595192, 0.5676070216581541, 0.37040752739646154, 0.0584433025939044, 0.7146533810518838, 0.47540827517177303, 0.32903711796064733, 0.048521155014650826, 0.99, 0.24608303182552652, 0.13386796263655806, 0.3054370057942439]
Training loss = 0.023240704238414765
step = 0, Training Accuracy: 0.7
Validation Accuracy: 0.775
Training loss = 0.023239599466323854
step = 1, Training Accuracy: 0.6833333333333333
Training loss = 0.02198610852162043
step = 2, Training Accuracy: 0.7133333333333334
Training loss = 0.02076214790344238
step = 3, Training Accuracy: 0.7066666666666667
Training loss = 0.02102980355421702
step = 4, Training Accuracy: 0.7033333333333334
Training loss = 0.020776944955190023
step = 5, Training Accuracy: 0.6933333333333334
Training loss = 0.021203003724416098
step = 6, Training Accuracy: 0.7
Training loss = 0.019798353612422943
step = 7, Training Accuracy: 0.7133333333333334
Training loss = 0.018845312098662057
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01928575187921524
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.020507989823818205
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.020229011178016662
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.020138427317142486
step = 12, Training Accuracy: 0.72
Training loss = 0.020107851922512056
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.01978577196598053
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.74875
params:  [0.4452243907042647, 0.3604726065859908, 0.2470115798117958, 0.11279750073366265, 0.691244557486636, 0.99, 0.048259404644660914, 0.857328008118857, 0.33328488983598575, 0.5109255386030812, 0.347994668029982, 0.37845089644666674, 0.1830757679586633, 0.01, 0.99, 0.01, 0.09923633957483802, 0.6733979737136498]
Training loss = 0.024901728828748068
step = 0, Training Accuracy: 0.6533333333333333
Validation Accuracy: 0.755
Training loss = 0.02310492197672526
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.021647106607755026
step = 2, Training Accuracy: 0.6933333333333334
Training loss = 0.021806113521258035
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.020582905213038127
step = 4, Training Accuracy: 0.74
Training loss = 0.020382121205329895
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.019872601628303527
step = 6, Training Accuracy: 0.7
Training loss = 0.020134154359499612
step = 7, Training Accuracy: 0.73
Training loss = 0.021458923816680908
step = 8, Training Accuracy: 0.7133333333333334
Training loss = 0.02160739481449127
step = 9, Training Accuracy: 0.72
Training loss = 0.019655545055866242
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.020117795964082082
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.02129133959611257
step = 12, Training Accuracy: 0.7133333333333334
Training loss = 0.017935239175955454
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.02027003010114034
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.75875
params:  [0.4430818617248281, 0.09108049568646823, 0.09542976830770036, 0.3307271337549019, 0.6507268051861075, 0.967372163155649, 0.6386905978190007, 0.99, 0.34477851955960714, 0.5740977024080832, 0.5044832479969896, 0.39740793826371434, 0.5607885361114296, 0.01, 0.6971757219304818, 0.01, 0.01, 0.7243390620091873]
Training loss = 0.022700112760066987
step = 0, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.72125
Training loss = 0.022994094888369242
step = 1, Training Accuracy: 0.73
Training loss = 0.02120953619480133
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.021044735511144
step = 3, Training Accuracy: 0.71
Training loss = 0.02037034461895625
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.020388776262601216
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.02089900940656662
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.01987319588661194
step = 7, Training Accuracy: 0.73
Training loss = 0.020605111718177794
step = 8, Training Accuracy: 0.7133333333333334
Training loss = 0.019490927259127298
step = 9, Training Accuracy: 0.74
Training loss = 0.020066868861516318
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.020594267845153807
step = 11, Training Accuracy: 0.73
Training loss = 0.020720416804154713
step = 12, Training Accuracy: 0.7066666666666667
Training loss = 0.018608274658521017
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01850134611129761
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.765
params:  [0.8851780117956709, 0.5795726763137443, 0.2766717153892528, 0.42189978926884186, 0.6814220006438072, 0.99, 0.5469782659178859, 0.7308713673920222, 0.859112492253876, 0.4241187998976231, 0.5548177321085895, 0.6280455975214988, 0.6124486684954606, 0.01, 0.527358295537465, 0.30789700967439637, 0.10139911693109957, 0.61560552989938]
Training loss = 0.031091632246971132
step = 0, Training Accuracy: 0.62
Validation Accuracy: 0.76125
Training loss = 0.02758128523826599
step = 1, Training Accuracy: 0.63
Training loss = 0.027117687463760375
step = 2, Training Accuracy: 0.63
Training loss = 0.02484421690305074
step = 3, Training Accuracy: 0.63
Training loss = 0.02594636082649231
step = 4, Training Accuracy: 0.62
Training loss = 0.026759701768557232
step = 5, Training Accuracy: 0.6433333333333333
Training loss = 0.025779720544815064
step = 6, Training Accuracy: 0.6266666666666667
Training loss = 0.02582004904747009
step = 7, Training Accuracy: 0.64
Training loss = 0.025318739612897236
step = 8, Training Accuracy: 0.6833333333333333
Training loss = 0.025157405535380047
step = 9, Training Accuracy: 0.6233333333333333
Training loss = 0.025580718715985617
step = 10, Training Accuracy: 0.62
Training loss = 0.024903064370155336
step = 11, Training Accuracy: 0.6766666666666666
Training loss = 0.024442408283551535
step = 12, Training Accuracy: 0.6466666666666666
Training loss = 0.024268519282341004
step = 13, Training Accuracy: 0.67
Training loss = 0.025417208671569824
step = 14, Training Accuracy: 0.66
Validation Accuracy: 0.74625
params:  [0.7639676152770443, 0.01, 0.34843892353664907, 0.3543223768782695, 0.6319479139153763, 0.9583543818626857, 0.5218912309358221, 0.6252095483334665, 0.4565319233074818, 0.41422660367626185, 0.99, 0.5616153092710408, 0.7164075304863573, 0.28494289876871703, 0.8700031007550972, 0.33401314047959674, 0.01, 0.6570721931082304]
Training loss = 0.020325575470924378
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.77125
Training loss = 0.021642206410566966
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.019029372036457062
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.019756584366162618
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.0194221693277359
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.018920883536338806
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.018565300107002258
step = 6, Training Accuracy: 0.74
Training loss = 0.020520707070827485
step = 7, Training Accuracy: 0.71
Training loss = 0.018521949847539267
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.018996039827664693
step = 9, Training Accuracy: 0.71
Training loss = 0.02020267864068349
step = 10, Training Accuracy: 0.72
Training loss = 0.019312720696131387
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.020916177332401274
step = 12, Training Accuracy: 0.73
Training loss = 0.018411811292171478
step = 13, Training Accuracy: 0.75
Training loss = 0.01973685731490453
step = 14, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.775
params:  [0.36086999392702346, 0.01, 0.01374700676007129, 0.23872788950318438, 0.5182068734925507, 0.7517163000360828, 0.01, 0.9154748337513576, 0.235386685019864, 0.6755134881846723, 0.7322901409112362, 0.8540436379289462, 0.38291061520430364, 0.2186859937760739, 0.99, 0.5186473275975899, 0.2203348479695141, 0.9657531713176277]
Training loss = 0.024701881508032483
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.7675
Training loss = 0.022520058751106263
step = 1, Training Accuracy: 0.7
Training loss = 0.022466686765352885
step = 2, Training Accuracy: 0.71
Training loss = 0.019810272653897603
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.02133614887793859
step = 4, Training Accuracy: 0.7
Training loss = 0.019750051895777384
step = 5, Training Accuracy: 0.7033333333333334
Training loss = 0.021189812123775482
step = 6, Training Accuracy: 0.7266666666666667
Training loss = 0.021671199798583986
step = 7, Training Accuracy: 0.69
Training loss = 0.022636182606220245
step = 8, Training Accuracy: 0.6933333333333334
Training loss = 0.022445192734400432
step = 9, Training Accuracy: 0.6566666666666666
Training loss = 0.021029420991738636
step = 10, Training Accuracy: 0.6733333333333333
Training loss = 0.021144954065481822
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.021715864141782126
step = 12, Training Accuracy: 0.7033333333333334
Training loss = 0.02030933658281962
step = 13, Training Accuracy: 0.7
Training loss = 0.021756324569384256
step = 14, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.77
params:  [0.6747882304826465, 0.17631784567165215, 0.19697219372685662, 0.31981002448229123, 0.34977711572649195, 0.83133024104636, 0.7678087484817075, 0.48050948026750623, 0.17996083321651193, 0.6069265788063839, 0.09358729832735602, 0.6272215800411185, 0.6124618460548161, 0.30247508854002136, 0.496578060377555, 0.05886355489760065, 0.01, 0.814170867107374]
Training loss = 0.01924347976843516
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.77
Training loss = 0.020412073930104573
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.021549950738747915
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.017780892054239907
step = 3, Training Accuracy: 0.77
Training loss = 0.020731266140937805
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.021094059348106386
step = 5, Training Accuracy: 0.7166666666666667
Training loss = 0.019478454291820525
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.01908565988143285
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.018632753392060598
step = 8, Training Accuracy: 0.7133333333333334
Training loss = 0.018984933197498322
step = 9, Training Accuracy: 0.7066666666666667
Training loss = 0.018833709657192232
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.02079565038283666
step = 11, Training Accuracy: 0.7266666666666667
Training loss = 0.019300335844357808
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.01979124754667282
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.01919009894132614
step = 14, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.75875
[[0.6998889158202064, 0.07109134979656023, 0.2945658253707681, 0.09283312001562202, 0.4829021965610328, 0.8148653237778251, 0.18465627764673953, 0.799313737143105, 0.48780829769102124, 0.24734720529460164, 0.7683470174131818, 0.35224949204539674, 0.01, 0.4614979426575929, 0.8927122891467596, 0.01, 0.07074487187895914, 0.6235397770125558], [0.84908141515184, 0.13455296078691914, 0.29606292809355594, 0.9392753579280604, 0.4919835528080426, 0.6081373773908303, 0.17586582879595192, 0.5676070216581541, 0.37040752739646154, 0.0584433025939044, 0.7146533810518838, 0.47540827517177303, 0.32903711796064733, 0.048521155014650826, 0.99, 0.24608303182552652, 0.13386796263655806, 0.3054370057942439], [0.4452243907042647, 0.3604726065859908, 0.2470115798117958, 0.11279750073366265, 0.691244557486636, 0.99, 0.048259404644660914, 0.857328008118857, 0.33328488983598575, 0.5109255386030812, 0.347994668029982, 0.37845089644666674, 0.1830757679586633, 0.01, 0.99, 0.01, 0.09923633957483802, 0.6733979737136498], [0.4430818617248281, 0.09108049568646823, 0.09542976830770036, 0.3307271337549019, 0.6507268051861075, 0.967372163155649, 0.6386905978190007, 0.99, 0.34477851955960714, 0.5740977024080832, 0.5044832479969896, 0.39740793826371434, 0.5607885361114296, 0.01, 0.6971757219304818, 0.01, 0.01, 0.7243390620091873], [0.8851780117956709, 0.5795726763137443, 0.2766717153892528, 0.42189978926884186, 0.6814220006438072, 0.99, 0.5469782659178859, 0.7308713673920222, 0.859112492253876, 0.4241187998976231, 0.5548177321085895, 0.6280455975214988, 0.6124486684954606, 0.01, 0.527358295537465, 0.30789700967439637, 0.10139911693109957, 0.61560552989938], [0.7639676152770443, 0.01, 0.34843892353664907, 0.3543223768782695, 0.6319479139153763, 0.9583543818626857, 0.5218912309358221, 0.6252095483334665, 0.4565319233074818, 0.41422660367626185, 0.99, 0.5616153092710408, 0.7164075304863573, 0.28494289876871703, 0.8700031007550972, 0.33401314047959674, 0.01, 0.6570721931082304], [0.36086999392702346, 0.01, 0.01374700676007129, 0.23872788950318438, 0.5182068734925507, 0.7517163000360828, 0.01, 0.9154748337513576, 0.235386685019864, 0.6755134881846723, 0.7322901409112362, 0.8540436379289462, 0.38291061520430364, 0.2186859937760739, 0.99, 0.5186473275975899, 0.2203348479695141, 0.9657531713176277], [0.6747882304826465, 0.17631784567165215, 0.19697219372685662, 0.31981002448229123, 0.34977711572649195, 0.83133024104636, 0.7678087484817075, 0.48050948026750623, 0.17996083321651193, 0.6069265788063839, 0.09358729832735602, 0.6272215800411185, 0.6124618460548161, 0.30247508854002136, 0.496578060377555, 0.05886355489760065, 0.01, 0.814170867107374]]
6  	8     	0.762969	0.0114554	0.74625	0.78125
params:  [0.5533493824205031, 0.01, 0.20502809432880956, 0.01, 0.5732875079750959, 0.99, 0.055668095589150285, 0.9022803016127886, 0.5007229628503793, 0.22453760901122086, 0.800800171984982, 0.30841067684139767, 0.2456075582938266, 0.31267337117982374, 0.9847867900514019, 0.07904076867659235, 0.01, 0.9781204825744421]
Training loss = 0.01957572788000107
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.77
Training loss = 0.02022521764039993
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.02044798215230306
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.018661900162696837
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.018840610682964325
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.018776876330375673
step = 5, Training Accuracy: 0.75
Training loss = 0.018255373338858287
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01917439450820287
step = 7, Training Accuracy: 0.7333333333333333
Training loss = 0.019923741817474364
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.019236150781313577
step = 9, Training Accuracy: 0.76
Training loss = 0.016579695641994477
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01722599854071935
step = 11, Training Accuracy: 0.81
Training loss = 0.01814933180809021
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.017588763535022735
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.01843007783095042
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.77
params:  [0.4103407469468255, 0.20402630745287326, 0.4357464412422921, 0.01, 0.22636904285437776, 0.806496073700634, 0.20795288339946416, 0.9529124874383359, 0.8294840741031515, 0.11650944293441129, 0.99, 0.35348767133272607, 0.26526941889114924, 0.2874713034599471, 0.9242566597013144, 0.14524823724886543, 0.35236536753269776, 0.5508151451980204]
Training loss = 0.01966319849093755
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.78875
Training loss = 0.019325512647628783
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.01882078488667806
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.018766477604707083
step = 3, Training Accuracy: 0.77
Training loss = 0.019520806968212127
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.017922214965025583
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.017465912997722627
step = 6, Training Accuracy: 0.76
Training loss = 0.01714085360368093
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.015639753192663194
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015619021455446878
step = 9, Training Accuracy: 0.78
Training loss = 0.01639349857966105
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.017088196277618407
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017362823088963825
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.015449355940024058
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.01661483307679494
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.775
params:  [0.799501825062621, 0.2174005233837376, 0.3463860562306829, 0.5964930491626458, 0.30756862654824496, 0.7135867149727072, 0.34217643333634323, 0.5421922374792935, 0.21084042198402309, 0.05427232435894341, 0.99, 0.5917340157980425, 0.1774281892232133, 0.16195271168311345, 0.99, 0.29575757466043423, 0.16815888906908166, 0.48247531321041104]
Training loss = 0.028455757300059
step = 0, Training Accuracy: 0.65
Validation Accuracy: 0.77125
Training loss = 0.025605330765247344
step = 1, Training Accuracy: 0.6933333333333334
Training loss = 0.023748673796653748
step = 2, Training Accuracy: 0.6866666666666666
Training loss = 0.023199372986952464
step = 3, Training Accuracy: 0.7033333333333334
Training loss = 0.023059046069780986
step = 4, Training Accuracy: 0.7
Training loss = 0.024696842829386393
step = 5, Training Accuracy: 0.6766666666666666
Training loss = 0.025059502522150674
step = 6, Training Accuracy: 0.65
Training loss = 0.024641560514767964
step = 7, Training Accuracy: 0.68
Training loss = 0.022933355569839477
step = 8, Training Accuracy: 0.69
Training loss = 0.023619654973347982
step = 9, Training Accuracy: 0.68
Training loss = 0.021541995008786518
step = 10, Training Accuracy: 0.7066666666666667
Training loss = 0.022121418317159018
step = 11, Training Accuracy: 0.67
Training loss = 0.024283501704533896
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.020993645191192626
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.02325339337189992
step = 14, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.77375
params:  [0.8146586473002352, 0.01, 0.5146738824521572, 0.41668717991536364, 0.4879771514912605, 0.99, 0.2966594322009106, 0.9185857155101196, 0.2930168199039755, 0.4339644758161708, 0.48586342280823913, 0.2555721902267716, 0.36875112379024794, 0.01, 0.7243174824367389, 0.17606059976318583, 0.32145967443381956, 0.5905494596836627]
Training loss = 0.024237817724545797
step = 0, Training Accuracy: 0.6933333333333334
Validation Accuracy: 0.77625
Training loss = 0.02222899874051412
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.023371468285719555
step = 2, Training Accuracy: 0.6666666666666666
Training loss = 0.022370129227638244
step = 3, Training Accuracy: 0.69
Training loss = 0.021381161908308664
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.02284212112426758
step = 5, Training Accuracy: 0.7033333333333334
Training loss = 0.021002876063187917
step = 6, Training Accuracy: 0.7266666666666667
Training loss = 0.021292649507522583
step = 7, Training Accuracy: 0.7133333333333334
Training loss = 0.021505623956521353
step = 8, Training Accuracy: 0.73
Training loss = 0.022097665866216025
step = 9, Training Accuracy: 0.72
Training loss = 0.022079056004683177
step = 10, Training Accuracy: 0.7166666666666667
Training loss = 0.021044839024543762
step = 11, Training Accuracy: 0.6966666666666667
Training loss = 0.020886496901512147
step = 12, Training Accuracy: 0.7266666666666667
Training loss = 0.01990278363227844
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.021972577472527823
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.74375
params:  [0.6040301469353314, 0.22416086087565057, 0.01, 0.3431569461923085, 0.16983146442673697, 0.99, 0.17470907083678489, 0.7442923551571984, 0.22101149005491638, 0.593196458379175, 0.8712942254259801, 0.347867619316486, 0.3142125934651524, 0.5451662709258616, 0.7024658559876484, 0.14391422612914356, 0.4630078476441824, 0.3764326326209552]
Training loss = 0.024554285605748495
step = 0, Training Accuracy: 0.66
Validation Accuracy: 0.74625
Training loss = 0.023772433996200562
step = 1, Training Accuracy: 0.7
Training loss = 0.02314875284830729
step = 2, Training Accuracy: 0.6933333333333334
Training loss = 0.023565669854482014
step = 3, Training Accuracy: 0.71
Training loss = 0.02038018743197123
step = 4, Training Accuracy: 0.73
Training loss = 0.022824652791023255
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.02076896737019221
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.02065600275993347
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.020585453112920125
step = 8, Training Accuracy: 0.73
Training loss = 0.019548964500427247
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.021249314943949382
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.020082850952943165
step = 11, Training Accuracy: 0.75
Training loss = 0.018704874714215596
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.020314387679100036
step = 13, Training Accuracy: 0.75
Training loss = 0.020549547175566354
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.765
params:  [0.6492922962002692, 0.18280557974999317, 0.3360837989374441, 0.2754060851562401, 0.40524408413142077, 0.9028542343883742, 0.3655308917193253, 0.99, 0.6304769501108846, 0.5866846194287223, 0.5050666523694535, 0.27355384141585204, 0.21121494289908177, 0.14375243511996505, 0.8546883460276418, 0.1426812097546932, 0.27343496153200403, 0.6232333991689858]
Training loss = 0.02239050805568695
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.76875
Training loss = 0.02030176858107249
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.019050610264142353
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.018996461033821105
step = 3, Training Accuracy: 0.77
Training loss = 0.01829176753759384
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.018396822810173033
step = 5, Training Accuracy: 0.79
Training loss = 0.01850978344678879
step = 6, Training Accuracy: 0.7133333333333334
Training loss = 0.020417185723781584
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.018216373125712077
step = 8, Training Accuracy: 0.74
Training loss = 0.01903419315814972
step = 9, Training Accuracy: 0.72
Training loss = 0.01717230349779129
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.017164489527543385
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.017635744214057923
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01811769942442576
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.01936729649702708
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.77875
params:  [0.9011459547779699, 0.08241321197293486, 0.23262660439454194, 0.40246498964045363, 0.40424394599377506, 0.99, 0.4086772022936973, 0.8215428471695574, 0.9495652679508806, 0.29284055571449086, 0.99, 0.36611330330111946, 0.36954694905759145, 0.16915708456304207, 0.99, 0.01, 0.01, 0.5682551777457165]
Training loss = 0.023291649619738262
step = 0, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.7775
Training loss = 0.020349559783935548
step = 1, Training Accuracy: 0.75
Training loss = 0.020289673209190368
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01998051444689433
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01961216708024343
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01789711078008016
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.0194613179564476
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.017939879695574444
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.019035361806551614
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.018877366185188295
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.018229383130868276
step = 10, Training Accuracy: 0.77
Training loss = 0.017276898920536042
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01783425748348236
step = 12, Training Accuracy: 0.75
Training loss = 0.01712606370449066
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.016597743729750314
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.77625
params:  [0.8147387721199457, 0.04970103130957004, 0.03502839968557994, 0.2513161133933836, 0.8272671821684319, 0.8467211257202346, 0.08909370453501367, 0.741213592758396, 0.48778315521209303, 0.4525700865059446, 0.8419476492736513, 0.8120175090799425, 0.35379121550510645, 0.5699690734535455, 0.99, 0.21704531167223878, 0.11944949810261918, 0.5907335246043227]
Training loss = 0.020897451241811117
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.78
Training loss = 0.019371370573838552
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.021859919329484303
step = 2, Training Accuracy: 0.72
Training loss = 0.0197284193833669
step = 3, Training Accuracy: 0.73
Training loss = 0.018647269308567048
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018574763735135395
step = 5, Training Accuracy: 0.75
Training loss = 0.018799894352753956
step = 6, Training Accuracy: 0.7133333333333334
Training loss = 0.017589026093482972
step = 7, Training Accuracy: 0.76
Training loss = 0.01966894785563151
step = 8, Training Accuracy: 0.74
Training loss = 0.017781448264916736
step = 9, Training Accuracy: 0.76
Training loss = 0.019712050557136537
step = 10, Training Accuracy: 0.73
Training loss = 0.017689588169256847
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.016770280202229818
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.019381651282310487
step = 13, Training Accuracy: 0.7333333333333333
Training loss = 0.017908980945746104
step = 14, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.7725
[[0.5533493824205031, 0.01, 0.20502809432880956, 0.01, 0.5732875079750959, 0.99, 0.055668095589150285, 0.9022803016127886, 0.5007229628503793, 0.22453760901122086, 0.800800171984982, 0.30841067684139767, 0.2456075582938266, 0.31267337117982374, 0.9847867900514019, 0.07904076867659235, 0.01, 0.9781204825744421], [0.4103407469468255, 0.20402630745287326, 0.4357464412422921, 0.01, 0.22636904285437776, 0.806496073700634, 0.20795288339946416, 0.9529124874383359, 0.8294840741031515, 0.11650944293441129, 0.99, 0.35348767133272607, 0.26526941889114924, 0.2874713034599471, 0.9242566597013144, 0.14524823724886543, 0.35236536753269776, 0.5508151451980204], [0.799501825062621, 0.2174005233837376, 0.3463860562306829, 0.5964930491626458, 0.30756862654824496, 0.7135867149727072, 0.34217643333634323, 0.5421922374792935, 0.21084042198402309, 0.05427232435894341, 0.99, 0.5917340157980425, 0.1774281892232133, 0.16195271168311345, 0.99, 0.29575757466043423, 0.16815888906908166, 0.48247531321041104], [0.8146586473002352, 0.01, 0.5146738824521572, 0.41668717991536364, 0.4879771514912605, 0.99, 0.2966594322009106, 0.9185857155101196, 0.2930168199039755, 0.4339644758161708, 0.48586342280823913, 0.2555721902267716, 0.36875112379024794, 0.01, 0.7243174824367389, 0.17606059976318583, 0.32145967443381956, 0.5905494596836627], [0.6040301469353314, 0.22416086087565057, 0.01, 0.3431569461923085, 0.16983146442673697, 0.99, 0.17470907083678489, 0.7442923551571984, 0.22101149005491638, 0.593196458379175, 0.8712942254259801, 0.347867619316486, 0.3142125934651524, 0.5451662709258616, 0.7024658559876484, 0.14391422612914356, 0.4630078476441824, 0.3764326326209552], [0.6492922962002692, 0.18280557974999317, 0.3360837989374441, 0.2754060851562401, 0.40524408413142077, 0.9028542343883742, 0.3655308917193253, 0.99, 0.6304769501108846, 0.5866846194287223, 0.5050666523694535, 0.27355384141585204, 0.21121494289908177, 0.14375243511996505, 0.8546883460276418, 0.1426812097546932, 0.27343496153200403, 0.6232333991689858], [0.9011459547779699, 0.08241321197293486, 0.23262660439454194, 0.40246498964045363, 0.40424394599377506, 0.99, 0.4086772022936973, 0.8215428471695574, 0.9495652679508806, 0.29284055571449086, 0.99, 0.36611330330111946, 0.36954694905759145, 0.16915708456304207, 0.99, 0.01, 0.01, 0.5682551777457165], [0.8147387721199457, 0.04970103130957004, 0.03502839968557994, 0.2513161133933836, 0.8272671821684319, 0.8467211257202346, 0.08909370453501367, 0.741213592758396, 0.48778315521209303, 0.4525700865059446, 0.8419476492736513, 0.8120175090799425, 0.35379121550510645, 0.5699690734535455, 0.99, 0.21704531167223878, 0.11944949810261918, 0.5907335246043227]]
7  	8     	0.769375	0.0104396	0.74375	0.77875
params:  [0.7125452246318118, 0.26705162247082614, 0.449134479083093, 0.20434980900303418, 0.7092450686006055, 0.9414323605328914, 0.36951662927356754, 0.9016692230741914, 0.7068068413208327, 0.6076810862415374, 0.6555803780234915, 0.19044404957773953, 0.3142584830350849, 0.2879260942006143, 0.99, 0.01, 0.5490211734238747, 0.9100939296807031]
Training loss = 0.023190814157327017
step = 0, Training Accuracy: 0.7
Validation Accuracy: 0.765
Training loss = 0.020789709289868674
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.02042849987745285
step = 2, Training Accuracy: 0.7133333333333334
Training loss = 0.02022563715775808
step = 3, Training Accuracy: 0.74
Training loss = 0.022644614179929096
step = 4, Training Accuracy: 0.7166666666666667
Training loss = 0.019441042741139728
step = 5, Training Accuracy: 0.71
Training loss = 0.020254241824150084
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.019458861549695332
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.019542823632558187
step = 8, Training Accuracy: 0.7333333333333333
Training loss = 0.020422562658786773
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.01999583880106608
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.019664003650347393
step = 11, Training Accuracy: 0.73
Training loss = 0.0192925755182902
step = 12, Training Accuracy: 0.74
Training loss = 0.02074309527873993
step = 13, Training Accuracy: 0.74
Training loss = 0.01953163057565689
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.76375
params:  [0.6956343282506643, 0.09822451175806371, 0.3839507358897332, 0.7255749982730122, 0.4746691657004921, 0.8765933480052266, 0.1814676567802537, 0.7685661722334182, 0.8107919579853353, 0.5146925570593459, 0.393786191775846, 0.40828858896634207, 0.5323161317371354, 0.038648776359892306, 0.99, 0.01, 0.37451095309858884, 0.6459266992968993]
Training loss = 0.019451991617679597
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.77875
Training loss = 0.01942488228281339
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01856484830379486
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01958424359560013
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.016789702375729878
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018337226311365765
step = 5, Training Accuracy: 0.77
Training loss = 0.018507779240608216
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.01840389390786489
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.017583406468232473
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01830145051081975
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.01820055574178696
step = 10, Training Accuracy: 0.75
Training loss = 0.016887149314085644
step = 11, Training Accuracy: 0.79
Training loss = 0.017575756907463075
step = 12, Training Accuracy: 0.77
Training loss = 0.01730842302242915
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.01697074741125107
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.77
params:  [0.7265703577704042, 0.01, 0.39899891405758514, 0.035654109182023885, 0.5273237690994782, 0.99, 0.5658136372479338, 0.99, 0.6256430548633423, 0.5637287759957301, 0.8068004534397406, 0.24029272782500516, 0.06150891744118184, 0.05300826488816829, 0.99, 0.19150405871416598, 0.12266677562441074, 0.480620897375567]
Training loss = 0.02211368203163147
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.77625
Training loss = 0.023043735126654308
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.02085164248943329
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.0197511820991834
step = 3, Training Accuracy: 0.7133333333333334
Training loss = 0.02009768764177958
step = 4, Training Accuracy: 0.75
Training loss = 0.019475533962249755
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.019444549878438314
step = 6, Training Accuracy: 0.76
Training loss = 0.018728169600168865
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.01916790783405304
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.01804636647303899
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018915165265401206
step = 10, Training Accuracy: 0.7266666666666667
Training loss = 0.017344211439291636
step = 11, Training Accuracy: 0.79
Training loss = 0.019544617036978403
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.01885384202003479
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018446445564428964
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.76
params:  [0.91724773111214, 0.1573227571197027, 0.577375351884528, 0.2041879094702509, 0.01910107577975212, 0.6991357739435701, 0.10233703819352674, 0.5198683826168224, 0.41598208364557626, 0.022328100281544472, 0.9017740116325504, 0.4291308004005144, 0.4444571827594924, 0.29871928964877337, 0.8566626396160875, 0.09287853053277349, 0.3723088905806837, 0.6900154017105654]
Training loss = 0.01934662898381551
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.7525
Training loss = 0.022632900973161063
step = 1, Training Accuracy: 0.69
Training loss = 0.018879299362500507
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.019522561132907866
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.020939519504706065
step = 4, Training Accuracy: 0.72
Training loss = 0.018043704827626548
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01835501571496328
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.018907329241434734
step = 7, Training Accuracy: 0.77
Training loss = 0.018538198669751486
step = 8, Training Accuracy: 0.76
Training loss = 0.01793578584988912
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01817068745692571
step = 10, Training Accuracy: 0.75
Training loss = 0.01826955497264862
step = 11, Training Accuracy: 0.76
Training loss = 0.019236032962799073
step = 12, Training Accuracy: 0.74
Training loss = 0.017800638576348622
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.018734443485736847
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.76125
params:  [0.6458494604162328, 0.4925302711612202, 0.4501116223391372, 0.32240989587595004, 0.3147237365772058, 0.6529601401346137, 0.3006514773335346, 0.99, 0.6541580858982741, 0.49208612218528514, 0.7721050634866218, 0.2215144710677171, 0.38737633541279987, 0.28257868301673933, 0.8552164029343245, 0.01, 0.28721109558065216, 0.3988584426322772]
Training loss = 0.023859836359818778
step = 0, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.7575
Training loss = 0.023362836639086407
step = 1, Training Accuracy: 0.67
Training loss = 0.022415369947751364
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.021295804182688394
step = 3, Training Accuracy: 0.72
Training loss = 0.02083203097184499
step = 4, Training Accuracy: 0.7066666666666667
Training loss = 0.019900489846865335
step = 5, Training Accuracy: 0.75
Training loss = 0.02045980910460154
step = 6, Training Accuracy: 0.69
Training loss = 0.020512121617794036
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.02029034932454427
step = 8, Training Accuracy: 0.71
Training loss = 0.020899857183297475
step = 9, Training Accuracy: 0.69
Training loss = 0.017815609474976856
step = 10, Training Accuracy: 0.7066666666666667
Training loss = 0.01807040552298228
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.019846164286136628
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.01864628920952479
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.01762707730134328
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.73375
params:  [0.5944076469303845, 0.01, 0.40333041539609393, 0.2304380219028175, 0.3388912520596162, 0.8764206637200588, 0.10476561174770177, 0.9652370994106889, 0.5732802760770536, 0.12755021392940458, 0.9241317004248258, 0.3995218128907163, 0.35332029129960274, 0.46094490250230535, 0.7366050803326772, 0.11726788452124842, 0.564086098397204, 0.48832524372009467]
Training loss = 0.0191612712542216
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.75125
Training loss = 0.019974264601866406
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.01924707700808843
step = 2, Training Accuracy: 0.75
Training loss = 0.018063377539316812
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.017701528867085776
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01803116778532664
step = 5, Training Accuracy: 0.77
Training loss = 0.01820841779311498
step = 6, Training Accuracy: 0.75
Training loss = 0.01760794311761856
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.018308765093485516
step = 8, Training Accuracy: 0.76
Training loss = 0.018282033602396646
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.016913081804911297
step = 10, Training Accuracy: 0.8
Training loss = 0.017322859168052672
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.016108044981956483
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01755943844715754
step = 13, Training Accuracy: 0.78
Training loss = 0.015549014906088511
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7625
params:  [0.41769122964699396, 0.4122848650365839, 0.6325722281592472, 0.3372917420245477, 0.14433431169061184, 0.8002597688624477, 0.40354944350159894, 0.99, 0.8468832604840102, 0.3407801610689959, 0.47979573876723586, 0.12841423563114995, 0.01, 0.01, 0.99, 0.01, 0.20423243554714357, 0.6164438038407513]
Training loss = 0.01989047755797704
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.77125
Training loss = 0.0194678392012914
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01989601383606593
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.017568215330441794
step = 3, Training Accuracy: 0.78
Training loss = 0.018688213527202607
step = 4, Training Accuracy: 0.75
Training loss = 0.018367795348167418
step = 5, Training Accuracy: 0.73
Training loss = 0.018020897209644317
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01781054218610128
step = 7, Training Accuracy: 0.74
Training loss = 0.01827669084072113
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016411768992741903
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.01810228337844213
step = 10, Training Accuracy: 0.77
Training loss = 0.017185436089833576
step = 11, Training Accuracy: 0.78
Training loss = 0.01655580888191859
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.017762391765912374
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.017116232514381408
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.77
params:  [0.6056616764388034, 0.5031907915640434, 0.17460196898886984, 0.12549985386911283, 0.3052796947653377, 0.855764130828057, 0.395179127228421, 0.99, 0.8595312666952808, 0.5280060928113657, 0.5848966447867525, 0.01, 0.01, 0.01, 0.99, 0.01, 0.034750259646974646, 0.7519730487909204]
Training loss = 0.02109807292620341
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7775
Training loss = 0.01836491217215856
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.02002066344022751
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.018340795934200287
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.017607989410559338
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017721115251382192
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.01787574201822281
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.017315745850404105
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.017471760511398315
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01861979305744171
step = 9, Training Accuracy: 0.7366666666666667
Training loss = 0.01791214883327484
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.016868090430895488
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.015877801974614462
step = 12, Training Accuracy: 0.77
Training loss = 0.016868118345737457
step = 13, Training Accuracy: 0.77
Training loss = 0.016870824992656706
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.76375
[[0.7125452246318118, 0.26705162247082614, 0.449134479083093, 0.20434980900303418, 0.7092450686006055, 0.9414323605328914, 0.36951662927356754, 0.9016692230741914, 0.7068068413208327, 0.6076810862415374, 0.6555803780234915, 0.19044404957773953, 0.3142584830350849, 0.2879260942006143, 0.99, 0.01, 0.5490211734238747, 0.9100939296807031], [0.6956343282506643, 0.09822451175806371, 0.3839507358897332, 0.7255749982730122, 0.4746691657004921, 0.8765933480052266, 0.1814676567802537, 0.7685661722334182, 0.8107919579853353, 0.5146925570593459, 0.393786191775846, 0.40828858896634207, 0.5323161317371354, 0.038648776359892306, 0.99, 0.01, 0.37451095309858884, 0.6459266992968993], [0.7265703577704042, 0.01, 0.39899891405758514, 0.035654109182023885, 0.5273237690994782, 0.99, 0.5658136372479338, 0.99, 0.6256430548633423, 0.5637287759957301, 0.8068004534397406, 0.24029272782500516, 0.06150891744118184, 0.05300826488816829, 0.99, 0.19150405871416598, 0.12266677562441074, 0.480620897375567], [0.91724773111214, 0.1573227571197027, 0.577375351884528, 0.2041879094702509, 0.01910107577975212, 0.6991357739435701, 0.10233703819352674, 0.5198683826168224, 0.41598208364557626, 0.022328100281544472, 0.9017740116325504, 0.4291308004005144, 0.4444571827594924, 0.29871928964877337, 0.8566626396160875, 0.09287853053277349, 0.3723088905806837, 0.6900154017105654], [0.6458494604162328, 0.4925302711612202, 0.4501116223391372, 0.32240989587595004, 0.3147237365772058, 0.6529601401346137, 0.3006514773335346, 0.99, 0.6541580858982741, 0.49208612218528514, 0.7721050634866218, 0.2215144710677171, 0.38737633541279987, 0.28257868301673933, 0.8552164029343245, 0.01, 0.28721109558065216, 0.3988584426322772], [0.5944076469303845, 0.01, 0.40333041539609393, 0.2304380219028175, 0.3388912520596162, 0.8764206637200588, 0.10476561174770177, 0.9652370994106889, 0.5732802760770536, 0.12755021392940458, 0.9241317004248258, 0.3995218128907163, 0.35332029129960274, 0.46094490250230535, 0.7366050803326772, 0.11726788452124842, 0.564086098397204, 0.48832524372009467], [0.41769122964699396, 0.4122848650365839, 0.6325722281592472, 0.3372917420245477, 0.14433431169061184, 0.8002597688624477, 0.40354944350159894, 0.99, 0.8468832604840102, 0.3407801610689959, 0.47979573876723586, 0.12841423563114995, 0.01, 0.01, 0.99, 0.01, 0.20423243554714357, 0.6164438038407513], [0.6056616764388034, 0.5031907915640434, 0.17460196898886984, 0.12549985386911283, 0.3052796947653377, 0.855764130828057, 0.395179127228421, 0.99, 0.8595312666952808, 0.5280060928113657, 0.5848966447867525, 0.01, 0.01, 0.01, 0.99, 0.01, 0.034750259646974646, 0.7519730487909204]]
8  	8     	0.760625	0.0107347	0.73375	0.77   
params:  [0.5092683812819704, 0.22409506580120514, 0.34975216155478367, 0.4618876969435365, 0.48497217205033283, 0.9235559760121704, 0.01, 0.99, 0.8760059537393952, 0.31589404887486283, 0.6879802184287634, 0.518986273082984, 0.47833022667475666, 0.04365831347907177, 0.8261649511889795, 0.01, 0.3460977729097226, 0.511594842957206]
Training loss = 0.020991502304871876
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.765
Training loss = 0.019946394960085552
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.02125537157058716
step = 2, Training Accuracy: 0.75
Training loss = 0.01900022625923157
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.020142273604869844
step = 4, Training Accuracy: 0.7133333333333334
Training loss = 0.01928045282761256
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.018612012366453805
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.017336152096589407
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.020274076561133066
step = 8, Training Accuracy: 0.7166666666666667
Training loss = 0.02010733405749003
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.018325620889663698
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.018470651010672252
step = 11, Training Accuracy: 0.75
Training loss = 0.01668208767970403
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01746345450480779
step = 13, Training Accuracy: 0.76
Training loss = 0.01829201022783915
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.76
params:  [0.5795354846629979, 0.07802341094260296, 0.505607657450623, 0.6314520430154076, 0.699299230919649, 0.8515576905331731, 0.26785811038591856, 0.6280868311062942, 0.99, 0.4018612361601898, 0.5215206547613148, 0.0179815555992624, 0.2773326163107971, 0.021811280048921272, 0.901953400921722, 0.25028334629831467, 0.39904876856255084, 0.6719239487316228]
Training loss = 0.020033423801263172
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.76625
Training loss = 0.02022522876660029
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.019464114904403685
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.02118517239888509
step = 3, Training Accuracy: 0.72
Training loss = 0.018434740006923675
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018121110995610555
step = 5, Training Accuracy: 0.76
Training loss = 0.01615243355433146
step = 6, Training Accuracy: 0.78
Training loss = 0.018672354221343994
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.018390135169029237
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01821049064397812
step = 9, Training Accuracy: 0.76
Training loss = 0.018253526290257772
step = 10, Training Accuracy: 0.75
Training loss = 0.016035733620325725
step = 11, Training Accuracy: 0.75
Training loss = 0.016557486553986867
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01565008074045181
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.017871125936508178
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.77125
params:  [0.762608892063558, 0.1017298913319861, 0.27566446292241265, 0.3898021178617908, 0.4370941194792062, 0.99, 0.44874037596780697, 0.4480359097677195, 0.786949115957119, 0.24127850223062489, 0.5572237332109901, 0.1552977497011035, 0.2539381947664775, 0.01, 0.99, 0.01, 0.46950482181337805, 0.5617130302051697]
Training loss = 0.021069536010424297
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.775
Training loss = 0.01955770403146744
step = 1, Training Accuracy: 0.73
Training loss = 0.019406628509362537
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.018647150297959644
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.01639029065767924
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.019257833162943522
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.018344113032023112
step = 6, Training Accuracy: 0.72
Training loss = 0.017239624659220378
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.018685741821924846
step = 8, Training Accuracy: 0.74
Training loss = 0.01721768905719121
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.018074032962322236
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.0164548725883166
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.01711318184932073
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.015789399743080138
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.017402292092641193
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.76625
params:  [0.780150564087243, 0.5862444652845707, 0.4050975548911806, 0.81634404240162, 0.5180074824517025, 0.99, 0.4851657184294961, 0.7817446413707672, 0.5496070517382452, 0.25144703437667837, 0.5434333449136838, 0.0974444443953392, 0.556856874529605, 0.01, 0.99, 0.26363783905646343, 0.38502562512715777, 0.6029513237452271]
Training loss = 0.02422760456800461
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.76375
Training loss = 0.021686903735001883
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.02106476922829946
step = 2, Training Accuracy: 0.71
Training loss = 0.020692624847094217
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.021663712163766224
step = 4, Training Accuracy: 0.72
Training loss = 0.02172975262006124
step = 5, Training Accuracy: 0.7166666666666667
Training loss = 0.0206314492225647
step = 6, Training Accuracy: 0.74
Training loss = 0.020302423338095347
step = 7, Training Accuracy: 0.74
Training loss = 0.0199992772936821
step = 8, Training Accuracy: 0.7
Training loss = 0.0220458056529363
step = 9, Training Accuracy: 0.72
Training loss = 0.022148630817731222
step = 10, Training Accuracy: 0.7166666666666667
Training loss = 0.02100411633650462
step = 11, Training Accuracy: 0.71
Training loss = 0.020341873168945312
step = 12, Training Accuracy: 0.73
Training loss = 0.02259847442309062
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.021012702683607738
step = 14, Training Accuracy: 0.72
Validation Accuracy: 0.74875
params:  [0.5840795753033764, 0.17101907408821115, 0.8185766021200138, 0.5494030908073179, 0.21189671931631623, 0.9032617734472055, 0.013665201890848244, 0.760452791489327, 0.8842258529129088, 0.8672641266540456, 0.657032936318256, 0.2528858492471692, 0.12876945930182473, 0.14894790616526649, 0.7505295202631059, 0.2628338009040797, 0.03001759451557995, 0.99]
Training loss = 0.017649423877398172
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.75625
Training loss = 0.01918769856293996
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01848641445239385
step = 2, Training Accuracy: 0.75
Training loss = 0.018213675518830616
step = 3, Training Accuracy: 0.74
Training loss = 0.017359390556812286
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.017184436917304993
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.017481086750825246
step = 6, Training Accuracy: 0.77
Training loss = 0.016355683704217274
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.016790916224320728
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016974885761737824
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015723943213621774
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.01633993923664093
step = 11, Training Accuracy: 0.78
Training loss = 0.01874241401751836
step = 12, Training Accuracy: 0.8
Training loss = 0.014524593353271484
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014497702022393545
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.76375
params:  [0.31098317479582077, 0.01, 0.484464817539049, 0.5681385878680214, 0.28008751671727705, 0.99, 0.2273753836383017, 0.99, 0.5418927611172661, 0.3310979036540016, 0.27240197681180983, 0.01, 0.5327995094043532, 0.01, 0.99, 0.11150405630696424, 0.5267371216083221, 0.8185212975590019]
Training loss = 0.01832377235094706
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.76
Training loss = 0.019166402717431388
step = 1, Training Accuracy: 0.77
Training loss = 0.016822927991549174
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01726849506298701
step = 3, Training Accuracy: 0.8
Training loss = 0.018105187515417735
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01729166477918625
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.017296634316444397
step = 6, Training Accuracy: 0.8
Training loss = 0.017220710813999177
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.018958494663238526
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.015958529114723206
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01490754023194313
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.01535194089015325
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.014675665199756622
step = 12, Training Accuracy: 0.8
Training loss = 0.016324251492818197
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.015898661315441133
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.7775
params:  [0.8423772914705877, 0.013166609531162998, 0.4824668467260587, 0.5325949418255708, 0.182971922060338, 0.5323372737373055, 0.5515070892152085, 0.7555252043500997, 0.9025450663919004, 0.6458383901105934, 0.7011848208633245, 0.6259129321163251, 0.6755706289450832, 0.2887254632062924, 0.9472113379642138, 0.01, 0.5252114032365343, 0.726307237350771]
Training loss = 0.01766098101933797
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.77625
Training loss = 0.017414933641751607
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.017021460632483165
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016434854567050933
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.017123100260893503
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.015454003711541493
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.014990410308043161
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.017407940725485484
step = 7, Training Accuracy: 0.79
Training loss = 0.015112232565879822
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.014748495618502298
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015877621471881865
step = 10, Training Accuracy: 0.8
Training loss = 0.015273650387922923
step = 11, Training Accuracy: 0.8
Training loss = 0.014858521123727163
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.013904210329055786
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.014483341077963511
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7775
params:  [0.5629177248580296, 0.477256066841487, 0.21451276006327316, 0.5640964013194157, 0.4739868459567071, 0.99, 0.3518646611299461, 0.7875974819659242, 0.8361222382503919, 0.7066814747296405, 0.3277309909449516, 0.019031265245703533, 0.08927791698951987, 0.01, 0.99, 0.01, 0.24133567757224675, 0.8238190432813073]
Training loss = 0.022257381280263264
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.78625
Training loss = 0.023199079632759093
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.019850757122039795
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.021028944849967958
step = 3, Training Accuracy: 0.71
Training loss = 0.020451423227787018
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.02078161617120107
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.02108068307240804
step = 6, Training Accuracy: 0.71
Training loss = 0.02003925830125809
step = 7, Training Accuracy: 0.71
Training loss = 0.020433969000975293
step = 8, Training Accuracy: 0.7233333333333334
Training loss = 0.01976530849933624
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.018675082921981813
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.02074759840965271
step = 11, Training Accuracy: 0.69
Training loss = 0.019697305460770923
step = 12, Training Accuracy: 0.73
Training loss = 0.018182196815808613
step = 13, Training Accuracy: 0.74
Training loss = 0.01887403001387914
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.75
[[0.5092683812819704, 0.22409506580120514, 0.34975216155478367, 0.4618876969435365, 0.48497217205033283, 0.9235559760121704, 0.01, 0.99, 0.8760059537393952, 0.31589404887486283, 0.6879802184287634, 0.518986273082984, 0.47833022667475666, 0.04365831347907177, 0.8261649511889795, 0.01, 0.3460977729097226, 0.511594842957206], [0.5795354846629979, 0.07802341094260296, 0.505607657450623, 0.6314520430154076, 0.699299230919649, 0.8515576905331731, 0.26785811038591856, 0.6280868311062942, 0.99, 0.4018612361601898, 0.5215206547613148, 0.0179815555992624, 0.2773326163107971, 0.021811280048921272, 0.901953400921722, 0.25028334629831467, 0.39904876856255084, 0.6719239487316228], [0.762608892063558, 0.1017298913319861, 0.27566446292241265, 0.3898021178617908, 0.4370941194792062, 0.99, 0.44874037596780697, 0.4480359097677195, 0.786949115957119, 0.24127850223062489, 0.5572237332109901, 0.1552977497011035, 0.2539381947664775, 0.01, 0.99, 0.01, 0.46950482181337805, 0.5617130302051697], [0.780150564087243, 0.5862444652845707, 0.4050975548911806, 0.81634404240162, 0.5180074824517025, 0.99, 0.4851657184294961, 0.7817446413707672, 0.5496070517382452, 0.25144703437667837, 0.5434333449136838, 0.0974444443953392, 0.556856874529605, 0.01, 0.99, 0.26363783905646343, 0.38502562512715777, 0.6029513237452271], [0.5840795753033764, 0.17101907408821115, 0.8185766021200138, 0.5494030908073179, 0.21189671931631623, 0.9032617734472055, 0.013665201890848244, 0.760452791489327, 0.8842258529129088, 0.8672641266540456, 0.657032936318256, 0.2528858492471692, 0.12876945930182473, 0.14894790616526649, 0.7505295202631059, 0.2628338009040797, 0.03001759451557995, 0.99], [0.31098317479582077, 0.01, 0.484464817539049, 0.5681385878680214, 0.28008751671727705, 0.99, 0.2273753836383017, 0.99, 0.5418927611172661, 0.3310979036540016, 0.27240197681180983, 0.01, 0.5327995094043532, 0.01, 0.99, 0.11150405630696424, 0.5267371216083221, 0.8185212975590019], [0.8423772914705877, 0.013166609531162998, 0.4824668467260587, 0.5325949418255708, 0.182971922060338, 0.5323372737373055, 0.5515070892152085, 0.7555252043500997, 0.9025450663919004, 0.6458383901105934, 0.7011848208633245, 0.6259129321163251, 0.6755706289450832, 0.2887254632062924, 0.9472113379642138, 0.01, 0.5252114032365343, 0.726307237350771], [0.5629177248580296, 0.477256066841487, 0.21451276006327316, 0.5640964013194157, 0.4739868459567071, 0.99, 0.3518646611299461, 0.7875974819659242, 0.8361222382503919, 0.7066814747296405, 0.3277309909449516, 0.019031265245703533, 0.08927791698951987, 0.01, 0.99, 0.01, 0.24133567757224675, 0.8238190432813073]]
9  	8     	0.764375	0.0104021	0.74875	0.7775 
params:  [0.2781330280748407, 0.09853591101285, 0.9695791881684681, 0.887026160960652, 0.5292238095788105, 0.8894722781267911, 0.4266373641519387, 0.7880432635856541, 0.862062888335078, 0.18493218463429095, 0.25210269871912305, 0.01, 0.7369580414123624, 0.3617254335317898, 0.99, 0.01, 0.5906710705626006, 0.792006414240085]
Training loss = 0.020907682776451112
step = 0, Training Accuracy: 0.71
Validation Accuracy: 0.7625
Training loss = 0.019996895988782248
step = 1, Training Accuracy: 0.7033333333333334
Training loss = 0.020598373313744863
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.019566867450873056
step = 3, Training Accuracy: 0.72
Training loss = 0.018696941335995993
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.02001866728067398
step = 5, Training Accuracy: 0.7266666666666667
Training loss = 0.02025298833847046
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.018296291132767994
step = 7, Training Accuracy: 0.7
Training loss = 0.020752764344215392
step = 8, Training Accuracy: 0.7333333333333333
Training loss = 0.020456288059552512
step = 9, Training Accuracy: 0.7366666666666667
Training loss = 0.01849343240261078
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.019228894412517548
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.017937975426514943
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.017760565082232158
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018099913597106932
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.78125
params:  [0.448188413793721, 0.0298858527812183, 0.3878595844470257, 0.7964174737316361, 0.49963857750649493, 0.99, 0.14745318026916224, 0.7800135858173858, 0.99, 0.3874937661253308, 0.7768707077779087, 0.0740087890240247, 0.817160141573317, 0.0230853151841557, 0.99, 0.05053889143064031, 0.022904705211345855, 0.8399645806209095]
Training loss = 0.018369635939598082
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.775
Training loss = 0.017300642927487692
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.017169440587361653
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01686266760031382
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.015936697522799175
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015989030102888744
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.017465448081493376
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.01685704310735067
step = 7, Training Accuracy: 0.77
Training loss = 0.01838454614082972
step = 8, Training Accuracy: 0.77
Training loss = 0.017031329969565075
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.016141352256139118
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.015629715422789257
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.016484929919242858
step = 12, Training Accuracy: 0.76
Training loss = 0.016746045450369517
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.015870567659536997
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.77625
params:  [0.4654292332928445, 0.01, 0.4417684626331825, 0.7732711368098663, 0.37413161048648913, 0.8843941441921712, 0.24775444933910828, 0.8426220289100312, 0.7720120772676613, 0.6100447130563267, 0.37845067026239854, 0.0813031750206274, 0.5884749202689418, 0.38966128424182167, 0.99, 0.01, 0.41317097734257696, 0.5023968237501428]
Training loss = 0.017569839060306548
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7825
Training loss = 0.020340770979722343
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01757812350988388
step = 2, Training Accuracy: 0.79
Training loss = 0.017242606083552044
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.017169872124989827
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.01582743505636851
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.017800234655539194
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.015901105006535847
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.01789917786916097
step = 8, Training Accuracy: 0.79
Training loss = 0.0177578799923261
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.016493404308954875
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.016508253316084544
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.016234793066978455
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.017196243703365324
step = 13, Training Accuracy: 0.79
Training loss = 0.016459742585817973
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.77875
params:  [0.6329223501877395, 0.09501809935113809, 0.4228272365364739, 0.8043178819131995, 0.11588921663816601, 0.99, 0.11878090163123037, 0.99, 0.643567892381602, 0.840024090632737, 0.38656558741933733, 0.20103885212015116, 0.7278085677183744, 0.01, 0.915165191233439, 0.01, 0.25215303821601787, 0.6213945225231727]
Training loss = 0.01945410281419754
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.78
Training loss = 0.017208042840162915
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01900526980559031
step = 2, Training Accuracy: 0.76
Training loss = 0.01858888864517212
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.01603654404481252
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.015976627965768177
step = 5, Training Accuracy: 0.77
Training loss = 0.015768657128016155
step = 6, Training Accuracy: 0.8
Training loss = 0.016436565419038138
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015968710283438364
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.016600041488806408
step = 9, Training Accuracy: 0.76
Training loss = 0.015512018799781799
step = 10, Training Accuracy: 0.8
Training loss = 0.015538575748602549
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.0160475688179334
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014882978300253551
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.014547036687533061
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.77
params:  [0.5568130407014625, 0.17503783452088695, 0.3928716433847234, 0.42778431453813803, 0.10787572432331205, 0.5596331403965513, 0.37182423685927235, 0.9556673875395989, 0.6403336572111127, 0.5088765186667542, 0.4896237994996558, 0.3058811255147965, 0.6909725453219778, 0.01, 0.9072592199475767, 0.01, 0.5423984318818414, 0.901389971223019]
Training loss = 0.017573577960332234
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.77125
Training loss = 0.018346486389636995
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01694259524345398
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.018319589495658876
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016018563807010652
step = 4, Training Accuracy: 0.8
Training loss = 0.01618874470392863
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.016004829506079357
step = 6, Training Accuracy: 0.8
Training loss = 0.016084781289100646
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01623195270697276
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.01708195557196935
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016184144616127015
step = 10, Training Accuracy: 0.78
Training loss = 0.015895348290602365
step = 11, Training Accuracy: 0.8
Training loss = 0.01530226449171702
step = 12, Training Accuracy: 0.82
Training loss = 0.013394405792156856
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.015183814962704977
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.77375
params:  [0.32042553179595057, 0.26070784045781903, 0.531528840386662, 0.5713581040928144, 0.3113949190845267, 0.9671099360769521, 0.46494577522383185, 0.9164901038825054, 0.6589436043999481, 0.2089643764151567, 0.3813878292203913, 0.054455736686183226, 0.5525458183892936, 0.01, 0.7753737797058153, 0.06036350548413377, 0.30787196445208853, 0.99]
Training loss = 0.020453959008057913
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.77
Training loss = 0.0202234282096227
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.020013374785582225
step = 2, Training Accuracy: 0.72
Training loss = 0.01999826232592265
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.019901968439420065
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.019945252041021985
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.018034817576408388
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.017332479059696197
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.0186052742600441
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.019390953679879506
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.01868967721859614
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.020182015498479207
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.0181631276011467
step = 12, Training Accuracy: 0.74
Training loss = 0.01680637483795484
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018562199870745342
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.77
params:  [0.6419369567868202, 0.01, 0.46098946814543457, 0.6359451810913104, 0.12513567681996607, 0.8310849023020557, 0.255273891758832, 0.6488222828460521, 0.5402805044786207, 0.49869899256411254, 0.4241316023673703, 0.630584283089312, 0.794451364962472, 0.36247816421826, 0.99, 0.1689546190346659, 0.8195055551446536, 0.762994489136655]
Training loss = 0.020987841486930846
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.765
Training loss = 0.018725046614805858
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.018498376309871674
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.02047157257795334
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.01843315621217092
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.019060678283373513
step = 5, Training Accuracy: 0.73
Training loss = 0.019144153992335003
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.018554425140221913
step = 7, Training Accuracy: 0.7333333333333333
Training loss = 0.017601624627908073
step = 8, Training Accuracy: 0.77
Training loss = 0.01869885583718618
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.018342652122179667
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.018333278795083365
step = 11, Training Accuracy: 0.76
Training loss = 0.01869808445374171
step = 12, Training Accuracy: 0.6966666666666667
Training loss = 0.017393040359020232
step = 13, Training Accuracy: 0.76
Training loss = 0.01701424866914749
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.77375
params:  [0.6011370231677703, 0.22457620971956208, 0.27917851672855953, 0.6468770676217044, 0.2139277599799898, 0.99, 0.4234923264674442, 0.6670369876373843, 0.7302856028345083, 0.6095259679957481, 0.3593614138497504, 0.40348333119397795, 0.3996156799543562, 0.01, 0.99, 0.01, 0.4363750258463265, 0.7300294407484144]
Training loss = 0.018604185481866202
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.775
Training loss = 0.01817496806383133
step = 1, Training Accuracy: 0.75
Training loss = 0.01784967948993047
step = 2, Training Accuracy: 0.75
Training loss = 0.01682390660047531
step = 3, Training Accuracy: 0.78
Training loss = 0.019380141397317252
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01671576261520386
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.016292960345745088
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.01761840651432673
step = 7, Training Accuracy: 0.7333333333333333
Training loss = 0.015686214367548624
step = 8, Training Accuracy: 0.8
Training loss = 0.01863188177347183
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.017054152886072795
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.01949150621891022
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.016837387482325238
step = 12, Training Accuracy: 0.77
Training loss = 0.016138339440027873
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.016602245072523752
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7775
[[0.2781330280748407, 0.09853591101285, 0.9695791881684681, 0.887026160960652, 0.5292238095788105, 0.8894722781267911, 0.4266373641519387, 0.7880432635856541, 0.862062888335078, 0.18493218463429095, 0.25210269871912305, 0.01, 0.7369580414123624, 0.3617254335317898, 0.99, 0.01, 0.5906710705626006, 0.792006414240085], [0.448188413793721, 0.0298858527812183, 0.3878595844470257, 0.7964174737316361, 0.49963857750649493, 0.99, 0.14745318026916224, 0.7800135858173858, 0.99, 0.3874937661253308, 0.7768707077779087, 0.0740087890240247, 0.817160141573317, 0.0230853151841557, 0.99, 0.05053889143064031, 0.022904705211345855, 0.8399645806209095], [0.4654292332928445, 0.01, 0.4417684626331825, 0.7732711368098663, 0.37413161048648913, 0.8843941441921712, 0.24775444933910828, 0.8426220289100312, 0.7720120772676613, 0.6100447130563267, 0.37845067026239854, 0.0813031750206274, 0.5884749202689418, 0.38966128424182167, 0.99, 0.01, 0.41317097734257696, 0.5023968237501428], [0.6329223501877395, 0.09501809935113809, 0.4228272365364739, 0.8043178819131995, 0.11588921663816601, 0.99, 0.11878090163123037, 0.99, 0.643567892381602, 0.840024090632737, 0.38656558741933733, 0.20103885212015116, 0.7278085677183744, 0.01, 0.915165191233439, 0.01, 0.25215303821601787, 0.6213945225231727], [0.5568130407014625, 0.17503783452088695, 0.3928716433847234, 0.42778431453813803, 0.10787572432331205, 0.5596331403965513, 0.37182423685927235, 0.9556673875395989, 0.6403336572111127, 0.5088765186667542, 0.4896237994996558, 0.3058811255147965, 0.6909725453219778, 0.01, 0.9072592199475767, 0.01, 0.5423984318818414, 0.901389971223019], [0.32042553179595057, 0.26070784045781903, 0.531528840386662, 0.5713581040928144, 0.3113949190845267, 0.9671099360769521, 0.46494577522383185, 0.9164901038825054, 0.6589436043999481, 0.2089643764151567, 0.3813878292203913, 0.054455736686183226, 0.5525458183892936, 0.01, 0.7753737797058153, 0.06036350548413377, 0.30787196445208853, 0.99], [0.6419369567868202, 0.01, 0.46098946814543457, 0.6359451810913104, 0.12513567681996607, 0.8310849023020557, 0.255273891758832, 0.6488222828460521, 0.5402805044786207, 0.49869899256411254, 0.4241316023673703, 0.630584283089312, 0.794451364962472, 0.36247816421826, 0.99, 0.1689546190346659, 0.8195055551446536, 0.762994489136655], [0.6011370231677703, 0.22457620971956208, 0.27917851672855953, 0.6468770676217044, 0.2139277599799898, 0.99, 0.4234923264674442, 0.6670369876373843, 0.7302856028345083, 0.6095259679957481, 0.3593614138497504, 0.40348333119397795, 0.3996156799543562, 0.01, 0.99, 0.01, 0.4363750258463265, 0.7300294407484144]]
10 	8     	0.775156	0.00377272	0.77   	0.78125
params:  [0.41068320299046535, 0.0929636804630435, 0.7437771952180271, 0.8517123230109784, 0.34475037102354883, 0.9555859651470525, 0.01, 0.7067640206036552, 0.5767692064628269, 0.38464143906410797, 0.2332379387625592, 0.2869895119925385, 0.6821784541288571, 0.18156350385111933, 0.9406053958894501, 0.24745584636890047, 0.8888406938158606, 0.6348314852808922]
Training loss = 0.01923172801733017
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.78125
Training loss = 0.01990471363067627
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.018903207679589588
step = 2, Training Accuracy: 0.74
Training loss = 0.0192606587211291
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.018700305223464966
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.017451964815457664
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01844102660814921
step = 6, Training Accuracy: 0.78
Training loss = 0.01696077803770701
step = 7, Training Accuracy: 0.8
Training loss = 0.019073439637819926
step = 8, Training Accuracy: 0.77
Training loss = 0.017192123035589854
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.018184323608875275
step = 10, Training Accuracy: 0.77
Training loss = 0.017208897670110065
step = 11, Training Accuracy: 0.79
Training loss = 0.018114167253176373
step = 12, Training Accuracy: 0.77
Training loss = 0.017374586363633475
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01849910110235214
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.77
params:  [0.3491906961428582, 0.10882101364270193, 0.8742747108186515, 0.8549781708377893, 0.99, 0.902655808230108, 0.5021784108250333, 0.7945749077166876, 0.8639418548574734, 0.2876300054877846, 0.6070678772046091, 0.3612046519325108, 0.5610832394092738, 0.26128959576963373, 0.895787495041368, 0.04823502326602816, 0.42308766899525957, 0.45801855068729946]
Training loss = 0.019674353301525116
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.77375
Training loss = 0.018823099533716837
step = 1, Training Accuracy: 0.78
Training loss = 0.018324445883433023
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01734904537598292
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.019599406917889913
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.01815775732199351
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.018069103956222535
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017529064416885377
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01770468682050705
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01801344394683838
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.017289728422959647
step = 10, Training Accuracy: 0.79
Training loss = 0.018092736999193827
step = 11, Training Accuracy: 0.77
Training loss = 0.017190057237943014
step = 12, Training Accuracy: 0.78
Training loss = 0.017856674293677013
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.018630999326705932
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.775
params:  [0.6565461746543015, 0.01, 0.8257171945772565, 0.8948985569992538, 0.49571954907998567, 0.99, 0.44548391770357804, 0.9130794463058435, 0.6795793916283003, 0.6043147553169627, 0.4081580281859238, 0.2228630878495183, 0.7240819416776849, 0.3173064194032168, 0.99, 0.01, 0.7728412303096834, 0.6330303172542531]
Training loss = 0.01684582342704137
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.77
Training loss = 0.018265345990657808
step = 1, Training Accuracy: 0.77
Training loss = 0.01775703956683477
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.01541707123319308
step = 3, Training Accuracy: 0.79
Training loss = 0.01912247896194458
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.01770765781402588
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01567135900259018
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.017493058244387308
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.015259776214758556
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01777833968400955
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01613841752211253
step = 10, Training Accuracy: 0.8
Training loss = 0.015769872268040976
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015495837728182475
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.017607761025428773
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.015403604606787363
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.77875
params:  [0.4214312743347707, 0.01, 0.14145018745889681, 0.9437438172333756, 0.5245754990614602, 0.8429927186168021, 0.3134842789728022, 0.5881938289227984, 0.99, 0.5528569446496947, 0.06099334480276114, 0.21074213572217415, 0.4271748334463057, 0.49291142988232617, 0.9095588281332316, 0.01, 0.32800729051688515, 0.6002520300842111]
Training loss = 0.018891759912172953
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.775
Training loss = 0.018127079606056213
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.016821243365605674
step = 2, Training Accuracy: 0.76
Training loss = 0.016396243770917255
step = 3, Training Accuracy: 0.76
Training loss = 0.0184384020169576
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.01802256296078364
step = 5, Training Accuracy: 0.77
Training loss = 0.01723077436288198
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017548550963401795
step = 7, Training Accuracy: 0.77
Training loss = 0.01658348540465037
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01627028246720632
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016094344854354858
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.015233155091603596
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.016366821726163227
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01619391957918803
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.016961444516976676
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7875
params:  [0.06797311124797534, 0.01, 0.6991981404064831, 0.9332316603980515, 0.33711943216897466, 0.99, 0.5791714356500319, 0.6782191048492026, 0.99, 0.41665816588776045, 0.013380106675531578, 0.14069769125882334, 0.7238138604792793, 0.5939654879845847, 0.99, 0.039039075537420524, 0.426157436110739, 0.8524215654994894]
Training loss = 0.018644150495529175
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.79
Training loss = 0.019523435533046724
step = 1, Training Accuracy: 0.74
Training loss = 0.017983261247475943
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.017513215144475302
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017111766338348388
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.01691213846206665
step = 5, Training Accuracy: 0.78
Training loss = 0.016443239947160085
step = 6, Training Accuracy: 0.79
Training loss = 0.016819556554158527
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.015630836884180706
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015756247738997142
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.014834917982419331
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015607542196909586
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016349805394808452
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.01765574554602305
step = 13, Training Accuracy: 0.79
Training loss = 0.015520318349202474
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.785
params:  [0.3349880315944307, 0.01, 0.455109694499739, 0.99, 0.99, 0.8371639066800765, 0.06911175550089371, 0.7606893380381778, 0.99, 0.619568813663739, 0.01, 0.01, 0.7932585403856656, 0.3659761853194745, 0.9788793923542263, 0.3776558642081916, 0.3687161188810848, 0.6209201311072285]
Training loss = 0.017440956234931946
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.785
Training loss = 0.01673743297656377
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.01681906948486964
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.01495396355787913
step = 3, Training Accuracy: 0.82
Training loss = 0.016785417199134827
step = 4, Training Accuracy: 0.8
Training loss = 0.016280819873015086
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.013936330080032349
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015946730971336365
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.01502813071012497
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.014615119695663452
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01453831116358439
step = 10, Training Accuracy: 0.83
Training loss = 0.01392006794611613
step = 11, Training Accuracy: 0.83
Training loss = 0.015317980498075486
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.014998234411080679
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.01356329987446467
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.78125
params:  [0.021038768120366036, 0.40304852427587196, 0.5470111237929722, 0.814972584610937, 0.5231083778044151, 0.7497195163044188, 0.4101135016603803, 0.7227463105441175, 0.7823468989575029, 0.01, 0.30927288937391856, 0.01, 0.9080358306876926, 0.5880833728160495, 0.9090750260426848, 0.06520487137011395, 0.38551176290235395, 0.7569520686211098]
Training loss = 0.018683177530765534
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.785
Training loss = 0.020321404635906218
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.018016973634560902
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.018680737614631654
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017222103774547578
step = 4, Training Accuracy: 0.76
Training loss = 0.01752003361781438
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.01651080459356308
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.01796875536441803
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01824322799841563
step = 8, Training Accuracy: 0.7133333333333334
Training loss = 0.0175825971364975
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.01600139170885086
step = 10, Training Accuracy: 0.78
Training loss = 0.01841009557247162
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.016338811616102854
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.017764532367388407
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01760394364595413
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7925
params:  [0.23848046030802156, 0.01727350099595501, 0.99, 0.638264484336396, 0.38642472125584254, 0.99, 0.39996758772662133, 0.730377436998491, 0.5973397694734571, 0.2038358519574477, 0.6047148641397868, 0.01, 0.99, 0.17773819623428722, 0.813388317450775, 0.01, 0.38265939110014713, 0.7203897788861551]
Training loss = 0.01861887236436208
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.795
Training loss = 0.01796031991640727
step = 1, Training Accuracy: 0.76
Training loss = 0.018198977708816528
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.017219079335530598
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01666101406017939
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017355493903160095
step = 5, Training Accuracy: 0.78
Training loss = 0.01750570942958196
step = 6, Training Accuracy: 0.77
Training loss = 0.017900433838367463
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.018324413001537324
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01746832380692164
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.016724453022082645
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.015513446678717931
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.016783227523167927
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.016853146453698477
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.01639853626489639
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79125
[[0.41068320299046535, 0.0929636804630435, 0.7437771952180271, 0.8517123230109784, 0.34475037102354883, 0.9555859651470525, 0.01, 0.7067640206036552, 0.5767692064628269, 0.38464143906410797, 0.2332379387625592, 0.2869895119925385, 0.6821784541288571, 0.18156350385111933, 0.9406053958894501, 0.24745584636890047, 0.8888406938158606, 0.6348314852808922], [0.3491906961428582, 0.10882101364270193, 0.8742747108186515, 0.8549781708377893, 0.99, 0.902655808230108, 0.5021784108250333, 0.7945749077166876, 0.8639418548574734, 0.2876300054877846, 0.6070678772046091, 0.3612046519325108, 0.5610832394092738, 0.26128959576963373, 0.895787495041368, 0.04823502326602816, 0.42308766899525957, 0.45801855068729946], [0.6565461746543015, 0.01, 0.8257171945772565, 0.8948985569992538, 0.49571954907998567, 0.99, 0.44548391770357804, 0.9130794463058435, 0.6795793916283003, 0.6043147553169627, 0.4081580281859238, 0.2228630878495183, 0.7240819416776849, 0.3173064194032168, 0.99, 0.01, 0.7728412303096834, 0.6330303172542531], [0.4214312743347707, 0.01, 0.14145018745889681, 0.9437438172333756, 0.5245754990614602, 0.8429927186168021, 0.3134842789728022, 0.5881938289227984, 0.99, 0.5528569446496947, 0.06099334480276114, 0.21074213572217415, 0.4271748334463057, 0.49291142988232617, 0.9095588281332316, 0.01, 0.32800729051688515, 0.6002520300842111], [0.06797311124797534, 0.01, 0.6991981404064831, 0.9332316603980515, 0.33711943216897466, 0.99, 0.5791714356500319, 0.6782191048492026, 0.99, 0.41665816588776045, 0.013380106675531578, 0.14069769125882334, 0.7238138604792793, 0.5939654879845847, 0.99, 0.039039075537420524, 0.426157436110739, 0.8524215654994894], [0.3349880315944307, 0.01, 0.455109694499739, 0.99, 0.99, 0.8371639066800765, 0.06911175550089371, 0.7606893380381778, 0.99, 0.619568813663739, 0.01, 0.01, 0.7932585403856656, 0.3659761853194745, 0.9788793923542263, 0.3776558642081916, 0.3687161188810848, 0.6209201311072285], [0.021038768120366036, 0.40304852427587196, 0.5470111237929722, 0.814972584610937, 0.5231083778044151, 0.7497195163044188, 0.4101135016603803, 0.7227463105441175, 0.7823468989575029, 0.01, 0.30927288937391856, 0.01, 0.9080358306876926, 0.5880833728160495, 0.9090750260426848, 0.06520487137011395, 0.38551176290235395, 0.7569520686211098], [0.23848046030802156, 0.01727350099595501, 0.99, 0.638264484336396, 0.38642472125584254, 0.99, 0.39996758772662133, 0.730377436998491, 0.5973397694734571, 0.2038358519574477, 0.6047148641397868, 0.01, 0.99, 0.17773819623428722, 0.813388317450775, 0.01, 0.38265939110014713, 0.7203897788861551]]
11 	8     	0.782656	0.00738023	0.77   	0.7925 
params:  [0.01042756030563144, 0.08852691538319843, 0.5532000010820943, 0.7007261523570129, 0.42245633364616436, 0.555225174978845, 0.34268577422691937, 0.5934325707455713, 0.99, 0.01, 0.1937418929876113, 0.3123069453687135, 0.7036311157324422, 0.6421183856282309, 0.8613870418522263, 0.01, 0.5133649616042459, 0.6320625954048484]
Training loss = 0.017936848004659018
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.79125
Training loss = 0.019326552351315817
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018354928096135457
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.019322189788023632
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.01786144604285558
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.016475113729635876
step = 5, Training Accuracy: 0.78
Training loss = 0.0180733926097552
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.017289176086584725
step = 7, Training Accuracy: 0.76
Training loss = 0.016075390775998434
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.016359743972619373
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.0179997851451238
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.017223897178967794
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.015447453459103902
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.016731595695018767
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015474395950635274
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.76875
params:  [0.2860664339221959, 0.19994788714848152, 0.7750877999809611, 0.8940571253117089, 0.6900090596786266, 0.99, 0.25946160303818966, 0.34736640478450337, 0.865808945846306, 0.2544108064174835, 0.44948591248296565, 0.01, 0.7961570412939288, 0.48566239402629474, 0.7549272869373038, 0.01, 0.4402277594333611, 0.864459023317737]
Training loss = 0.02181203881899516
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.76875
Training loss = 0.022955636978149414
step = 1, Training Accuracy: 0.6833333333333333
Training loss = 0.019787585238615672
step = 2, Training Accuracy: 0.72
Training loss = 0.02068846066792806
step = 3, Training Accuracy: 0.72
Training loss = 0.019973672926425934
step = 4, Training Accuracy: 0.73
Training loss = 0.018477211991945904
step = 5, Training Accuracy: 0.76
Training loss = 0.02032645672559738
step = 6, Training Accuracy: 0.74
Training loss = 0.020228304664293924
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.019039240578810374
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.0199740532040596
step = 9, Training Accuracy: 0.72
Training loss = 0.019429928064346312
step = 10, Training Accuracy: 0.73
Training loss = 0.02057659496863683
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.021344220836957296
step = 12, Training Accuracy: 0.73
Training loss = 0.018063400387763977
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.018317878743012748
step = 14, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.78375
params:  [0.2397594739804383, 0.01, 0.3898335417693979, 0.7883527993704154, 0.43795925594270907, 0.9153444812595859, 0.5755736049459825, 0.43381638808559586, 0.7000650751674666, 0.26390685013111914, 0.5168435810469432, 0.12927187159186643, 0.99, 0.6600374955162143, 0.8442936479499022, 0.3799855032249343, 0.06944504171556448, 0.99]
Training loss = 0.01847947895526886
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.77625
Training loss = 0.01818334142367045
step = 1, Training Accuracy: 0.77
Training loss = 0.019328094522158303
step = 2, Training Accuracy: 0.72
Training loss = 0.017401933570702872
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01917399913072586
step = 4, Training Accuracy: 0.74
Training loss = 0.018572202920913695
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.017638378341992698
step = 6, Training Accuracy: 0.78
Training loss = 0.016888898015022278
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.01873369038105011
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.019147510925928753
step = 9, Training Accuracy: 0.75
Training loss = 0.017505457897981008
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.018747822841008503
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.016867397427558897
step = 12, Training Accuracy: 0.78
Training loss = 0.017041696310043333
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.017448578973611194
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.78375
params:  [0.21549676272007903, 0.01, 0.794136412964668, 0.6975991114235033, 0.3966224167483018, 0.99, 0.1601425472556377, 0.7777140572788983, 0.9043507497730543, 0.6219605658047068, 0.6072581957220593, 0.01, 0.99, 0.36217407343132707, 0.8732450965953859, 0.01, 0.5629414302312084, 0.7009751967573958]
Training loss = 0.019053958306709924
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7925
Training loss = 0.019833612143993377
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.01848070154587428
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.01913575271765391
step = 3, Training Accuracy: 0.77
Training loss = 0.01890800952911377
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.018216488659381868
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.017898676097393037
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.020944587886333466
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.01821843554576238
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01700524757305781
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.017398185829321542
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.019551842908064523
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.016404512425263723
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.01713292479515076
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01704894830783208
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.79
params:  [0.4082562531707693, 0.0999225028442327, 0.5965497934244156, 0.6659099950187755, 0.5690659903743599, 0.6244408102407387, 0.4674867315087023, 0.7088170386968939, 0.8179046120420068, 0.46771362207691736, 0.3367339687427021, 0.09867835975890288, 0.7760219180808875, 0.4057747615692982, 0.8312345891594979, 0.14134375210431563, 0.1904856950984831, 0.99]
Training loss = 0.01804440180460612
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7825
Training loss = 0.01592445602019628
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.01666995515426
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.017977095941702523
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016006885468959807
step = 4, Training Accuracy: 0.79
Training loss = 0.01486391951640447
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.016822062929471335
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.015927328566710156
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.01692372510830561
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.016624313592910767
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.015416584213574728
step = 10, Training Accuracy: 0.78
Training loss = 0.0150433811545372
step = 11, Training Accuracy: 0.77
Training loss = 0.015402511854966482
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01562319000562032
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01634918709595998
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78375
params:  [0.01, 0.01, 0.3782290607368435, 0.6634890218481952, 0.42932486464869274, 0.6601603879505218, 0.5335093294324562, 0.4711893166555145, 0.9557531998431212, 0.01839095963660245, 0.744121939225846, 0.01, 0.9557357745506267, 0.2013220752577642, 0.99, 0.01, 0.13885140211996944, 0.8348106671154752]
Training loss = 0.020262175699075062
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.78625
Training loss = 0.018131179213523866
step = 1, Training Accuracy: 0.75
Training loss = 0.017919716636339823
step = 2, Training Accuracy: 0.75
Training loss = 0.017972219636042914
step = 3, Training Accuracy: 0.76
Training loss = 0.01727436016003291
step = 4, Training Accuracy: 0.78
Training loss = 0.01709625740845998
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01778825968503952
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.01787461171547572
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.01757309406995773
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.017082908650239308
step = 9, Training Accuracy: 0.76
Training loss = 0.016381446421146393
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.015831218163172404
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.01740401764710744
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01612686534722646
step = 13, Training Accuracy: 0.77
Training loss = 0.018899344305197397
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.78125
params:  [0.05983334084567456, 0.08489814266167464, 0.6474492885316006, 0.99, 0.13161571160599517, 0.6151699265292885, 0.4711032032434428, 0.667906055473884, 0.99, 0.0324658439795161, 0.25778907892811387, 0.01, 0.7686091147299698, 0.38116443385383025, 0.8314299158456039, 0.01515973362360436, 0.48760572687990733, 0.6406916844728814]
Training loss = 0.020764111280441283
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.785
Training loss = 0.01686022162437439
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.019297827184200287
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.017099737028280895
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017160122990608217
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.016019037415583928
step = 5, Training Accuracy: 0.75
Training loss = 0.01828639070192973
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.016834327081839243
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.016464494665463767
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01729627698659897
step = 9, Training Accuracy: 0.77
Training loss = 0.016828037897745767
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017585612138112387
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.017468356788158418
step = 12, Training Accuracy: 0.74
Training loss = 0.015964817106723785
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.017495236297448476
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7725
params:  [0.29100401504305484, 0.11280580771553052, 0.9687457905011907, 0.7965667474232316, 0.2287652041535163, 0.5235839351146405, 0.2819179620295408, 0.2192812201864252, 0.7189627523520824, 0.3950137583202342, 0.1208887992368585, 0.214217077396721, 0.99, 0.5698589929475038, 0.7283704735283826, 0.02487233772334296, 0.147749861630406, 0.8088229555328179]
Training loss = 0.022540690302848818
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.77625
Training loss = 0.020019692877928416
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.019596256713072458
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.022937406996885935
step = 3, Training Accuracy: 0.71
Training loss = 0.02073286533355713
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.019957012832164764
step = 5, Training Accuracy: 0.6933333333333334
Training loss = 0.018372855385144552
step = 6, Training Accuracy: 0.74
Training loss = 0.018179713884989422
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.01873016615708669
step = 8, Training Accuracy: 0.74
Training loss = 0.018460047940413157
step = 9, Training Accuracy: 0.75
Training loss = 0.01827929069598516
step = 10, Training Accuracy: 0.75
Training loss = 0.018860180974006654
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.017913815081119538
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.018538200557231904
step = 13, Training Accuracy: 0.76
Training loss = 0.017304788132508596
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.77125
[[0.01042756030563144, 0.08852691538319843, 0.5532000010820943, 0.7007261523570129, 0.42245633364616436, 0.555225174978845, 0.34268577422691937, 0.5934325707455713, 0.99, 0.01, 0.1937418929876113, 0.3123069453687135, 0.7036311157324422, 0.6421183856282309, 0.8613870418522263, 0.01, 0.5133649616042459, 0.6320625954048484], [0.2860664339221959, 0.19994788714848152, 0.7750877999809611, 0.8940571253117089, 0.6900090596786266, 0.99, 0.25946160303818966, 0.34736640478450337, 0.865808945846306, 0.2544108064174835, 0.44948591248296565, 0.01, 0.7961570412939288, 0.48566239402629474, 0.7549272869373038, 0.01, 0.4402277594333611, 0.864459023317737], [0.2397594739804383, 0.01, 0.3898335417693979, 0.7883527993704154, 0.43795925594270907, 0.9153444812595859, 0.5755736049459825, 0.43381638808559586, 0.7000650751674666, 0.26390685013111914, 0.5168435810469432, 0.12927187159186643, 0.99, 0.6600374955162143, 0.8442936479499022, 0.3799855032249343, 0.06944504171556448, 0.99], [0.21549676272007903, 0.01, 0.794136412964668, 0.6975991114235033, 0.3966224167483018, 0.99, 0.1601425472556377, 0.7777140572788983, 0.9043507497730543, 0.6219605658047068, 0.6072581957220593, 0.01, 0.99, 0.36217407343132707, 0.8732450965953859, 0.01, 0.5629414302312084, 0.7009751967573958], [0.4082562531707693, 0.0999225028442327, 0.5965497934244156, 0.6659099950187755, 0.5690659903743599, 0.6244408102407387, 0.4674867315087023, 0.7088170386968939, 0.8179046120420068, 0.46771362207691736, 0.3367339687427021, 0.09867835975890288, 0.7760219180808875, 0.4057747615692982, 0.8312345891594979, 0.14134375210431563, 0.1904856950984831, 0.99], [0.01, 0.01, 0.3782290607368435, 0.6634890218481952, 0.42932486464869274, 0.6601603879505218, 0.5335093294324562, 0.4711893166555145, 0.9557531998431212, 0.01839095963660245, 0.744121939225846, 0.01, 0.9557357745506267, 0.2013220752577642, 0.99, 0.01, 0.13885140211996944, 0.8348106671154752], [0.05983334084567456, 0.08489814266167464, 0.6474492885316006, 0.99, 0.13161571160599517, 0.6151699265292885, 0.4711032032434428, 0.667906055473884, 0.99, 0.0324658439795161, 0.25778907892811387, 0.01, 0.7686091147299698, 0.38116443385383025, 0.8314299158456039, 0.01515973362360436, 0.48760572687990733, 0.6406916844728814], [0.29100401504305484, 0.11280580771553052, 0.9687457905011907, 0.7965667474232316, 0.2287652041535163, 0.5235839351146405, 0.2819179620295408, 0.2192812201864252, 0.7189627523520824, 0.3950137583202342, 0.1208887992368585, 0.214217077396721, 0.99, 0.5698589929475038, 0.7283704735283826, 0.02487233772334296, 0.147749861630406, 0.8088229555328179]]
12 	8     	0.779375	0.00707107	0.76875	0.79   
params:  [0.35344145741386307, 0.01, 0.802767907152503, 0.6817983433465961, 0.7354410143987735, 0.99, 0.3187674483085009, 0.5719121182768959, 0.99, 0.5584507754841385, 0.6359391052229572, 0.10997937181411228, 0.8150404733233165, 0.6758286533986115, 0.99, 0.01, 0.39560205166392926, 0.99]
Training loss = 0.018126455346743266
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7725
Training loss = 0.018917142351468404
step = 1, Training Accuracy: 0.76
Training loss = 0.01855588227510452
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.01787169446547826
step = 3, Training Accuracy: 0.77
Training loss = 0.018492994606494905
step = 4, Training Accuracy: 0.76
Training loss = 0.0162079976995786
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.017248016198476157
step = 6, Training Accuracy: 0.77
Training loss = 0.01803075780471166
step = 7, Training Accuracy: 0.78
Training loss = 0.01830682178338369
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.016723784108956655
step = 9, Training Accuracy: 0.78
Training loss = 0.01690774361292521
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01633937080701192
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.017122102081775666
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017856384615103405
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.01711017628510793
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.78
params:  [0.3446051170869211, 0.01, 0.6484576628123323, 0.5599311743824292, 0.5942807274825166, 0.99, 0.33968409836543806, 0.34919696523910015, 0.7511936966194472, 0.2902035964054216, 0.6197905114311746, 0.16813464684145749, 0.9799253988165513, 0.5346989690869232, 0.9234361405469929, 0.01, 0.2678344513509448, 0.7759630886241407]
Training loss = 0.02011716812849045
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.775
Training loss = 0.01900226980447769
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.018372863034407296
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.018160364429155987
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.018261503477891287
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.019128633340199788
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.01815539926290512
step = 6, Training Accuracy: 0.77
Training loss = 0.018357981741428376
step = 7, Training Accuracy: 0.74
Training loss = 0.0176571582754453
step = 8, Training Accuracy: 0.77
Training loss = 0.017614456514517467
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018711433410644532
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01737247039874395
step = 11, Training Accuracy: 0.75
Training loss = 0.01769765575726827
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01824270357688268
step = 13, Training Accuracy: 0.74
Training loss = 0.018102462391058605
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79375
params:  [0.23289256862042884, 0.01, 0.7350045437597429, 0.6767056677367522, 0.6366086027113531, 0.46655566649809777, 0.23308281286602367, 0.6434610738916945, 0.8745540868617323, 0.43526089432498716, 0.6049460069230014, 0.3395559305904215, 0.9516593259335193, 0.46025797366259524, 0.99, 0.21331617021643987, 0.25402170784956735, 0.6942487358909525]
Training loss = 0.01804925352334976
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.7925
Training loss = 0.01929448594649633
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01924090415239334
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01998261352380117
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.016891050040721892
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.017456876436869304
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.015995718638102212
step = 6, Training Accuracy: 0.8
Training loss = 0.017189975579579672
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01775413955251376
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.016559638480345407
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.017565028170744578
step = 10, Training Accuracy: 0.77
Training loss = 0.017490883370240528
step = 11, Training Accuracy: 0.76
Training loss = 0.016822573641935984
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.016411389907201132
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.016426506141821545
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.785
params:  [0.08943066636520872, 0.05534738422851086, 0.8446932169676251, 0.7712693009536848, 0.5368325262793253, 0.7590843888889567, 0.18211260304162907, 0.7110301031191988, 0.7322922743843224, 0.5401396726268529, 0.5908123202641358, 0.01, 0.99, 0.5305434140563683, 0.786991756758896, 0.09025963243463286, 0.8287706411540805, 0.99]
Training loss = 0.017833978136380515
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.78
Training loss = 0.01623949646949768
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.01635636826356252
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.016294361849625905
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.016133269270261128
step = 4, Training Accuracy: 0.81
Training loss = 0.016536996265252433
step = 5, Training Accuracy: 0.78
Training loss = 0.016344550251960754
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01603851467370987
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.016721686422824858
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.015577660202980041
step = 9, Training Accuracy: 0.78
Training loss = 0.015220730106035868
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.014972435434659322
step = 11, Training Accuracy: 0.81
Training loss = 0.015152997573216757
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.01588043620189031
step = 13, Training Accuracy: 0.81
Training loss = 0.014953146775563558
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.7825
params:  [0.06424147193778545, 0.07281122181920492, 0.8410465215187443, 0.6887635536909769, 0.4686221614989478, 0.99, 0.34668929477609717, 0.8955595585674119, 0.9291538372132208, 0.4085602120783337, 0.4801658101223552, 0.01, 0.99, 0.32349518924734694, 0.689089414717582, 0.01, 0.530990225083868, 0.99]
Training loss = 0.016678079068660735
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7825
Training loss = 0.019521474440892538
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.0177063317100207
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01897730678319931
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.01868597447872162
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.017978449761867524
step = 5, Training Accuracy: 0.78
Training loss = 0.017269051373004912
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016735257506370546
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.016886710921923318
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.017424889405568442
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01840542842944463
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.015160413285096486
step = 11, Training Accuracy: 0.8
Training loss = 0.01771593044201533
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.015913808643817903
step = 13, Training Accuracy: 0.78
Training loss = 0.01538676381111145
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78
params:  [0.2760975316963073, 0.4097669851796053, 0.4836425806513571, 0.48582958930616704, 0.6148345502009349, 0.99, 0.3024313468253552, 0.5441726604439522, 0.7918512119265855, 0.31505120049716007, 0.5521311299859456, 0.154650358369763, 0.961033259813188, 0.43625099907582476, 0.7172165329039477, 0.2567189736371235, 0.44263372371091586, 0.99]
Training loss = 0.020353067020575207
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.77875
Training loss = 0.01894888937473297
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.019743259847164154
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.019470902383327483
step = 3, Training Accuracy: 0.7166666666666667
Training loss = 0.018592986464500427
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.019306001961231233
step = 5, Training Accuracy: 0.75
Training loss = 0.019923091232776642
step = 6, Training Accuracy: 0.72
Training loss = 0.019408097465833028
step = 7, Training Accuracy: 0.73
Training loss = 0.018585563997427623
step = 8, Training Accuracy: 0.75
Training loss = 0.019005933403968812
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.01929015149672826
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.018464681804180146
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.020949087242285412
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.019370214343070985
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.019487712979316712
step = 14, Training Accuracy: 0.72
Validation Accuracy: 0.7775
params:  [0.36662784606309146, 0.01, 0.781556284567873, 0.9198894763580984, 0.6072649381341643, 0.99, 0.3463390453214378, 0.5857735479815348, 0.919949299635412, 0.26019031211951904, 0.8513833883729682, 0.01, 0.99, 0.43750842513353744, 0.99, 0.18829915400989097, 0.4437352716146771, 0.7462696198504399]
Training loss = 0.020687137146790824
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.77625
Training loss = 0.01956320901711782
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.020494269132614137
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.02114450494448344
step = 3, Training Accuracy: 0.75
Training loss = 0.017883670032024384
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.01948925862709681
step = 5, Training Accuracy: 0.7266666666666667
Training loss = 0.02094102531671524
step = 6, Training Accuracy: 0.74
Training loss = 0.018868908484776816
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.018639572660128275
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.018683985571066538
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.019304719865322114
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.019269354740778604
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.01809582660595576
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.017661238213380177
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.018105360368887585
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.77625
params:  [0.058171258633378714, 0.05443164921847746, 0.9551919333704962, 0.99, 0.46590697768733297, 0.99, 0.01, 0.8265052554024297, 0.8848342639322787, 0.35655681925220883, 0.34764588905576665, 0.018033895702596385, 0.9578228124432536, 0.4965066626039847, 0.7056260476333809, 0.24551807692773872, 0.5993359609311419, 0.9587860868389941]
Training loss = 0.01825691044330597
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.7825
Training loss = 0.017587387561798097
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.018691740334033966
step = 2, Training Accuracy: 0.75
Training loss = 0.01869444896777471
step = 3, Training Accuracy: 0.72
Training loss = 0.017124402324358624
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017688000996907552
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.01779951463143031
step = 6, Training Accuracy: 0.76
Training loss = 0.016767873565355938
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.015583226084709167
step = 8, Training Accuracy: 0.79
Training loss = 0.017347065607706706
step = 9, Training Accuracy: 0.78
Training loss = 0.017833270331223807
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.018996139367421467
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.018809576133886972
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.017238251765569052
step = 13, Training Accuracy: 0.77
Training loss = 0.015370902121067048
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.78625
[[0.35344145741386307, 0.01, 0.802767907152503, 0.6817983433465961, 0.7354410143987735, 0.99, 0.3187674483085009, 0.5719121182768959, 0.99, 0.5584507754841385, 0.6359391052229572, 0.10997937181411228, 0.8150404733233165, 0.6758286533986115, 0.99, 0.01, 0.39560205166392926, 0.99], [0.3446051170869211, 0.01, 0.6484576628123323, 0.5599311743824292, 0.5942807274825166, 0.99, 0.33968409836543806, 0.34919696523910015, 0.7511936966194472, 0.2902035964054216, 0.6197905114311746, 0.16813464684145749, 0.9799253988165513, 0.5346989690869232, 0.9234361405469929, 0.01, 0.2678344513509448, 0.7759630886241407], [0.23289256862042884, 0.01, 0.7350045437597429, 0.6767056677367522, 0.6366086027113531, 0.46655566649809777, 0.23308281286602367, 0.6434610738916945, 0.8745540868617323, 0.43526089432498716, 0.6049460069230014, 0.3395559305904215, 0.9516593259335193, 0.46025797366259524, 0.99, 0.21331617021643987, 0.25402170784956735, 0.6942487358909525], [0.08943066636520872, 0.05534738422851086, 0.8446932169676251, 0.7712693009536848, 0.5368325262793253, 0.7590843888889567, 0.18211260304162907, 0.7110301031191988, 0.7322922743843224, 0.5401396726268529, 0.5908123202641358, 0.01, 0.99, 0.5305434140563683, 0.786991756758896, 0.09025963243463286, 0.8287706411540805, 0.99], [0.06424147193778545, 0.07281122181920492, 0.8410465215187443, 0.6887635536909769, 0.4686221614989478, 0.99, 0.34668929477609717, 0.8955595585674119, 0.9291538372132208, 0.4085602120783337, 0.4801658101223552, 0.01, 0.99, 0.32349518924734694, 0.689089414717582, 0.01, 0.530990225083868, 0.99], [0.2760975316963073, 0.4097669851796053, 0.4836425806513571, 0.48582958930616704, 0.6148345502009349, 0.99, 0.3024313468253552, 0.5441726604439522, 0.7918512119265855, 0.31505120049716007, 0.5521311299859456, 0.154650358369763, 0.961033259813188, 0.43625099907582476, 0.7172165329039477, 0.2567189736371235, 0.44263372371091586, 0.99], [0.36662784606309146, 0.01, 0.781556284567873, 0.9198894763580984, 0.6072649381341643, 0.99, 0.3463390453214378, 0.5857735479815348, 0.919949299635412, 0.26019031211951904, 0.8513833883729682, 0.01, 0.99, 0.43750842513353744, 0.99, 0.18829915400989097, 0.4437352716146771, 0.7462696198504399], [0.058171258633378714, 0.05443164921847746, 0.9551919333704962, 0.99, 0.46590697768733297, 0.99, 0.01, 0.8265052554024297, 0.8848342639322787, 0.35655681925220883, 0.34764588905576665, 0.018033895702596385, 0.9578228124432536, 0.4965066626039847, 0.7056260476333809, 0.24551807692773872, 0.5993359609311419, 0.9587860868389941]]
13 	8     	0.782656	0.00528254	0.77625	0.79375
params:  [0.1439747598503092, 0.3320846958824242, 0.9819602095515549, 0.41716472859071274, 0.34762998581288956, 0.8533949180426788, 0.09394325300948023, 0.4727943600293524, 0.99, 0.2770951591550786, 0.41006769686519196, 0.45353378865754046, 0.99, 0.6051645801431333, 0.99, 0.07911618163297177, 0.48556050984169374, 0.8061767869830907]
Training loss = 0.019739820162455242
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.78375
Training loss = 0.018268722395102185
step = 1, Training Accuracy: 0.75
Training loss = 0.017037348945935567
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.01628832280635834
step = 3, Training Accuracy: 0.77
Training loss = 0.016862052579720815
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017697502175966898
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.017410581807295482
step = 6, Training Accuracy: 0.75
Training loss = 0.0188158118724823
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01751242071390152
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.01682173599799474
step = 9, Training Accuracy: 0.79
Training loss = 0.01842828243970871
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.019187127351760865
step = 11, Training Accuracy: 0.72
Training loss = 0.017130475342273712
step = 12, Training Accuracy: 0.77
Training loss = 0.017319321036338806
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.017611514925956726
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.78125
params:  [0.32940018753852834, 0.046079117429930275, 0.655979953341634, 0.643380458823663, 0.7196204541606787, 0.9563917300815384, 0.45473664474742104, 0.5392543079289353, 0.9345647089473915, 0.5561490915426467, 0.7435535853318984, 0.09570713405365906, 0.99, 0.5772906889788648, 0.99, 0.022263615981890555, 0.4339072969869163, 0.99]
Training loss = 0.018904153307278952
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.78875
Training loss = 0.019585209786891936
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.021829540928204855
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01906447947025299
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.01979472130537033
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.017875075340270996
step = 5, Training Accuracy: 0.78
Training loss = 0.01933069626490275
step = 6, Training Accuracy: 0.72
Training loss = 0.018584779103597005
step = 7, Training Accuracy: 0.74
Training loss = 0.018931924005349477
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01805956373612086
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01902421534061432
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.018281102577845255
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.018045979340871175
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.01803332656621933
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.017001486967007318
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.78125
params:  [0.3529308087924474, 0.12193223264603609, 0.7644300062410877, 0.590032272985544, 0.8199528611616185, 0.853996994487778, 0.14967871453387188, 0.5879336027880291, 0.6140373545739883, 0.6177754265975435, 0.5028415895106579, 0.286471459924582, 0.99, 0.160996924616024, 0.836463089017614, 0.08542826024832723, 0.5443812509462608, 0.46309111670637787]
Training loss = 0.01632644861936569
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.785
Training loss = 0.017612991333007814
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01688098341226578
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.015623401006062826
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016947500705718994
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.016231694718201954
step = 5, Training Accuracy: 0.79
Training loss = 0.016271908084551492
step = 6, Training Accuracy: 0.8
Training loss = 0.015318255424499511
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.015022491812705993
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.015755924085776012
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.014768787821133932
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.014764615794022879
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.014892248709996541
step = 12, Training Accuracy: 0.81
Training loss = 0.015980854233105978
step = 13, Training Accuracy: 0.78
Training loss = 0.013837078015009562
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.78625
params:  [0.3339137797928727, 0.15172756369869764, 0.3487315530915752, 0.9687866266991161, 0.5217382149610998, 0.8203149100131587, 0.1897922929962427, 0.42660096869843767, 0.8888684906480804, 0.14543422870575742, 0.5221453367462057, 0.42189714235935327, 0.6897020296637937, 0.6088299964893257, 0.99, 0.01, 0.01, 0.99]
Training loss = 0.019967667162418365
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.78125
Training loss = 0.01906630426645279
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01803665667772293
step = 2, Training Accuracy: 0.77
Training loss = 0.019449284076690675
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.01728947748740514
step = 4, Training Accuracy: 0.82
Training loss = 0.018532837828000387
step = 5, Training Accuracy: 0.8
Training loss = 0.018848175704479216
step = 6, Training Accuracy: 0.77
Training loss = 0.018440881669521333
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.01653633624315262
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.016415480375289917
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.018176553944746654
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.016414549549420676
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.016193850636482238
step = 12, Training Accuracy: 0.8
Training loss = 0.017607989807923635
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.016364065110683442
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.77
params:  [0.2934334355131077, 0.01, 0.7223565169669317, 0.46242643301642694, 0.43324601380456884, 0.6817975367592382, 0.23870613959194806, 0.6458431716213472, 0.99, 0.4083476438772369, 0.38241591679537446, 0.01, 0.8803464803782881, 0.47821169249591944, 0.937275651793024, 0.01, 0.01, 0.9140111150308974]
Training loss = 0.017712013920148213
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.77625
Training loss = 0.017079781889915466
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017758932610352832
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.016558079222838085
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016962220966815947
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01757980575164159
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.016931500335534415
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.016089620192845662
step = 7, Training Accuracy: 0.8
Training loss = 0.016554961800575255
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.017121366560459136
step = 9, Training Accuracy: 0.75
Training loss = 0.016995192766189576
step = 10, Training Accuracy: 0.8
Training loss = 0.0159850005308787
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016987657447655995
step = 12, Training Accuracy: 0.78
Training loss = 0.016241456468900045
step = 13, Training Accuracy: 0.8
Training loss = 0.016416876713434857
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.78875
params:  [0.4774164973537989, 0.2749596102481198, 0.5799340829460786, 0.8017109123729713, 0.9306677870162818, 0.9465821329579489, 0.14382083741080398, 0.544768011703456, 0.6273778743809857, 0.25479298132303657, 0.722019795580363, 0.02241784961943598, 0.99, 0.5017140893142387, 0.7480702935046593, 0.11248768566428632, 0.21247236497514985, 0.5078217240491918]
Training loss = 0.019188977678616843
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7925
Training loss = 0.017138915956020354
step = 1, Training Accuracy: 0.77
Training loss = 0.01901609738667806
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.019825217326482136
step = 3, Training Accuracy: 0.73
Training loss = 0.01746298392613729
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01798963894446691
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.018737588127454123
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.018748158911863964
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01826609969139099
step = 8, Training Accuracy: 0.77
Training loss = 0.018511997163295747
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.018713118731975554
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.018315354983011883
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.01966595560312271
step = 12, Training Accuracy: 0.74
Training loss = 0.017175231575965882
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.018078090051809947
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7775
params:  [0.01, 0.03322629922441564, 0.6313811049375947, 0.6727075492278265, 0.43236597726755077, 0.8768741392822421, 0.3237886657213126, 0.5015650792401204, 0.9582891388485963, 0.3095076296287623, 0.797951517102129, 0.01, 0.8485175499163409, 0.7150811106960318, 0.879717545000649, 0.19859402252410444, 0.46712075050064567, 0.8720554550256575]
Training loss = 0.018097804188728334
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7725
Training loss = 0.02050596336523692
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.018336726327737172
step = 2, Training Accuracy: 0.74
Training loss = 0.018946442306041717
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.018795188268025717
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.018500206271807353
step = 5, Training Accuracy: 0.75
Training loss = 0.01928584525982539
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.018610464731852214
step = 7, Training Accuracy: 0.75
Training loss = 0.01865224152803421
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01893243392308553
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018221158484617868
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.018658664226531983
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.01710118442773819
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.018837016224861145
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.01644613633553187
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.77375
params:  [0.10953725132916559, 0.01, 0.8090957426507306, 0.43641120798570676, 0.557779175119785, 0.8116954775991174, 0.4945129772057206, 0.46274159839717477, 0.7139980310482031, 0.2725972350154339, 0.5426061066085976, 0.14145727178278394, 0.8122658078289214, 0.2565929929140909, 0.99, 0.1088234745871962, 0.23578775824682371, 0.7861051734877741]
Training loss = 0.016463228762149812
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78
Training loss = 0.017563575009504954
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.01739593595266342
step = 2, Training Accuracy: 0.78
Training loss = 0.016216775774955748
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.017289297382036845
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01764677047729492
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.015535714626312256
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.015346845189730327
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01660680115222931
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.015531773467858632
step = 9, Training Accuracy: 0.81
Training loss = 0.015880149404207865
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.015022535125414531
step = 11, Training Accuracy: 0.81
Training loss = 0.015239907105763754
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.01675492376089096
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.014911107321580252
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.785
[[0.1439747598503092, 0.3320846958824242, 0.9819602095515549, 0.41716472859071274, 0.34762998581288956, 0.8533949180426788, 0.09394325300948023, 0.4727943600293524, 0.99, 0.2770951591550786, 0.41006769686519196, 0.45353378865754046, 0.99, 0.6051645801431333, 0.99, 0.07911618163297177, 0.48556050984169374, 0.8061767869830907], [0.32940018753852834, 0.046079117429930275, 0.655979953341634, 0.643380458823663, 0.7196204541606787, 0.9563917300815384, 0.45473664474742104, 0.5392543079289353, 0.9345647089473915, 0.5561490915426467, 0.7435535853318984, 0.09570713405365906, 0.99, 0.5772906889788648, 0.99, 0.022263615981890555, 0.4339072969869163, 0.99], [0.3529308087924474, 0.12193223264603609, 0.7644300062410877, 0.590032272985544, 0.8199528611616185, 0.853996994487778, 0.14967871453387188, 0.5879336027880291, 0.6140373545739883, 0.6177754265975435, 0.5028415895106579, 0.286471459924582, 0.99, 0.160996924616024, 0.836463089017614, 0.08542826024832723, 0.5443812509462608, 0.46309111670637787], [0.3339137797928727, 0.15172756369869764, 0.3487315530915752, 0.9687866266991161, 0.5217382149610998, 0.8203149100131587, 0.1897922929962427, 0.42660096869843767, 0.8888684906480804, 0.14543422870575742, 0.5221453367462057, 0.42189714235935327, 0.6897020296637937, 0.6088299964893257, 0.99, 0.01, 0.01, 0.99], [0.2934334355131077, 0.01, 0.7223565169669317, 0.46242643301642694, 0.43324601380456884, 0.6817975367592382, 0.23870613959194806, 0.6458431716213472, 0.99, 0.4083476438772369, 0.38241591679537446, 0.01, 0.8803464803782881, 0.47821169249591944, 0.937275651793024, 0.01, 0.01, 0.9140111150308974], [0.4774164973537989, 0.2749596102481198, 0.5799340829460786, 0.8017109123729713, 0.9306677870162818, 0.9465821329579489, 0.14382083741080398, 0.544768011703456, 0.6273778743809857, 0.25479298132303657, 0.722019795580363, 0.02241784961943598, 0.99, 0.5017140893142387, 0.7480702935046593, 0.11248768566428632, 0.21247236497514985, 0.5078217240491918], [0.01, 0.03322629922441564, 0.6313811049375947, 0.6727075492278265, 0.43236597726755077, 0.8768741392822421, 0.3237886657213126, 0.5015650792401204, 0.9582891388485963, 0.3095076296287623, 0.797951517102129, 0.01, 0.8485175499163409, 0.7150811106960318, 0.879717545000649, 0.19859402252410444, 0.46712075050064567, 0.8720554550256575], [0.10953725132916559, 0.01, 0.8090957426507306, 0.43641120798570676, 0.557779175119785, 0.8116954775991174, 0.4945129772057206, 0.46274159839717477, 0.7139980310482031, 0.2725972350154339, 0.5426061066085976, 0.14145727178278394, 0.8122658078289214, 0.2565929929140909, 0.99, 0.1088234745871962, 0.23578775824682371, 0.7861051734877741]]
14 	8     	0.780469	0.00599275	0.77   	0.78875
params:  [0.2989280314557901, 0.24267701466022687, 0.99, 0.42033069183312766, 0.5733744747466234, 0.7408629586136286, 0.3773690105268338, 0.6360668270233794, 0.938233190866511, 0.5901774984676071, 0.43691267532507905, 0.2840085325406686, 0.874034021968698, 0.6148922110586863, 0.99, 0.01, 0.212869263512363, 0.7238824117200645]
Training loss = 0.01873580733935038
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.78375
Training loss = 0.018910741011301677
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.01818120002746582
step = 2, Training Accuracy: 0.72
Training loss = 0.0175333630045255
step = 3, Training Accuracy: 0.78
Training loss = 0.017896616061528522
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.01732327212889989
step = 5, Training Accuracy: 0.78
Training loss = 0.018430664837360382
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.017057926853497822
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01826839288075765
step = 8, Training Accuracy: 0.75
Training loss = 0.018670666019121807
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.016951798697312673
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.017560323774814604
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.017402236362298328
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.0186998384197553
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.016783165037631987
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.78875
params:  [0.35413032747666884, 0.01, 0.9012459884402209, 0.5435536479636992, 0.35909149162063886, 0.36053903076730365, 0.05807424213439635, 0.7538423819594302, 0.7863130859002783, 0.5477868885613931, 0.12050170945248095, 0.3440389713625709, 0.7402105894018347, 0.6653097870164766, 0.5988311464558382, 0.01, 0.41510659428923585, 0.5825643488152459]
Training loss = 0.01724257568518321
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79
Training loss = 0.018960689306259156
step = 1, Training Accuracy: 0.76
Training loss = 0.018239011069138843
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.01698032816251119
step = 3, Training Accuracy: 0.79
Training loss = 0.017455403606096903
step = 4, Training Accuracy: 0.76
Training loss = 0.015419781108697255
step = 5, Training Accuracy: 0.82
Training loss = 0.016171984374523163
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.01615631232659022
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016695507069428763
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.016733620266119638
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01804610063632329
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.015294149418671925
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01574937403202057
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.015664570530255637
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015947709282239278
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79
params:  [0.13018564665910265, 0.01, 0.45480315059320336, 0.606575045161427, 0.2729460329570184, 0.6320409042124577, 0.20790958185732022, 0.8305429638037558, 0.7221936703083317, 0.5897578965272482, 0.4439859958893243, 0.05937361216711701, 0.99, 0.4669578442417387, 0.5889058444650577, 0.01, 0.22780934589877283, 0.6410341152778586]
Training loss = 0.017523821095625558
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.795
Training loss = 0.017776615719000497
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.019973151783148447
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.01842151830593745
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016412917325894037
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.016676001648108164
step = 5, Training Accuracy: 0.78
Training loss = 0.017058665752410888
step = 6, Training Accuracy: 0.78
Training loss = 0.016233712136745453
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.016947406033674877
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01555045485496521
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016945799589157106
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01665002683798472
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.016707725524902343
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.016645293335119882
step = 13, Training Accuracy: 0.78
Training loss = 0.017296475768089296
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78125
params:  [0.4201916707523661, 0.02092167921394466, 0.6564975204891031, 0.4000028783982703, 0.5590096445725118, 0.588757146401571, 0.15655764652703402, 0.5350801002370558, 0.6164625557100634, 0.5825323283705377, 0.40161725157449435, 0.018319313807556442, 0.691760692124219, 0.19877090226652663, 0.8152366969139566, 0.01, 0.3470281798174155, 0.9059894646725879]
Training loss = 0.01658771534760793
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7825
Training loss = 0.014289335409800211
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.01653499315182368
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.015343300898869832
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015250966846942902
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.014117561380068462
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.013781785865624745
step = 6, Training Accuracy: 0.82
Training loss = 0.013508678575356802
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.01454073742032051
step = 8, Training Accuracy: 0.8333333333333334
Training loss = 0.014084056218465169
step = 9, Training Accuracy: 0.83
Training loss = 0.014052958885828654
step = 10, Training Accuracy: 0.82
Training loss = 0.013484117339054743
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.01443113073706627
step = 12, Training Accuracy: 0.83
Training loss = 0.013809689581394195
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.01408763587474823
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7875
params:  [0.18733987980786226, 0.1493097862603331, 0.6851658957476412, 0.5941605077504181, 0.6032810949575753, 0.99, 0.12619640783920044, 0.37207707627143877, 0.9791889658504265, 0.583302242152232, 0.4471612316167705, 0.14640635800586746, 0.7082740521113269, 0.12876613918559834, 0.99, 0.01, 0.12302504404503174, 0.7028491823515424]
Training loss = 0.01732670336961746
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.78875
Training loss = 0.017760683794816334
step = 1, Training Accuracy: 0.73
Training loss = 0.017746415535608927
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.020204002956549328
step = 3, Training Accuracy: 0.7033333333333334
Training loss = 0.015996980567773184
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017599511543909707
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.017596090336640675
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.017896584570407867
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.016840543150901794
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.01796658178170522
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.016238896052042644
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01791940599679947
step = 11, Training Accuracy: 0.74
Training loss = 0.01555212159951528
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.017471234252055486
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.017794855535030366
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.78125
params:  [0.15418801931526702, 0.12643388474471373, 0.6287782933015219, 0.524256700114829, 0.3409353945450084, 0.8464691878440228, 0.2808447558793443, 0.7941910468297455, 0.6340610723154088, 0.10593169082399606, 0.344132872456339, 0.23099124640617746, 0.99, 0.27333130255160354, 0.99, 0.276289584546482, 0.40869585557733307, 0.99]
Training loss = 0.015682725161314012
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78125
Training loss = 0.01578086882829666
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014193639854590098
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.015883374512195587
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.0134652275343736
step = 4, Training Accuracy: 0.82
Training loss = 0.016784698714812595
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.014416584173838298
step = 6, Training Accuracy: 0.83
Training loss = 0.014940167864163717
step = 7, Training Accuracy: 0.81
Training loss = 0.013489212890466054
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.015009793043136597
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.014980371693770091
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.013967010080814362
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.014418984005848567
step = 12, Training Accuracy: 0.81
Training loss = 0.013632021148999533
step = 13, Training Accuracy: 0.82
Training loss = 0.013013516068458558
step = 14, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.78
params:  [0.37846710594381033, 0.01, 0.6811504739927687, 0.19372135552355896, 0.5697994693581134, 0.7377099504160262, 0.3010519676352704, 0.6211315866149759, 0.6163795278366484, 0.43038896929872006, 0.4838885473857536, 0.2390327060157284, 0.8266967404548107, 0.3176116941725507, 0.9006409323598633, 0.068698310775888, 0.2100509699167272, 0.1747216127960135]
Training loss = 0.01678092747926712
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7825
Training loss = 0.015721588432788848
step = 1, Training Accuracy: 0.81
Training loss = 0.017884918451309205
step = 2, Training Accuracy: 0.78
Training loss = 0.01797168766458829
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016010246276855468
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.017548473278681437
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.017321250240008038
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.018047717610994975
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.016345251500606537
step = 8, Training Accuracy: 0.79
Training loss = 0.016944494744141895
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.017137992779413858
step = 10, Training Accuracy: 0.8
Training loss = 0.01605197767416636
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.015481783052285512
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014992686609427134
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015744713842868806
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.78625
params:  [0.24738929662736286, 0.05636984348003551, 0.6239290363304468, 0.5067691434459424, 0.4753581017728614, 0.7554514839129071, 0.01, 0.31392602738108744, 0.9577069588868494, 0.41192932545908306, 0.604081965482038, 0.01, 0.6443339183307873, 0.5842690699264561, 0.99, 0.2047559138593123, 0.02523014989661876, 0.7865176330114978]
Training loss = 0.01740671883026759
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.8
Training loss = 0.016162983079751333
step = 1, Training Accuracy: 0.8
Training loss = 0.016523935000101724
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.015194370448589324
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016910623808701834
step = 4, Training Accuracy: 0.77
Training loss = 0.014752664466698965
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.01617969274520874
step = 6, Training Accuracy: 0.78
Training loss = 0.01662990927696228
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015520749390125274
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01436064749956131
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.014908258318901062
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.014491155296564102
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.013930569191773733
step = 12, Training Accuracy: 0.82
Training loss = 0.01594067672888438
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.014085192680358887
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.795
[[0.2989280314557901, 0.24267701466022687, 0.99, 0.42033069183312766, 0.5733744747466234, 0.7408629586136286, 0.3773690105268338, 0.6360668270233794, 0.938233190866511, 0.5901774984676071, 0.43691267532507905, 0.2840085325406686, 0.874034021968698, 0.6148922110586863, 0.99, 0.01, 0.212869263512363, 0.7238824117200645], [0.35413032747666884, 0.01, 0.9012459884402209, 0.5435536479636992, 0.35909149162063886, 0.36053903076730365, 0.05807424213439635, 0.7538423819594302, 0.7863130859002783, 0.5477868885613931, 0.12050170945248095, 0.3440389713625709, 0.7402105894018347, 0.6653097870164766, 0.5988311464558382, 0.01, 0.41510659428923585, 0.5825643488152459], [0.13018564665910265, 0.01, 0.45480315059320336, 0.606575045161427, 0.2729460329570184, 0.6320409042124577, 0.20790958185732022, 0.8305429638037558, 0.7221936703083317, 0.5897578965272482, 0.4439859958893243, 0.05937361216711701, 0.99, 0.4669578442417387, 0.5889058444650577, 0.01, 0.22780934589877283, 0.6410341152778586], [0.4201916707523661, 0.02092167921394466, 0.6564975204891031, 0.4000028783982703, 0.5590096445725118, 0.588757146401571, 0.15655764652703402, 0.5350801002370558, 0.6164625557100634, 0.5825323283705377, 0.40161725157449435, 0.018319313807556442, 0.691760692124219, 0.19877090226652663, 0.8152366969139566, 0.01, 0.3470281798174155, 0.9059894646725879], [0.18733987980786226, 0.1493097862603331, 0.6851658957476412, 0.5941605077504181, 0.6032810949575753, 0.99, 0.12619640783920044, 0.37207707627143877, 0.9791889658504265, 0.583302242152232, 0.4471612316167705, 0.14640635800586746, 0.7082740521113269, 0.12876613918559834, 0.99, 0.01, 0.12302504404503174, 0.7028491823515424], [0.15418801931526702, 0.12643388474471373, 0.6287782933015219, 0.524256700114829, 0.3409353945450084, 0.8464691878440228, 0.2808447558793443, 0.7941910468297455, 0.6340610723154088, 0.10593169082399606, 0.344132872456339, 0.23099124640617746, 0.99, 0.27333130255160354, 0.99, 0.276289584546482, 0.40869585557733307, 0.99], [0.37846710594381033, 0.01, 0.6811504739927687, 0.19372135552355896, 0.5697994693581134, 0.7377099504160262, 0.3010519676352704, 0.6211315866149759, 0.6163795278366484, 0.43038896929872006, 0.4838885473857536, 0.2390327060157284, 0.8266967404548107, 0.3176116941725507, 0.9006409323598633, 0.068698310775888, 0.2100509699167272, 0.1747216127960135], [0.24738929662736286, 0.05636984348003551, 0.6239290363304468, 0.5067691434459424, 0.4753581017728614, 0.7554514839129071, 0.01, 0.31392602738108744, 0.9577069588868494, 0.41192932545908306, 0.604081965482038, 0.01, 0.6443339183307873, 0.5842690699264561, 0.99, 0.2047559138593123, 0.02523014989661876, 0.7865176330114978]]
15 	8     	0.78625 	0.00484123	0.78   	0.795  
params:  [0.5090865255959343, 0.15389490850125256, 0.6299485440326379, 0.36374931295125945, 0.5343332446024704, 0.5193489952661436, 0.01, 0.5344305400902463, 0.9096389950954293, 0.40479080980818616, 0.5671667656994154, 0.3840166732017916, 0.9004393630840655, 0.8677342610177187, 0.9727049795312354, 0.2357307514594303, 0.22475457456235376, 0.48919778942064823]
Training loss = 0.019346713622411093
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.7925
Training loss = 0.018231638272603354
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.019599305987358092
step = 2, Training Accuracy: 0.77
Training loss = 0.018250906268755595
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.020017983118693034
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.01863114098707835
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.017118039826552075
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.018692642649014792
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.018271028995513916
step = 8, Training Accuracy: 0.78
Training loss = 0.018712961971759798
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01800101826588313
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.018107478618621827
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.017653019825617473
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017048560082912445
step = 13, Training Accuracy: 0.76
Training loss = 0.016610295474529267
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78625
params:  [0.307282660021041, 0.12108056009198986, 0.8132778381078001, 0.6819166618442402, 0.37808230777753576, 0.696986333386518, 0.03551074549851367, 0.5930945807281549, 0.9783325268342304, 0.47222777268662514, 0.22799610434398015, 0.31481323037994446, 0.692542797119856, 0.49319555109123725, 0.5867362582744617, 0.1737729560235265, 0.05311977027723418, 0.9459530183734552]
Training loss = 0.020267359614372253
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79125
Training loss = 0.018599352240562438
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.019824980199337004
step = 2, Training Accuracy: 0.77
Training loss = 0.0187571386496226
step = 3, Training Accuracy: 0.72
Training loss = 0.018306228816509246
step = 4, Training Accuracy: 0.76
Training loss = 0.019802497526009877
step = 5, Training Accuracy: 0.7333333333333333
Training loss = 0.01821980267763138
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.019260908365249633
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.01763035461306572
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.02002683460712433
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.01882315903902054
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.018173165023326873
step = 11, Training Accuracy: 0.76
Training loss = 0.018796433707078296
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01838616649309794
step = 13, Training Accuracy: 0.74
Training loss = 0.017718955477078756
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.79625
params:  [0.01, 0.26731453043021236, 0.7930306804123441, 0.4681487840137856, 0.4697397400715755, 0.7779123936111044, 0.01, 0.3561334990445907, 0.8222028650542073, 0.3475793259952142, 0.3773818258130089, 0.014820927197052636, 0.7031518019614238, 0.6416403195719904, 0.9150936485406664, 0.12739907744837783, 0.036955128166280576, 0.7289756158275469]
Training loss = 0.01537522037823995
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.7925
Training loss = 0.01691085010766983
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01582547108332316
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.01636333256959915
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.017920387585957844
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.016053630808989208
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01739543636639913
step = 6, Training Accuracy: 0.76
Training loss = 0.01567685087521871
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016127772827943167
step = 8, Training Accuracy: 0.79
Training loss = 0.016502767503261566
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.014943300982316336
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015297472178936004
step = 11, Training Accuracy: 0.81
Training loss = 0.016626144846280416
step = 12, Training Accuracy: 0.78
Training loss = 0.015211215615272522
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.017935846547285715
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.8
params:  [0.38602816038296506, 0.24979133140402804, 0.9435741530685725, 0.5299355447605939, 0.46808880089623633, 0.5053414160156787, 0.01, 0.5849640842017931, 0.9263647004797175, 0.46069535466425565, 0.49803954381179916, 0.2518741053242294, 0.7769754738861568, 0.4107093029056483, 0.5888533361808092, 0.1170438542717157, 0.01, 0.7129804083415903]
Training loss = 0.015828792651494342
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.79375
Training loss = 0.015564743479092915
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.01572544753551483
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.016360354622205097
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.015571516851584116
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.014147171080112457
step = 5, Training Accuracy: 0.79
Training loss = 0.015239410499731699
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.01474005768696467
step = 7, Training Accuracy: 0.8
Training loss = 0.014468929370244344
step = 8, Training Accuracy: 0.81
Training loss = 0.01657446493705114
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.015355396767457325
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.016920936206976572
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.0150335360566775
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.014863326152165731
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.014771246016025544
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79125
params:  [0.2788395923011258, 0.28382785739118366, 0.7602739182240846, 0.5303285177717885, 0.6433238492074049, 0.5278855086562202, 0.01, 0.3668370148984433, 0.9512792960362775, 0.5426261660977579, 0.22575839255575922, 0.284267909556689, 0.5195063772953318, 0.5056382772033061, 0.99, 0.22937736125868508, 0.19487478189486407, 0.5598742538358522]
Training loss = 0.017675053675969443
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79
Training loss = 0.01611321379741033
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.01496582676966985
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01688624272743861
step = 3, Training Accuracy: 0.78
Training loss = 0.015993655721346537
step = 4, Training Accuracy: 0.8
Training loss = 0.015485256016254424
step = 5, Training Accuracy: 0.78
Training loss = 0.017148911952972412
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.015444225072860718
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015939589341481528
step = 8, Training Accuracy: 0.78
Training loss = 0.014993913074334463
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015328295727570851
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.016095710496107737
step = 11, Training Accuracy: 0.77
Training loss = 0.014401108125845592
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.015611193478107452
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015577083726723989
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78875
params:  [0.24422552932614428, 0.18151005470990283, 0.8064291674367677, 0.4393516659489095, 0.2775232786098646, 0.31147753407010287, 0.07124941349213251, 0.5369319044155204, 0.863888878986057, 0.4541500347972799, 0.5541714858359966, 0.01, 0.7499872156149199, 0.6836207062558887, 0.8385025573071077, 0.04517394479585361, 0.45816388244183737, 0.8661325432419313]
Training loss = 0.01812413310011228
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.7925
Training loss = 0.01685455411672592
step = 1, Training Accuracy: 0.77
Training loss = 0.016477336486180622
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.016488673786322277
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.01568848788738251
step = 4, Training Accuracy: 0.81
Training loss = 0.016380003243684767
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.016188751856486
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.018358184695243834
step = 7, Training Accuracy: 0.79
Training loss = 0.015487233102321625
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.014895004034042359
step = 9, Training Accuracy: 0.8
Training loss = 0.015345796048641204
step = 10, Training Accuracy: 0.8
Training loss = 0.015892019271850587
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.015036588609218597
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015528164307276408
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.015736790994803112
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.785
params:  [0.5777358749410908, 0.01, 0.673574072138476, 0.2998114942533947, 0.12546921860264665, 0.7927443977508226, 0.01, 0.43591060047868807, 0.99, 0.24614821805918588, 0.1529197411211733, 0.2406273032849704, 0.8799120785805865, 0.7166250357545871, 0.7979244780515652, 0.3275364178807182, 0.30969716901918837, 0.7056542534482848]
Training loss = 0.016828779876232148
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.78875
Training loss = 0.01656916429599126
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.0161184890071551
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.018041149874528248
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.01683234473069509
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016275801261266074
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01807081272204717
step = 6, Training Accuracy: 0.7266666666666667
Training loss = 0.016258749266465505
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.016247744858264922
step = 8, Training Accuracy: 0.78
Training loss = 0.016265937785307567
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015898918559153874
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.014598820159832637
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.01498148888349533
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.016268165508906047
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.0156514506538709
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.7725
params:  [0.051471162455561326, 0.09533794949136176, 0.9688317111912041, 0.41126771219627833, 0.6978526766748205, 0.9637409383250198, 0.18192622517456666, 0.42194771364806627, 0.7627027430225435, 0.5665493914098958, 0.4277194158879349, 0.355663902278182, 0.714183505122182, 0.5398968715803978, 0.5780941509063303, 0.1290938777427456, 0.014382503716890316, 0.6204501696700243]
Training loss = 0.01733955482641856
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.7875
Training loss = 0.016105164885520936
step = 1, Training Accuracy: 0.79
Training loss = 0.01684676150480906
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01712745666503906
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016568375527858736
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.016281072696050006
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.016376245121161145
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015911191751559577
step = 7, Training Accuracy: 0.81
Training loss = 0.01791966607173284
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.017227557400862376
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.017326600054899853
step = 10, Training Accuracy: 0.79
Training loss = 0.016228001018365225
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014788001875082652
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.01796789526939392
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.017456878821055094
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.7925
[[0.5090865255959343, 0.15389490850125256, 0.6299485440326379, 0.36374931295125945, 0.5343332446024704, 0.5193489952661436, 0.01, 0.5344305400902463, 0.9096389950954293, 0.40479080980818616, 0.5671667656994154, 0.3840166732017916, 0.9004393630840655, 0.8677342610177187, 0.9727049795312354, 0.2357307514594303, 0.22475457456235376, 0.48919778942064823], [0.307282660021041, 0.12108056009198986, 0.8132778381078001, 0.6819166618442402, 0.37808230777753576, 0.696986333386518, 0.03551074549851367, 0.5930945807281549, 0.9783325268342304, 0.47222777268662514, 0.22799610434398015, 0.31481323037994446, 0.692542797119856, 0.49319555109123725, 0.5867362582744617, 0.1737729560235265, 0.05311977027723418, 0.9459530183734552], [0.01, 0.26731453043021236, 0.7930306804123441, 0.4681487840137856, 0.4697397400715755, 0.7779123936111044, 0.01, 0.3561334990445907, 0.8222028650542073, 0.3475793259952142, 0.3773818258130089, 0.014820927197052636, 0.7031518019614238, 0.6416403195719904, 0.9150936485406664, 0.12739907744837783, 0.036955128166280576, 0.7289756158275469], [0.38602816038296506, 0.24979133140402804, 0.9435741530685725, 0.5299355447605939, 0.46808880089623633, 0.5053414160156787, 0.01, 0.5849640842017931, 0.9263647004797175, 0.46069535466425565, 0.49803954381179916, 0.2518741053242294, 0.7769754738861568, 0.4107093029056483, 0.5888533361808092, 0.1170438542717157, 0.01, 0.7129804083415903], [0.2788395923011258, 0.28382785739118366, 0.7602739182240846, 0.5303285177717885, 0.6433238492074049, 0.5278855086562202, 0.01, 0.3668370148984433, 0.9512792960362775, 0.5426261660977579, 0.22575839255575922, 0.284267909556689, 0.5195063772953318, 0.5056382772033061, 0.99, 0.22937736125868508, 0.19487478189486407, 0.5598742538358522], [0.24422552932614428, 0.18151005470990283, 0.8064291674367677, 0.4393516659489095, 0.2775232786098646, 0.31147753407010287, 0.07124941349213251, 0.5369319044155204, 0.863888878986057, 0.4541500347972799, 0.5541714858359966, 0.01, 0.7499872156149199, 0.6836207062558887, 0.8385025573071077, 0.04517394479585361, 0.45816388244183737, 0.8661325432419313], [0.5777358749410908, 0.01, 0.673574072138476, 0.2998114942533947, 0.12546921860264665, 0.7927443977508226, 0.01, 0.43591060047868807, 0.99, 0.24614821805918588, 0.1529197411211733, 0.2406273032849704, 0.8799120785805865, 0.7166250357545871, 0.7979244780515652, 0.3275364178807182, 0.30969716901918837, 0.7056542534482848], [0.051471162455561326, 0.09533794949136176, 0.9688317111912041, 0.41126771219627833, 0.6978526766748205, 0.9637409383250198, 0.18192622517456666, 0.42194771364806627, 0.7627027430225435, 0.5665493914098958, 0.4277194158879349, 0.355663902278182, 0.714183505122182, 0.5398968715803978, 0.5780941509063303, 0.1290938777427456, 0.014382503716890316, 0.6204501696700243]]
16 	8     	0.789062	0.00779999	0.7725 	0.8    
params:  [0.012776157173579841, 0.01, 0.9819700092815075, 0.5525947830259389, 0.5530702142441954, 0.7853276659352338, 0.222902788553238, 0.5579934387388122, 0.8156884023839659, 0.5717522892182914, 0.2960167599532463, 0.1936209401749328, 0.9149051052757456, 0.6591901234022722, 0.99, 0.16182713864271567, 0.01, 0.7198697806252418]
Training loss = 0.017324562172094982
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79875
Training loss = 0.017077537427345912
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.017626410325368245
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.018077519635359446
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017923030654589334
step = 4, Training Accuracy: 0.78
Training loss = 0.017236228088537853
step = 5, Training Accuracy: 0.77
Training loss = 0.016048805316289265
step = 6, Training Accuracy: 0.78
Training loss = 0.015518096089363099
step = 7, Training Accuracy: 0.82
Training loss = 0.01542187641064326
step = 8, Training Accuracy: 0.8333333333333334
Training loss = 0.01632469614346822
step = 9, Training Accuracy: 0.8
Training loss = 0.015111033221085866
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016486918330192567
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.0155394846200943
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.015269275506337483
step = 13, Training Accuracy: 0.82
Training loss = 0.016589625378449758
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.78375
params:  [0.01, 0.2893851638356955, 0.7077513466053226, 0.5583516471712358, 0.4077419975918679, 0.744677516080009, 0.17478635982849755, 0.551986206172551, 0.99, 0.5582278633399578, 0.36219961214220947, 0.18392253926165836, 0.8147541164668546, 0.7941482075872098, 0.8784675631471102, 0.18435791237626642, 0.01, 0.6794957356261667]
Training loss = 0.019073394536972047
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7875
Training loss = 0.019009554982185362
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.017035520871480306
step = 2, Training Accuracy: 0.77
Training loss = 0.016799772679805754
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.02061679313580195
step = 4, Training Accuracy: 0.71
Training loss = 0.018916137715180716
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.01896232436100642
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.01710781455039978
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.016791940728823344
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.01980513870716095
step = 9, Training Accuracy: 0.74
Training loss = 0.016741650998592376
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017902790705362957
step = 11, Training Accuracy: 0.78
Training loss = 0.017545596460501352
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.0187710103392601
step = 13, Training Accuracy: 0.75
Training loss = 0.016883052090803784
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.79
params:  [0.12732249823500097, 0.26316195714624796, 0.497097856339858, 0.32684819704378765, 0.53631759727839, 0.7133689135992363, 0.01, 0.4681188002848565, 0.8180491096096771, 0.40909985742338806, 0.4657427995715168, 0.09779580490427262, 0.6777500053294055, 0.7532653345988232, 0.8002931404341699, 0.01, 0.2745533926020819, 0.99]
Training loss = 0.01758747677008311
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.795
Training loss = 0.02006596714258194
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.018598642746607462
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.017042091290156047
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016497268875439963
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.017787881791591645
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.016264624198277792
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.015920244852701822
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.017278472185134886
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016250266830126446
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.016819090743859608
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.016622759302457175
step = 11, Training Accuracy: 0.76
Training loss = 0.015458088616530101
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.014831061859925589
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.016100833018620808
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.77875
params:  [0.30190142899235156, 0.29415450166347495, 0.5576053841504688, 0.45080405186404626, 0.3293551580664743, 0.7621768271153547, 0.09560478969837899, 0.3402689094561273, 0.8620975399442442, 0.5568572259505178, 0.3365548543973843, 0.16528600136621013, 0.4223105413042775, 0.6398116118746413, 0.7820012189156499, 0.08710421432224957, 0.01, 0.99]
Training loss = 0.01611082245906194
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7775
Training loss = 0.017415504356225332
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01729474465052287
step = 2, Training Accuracy: 0.75
Training loss = 0.018716207643349966
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.018937849998474122
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01711613267660141
step = 5, Training Accuracy: 0.77
Training loss = 0.018195918103059133
step = 6, Training Accuracy: 0.75
Training loss = 0.01786488244930903
step = 7, Training Accuracy: 0.78
Training loss = 0.0161439256866773
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016854220827420552
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015897752940654753
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.016002996961275738
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.016341191033522287
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.01667278985182444
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.015360036889712016
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79125
params:  [0.17275544142468288, 0.37185610169320055, 0.7725479467718572, 0.7021860782242155, 0.41965855110582423, 0.9585531728326879, 0.01, 0.52439731152977, 0.9258839292395882, 0.5915960158707405, 0.42850118940489534, 0.01, 0.44995709679323426, 0.6850816145742498, 0.9602321692253558, 0.2965653451860326, 0.07926221204273307, 0.6432643481433655]
Training loss = 0.021880013545354206
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.795
Training loss = 0.01807589679956436
step = 1, Training Accuracy: 0.75
Training loss = 0.018255810836950936
step = 2, Training Accuracy: 0.75
Training loss = 0.019335350791613262
step = 3, Training Accuracy: 0.76
Training loss = 0.01881180187066396
step = 4, Training Accuracy: 0.72
Training loss = 0.01965934892495473
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.017094923357168834
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.019172535836696626
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.01726000557343165
step = 8, Training Accuracy: 0.76
Training loss = 0.01915707329909007
step = 9, Training Accuracy: 0.73
Training loss = 0.019977276225884754
step = 10, Training Accuracy: 0.72
Training loss = 0.016851451694965363
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.018648065427939096
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.018368451992670697
step = 13, Training Accuracy: 0.7333333333333333
Training loss = 0.016644359529018403
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.77875
params:  [0.20722486593504236, 0.01, 0.9360251118701809, 0.5364399077409354, 0.4531697103769516, 0.8020677204789771, 0.01, 0.5735756037637922, 0.8665370536587301, 0.8563737979039181, 0.3532087923104042, 0.33695281428878826, 0.5321462944984094, 0.5545820654701377, 0.7103417145054013, 0.23439667834720063, 0.16371540526010137, 0.8505453712825558]
Training loss = 0.017104385097821553
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.78625
Training loss = 0.015754072368144988
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.015655649304389955
step = 2, Training Accuracy: 0.79
Training loss = 0.017422888477643332
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.016356021761894227
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015206619352102279
step = 5, Training Accuracy: 0.8
Training loss = 0.01621886024872462
step = 6, Training Accuracy: 0.79
Training loss = 0.017283470531304676
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.01691688617070516
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015133415559927622
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015564564168453216
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.014796664118766784
step = 11, Training Accuracy: 0.81
Training loss = 0.0165385310848554
step = 12, Training Accuracy: 0.8
Training loss = 0.016235583225886027
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.017216338415940603
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.785
params:  [0.052431691430838034, 0.05417999513168992, 0.8778966774163228, 0.6623606317856565, 0.2637380839981667, 0.8534263618888571, 0.3930997811434165, 0.3848000679572913, 0.99, 0.3009292475493482, 0.5718046097771148, 0.24493071002461203, 0.6656381805838893, 0.5339968739098997, 0.9413585442781485, 0.01, 0.07897139064078554, 0.7209839473790087]
Training loss = 0.017234593629837036
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.77625
Training loss = 0.018247426251570383
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.021689065893491108
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.0191402476032575
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016641940971215567
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.01810212271908919
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.018687364161014557
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01903424600760142
step = 7, Training Accuracy: 0.76
Training loss = 0.017335043648878733
step = 8, Training Accuracy: 0.78
Training loss = 0.01763530323902766
step = 9, Training Accuracy: 0.79
Training loss = 0.018705506722132365
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.01764883836110433
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.017955757876237235
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.01861620585123698
step = 13, Training Accuracy: 0.78
Training loss = 0.018090759813785554
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.78625
params:  [0.035745047343717945, 0.06488993036241908, 0.9366610365665768, 0.33756843242969053, 0.24768405953200695, 0.8065210489455978, 0.22495884097126886, 0.622670532968433, 0.8135265304083034, 0.38605628792940233, 0.2775762278991286, 0.1494321802001102, 0.6929419212979389, 0.5577300790990869, 0.8398137898586386, 0.01, 0.01, 0.99]
Training loss = 0.018240684966246288
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79125
Training loss = 0.017170394857724508
step = 1, Training Accuracy: 0.77
Training loss = 0.01772915869951248
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018921195964018502
step = 3, Training Accuracy: 0.75
Training loss = 0.017860620220502218
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.017678223749001822
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.017691196699937184
step = 6, Training Accuracy: 0.78
Training loss = 0.017303301990032195
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.01783434102932612
step = 8, Training Accuracy: 0.74
Training loss = 0.01832171966632207
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.017793990472952523
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.0184186131755511
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018398759762446086
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.017025638123353323
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.016412890752156576
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77625
[[0.012776157173579841, 0.01, 0.9819700092815075, 0.5525947830259389, 0.5530702142441954, 0.7853276659352338, 0.222902788553238, 0.5579934387388122, 0.8156884023839659, 0.5717522892182914, 0.2960167599532463, 0.1936209401749328, 0.9149051052757456, 0.6591901234022722, 0.99, 0.16182713864271567, 0.01, 0.7198697806252418], [0.01, 0.2893851638356955, 0.7077513466053226, 0.5583516471712358, 0.4077419975918679, 0.744677516080009, 0.17478635982849755, 0.551986206172551, 0.99, 0.5582278633399578, 0.36219961214220947, 0.18392253926165836, 0.8147541164668546, 0.7941482075872098, 0.8784675631471102, 0.18435791237626642, 0.01, 0.6794957356261667], [0.12732249823500097, 0.26316195714624796, 0.497097856339858, 0.32684819704378765, 0.53631759727839, 0.7133689135992363, 0.01, 0.4681188002848565, 0.8180491096096771, 0.40909985742338806, 0.4657427995715168, 0.09779580490427262, 0.6777500053294055, 0.7532653345988232, 0.8002931404341699, 0.01, 0.2745533926020819, 0.99], [0.30190142899235156, 0.29415450166347495, 0.5576053841504688, 0.45080405186404626, 0.3293551580664743, 0.7621768271153547, 0.09560478969837899, 0.3402689094561273, 0.8620975399442442, 0.5568572259505178, 0.3365548543973843, 0.16528600136621013, 0.4223105413042775, 0.6398116118746413, 0.7820012189156499, 0.08710421432224957, 0.01, 0.99], [0.17275544142468288, 0.37185610169320055, 0.7725479467718572, 0.7021860782242155, 0.41965855110582423, 0.9585531728326879, 0.01, 0.52439731152977, 0.9258839292395882, 0.5915960158707405, 0.42850118940489534, 0.01, 0.44995709679323426, 0.6850816145742498, 0.9602321692253558, 0.2965653451860326, 0.07926221204273307, 0.6432643481433655], [0.20722486593504236, 0.01, 0.9360251118701809, 0.5364399077409354, 0.4531697103769516, 0.8020677204789771, 0.01, 0.5735756037637922, 0.8665370536587301, 0.8563737979039181, 0.3532087923104042, 0.33695281428878826, 0.5321462944984094, 0.5545820654701377, 0.7103417145054013, 0.23439667834720063, 0.16371540526010137, 0.8505453712825558], [0.052431691430838034, 0.05417999513168992, 0.8778966774163228, 0.6623606317856565, 0.2637380839981667, 0.8534263618888571, 0.3930997811434165, 0.3848000679572913, 0.99, 0.3009292475493482, 0.5718046097771148, 0.24493071002461203, 0.6656381805838893, 0.5339968739098997, 0.9413585442781485, 0.01, 0.07897139064078554, 0.7209839473790087], [0.035745047343717945, 0.06488993036241908, 0.9366610365665768, 0.33756843242969053, 0.24768405953200695, 0.8065210489455978, 0.22495884097126886, 0.622670532968433, 0.8135265304083034, 0.38605628792940233, 0.2775762278991286, 0.1494321802001102, 0.6929419212979389, 0.5577300790990869, 0.8398137898586386, 0.01, 0.01, 0.99]]
17 	8     	0.78375 	0.00511585	0.77625	0.79125
params:  [0.01, 0.21410743207039015, 0.34514852139733043, 0.563876864371746, 0.2150187933420724, 0.9760926706599915, 0.3304216020780949, 0.573986734302371, 0.7751094802472247, 0.5938485161318491, 0.342356674828656, 0.06630809961568465, 0.7596822109860268, 0.9187165720084794, 0.9681481794176403, 0.2781044745318596, 0.01, 0.8564416086764205]
Training loss = 0.019012079238891602
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.77875
Training loss = 0.01731533259153366
step = 1, Training Accuracy: 0.78
Training loss = 0.017800060510635377
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.017885568539301556
step = 3, Training Accuracy: 0.8
Training loss = 0.01835748811562856
step = 4, Training Accuracy: 0.77
Training loss = 0.017686316967010497
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.015748655299345653
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.015919384956359865
step = 7, Training Accuracy: 0.8
Training loss = 0.017294175326824188
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01798341244459152
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016284638941287996
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.01716036170721054
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.018580464124679567
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016474642753601075
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.017520758310953777
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.785
params:  [0.30103413362354514, 0.1266765473379839, 0.5918603609187176, 0.527141505789378, 0.3178946979377605, 0.6440749779570247, 0.2043360628139681, 0.5226996742002307, 0.8393697047160407, 0.5648919367703001, 0.621976616011635, 0.2649631569978424, 0.4811983616856619, 0.6232981894080246, 0.9051404151540297, 0.15101596495803868, 0.23149194041000862, 0.8273115280155589]
Training loss = 0.018166956504185993
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.78375
Training loss = 0.017044986585776013
step = 1, Training Accuracy: 0.75
Training loss = 0.01707230399052302
step = 2, Training Accuracy: 0.75
Training loss = 0.01796981930732727
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01657905101776123
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.016692145268122356
step = 5, Training Accuracy: 0.77
Training loss = 0.016264656980832418
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.016467914382616678
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.017526779373486838
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01765768547852834
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.017026316424210867
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.01656577577193578
step = 11, Training Accuracy: 0.78
Training loss = 0.015193425516287485
step = 12, Training Accuracy: 0.8
Training loss = 0.016731832027435303
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.017831496993700662
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.79875
params:  [0.3664768813133127, 0.10631334892269659, 0.6024287117403777, 0.5563005662079032, 0.49084341507710183, 0.8343805576042085, 0.01, 0.36289456689376215, 0.99, 0.39553565324064455, 0.48740556741050406, 0.04898596786094414, 0.4699478012032134, 0.6371677076403397, 0.9159672345784808, 0.01, 0.01, 0.99]
Training loss = 0.017872247298558554
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7925
Training loss = 0.01705793450276057
step = 1, Training Accuracy: 0.75
Training loss = 0.01630898078282674
step = 2, Training Accuracy: 0.73
Training loss = 0.01656891018152237
step = 3, Training Accuracy: 0.75
Training loss = 0.01732253611087799
step = 4, Training Accuracy: 0.77
Training loss = 0.01805357406536738
step = 5, Training Accuracy: 0.74
Training loss = 0.01718630035718282
step = 6, Training Accuracy: 0.77
Training loss = 0.01601469486951828
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.017932231426239013
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.018101526697476705
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.0179000186920166
step = 10, Training Accuracy: 0.76
Training loss = 0.017553194761276245
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.016642652054627737
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01609439412752787
step = 13, Training Accuracy: 0.79
Training loss = 0.01816014736890793
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.795
params:  [0.17987301360242322, 0.11890854756842897, 0.3474723236704443, 0.3764641825320314, 0.5242966481748406, 0.99, 0.17061530793720403, 0.5203888217293319, 0.99, 0.6272668542955087, 0.5932199279800855, 0.01, 0.8223397576161493, 0.37162318167822894, 0.6283770255066748, 0.01, 0.12216217945573174, 0.7943172352659585]
Training loss = 0.017366816103458405
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.79625
Training loss = 0.017633553445339203
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017790723542372384
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01821288247903188
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.017617334028085074
step = 4, Training Accuracy: 0.76
Training loss = 0.019037317236264548
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.01972700665394465
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.016464290221532184
step = 7, Training Accuracy: 0.8
Training loss = 0.017994199593861896
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.017952308158079782
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.01678854743639628
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017481416761875153
step = 11, Training Accuracy: 0.76
Training loss = 0.01738790988922119
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.019039160311222075
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.018113033374150593
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.79625
params:  [0.01, 0.12421926747744087, 0.7201416936034727, 0.5907184408149467, 0.516183758029934, 0.5682012574836997, 0.04888717135297149, 0.4522173386217414, 0.99, 0.6379064923160541, 0.4720851654751014, 0.010458808996942737, 0.909773737503941, 0.56835070455161, 0.99, 0.2822004210012596, 0.30907454028639453, 0.9878082769844294]
Training loss = 0.015331858694553375
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7925
Training loss = 0.016477983593940735
step = 1, Training Accuracy: 0.77
Training loss = 0.014232988357543946
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.014584573010603587
step = 3, Training Accuracy: 0.8
Training loss = 0.014490471184253693
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.014856254359086355
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.01592465043067932
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.016169879138469696
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014167096813519795
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.014989428222179413
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.015432914594809215
step = 10, Training Accuracy: 0.8
Training loss = 0.014311535159746806
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.014510978162288666
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013182776272296906
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.014628687699635823
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7875
params:  [0.11203219831785215, 0.01, 0.75035595550498, 0.5742915335924882, 0.31046469859472803, 0.551476415624671, 0.3176485817996181, 0.40515481518223406, 0.99, 0.5971682055196186, 0.915307791960146, 0.2436778786489556, 0.7952819574970522, 0.688223018106116, 0.7164118545664558, 0.30583238897715553, 0.01, 0.8062101313772095]
Training loss = 0.018723352551460265
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.79375
Training loss = 0.018081283072630565
step = 1, Training Accuracy: 0.75
Training loss = 0.017254671951135
step = 2, Training Accuracy: 0.77
Training loss = 0.018333644370237986
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.016870089073975882
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.016571580370267232
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.017393776575724284
step = 6, Training Accuracy: 0.77
Training loss = 0.016777671376864114
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.016412989993890126
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01516868603726228
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.017738315165042876
step = 10, Training Accuracy: 0.77
Training loss = 0.016037454009056092
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.016555215219656628
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.016179002622763314
step = 13, Training Accuracy: 0.77
Training loss = 0.017712657848993937
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7925
params:  [0.15646953562514732, 0.01, 0.36396398486078985, 0.7781446358191255, 0.1770953102452923, 0.9245006037338195, 0.2518593796841587, 0.08774001922745156, 0.99, 0.28250504084095357, 0.46282136192992074, 0.219665389305965, 0.34313877305561435, 0.8568019046549846, 0.9750428712622617, 0.11219760367588545, 0.01, 0.7953892076157874]
Training loss = 0.01929961164792379
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7925
Training loss = 0.020687555571397145
step = 1, Training Accuracy: 0.74
Training loss = 0.01870996246735255
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.02035175194342931
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.017659390072027843
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.018516916831334433
step = 5, Training Accuracy: 0.73
Training loss = 0.018435555199782055
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.018654249807198844
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.018507912755012512
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.017106404801209767
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01875496596097946
step = 10, Training Accuracy: 0.71
Training loss = 0.017343056599299113
step = 11, Training Accuracy: 0.77
Training loss = 0.017502901256084443
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.01768883228302002
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01837309161822001
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.7775
params:  [0.2064375822128601, 0.5313149662294199, 0.7977400188540273, 0.48617427086959, 0.2300442508601296, 0.7790175553538684, 0.07126170078624842, 0.032501427176692965, 0.99, 0.5800514210049407, 0.4882201338107033, 0.15670143987884183, 0.40077998026370365, 0.9683612407274467, 0.7386515814378103, 0.05600789304650016, 0.01962402681925183, 0.985059091849399]
Training loss = 0.017793812255064646
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78875
Training loss = 0.018126245935757956
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.018483109573523202
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.019071545700232187
step = 3, Training Accuracy: 0.74
Training loss = 0.0184799458583196
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01811011473337809
step = 5, Training Accuracy: 0.77
Training loss = 0.018335252503554025
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01688232700030009
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.0171796382466952
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.017096846501032513
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01750229557355245
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.018358480433622995
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.016748415331045787
step = 12, Training Accuracy: 0.78
Training loss = 0.01887288600206375
step = 13, Training Accuracy: 0.7333333333333333
Training loss = 0.017897725502649942
step = 14, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.79
[[0.01, 0.21410743207039015, 0.34514852139733043, 0.563876864371746, 0.2150187933420724, 0.9760926706599915, 0.3304216020780949, 0.573986734302371, 0.7751094802472247, 0.5938485161318491, 0.342356674828656, 0.06630809961568465, 0.7596822109860268, 0.9187165720084794, 0.9681481794176403, 0.2781044745318596, 0.01, 0.8564416086764205], [0.30103413362354514, 0.1266765473379839, 0.5918603609187176, 0.527141505789378, 0.3178946979377605, 0.6440749779570247, 0.2043360628139681, 0.5226996742002307, 0.8393697047160407, 0.5648919367703001, 0.621976616011635, 0.2649631569978424, 0.4811983616856619, 0.6232981894080246, 0.9051404151540297, 0.15101596495803868, 0.23149194041000862, 0.8273115280155589], [0.3664768813133127, 0.10631334892269659, 0.6024287117403777, 0.5563005662079032, 0.49084341507710183, 0.8343805576042085, 0.01, 0.36289456689376215, 0.99, 0.39553565324064455, 0.48740556741050406, 0.04898596786094414, 0.4699478012032134, 0.6371677076403397, 0.9159672345784808, 0.01, 0.01, 0.99], [0.17987301360242322, 0.11890854756842897, 0.3474723236704443, 0.3764641825320314, 0.5242966481748406, 0.99, 0.17061530793720403, 0.5203888217293319, 0.99, 0.6272668542955087, 0.5932199279800855, 0.01, 0.8223397576161493, 0.37162318167822894, 0.6283770255066748, 0.01, 0.12216217945573174, 0.7943172352659585], [0.01, 0.12421926747744087, 0.7201416936034727, 0.5907184408149467, 0.516183758029934, 0.5682012574836997, 0.04888717135297149, 0.4522173386217414, 0.99, 0.6379064923160541, 0.4720851654751014, 0.010458808996942737, 0.909773737503941, 0.56835070455161, 0.99, 0.2822004210012596, 0.30907454028639453, 0.9878082769844294], [0.11203219831785215, 0.01, 0.75035595550498, 0.5742915335924882, 0.31046469859472803, 0.551476415624671, 0.3176485817996181, 0.40515481518223406, 0.99, 0.5971682055196186, 0.915307791960146, 0.2436778786489556, 0.7952819574970522, 0.688223018106116, 0.7164118545664558, 0.30583238897715553, 0.01, 0.8062101313772095], [0.15646953562514732, 0.01, 0.36396398486078985, 0.7781446358191255, 0.1770953102452923, 0.9245006037338195, 0.2518593796841587, 0.08774001922745156, 0.99, 0.28250504084095357, 0.46282136192992074, 0.219665389305965, 0.34313877305561435, 0.8568019046549846, 0.9750428712622617, 0.11219760367588545, 0.01, 0.7953892076157874], [0.2064375822128601, 0.5313149662294199, 0.7977400188540273, 0.48617427086959, 0.2300442508601296, 0.7790175553538684, 0.07126170078624842, 0.032501427176692965, 0.99, 0.5800514210049407, 0.4882201338107033, 0.15670143987884183, 0.40077998026370365, 0.9683612407274467, 0.7386515814378103, 0.05600789304650016, 0.01962402681925183, 0.985059091849399]]
18 	8     	0.790312	0.00645749	0.7775 	0.79875
params:  [0.40195095956303983, 0.18930067033209522, 0.3951334881624762, 0.4182376035928181, 0.5095047242940219, 0.8070226465238803, 0.1793630683113204, 0.6552001331346619, 0.953405076420208, 0.4661155107837284, 0.26052014439222193, 0.07030655356282293, 0.3388517968048346, 0.6206016466232376, 0.7421700203780619, 0.05655997931774619, 0.1691053142463936, 0.7930935258565005]
Training loss = 0.016596115827560425
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.79375
Training loss = 0.017762071390946706
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.018184756835301717
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018334526618321738
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.01631296902894974
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.017026492754618327
step = 5, Training Accuracy: 0.79
Training loss = 0.017344477822383245
step = 6, Training Accuracy: 0.76
Training loss = 0.016586271027723948
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01801964968442917
step = 8, Training Accuracy: 0.77
Training loss = 0.018010710875193278
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.01664264380931854
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.016373860438664754
step = 11, Training Accuracy: 0.79
Training loss = 0.01578548272450765
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.017334117194016774
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.016233000059922537
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79
params:  [0.23212564603794333, 0.23585799409747343, 0.5004174895556666, 0.4897969188689348, 0.4316331230928398, 0.7373059217512123, 0.2247883549907607, 0.6434785071204417, 0.9095856994190232, 0.6219206994251233, 0.5529415966036306, 0.10549686576083206, 0.872197576388378, 0.549292328506873, 0.99, 0.01, 0.04249003920138836, 0.6005047785540695]
Training loss = 0.01811684568723043
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78875
Training loss = 0.01619940827290217
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.019277227222919465
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.016543107132116954
step = 3, Training Accuracy: 0.76
Training loss = 0.017118104497591654
step = 4, Training Accuracy: 0.73
Training loss = 0.016425681511561076
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.017491402328014372
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.01635994335015615
step = 7, Training Accuracy: 0.76
Training loss = 0.015798751513163248
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015724588433901468
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01730428506930669
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.016753576000531515
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.016741388539473216
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.015425992012023926
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015826009015242257
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.79375
params:  [0.3312934206883683, 0.01, 0.5617540441268156, 0.26185192450270445, 0.5745351403586706, 0.8436001905515563, 0.15858037929979724, 0.617776690099184, 0.8886636732592261, 0.6856064527847647, 0.4323153394418868, 0.1725254682908995, 0.493054644416995, 0.520524718790148, 0.9077978160699554, 0.2606412664700541, 0.2935217058983155, 0.7809266909504121]
Training loss = 0.014290978362162908
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.795
Training loss = 0.01438542664051056
step = 1, Training Accuracy: 0.8
Training loss = 0.013808636963367461
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01588861326376597
step = 3, Training Accuracy: 0.8
Training loss = 0.01464432676633199
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.016730141441027323
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.016345968743165334
step = 6, Training Accuracy: 0.79
Training loss = 0.014522525121768316
step = 7, Training Accuracy: 0.81
Training loss = 0.016029817561308543
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.01613179604212443
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015436103542645772
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.01495493769645691
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.014317598541577657
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.016971544722716014
step = 13, Training Accuracy: 0.76
Training loss = 0.0144153360525767
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79375
params:  [0.17731952737324352, 0.23194817453842834, 0.40341138665861453, 0.6103526044492065, 0.5361554758522784, 0.8599060554410138, 0.20409323146490527, 0.40613934183676337, 0.9614456953455653, 0.5958950918329202, 0.8238946786221781, 0.1909106503005775, 0.5795863899891374, 0.5905904731654903, 0.5492517380441478, 0.2196534301644536, 0.026738465835170755, 0.7653799539778112]
Training loss = 0.015655143558979033
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.795
Training loss = 0.016811688443024952
step = 1, Training Accuracy: 0.79
Training loss = 0.017533734838167826
step = 2, Training Accuracy: 0.78
Training loss = 0.016460864742596944
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.01693862328926722
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01609626621007919
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.016774886747201283
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.015409337729215622
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.016997945656379063
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.01625062565008799
step = 9, Training Accuracy: 0.79
Training loss = 0.015172175566355387
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01590823769569397
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.01626810957988103
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.015704479118188223
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01761524091164271
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.80125
params:  [0.4016603193649567, 0.01, 0.5112628371922692, 0.41227752917628807, 0.3124630069133146, 0.83289286852766, 0.01, 0.6644066974692865, 0.9552307532686174, 0.7498539295402427, 0.6841417009980931, 0.03612781091386068, 0.6641400592165153, 0.7179645730559385, 0.9017058187544765, 0.01, 0.04144891871746062, 0.6140161366161161]
Training loss = 0.017861050566037494
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.80375
Training loss = 0.01704471468925476
step = 1, Training Accuracy: 0.79
Training loss = 0.016502353151639303
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.017702330152193704
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01655057410399119
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.016853109548489252
step = 5, Training Accuracy: 0.77
Training loss = 0.016999249060948688
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.0176663738489151
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016162985066572825
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.015944724678993227
step = 9, Training Accuracy: 0.77
Training loss = 0.015235839585463207
step = 10, Training Accuracy: 0.8
Training loss = 0.015604448318481446
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015617839694023132
step = 12, Training Accuracy: 0.81
Training loss = 0.015014070868492126
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015743831396102904
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.79625
params:  [0.19454184581292683, 0.03915004392011297, 0.5113452555577348, 0.6808795280162481, 0.22793421124056226, 0.8133962775384798, 0.08004686050068655, 0.44828215725659126, 0.9565305143313539, 0.5519336981962437, 0.7639733300480671, 0.36670383109497073, 0.7217668628730914, 0.6256465230020275, 0.79147916859087, 0.13211587902006344, 0.01, 0.8385419296360995]
Training loss = 0.01595927933851878
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79
Training loss = 0.01747281084458033
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01651464174191157
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.016669989029566446
step = 3, Training Accuracy: 0.79
Training loss = 0.016419771413008374
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.015873837272326153
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.015457905232906341
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.016000483334064484
step = 7, Training Accuracy: 0.77
Training loss = 0.016835873027642567
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016934898098309836
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.01481138696273168
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.0169537748893102
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.015138175090154011
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.015469972342252732
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015594195524851482
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.79875
params:  [0.43567387087011666, 0.01, 0.5135846954413157, 0.421534163863964, 0.528607017931924, 0.7681428223915223, 0.09385540324854999, 0.6106457330675024, 0.99, 0.741630397248132, 0.6677977470192824, 0.11897393458422588, 0.5663930197475187, 0.6286163939133006, 0.8527068938855457, 0.05258849054062141, 0.05536306074917509, 0.8693793640294021]
Training loss = 0.018157945275306703
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.8
Training loss = 0.019689619143803915
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.01859260489543279
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018604452610015868
step = 3, Training Accuracy: 0.73
Training loss = 0.01761001855134964
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01835228164990743
step = 5, Training Accuracy: 0.77
Training loss = 0.01929684966802597
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.018697074155012768
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.017941482166449228
step = 8, Training Accuracy: 0.77
Training loss = 0.017860152820746104
step = 9, Training Accuracy: 0.75
Training loss = 0.017641470233599344
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.017787333031495413
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01736540933450063
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016167794466018678
step = 13, Training Accuracy: 0.79
Training loss = 0.015332473516464233
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.795
params:  [0.4363119985043341, 0.47125768648411814, 0.5106881973654792, 0.6297563353169325, 0.49936533100542885, 0.9699226799287838, 0.10084334971167301, 0.41842299264564536, 0.8120417937117215, 0.5448072509490689, 0.7952789001753869, 0.4023848256982441, 0.3897517773018452, 0.3642803608220385, 0.7392531555126478, 0.10655437115066607, 0.26913120934894796, 0.7558138587184011]
Training loss = 0.022904253304004668
step = 0, Training Accuracy: 0.71
Validation Accuracy: 0.79875
Training loss = 0.019405856430530548
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.021852828959623972
step = 2, Training Accuracy: 0.6766666666666666
Training loss = 0.02183328241109848
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.019869203269481658
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.019065264562765756
step = 5, Training Accuracy: 0.7333333333333333
Training loss = 0.019056252539157867
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.020512997210025787
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.02046614170074463
step = 8, Training Accuracy: 0.7166666666666667
Training loss = 0.02084728568792343
step = 9, Training Accuracy: 0.7166666666666667
Training loss = 0.02014185776313146
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.021826657950878143
step = 11, Training Accuracy: 0.7166666666666667
Training loss = 0.020760639508565267
step = 12, Training Accuracy: 0.75
Training loss = 0.01969230512777964
step = 13, Training Accuracy: 0.7333333333333333
Training loss = 0.019977625012397766
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.78625
[[0.40195095956303983, 0.18930067033209522, 0.3951334881624762, 0.4182376035928181, 0.5095047242940219, 0.8070226465238803, 0.1793630683113204, 0.6552001331346619, 0.953405076420208, 0.4661155107837284, 0.26052014439222193, 0.07030655356282293, 0.3388517968048346, 0.6206016466232376, 0.7421700203780619, 0.05655997931774619, 0.1691053142463936, 0.7930935258565005], [0.23212564603794333, 0.23585799409747343, 0.5004174895556666, 0.4897969188689348, 0.4316331230928398, 0.7373059217512123, 0.2247883549907607, 0.6434785071204417, 0.9095856994190232, 0.6219206994251233, 0.5529415966036306, 0.10549686576083206, 0.872197576388378, 0.549292328506873, 0.99, 0.01, 0.04249003920138836, 0.6005047785540695], [0.3312934206883683, 0.01, 0.5617540441268156, 0.26185192450270445, 0.5745351403586706, 0.8436001905515563, 0.15858037929979724, 0.617776690099184, 0.8886636732592261, 0.6856064527847647, 0.4323153394418868, 0.1725254682908995, 0.493054644416995, 0.520524718790148, 0.9077978160699554, 0.2606412664700541, 0.2935217058983155, 0.7809266909504121], [0.17731952737324352, 0.23194817453842834, 0.40341138665861453, 0.6103526044492065, 0.5361554758522784, 0.8599060554410138, 0.20409323146490527, 0.40613934183676337, 0.9614456953455653, 0.5958950918329202, 0.8238946786221781, 0.1909106503005775, 0.5795863899891374, 0.5905904731654903, 0.5492517380441478, 0.2196534301644536, 0.026738465835170755, 0.7653799539778112], [0.4016603193649567, 0.01, 0.5112628371922692, 0.41227752917628807, 0.3124630069133146, 0.83289286852766, 0.01, 0.6644066974692865, 0.9552307532686174, 0.7498539295402427, 0.6841417009980931, 0.03612781091386068, 0.6641400592165153, 0.7179645730559385, 0.9017058187544765, 0.01, 0.04144891871746062, 0.6140161366161161], [0.19454184581292683, 0.03915004392011297, 0.5113452555577348, 0.6808795280162481, 0.22793421124056226, 0.8133962775384798, 0.08004686050068655, 0.44828215725659126, 0.9565305143313539, 0.5519336981962437, 0.7639733300480671, 0.36670383109497073, 0.7217668628730914, 0.6256465230020275, 0.79147916859087, 0.13211587902006344, 0.01, 0.8385419296360995], [0.43567387087011666, 0.01, 0.5135846954413157, 0.421534163863964, 0.528607017931924, 0.7681428223915223, 0.09385540324854999, 0.6106457330675024, 0.99, 0.741630397248132, 0.6677977470192824, 0.11897393458422588, 0.5663930197475187, 0.6286163939133006, 0.8527068938855457, 0.05258849054062141, 0.05536306074917509, 0.8693793640294021], [0.4363119985043341, 0.47125768648411814, 0.5106881973654792, 0.6297563353169325, 0.49936533100542885, 0.9699226799287838, 0.10084334971167301, 0.41842299264564536, 0.8120417937117215, 0.5448072509490689, 0.7952789001753869, 0.4023848256982441, 0.3897517773018452, 0.3642803608220385, 0.7392531555126478, 0.10655437115066607, 0.26913120934894796, 0.7558138587184011]]
19 	8     	0.794375	0.00441942	0.78625	0.80125
params:  [0.33736462245144055, 0.23936992919607217, 0.5252124271837919, 0.5587295569747289, 0.29866716822275835, 0.7892743401953793, 0.4157059873713703, 0.41592997864620557, 0.8101253315925615, 0.8189274231117931, 0.6993036776002545, 0.24681412837250438, 0.6125589861986689, 0.4611134795156739, 0.909410291419082, 0.06081467901790576, 0.01, 0.844192146490886]
Training loss = 0.016495631138483683
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79125
Training loss = 0.017127818763256072
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017192151844501496
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.019426657954851788
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017445229391256967
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.016792334616184235
step = 5, Training Accuracy: 0.78
Training loss = 0.016878298620382946
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.016225703259309134
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.018628371357917787
step = 8, Training Accuracy: 0.75
Training loss = 0.0185248597462972
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.017817741135756175
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.016917907198270164
step = 11, Training Accuracy: 0.78
Training loss = 0.016627172231674193
step = 12, Training Accuracy: 0.75
Training loss = 0.01843159705400467
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.015387512942155202
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79125
params:  [0.10594499817649174, 0.1792396806258057, 0.31523697730030653, 0.5831115354710499, 0.3983250753784565, 0.8836692200296068, 0.20157548369321593, 0.332640695344609, 0.9297198431030427, 0.645005590910218, 0.7860061363208914, 0.15306939780017273, 0.8638516162318695, 0.6502747918032971, 0.8389945615590955, 0.21131584214664056, 0.01, 0.8949635555759095]
Training loss = 0.01979887157678604
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.785
Training loss = 0.01757776826620102
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.020027714868386587
step = 2, Training Accuracy: 0.72
Training loss = 0.01967591146628062
step = 3, Training Accuracy: 0.77
Training loss = 0.017965486844380697
step = 4, Training Accuracy: 0.78
Training loss = 0.020359049538771313
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.01821225553750992
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.018616117934385935
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.01808570881684621
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.018908095558484394
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018420962889989217
step = 10, Training Accuracy: 0.76
Training loss = 0.01752274493376414
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.01820377310117086
step = 12, Training Accuracy: 0.76
Training loss = 0.017691581348578136
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.017516760726769765
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7875
params:  [0.2693342591994577, 0.16670194013595313, 0.5134006322586138, 0.6822598609463335, 0.3826472341632896, 0.8302376014104821, 0.01, 0.4269075886202876, 0.8091463034414531, 0.7659064657965452, 0.6313453247060072, 0.08648225011797, 0.4726850054503374, 0.5890232226997189, 0.7228010019548605, 0.18548036477370783, 0.28229066189438307, 0.750184459566756]
Training loss = 0.021378580629825592
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.7875
Training loss = 0.020318845609823864
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.017996679544448852
step = 2, Training Accuracy: 0.7133333333333334
Training loss = 0.018457672297954558
step = 3, Training Accuracy: 0.75
Training loss = 0.019979449113210042
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.019883578618367512
step = 5, Training Accuracy: 0.73
Training loss = 0.019713225861390432
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.020292691091696423
step = 7, Training Accuracy: 0.7233333333333334
Training loss = 0.018409291505813597
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.019267920156319937
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.019164002339045208
step = 10, Training Accuracy: 0.75
Training loss = 0.018455563088258107
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01901451975107193
step = 12, Training Accuracy: 0.75
Training loss = 0.020375976264476778
step = 13, Training Accuracy: 0.71
Training loss = 0.017976661026477814
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.78875
params:  [0.13046564680181055, 0.2513533745887681, 0.34913501399938973, 0.8075409060442104, 0.33531259677332387, 0.8553678859986803, 0.1300824644171913, 0.5844151303710364, 0.9209250480218761, 0.6272869539121053, 0.7887697807379093, 0.3189959834086107, 0.5988079272731762, 0.6520421218379145, 0.3875629730035702, 0.390241591712254, 0.07658602766258893, 0.8621643512752668]
Training loss = 0.016387050052483875
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.78875
Training loss = 0.016123884320259095
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01701930890480677
step = 2, Training Accuracy: 0.81
Training loss = 0.01627613037824631
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016786545515060425
step = 4, Training Accuracy: 0.76
Training loss = 0.016504524449507396
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.016755278706550598
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.016192734042803445
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.01615756740172704
step = 8, Training Accuracy: 0.78
Training loss = 0.015452605485916138
step = 9, Training Accuracy: 0.77
Training loss = 0.016150629619757335
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01688912123441696
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01553628961245219
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014745421359936397
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.016461222370465597
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79125
params:  [0.29926586386691706, 0.10559842536943988, 0.5778843041679332, 0.6513595367976602, 0.38976473334547346, 0.7653223853157798, 0.01214461979760556, 0.34717402888254156, 0.99, 0.476799418019824, 0.6651886740641818, 0.04745608038813412, 0.6703512012001489, 0.827500068111763, 0.7410741901189217, 0.07900729654977204, 0.01, 0.644172619189946]
Training loss = 0.01789331833521525
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.78875
Training loss = 0.01695439358552297
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01568390558163325
step = 2, Training Accuracy: 0.8
Training loss = 0.017633197406927745
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.016874301036198934
step = 4, Training Accuracy: 0.78
Training loss = 0.015687469442685446
step = 5, Training Accuracy: 0.82
Training loss = 0.01630911707878113
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.015871956149737042
step = 7, Training Accuracy: 0.78
Training loss = 0.016230370302995047
step = 8, Training Accuracy: 0.78
Training loss = 0.01582040419181188
step = 9, Training Accuracy: 0.78
Training loss = 0.016759620110193888
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016517404317855835
step = 11, Training Accuracy: 0.8
Training loss = 0.01726802219947179
step = 12, Training Accuracy: 0.81
Training loss = 0.015980240603288013
step = 13, Training Accuracy: 0.82
Training loss = 0.015813043514887493
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7925
params:  [0.2723097753764282, 0.10995133146035474, 0.4195585430710162, 0.5466831038171377, 0.3786710406770035, 0.7057295442023261, 0.15316092607892337, 0.12833105188168986, 0.99, 0.5218240658053303, 0.7123844935870511, 0.15238572571421524, 0.6420519300997335, 0.5647740620461678, 0.8559856410560591, 0.029824452862782708, 0.01, 0.8821774322964664]
Training loss = 0.015298334459463755
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.79125
Training loss = 0.015314266284306844
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01646123766899109
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.013952014346917471
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.014183307190736134
step = 4, Training Accuracy: 0.81
Training loss = 0.0146624423066775
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015427420735359193
step = 6, Training Accuracy: 0.81
Training loss = 0.014385490218798319
step = 7, Training Accuracy: 0.82
Training loss = 0.014930229882399242
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.015119623641173046
step = 9, Training Accuracy: 0.79
Training loss = 0.015656759937604268
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.014015210171540579
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015731510321299234
step = 12, Training Accuracy: 0.78
Training loss = 0.01472021520137787
step = 13, Training Accuracy: 0.83
Training loss = 0.015338934659957885
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.79375
params:  [0.32977737491456544, 0.12542725120093232, 0.5113352589730173, 0.5310081748113756, 0.48264122748647287, 0.7330889805649407, 0.04625153718302699, 0.4226947841077583, 0.99, 0.7548750875895911, 0.8582278988909255, 0.21156706138133252, 0.776541991300341, 0.5356162602676768, 0.7250581562263448, 0.01, 0.16057075558654513, 0.7565065052562417]
Training loss = 0.016582661469777424
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79625
Training loss = 0.017152503927548725
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01633208731810252
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01685119887193044
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01777412752310435
step = 4, Training Accuracy: 0.77
Training loss = 0.016189374128977457
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.013870869676272074
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.016383765240510304
step = 7, Training Accuracy: 0.8
Training loss = 0.016308204929033916
step = 8, Training Accuracy: 0.78
Training loss = 0.014404987891515095
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.016793562918901443
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.015056782166163126
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015968573490778605
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.015850689113140106
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.01573335995276769
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.79625
params:  [0.3170524416684988, 0.19208084980001, 0.5189056604161226, 0.5979825004444483, 0.5071868581035686, 0.7347956625122927, 0.026101484034444983, 0.4635817312800869, 0.7942911834601944, 0.48111242271047344, 0.7831706294020532, 0.3061999030316491, 0.9212443482502695, 0.7694504766925566, 0.4740074884582657, 0.21605295281919307, 0.01, 0.7361421294592212]
Training loss = 0.016247230172157286
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.795
Training loss = 0.016328075230121614
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.015642835994561514
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.014660297731558481
step = 3, Training Accuracy: 0.81
Training loss = 0.017181754608949027
step = 4, Training Accuracy: 0.75
Training loss = 0.016236743877331417
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.016192019879817963
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.017559187809626262
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.016428182721138
step = 8, Training Accuracy: 0.77
Training loss = 0.015444908738136292
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01562715689341227
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015159400403499603
step = 11, Training Accuracy: 0.78
Training loss = 0.01596302111943563
step = 12, Training Accuracy: 0.79
Training loss = 0.01620240290959676
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.014808139453331629
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.79125
[[0.33736462245144055, 0.23936992919607217, 0.5252124271837919, 0.5587295569747289, 0.29866716822275835, 0.7892743401953793, 0.4157059873713703, 0.41592997864620557, 0.8101253315925615, 0.8189274231117931, 0.6993036776002545, 0.24681412837250438, 0.6125589861986689, 0.4611134795156739, 0.909410291419082, 0.06081467901790576, 0.01, 0.844192146490886], [0.10594499817649174, 0.1792396806258057, 0.31523697730030653, 0.5831115354710499, 0.3983250753784565, 0.8836692200296068, 0.20157548369321593, 0.332640695344609, 0.9297198431030427, 0.645005590910218, 0.7860061363208914, 0.15306939780017273, 0.8638516162318695, 0.6502747918032971, 0.8389945615590955, 0.21131584214664056, 0.01, 0.8949635555759095], [0.2693342591994577, 0.16670194013595313, 0.5134006322586138, 0.6822598609463335, 0.3826472341632896, 0.8302376014104821, 0.01, 0.4269075886202876, 0.8091463034414531, 0.7659064657965452, 0.6313453247060072, 0.08648225011797, 0.4726850054503374, 0.5890232226997189, 0.7228010019548605, 0.18548036477370783, 0.28229066189438307, 0.750184459566756], [0.13046564680181055, 0.2513533745887681, 0.34913501399938973, 0.8075409060442104, 0.33531259677332387, 0.8553678859986803, 0.1300824644171913, 0.5844151303710364, 0.9209250480218761, 0.6272869539121053, 0.7887697807379093, 0.3189959834086107, 0.5988079272731762, 0.6520421218379145, 0.3875629730035702, 0.390241591712254, 0.07658602766258893, 0.8621643512752668], [0.29926586386691706, 0.10559842536943988, 0.5778843041679332, 0.6513595367976602, 0.38976473334547346, 0.7653223853157798, 0.01214461979760556, 0.34717402888254156, 0.99, 0.476799418019824, 0.6651886740641818, 0.04745608038813412, 0.6703512012001489, 0.827500068111763, 0.7410741901189217, 0.07900729654977204, 0.01, 0.644172619189946], [0.2723097753764282, 0.10995133146035474, 0.4195585430710162, 0.5466831038171377, 0.3786710406770035, 0.7057295442023261, 0.15316092607892337, 0.12833105188168986, 0.99, 0.5218240658053303, 0.7123844935870511, 0.15238572571421524, 0.6420519300997335, 0.5647740620461678, 0.8559856410560591, 0.029824452862782708, 0.01, 0.8821774322964664], [0.32977737491456544, 0.12542725120093232, 0.5113352589730173, 0.5310081748113756, 0.48264122748647287, 0.7330889805649407, 0.04625153718302699, 0.4226947841077583, 0.99, 0.7548750875895911, 0.8582278988909255, 0.21156706138133252, 0.776541991300341, 0.5356162602676768, 0.7250581562263448, 0.01, 0.16057075558654513, 0.7565065052562417], [0.3170524416684988, 0.19208084980001, 0.5189056604161226, 0.5979825004444483, 0.5071868581035686, 0.7347956625122927, 0.026101484034444983, 0.4635817312800869, 0.7942911834601944, 0.48111242271047344, 0.7831706294020532, 0.3061999030316491, 0.9212443482502695, 0.7694504766925566, 0.4740074884582657, 0.21605295281919307, 0.01, 0.7361421294592212]]
20 	8     	0.791562	0.00255792	0.7875 	0.79625
params:  [0.2834158141443113, 0.01, 0.6923622736739552, 0.5593755566915563, 0.7757359799679593, 0.6739962165655049, 0.15400636538508647, 0.28808418243918477, 0.99, 0.503163530778874, 0.9618979491186241, 0.11681804666018702, 0.7883124007027308, 0.764056027677849, 0.7863784434048827, 0.01, 0.01, 0.7120458676756583]
Training loss = 0.01667845368385315
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.795
Training loss = 0.01564834117889404
step = 1, Training Accuracy: 0.8
Training loss = 0.016097199022769928
step = 2, Training Accuracy: 0.8
Training loss = 0.015968149105707805
step = 3, Training Accuracy: 0.79
Training loss = 0.01642512023448944
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.0157233257095019
step = 5, Training Accuracy: 0.79
Training loss = 0.015396826366583505
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.015897318621476492
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.017515975634256997
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01610664536555608
step = 9, Training Accuracy: 0.78
Training loss = 0.01563695043325424
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015814276486635206
step = 11, Training Accuracy: 0.8
Training loss = 0.01464252640803655
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.01625687062740326
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015233105719089509
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
params:  [0.252709712271602, 0.1061982519315858, 0.8213808094959149, 0.549819003087127, 0.49738128809720306, 0.6010522353429528, 0.26987095998089383, 0.21469964420931115, 0.9443243355821352, 0.6035196725378514, 0.8028676411333298, 0.10173383943250322, 0.8516931691271779, 0.5404927666463827, 0.8644813714982386, 0.01, 0.046049152093467015, 0.6693166199006195]
Training loss = 0.01637194722890854
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7925
Training loss = 0.01591186096270879
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.016420779327551524
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.015149879654248556
step = 3, Training Accuracy: 0.81
Training loss = 0.016085879504680635
step = 4, Training Accuracy: 0.77
Training loss = 0.016628676652908327
step = 5, Training Accuracy: 0.77
Training loss = 0.015251596421003343
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01551660527785619
step = 7, Training Accuracy: 0.8
Training loss = 0.015067759454250335
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.015784092247486115
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.014689813951651256
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.01816033720970154
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.01610171506802241
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016705939173698427
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015501122772693634
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.79
params:  [0.5118844336027224, 0.01, 0.3774130950759728, 0.6837431166723789, 0.45224760721721485, 0.9271381863819086, 0.20584598460842712, 0.22571767997478354, 0.99, 0.9894180612173915, 0.99, 0.1393788541592074, 0.8917377410038089, 0.4672468191428547, 0.72558941998426, 0.018568906628333166, 0.07527337232439767, 0.6038183391672828]
Training loss = 0.0168913334608078
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.795
Training loss = 0.018455330828825632
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.018368895252545675
step = 2, Training Accuracy: 0.75
Training loss = 0.019805422524611156
step = 3, Training Accuracy: 0.78
Training loss = 0.017723662455876667
step = 4, Training Accuracy: 0.79
Training loss = 0.0188078765074412
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.01898689806461334
step = 6, Training Accuracy: 0.78
Training loss = 0.01806026170651118
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.017651875515778858
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.01713425000508626
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.017638227641582488
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017763836085796358
step = 11, Training Accuracy: 0.81
Training loss = 0.018026521106561024
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.01812897672255834
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.01798795153697332
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.79625
params:  [0.18712192461924937, 0.15277946716225313, 0.2981265201126094, 0.5464764911870885, 0.3690210438652413, 0.7046458040968498, 0.03224290887595924, 0.07401787238010954, 0.99, 0.8511193586844237, 0.99, 0.19803264856946914, 0.6297339571766796, 0.5205139220276737, 0.7199156799132221, 0.01, 0.01, 0.9748925900174599]
Training loss = 0.01785084843635559
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79625
Training loss = 0.018331013719240823
step = 1, Training Accuracy: 0.76
Training loss = 0.01630348970492681
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.016568645338217416
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.01608594705661138
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015631328523159026
step = 5, Training Accuracy: 0.77
Training loss = 0.016262741684913637
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016080517172813415
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.016528542836507162
step = 8, Training Accuracy: 0.78
Training loss = 0.01565937618414561
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.01594481666882833
step = 10, Training Accuracy: 0.75
Training loss = 0.014908126095930735
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016790334781010944
step = 12, Training Accuracy: 0.77
Training loss = 0.01631109893321991
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.01844460884730021
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79375
params:  [0.3006723524118936, 0.1267877464868104, 0.6621083632858349, 0.6589360999766153, 0.682226397506677, 0.6286585339881455, 0.19699092914141292, 0.27218329199893443, 0.99, 0.6871426555028421, 0.7536333512780728, 0.12605232313179876, 0.4681677092417762, 0.7467911245744064, 0.9438390761662216, 0.01, 0.01, 0.7723178544918433]
Training loss = 0.017951768239339194
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7875
Training loss = 0.01910151163736979
step = 1, Training Accuracy: 0.77
Training loss = 0.01567919448018074
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01739642788966497
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01695026914278666
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.018359917203585308
step = 5, Training Accuracy: 0.74
Training loss = 0.018187179466088613
step = 6, Training Accuracy: 0.77
Training loss = 0.01692265470822652
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.018118177553017933
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01772566606601079
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018149649798870085
step = 10, Training Accuracy: 0.78
Training loss = 0.018372748295466104
step = 11, Training Accuracy: 0.76
Training loss = 0.017790965239206948
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.018528947432835896
step = 13, Training Accuracy: 0.76
Training loss = 0.017321879069010417
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.78375
params:  [0.34377162916844806, 0.07274997488097852, 0.4239743717305179, 0.55363860940443, 0.44255208781201055, 0.561112628681534, 0.10471971271366808, 0.41784714860087074, 0.933694275429777, 0.5349507517238699, 0.8660193375884051, 0.21483335213845514, 0.6346304004194026, 0.6708287869489727, 0.902789781515279, 0.1201120059271798, 0.025269280058152502, 0.83597349003676]
Training loss = 0.017123083174228667
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.77875
Training loss = 0.017888102531433105
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.017475448846817017
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01835891087849935
step = 3, Training Accuracy: 0.76
Training loss = 0.017760896186033884
step = 4, Training Accuracy: 0.77
Training loss = 0.01750821739435196
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.016575319170951845
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.01847886174917221
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.016985121965408325
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.017360185583432514
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.016136928300062817
step = 10, Training Accuracy: 0.79
Training loss = 0.01709185560544332
step = 11, Training Accuracy: 0.76
Training loss = 0.0168479589621226
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.02047606259584427
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.015757112701733907
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78125
params:  [0.27597594983033613, 0.38178931557987184, 0.4433648264388221, 0.5308693396434698, 0.41767333255234496, 0.6012881585520988, 0.20796172134486318, 0.31206882277647663, 0.99, 0.6525002203791889, 0.7845879145212623, 0.01, 0.7751932547093503, 0.7206101602331945, 0.8097004925678549, 0.1315405287876102, 0.01, 0.9312281047163075]
Training loss = 0.018263634542624155
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.785
Training loss = 0.01720087985197703
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.019037731289863587
step = 2, Training Accuracy: 0.72
Training loss = 0.01751459151506424
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.017344967722892762
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01955379138390223
step = 5, Training Accuracy: 0.74
Training loss = 0.017510493497053782
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01708629379669825
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.01695459008216858
step = 8, Training Accuracy: 0.78
Training loss = 0.018580574889977775
step = 9, Training Accuracy: 0.7166666666666667
Training loss = 0.018010137379169466
step = 10, Training Accuracy: 0.74
Training loss = 0.016169803539911907
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.01827569236358007
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.018001586397488913
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.017280114491780598
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.77875
params:  [0.28592412696413444, 0.08570489329939401, 0.3810795842262376, 0.5612631266815765, 0.41000030426884826, 0.642605751188986, 0.06503641518896665, 0.2683994085873462, 0.9781316027769147, 0.5288031257914847, 0.9873726753571667, 0.09914121546601297, 0.8728789833883197, 0.5856032227694551, 0.753017716082882, 0.09327050620960299, 0.14455537694569648, 0.9547391567633156]
Training loss = 0.016767407258351644
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.77375
Training loss = 0.01567680497964223
step = 1, Training Accuracy: 0.79
Training loss = 0.015608983635902405
step = 2, Training Accuracy: 0.79
Training loss = 0.015924825270970663
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.015135985612869263
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.015406050582726797
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.016161291201909383
step = 6, Training Accuracy: 0.82
Training loss = 0.01422334353129069
step = 7, Training Accuracy: 0.8
Training loss = 0.015175237655639648
step = 8, Training Accuracy: 0.77
Training loss = 0.015676354467868806
step = 9, Training Accuracy: 0.79
Training loss = 0.015206613192955652
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.015268477300802866
step = 11, Training Accuracy: 0.78
Training loss = 0.015657310485839845
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015198652644952139
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.015005158086617787
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.785
[[0.2834158141443113, 0.01, 0.6923622736739552, 0.5593755566915563, 0.7757359799679593, 0.6739962165655049, 0.15400636538508647, 0.28808418243918477, 0.99, 0.503163530778874, 0.9618979491186241, 0.11681804666018702, 0.7883124007027308, 0.764056027677849, 0.7863784434048827, 0.01, 0.01, 0.7120458676756583], [0.252709712271602, 0.1061982519315858, 0.8213808094959149, 0.549819003087127, 0.49738128809720306, 0.6010522353429528, 0.26987095998089383, 0.21469964420931115, 0.9443243355821352, 0.6035196725378514, 0.8028676411333298, 0.10173383943250322, 0.8516931691271779, 0.5404927666463827, 0.8644813714982386, 0.01, 0.046049152093467015, 0.6693166199006195], [0.5118844336027224, 0.01, 0.3774130950759728, 0.6837431166723789, 0.45224760721721485, 0.9271381863819086, 0.20584598460842712, 0.22571767997478354, 0.99, 0.9894180612173915, 0.99, 0.1393788541592074, 0.8917377410038089, 0.4672468191428547, 0.72558941998426, 0.018568906628333166, 0.07527337232439767, 0.6038183391672828], [0.18712192461924937, 0.15277946716225313, 0.2981265201126094, 0.5464764911870885, 0.3690210438652413, 0.7046458040968498, 0.03224290887595924, 0.07401787238010954, 0.99, 0.8511193586844237, 0.99, 0.19803264856946914, 0.6297339571766796, 0.5205139220276737, 0.7199156799132221, 0.01, 0.01, 0.9748925900174599], [0.3006723524118936, 0.1267877464868104, 0.6621083632858349, 0.6589360999766153, 0.682226397506677, 0.6286585339881455, 0.19699092914141292, 0.27218329199893443, 0.99, 0.6871426555028421, 0.7536333512780728, 0.12605232313179876, 0.4681677092417762, 0.7467911245744064, 0.9438390761662216, 0.01, 0.01, 0.7723178544918433], [0.34377162916844806, 0.07274997488097852, 0.4239743717305179, 0.55363860940443, 0.44255208781201055, 0.561112628681534, 0.10471971271366808, 0.41784714860087074, 0.933694275429777, 0.5349507517238699, 0.8660193375884051, 0.21483335213845514, 0.6346304004194026, 0.6708287869489727, 0.902789781515279, 0.1201120059271798, 0.025269280058152502, 0.83597349003676], [0.27597594983033613, 0.38178931557987184, 0.4433648264388221, 0.5308693396434698, 0.41767333255234496, 0.6012881585520988, 0.20796172134486318, 0.31206882277647663, 0.99, 0.6525002203791889, 0.7845879145212623, 0.01, 0.7751932547093503, 0.7206101602331945, 0.8097004925678549, 0.1315405287876102, 0.01, 0.9312281047163075], [0.28592412696413444, 0.08570489329939401, 0.3810795842262376, 0.5612631266815765, 0.41000030426884826, 0.642605751188986, 0.06503641518896665, 0.2683994085873462, 0.9781316027769147, 0.5288031257914847, 0.9873726753571667, 0.09914121546601297, 0.8728789833883197, 0.5856032227694551, 0.753017716082882, 0.09327050620960299, 0.14455537694569648, 0.9547391567633156]]
21 	8     	0.787187	0.00568544	0.77875	0.79625
params:  [0.3733209207941258, 0.10518508628817319, 0.39699043229271436, 0.43051515048444944, 0.4524483196533734, 0.7456802494895827, 0.18178495760146274, 0.14820967500740917, 0.99, 0.7650245898870285, 0.99, 0.01, 0.6110027013367163, 0.4198617889236402, 0.6968886547724106, 0.15944789467307083, 0.01, 0.7844195426559172]
Training loss = 0.015650527874628704
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
Training loss = 0.01620102236668269
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.015520413219928742
step = 2, Training Accuracy: 0.78
Training loss = 0.01589422921339671
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.015939127703507742
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.015044101675351461
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.01633120854695638
step = 6, Training Accuracy: 0.78
Training loss = 0.015008637706438701
step = 7, Training Accuracy: 0.81
Training loss = 0.016975804169972738
step = 8, Training Accuracy: 0.79
Training loss = 0.015692261556784312
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.016417577465375265
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.015151825944582621
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.015180855890115101
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01626424471537272
step = 13, Training Accuracy: 0.78
Training loss = 0.01676009237766266
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7875
params:  [0.24751149022506538, 0.1516108667210656, 0.31280939765168647, 0.4438727586731098, 0.432467864770293, 0.8225384371903844, 0.09799308532200914, 0.21757147745931646, 0.8970027167310228, 0.7656483241810419, 0.9208701651744394, 0.3182761122751575, 0.9600907996239074, 0.6645704109851397, 0.7710527077738005, 0.03963944893628609, 0.01, 0.6391731533297491]
Training loss = 0.01579377075036367
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.785
Training loss = 0.017003674606482187
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.016415907243887584
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.01712477664152781
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.01809534102678299
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.017296636203924815
step = 5, Training Accuracy: 0.81
Training loss = 0.015742961366971335
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.017069320877393088
step = 7, Training Accuracy: 0.8
Training loss = 0.01733254353205363
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.017148391902446748
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016097579995791117
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.017256077925364176
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.016429422597090404
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.01710111250480016
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.018078524569670358
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7925
params:  [0.5199613071782234, 0.2114980925675212, 0.3117278894707577, 0.5662439856911708, 0.17963407407139675, 0.99, 0.2981748086112733, 0.22625389175064498, 0.99, 0.8416960220611552, 0.99, 0.08987664699769195, 0.5276899822012444, 0.35862374029442046, 0.99, 0.01, 0.01, 0.611836365046753]
Training loss = 0.01893704166014989
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.78375
Training loss = 0.01774792363246282
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.018342548310756684
step = 2, Training Accuracy: 0.75
Training loss = 0.019168772598107654
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.017934725880622865
step = 4, Training Accuracy: 0.76
Training loss = 0.018265987932682037
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.019752814173698424
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.01868039906024933
step = 7, Training Accuracy: 0.79
Training loss = 0.019433723986148833
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.017848614752292633
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.018938089807828268
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.01823117693265279
step = 11, Training Accuracy: 0.78
Training loss = 0.018656840026378633
step = 12, Training Accuracy: 0.75
Training loss = 0.019699237843354544
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.019105989933013916
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.785
params:  [0.33111856846654464, 0.22258343823043553, 0.5552973190064655, 0.5017758944217411, 0.5379360447434209, 0.9438599035929587, 0.0947990233278831, 0.12870742387594547, 0.9857794096642435, 0.99, 0.9205855255704346, 0.09561413614341426, 0.8433423779766464, 0.3692518751554833, 0.788411321922052, 0.01, 0.24196810984391223, 0.5786540085001111]
Training loss = 0.018176052769025168
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7825
Training loss = 0.01725033978621165
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01677426387866338
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.018150844772656757
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.018510096073150635
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.02019595921039581
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.018633980254332223
step = 6, Training Accuracy: 0.77
Training loss = 0.017617526054382323
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.020212271213531495
step = 8, Training Accuracy: 0.75
Training loss = 0.018461575806140898
step = 9, Training Accuracy: 0.76
Training loss = 0.019618300000826518
step = 10, Training Accuracy: 0.74
Training loss = 0.017186811963717144
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.017615309258302052
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.019287042617797852
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.017080927590529125
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7875
params:  [0.21318346207876693, 0.20482454704612213, 0.4155625241755866, 0.6331149832863373, 0.4791023945274544, 0.725694890176346, 0.17151931795710915, 0.01, 0.99, 0.99, 0.952633508227282, 0.14123694552130758, 0.8258216566437024, 0.5449082496986853, 0.7572527495355907, 0.01, 0.06811232086802424, 0.754170879473157]
Training loss = 0.019302915334701538
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7875
Training loss = 0.019996807177861533
step = 1, Training Accuracy: 0.73
Training loss = 0.01933371921380361
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.01850815922021866
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.020506110688050586
step = 4, Training Accuracy: 0.6966666666666667
Training loss = 0.01992934097846349
step = 5, Training Accuracy: 0.71
Training loss = 0.018768470784028372
step = 6, Training Accuracy: 0.77
Training loss = 0.02021544913450877
step = 7, Training Accuracy: 0.7333333333333333
Training loss = 0.01988416055838267
step = 8, Training Accuracy: 0.7033333333333334
Training loss = 0.017780175705750783
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.0203997736175855
step = 10, Training Accuracy: 0.72
Training loss = 0.01879196971654892
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.020005564391613006
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.020032128691673277
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.0202871439854304
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.79375
params:  [0.33220634305439006, 0.01, 0.29679757255306516, 0.4980201189404499, 0.40515792828655106, 0.752458344853176, 0.3944481090448719, 0.23516689870478197, 0.8924020574458529, 0.9379622855952237, 0.99, 0.4074939047999184, 0.7647814833439845, 0.32464806903722465, 0.7667506790913259, 0.01, 0.14840811114027375, 0.7308204037684961]
Training loss = 0.016980234682559967
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.7925
Training loss = 0.01652633418639501
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.018185576299826305
step = 2, Training Accuracy: 0.74
Training loss = 0.016109122534592946
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.01834552268187205
step = 4, Training Accuracy: 0.76
Training loss = 0.01808073788881302
step = 5, Training Accuracy: 0.7266666666666667
Training loss = 0.017489993770917256
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.017491398851076763
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.015418879489103953
step = 8, Training Accuracy: 0.79
Training loss = 0.019218664864699045
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.016932777017354964
step = 10, Training Accuracy: 0.7366666666666667
Training loss = 0.017151067753632863
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.016627277731895446
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.015911282896995546
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.016968316237131756
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.795
params:  [0.518614791543248, 0.17305781508780813, 0.3300272593001135, 0.601615801697507, 0.30202843068597973, 0.4845402328904996, 0.1206831226826241, 0.08446070380647924, 0.7555901935123959, 0.99, 0.99, 0.041812968412072854, 0.7825154338800364, 0.433511192414701, 0.8044111452597058, 0.01, 0.07198819898673663, 0.6918902622189952]
Training loss = 0.016686346232891083
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7975
Training loss = 0.01824643741051356
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.016920513461033505
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.018750063280264538
step = 3, Training Accuracy: 0.78
Training loss = 0.0164965158700943
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.018161420822143556
step = 5, Training Accuracy: 0.77
Training loss = 0.018180463115374246
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.019334110617637634
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.01921143223841985
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01748997747898102
step = 9, Training Accuracy: 0.77
Training loss = 0.017789550522963205
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.018213162422180174
step = 11, Training Accuracy: 0.76
Training loss = 0.017841575145721437
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.017179474333922067
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018214884102344512
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.795
params:  [0.291517364622207, 0.01, 0.6383363568566782, 0.7017263495530592, 0.4079641339739945, 0.8696733081453671, 0.09159996508667696, 0.18912017827401414, 0.923717658692714, 0.99, 0.99, 0.3197167710871364, 0.99, 0.3722092245985774, 0.8396719570975378, 0.01, 0.12700335708042226, 0.4400603320978148]
Training loss = 0.016125610371430715
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.79
Training loss = 0.016551584005355835
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.015747017661730447
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.014280144274234772
step = 3, Training Accuracy: 0.84
Training loss = 0.016609051525592805
step = 4, Training Accuracy: 0.78
Training loss = 0.015646713972091674
step = 5, Training Accuracy: 0.82
Training loss = 0.014839364687601725
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.01565411577622096
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.014133320053418478
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.016045433382193247
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015093175570170085
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.014777307609717051
step = 11, Training Accuracy: 0.82
Training loss = 0.015119010210037231
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.01578002115090688
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015224608580271403
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.795
[[0.3733209207941258, 0.10518508628817319, 0.39699043229271436, 0.43051515048444944, 0.4524483196533734, 0.7456802494895827, 0.18178495760146274, 0.14820967500740917, 0.99, 0.7650245898870285, 0.99, 0.01, 0.6110027013367163, 0.4198617889236402, 0.6968886547724106, 0.15944789467307083, 0.01, 0.7844195426559172], [0.24751149022506538, 0.1516108667210656, 0.31280939765168647, 0.4438727586731098, 0.432467864770293, 0.8225384371903844, 0.09799308532200914, 0.21757147745931646, 0.8970027167310228, 0.7656483241810419, 0.9208701651744394, 0.3182761122751575, 0.9600907996239074, 0.6645704109851397, 0.7710527077738005, 0.03963944893628609, 0.01, 0.6391731533297491], [0.5199613071782234, 0.2114980925675212, 0.3117278894707577, 0.5662439856911708, 0.17963407407139675, 0.99, 0.2981748086112733, 0.22625389175064498, 0.99, 0.8416960220611552, 0.99, 0.08987664699769195, 0.5276899822012444, 0.35862374029442046, 0.99, 0.01, 0.01, 0.611836365046753], [0.33111856846654464, 0.22258343823043553, 0.5552973190064655, 0.5017758944217411, 0.5379360447434209, 0.9438599035929587, 0.0947990233278831, 0.12870742387594547, 0.9857794096642435, 0.99, 0.9205855255704346, 0.09561413614341426, 0.8433423779766464, 0.3692518751554833, 0.788411321922052, 0.01, 0.24196810984391223, 0.5786540085001111], [0.21318346207876693, 0.20482454704612213, 0.4155625241755866, 0.6331149832863373, 0.4791023945274544, 0.725694890176346, 0.17151931795710915, 0.01, 0.99, 0.99, 0.952633508227282, 0.14123694552130758, 0.8258216566437024, 0.5449082496986853, 0.7572527495355907, 0.01, 0.06811232086802424, 0.754170879473157], [0.33220634305439006, 0.01, 0.29679757255306516, 0.4980201189404499, 0.40515792828655106, 0.752458344853176, 0.3944481090448719, 0.23516689870478197, 0.8924020574458529, 0.9379622855952237, 0.99, 0.4074939047999184, 0.7647814833439845, 0.32464806903722465, 0.7667506790913259, 0.01, 0.14840811114027375, 0.7308204037684961], [0.518614791543248, 0.17305781508780813, 0.3300272593001135, 0.601615801697507, 0.30202843068597973, 0.4845402328904996, 0.1206831226826241, 0.08446070380647924, 0.7555901935123959, 0.99, 0.99, 0.041812968412072854, 0.7825154338800364, 0.433511192414701, 0.8044111452597058, 0.01, 0.07198819898673663, 0.6918902622189952], [0.291517364622207, 0.01, 0.6383363568566782, 0.7017263495530592, 0.4079641339739945, 0.8696733081453671, 0.09159996508667696, 0.18912017827401414, 0.923717658692714, 0.99, 0.99, 0.3197167710871364, 0.99, 0.3722092245985774, 0.8396719570975378, 0.01, 0.12700335708042226, 0.4400603320978148]]
22 	8     	0.791406	0.00382414	0.785  	0.795  
params:  [0.21516272888279464, 0.01, 0.01, 0.801855491397888, 0.15769895388937874, 0.6309429520595699, 0.037964224392974394, 0.20424959805719622, 0.7912179006830472, 0.5697575829493001, 0.99, 0.3550426793859783, 0.9404250995700321, 0.4742239734509098, 0.8963265467087034, 0.01, 0.07430320511059138, 0.49888485039783204]
Training loss = 0.017036127249399822
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79875
Training loss = 0.016478557586669922
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.01772343158721924
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.016086297531922658
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017011126180489857
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01903817683458328
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.016847656071186067
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.015444760421911875
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.017060057421525318
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016698782444000245
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016031914552052814
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015867946843306224
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015954559644063313
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.015458585321903228
step = 13, Training Accuracy: 0.78
Training loss = 0.016900136669476828
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7975
params:  [0.5353033095050068, 0.043746958676444875, 0.2795225890768406, 0.43508087925787475, 0.36616149836212464, 0.7873030875584444, 0.2121421393105074, 0.36117326697126245, 0.7406028456960619, 0.9185499393705596, 0.8365141270623834, 0.3082614196942166, 0.8580511190380584, 0.1790805591275004, 0.6839848682402356, 0.0632730442967086, 0.01, 0.6254884125174087]
Training loss = 0.01564109613498052
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.80125
Training loss = 0.015671408871809643
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.016255210638046264
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.015344446996847788
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.017468392252922058
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015591458082199097
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014935413400332134
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.01635294109582901
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.017146285672982535
step = 8, Training Accuracy: 0.77
Training loss = 0.015175036589304606
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.01689913034439087
step = 10, Training Accuracy: 0.76
Training loss = 0.014981625080108642
step = 11, Training Accuracy: 0.81
Training loss = 0.015680018862088523
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014947887063026428
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01579485793908437
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.805
params:  [0.522001030721724, 0.20387241279597373, 0.31066699382509777, 0.5926060144657784, 0.18489280744791695, 0.726667843111166, 0.26830067341793956, 0.06520497086270959, 0.7053421840625749, 0.9541719463750592, 0.8717585986938835, 0.03352745462298137, 0.638650235362407, 0.15061383056946326, 0.7290811304268587, 0.01, 0.09213231229973091, 0.8211248711950224]
Training loss = 0.015661882758140563
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.79625
Training loss = 0.01620024303595225
step = 1, Training Accuracy: 0.79
Training loss = 0.01650031954050064
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.016187793811162313
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016932985484600066
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015145089030265808
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.014620970288912455
step = 6, Training Accuracy: 0.8
Training loss = 0.014537355105082195
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014141206045945485
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.016128531098365782
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.014320667485396068
step = 10, Training Accuracy: 0.78
Training loss = 0.014117334385712941
step = 11, Training Accuracy: 0.81
Training loss = 0.014888182779153188
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.017129664719104768
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.014801032543182373
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79
params:  [0.21948165203977552, 0.01, 0.24964030586614247, 0.6035032018676557, 0.07645850922452452, 0.5596746438502771, 0.3151934846110011, 0.13172355596280771, 0.5867359942851755, 0.99, 0.8953536749828994, 0.16488477584659153, 0.7447112121742199, 0.2841525953865781, 0.7979127424011653, 0.08661276592192352, 0.13181326423978804, 0.6119338507942313]
Training loss = 0.015729805529117583
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.7975
Training loss = 0.01638946056365967
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01765463868776957
step = 2, Training Accuracy: 0.8
Training loss = 0.016895039975643157
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016840901374816895
step = 4, Training Accuracy: 0.78
Training loss = 0.016480782975753148
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.01773525282740593
step = 6, Training Accuracy: 0.79
Training loss = 0.015467274487018585
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.014471511840820312
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01684168299039205
step = 9, Training Accuracy: 0.77
Training loss = 0.01581089824438095
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.015394835571448008
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.016518777509530385
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.01550347218910853
step = 13, Training Accuracy: 0.79
Training loss = 0.017227111061414082
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.8025
params:  [0.2919749335714575, 0.13182419878385682, 0.32298973882243, 0.5597059428053981, 0.6055072173452118, 0.8247387969351062, 0.344393255719858, 0.22048383298744784, 0.8657616539262492, 0.99, 0.936748880397347, 0.3271118151029462, 0.9506137198369203, 0.5907725824628205, 0.8620825118098834, 0.01, 0.07195934659764242, 0.6513166711793549]
Training loss = 0.016429119606812793
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79875
Training loss = 0.017048030694325766
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01574981967608134
step = 2, Training Accuracy: 0.8
Training loss = 0.01666892260313034
step = 3, Training Accuracy: 0.76
Training loss = 0.016132077475388844
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01523450473944346
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.01734918177127838
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016767291923364003
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01714084247748057
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.016371277670065562
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016273546616236367
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.017201514343420665
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.017684824665387473
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.016802866756916047
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01666055828332901
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79375
params:  [0.31894715231278276, 0.04855502259270082, 0.23468094272698417, 0.5930067727587661, 0.2939102788254382, 0.6252801451482959, 0.2635980426156322, 0.04913382130556565, 0.7612358006368053, 0.8400292463160641, 0.9391315802247873, 0.506852906556363, 0.8142933205866908, 0.5799447717049321, 0.8297933190733677, 0.14693944066920125, 0.25634008811265013, 0.467195468104175]
Training loss = 0.017487455308437348
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.795
Training loss = 0.017878920634587604
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.018006381293137867
step = 2, Training Accuracy: 0.76
Training loss = 0.016068432331085205
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.018415557245413463
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.017116875251134235
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.016611453592777253
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.0157369856039683
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.017399209439754485
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.016441635489463806
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.017085086504618326
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.016384506126244862
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015070470571517945
step = 12, Training Accuracy: 0.79
Training loss = 0.015675064424673715
step = 13, Training Accuracy: 0.79
Training loss = 0.015752671162287395
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.78875
params:  [0.3993036960090568, 0.12161365679262731, 0.369027318647233, 0.6367527148021493, 0.4292050806385569, 0.8241079541564647, 0.31277556223556185, 0.21926619914593132, 0.8042701358688523, 0.9055957542323174, 0.99, 0.24526915946531624, 0.7300021590567483, 0.314046566479695, 0.812213106016449, 0.01, 0.24738017627834286, 0.6555991409215991]
Training loss = 0.016435124973456065
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79125
Training loss = 0.016686837176481884
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017675131658713022
step = 2, Training Accuracy: 0.77
Training loss = 0.016253828207651776
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.015142056147257487
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.017649254004160564
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.017876008152961732
step = 6, Training Accuracy: 0.78
Training loss = 0.015775441726048788
step = 7, Training Accuracy: 0.78
Training loss = 0.01562581385175387
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.016287477413813273
step = 9, Training Accuracy: 0.78
Training loss = 0.01632976363102595
step = 10, Training Accuracy: 0.79
Training loss = 0.016867187817891437
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01705735315879186
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.01687656213839849
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.018023724257946013
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79375
params:  [0.5684170986455721, 0.16358264689167443, 0.42456062482894613, 0.5254676057260322, 0.3257590439287886, 0.8342664753837733, 0.3856268717233079, 0.414978672377874, 0.939914531397458, 0.883178286378026, 0.9803402292296535, 0.10460651675652421, 0.9179242854011602, 0.5471721524551001, 0.7953769890861968, 0.01, 0.203346844652862, 0.5839960575552386]
Training loss = 0.01735601599017779
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.795
Training loss = 0.01767698069413503
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.020670066873232525
step = 2, Training Accuracy: 0.76
Training loss = 0.017055324216683706
step = 3, Training Accuracy: 0.79
Training loss = 0.017902090847492218
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.017561571995417278
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.0184004678328832
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.017133383452892302
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01644903063774109
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.018189254800478616
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.015889003177483877
step = 10, Training Accuracy: 0.78
Training loss = 0.016917594174544016
step = 11, Training Accuracy: 0.81
Training loss = 0.01800208866596222
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016720877091089884
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.017087719837824502
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.79125
[[0.21516272888279464, 0.01, 0.01, 0.801855491397888, 0.15769895388937874, 0.6309429520595699, 0.037964224392974394, 0.20424959805719622, 0.7912179006830472, 0.5697575829493001, 0.99, 0.3550426793859783, 0.9404250995700321, 0.4742239734509098, 0.8963265467087034, 0.01, 0.07430320511059138, 0.49888485039783204], [0.5353033095050068, 0.043746958676444875, 0.2795225890768406, 0.43508087925787475, 0.36616149836212464, 0.7873030875584444, 0.2121421393105074, 0.36117326697126245, 0.7406028456960619, 0.9185499393705596, 0.8365141270623834, 0.3082614196942166, 0.8580511190380584, 0.1790805591275004, 0.6839848682402356, 0.0632730442967086, 0.01, 0.6254884125174087], [0.522001030721724, 0.20387241279597373, 0.31066699382509777, 0.5926060144657784, 0.18489280744791695, 0.726667843111166, 0.26830067341793956, 0.06520497086270959, 0.7053421840625749, 0.9541719463750592, 0.8717585986938835, 0.03352745462298137, 0.638650235362407, 0.15061383056946326, 0.7290811304268587, 0.01, 0.09213231229973091, 0.8211248711950224], [0.21948165203977552, 0.01, 0.24964030586614247, 0.6035032018676557, 0.07645850922452452, 0.5596746438502771, 0.3151934846110011, 0.13172355596280771, 0.5867359942851755, 0.99, 0.8953536749828994, 0.16488477584659153, 0.7447112121742199, 0.2841525953865781, 0.7979127424011653, 0.08661276592192352, 0.13181326423978804, 0.6119338507942313], [0.2919749335714575, 0.13182419878385682, 0.32298973882243, 0.5597059428053981, 0.6055072173452118, 0.8247387969351062, 0.344393255719858, 0.22048383298744784, 0.8657616539262492, 0.99, 0.936748880397347, 0.3271118151029462, 0.9506137198369203, 0.5907725824628205, 0.8620825118098834, 0.01, 0.07195934659764242, 0.6513166711793549], [0.31894715231278276, 0.04855502259270082, 0.23468094272698417, 0.5930067727587661, 0.2939102788254382, 0.6252801451482959, 0.2635980426156322, 0.04913382130556565, 0.7612358006368053, 0.8400292463160641, 0.9391315802247873, 0.506852906556363, 0.8142933205866908, 0.5799447717049321, 0.8297933190733677, 0.14693944066920125, 0.25634008811265013, 0.467195468104175], [0.3993036960090568, 0.12161365679262731, 0.369027318647233, 0.6367527148021493, 0.4292050806385569, 0.8241079541564647, 0.31277556223556185, 0.21926619914593132, 0.8042701358688523, 0.9055957542323174, 0.99, 0.24526915946531624, 0.7300021590567483, 0.314046566479695, 0.812213106016449, 0.01, 0.24738017627834286, 0.6555991409215991], [0.5684170986455721, 0.16358264689167443, 0.42456062482894613, 0.5254676057260322, 0.3257590439287886, 0.8342664753837733, 0.3856268717233079, 0.414978672377874, 0.939914531397458, 0.883178286378026, 0.9803402292296535, 0.10460651675652421, 0.9179242854011602, 0.5471721524551001, 0.7953769890861968, 0.01, 0.203346844652862, 0.5839960575552386]]
23 	8     	0.795312	0.005511  	0.78875	0.805  
params:  [0.2968145325750992, 0.08033736455705759, 0.21839258676390597, 0.4922206411577652, 0.41529680130576074, 0.8326001997504132, 0.3601450133538541, 0.192511736724276, 0.6423428983738237, 0.75495255924487, 0.854949864686769, 0.17680465262788597, 0.6273141990366851, 0.193838232322698, 0.9672128500374526, 0.04169587408346416, 0.24644886965607984, 0.6444713789813102]
Training loss = 0.01762392113606135
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7975
Training loss = 0.017924019793669383
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.018589481910069785
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.019685464402039846
step = 3, Training Accuracy: 0.77
Training loss = 0.01894972970088323
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01825018525123596
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.019215694268544515
step = 6, Training Accuracy: 0.76
Training loss = 0.017908425231774647
step = 7, Training Accuracy: 0.76
Training loss = 0.018593081335226695
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.017737948795159657
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.01554099420706431
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.017412020762761434
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.018274533053239186
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.017175018588701883
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.01798592448234558
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.795
params:  [0.3500221613377124, 0.01, 0.46523395367158993, 0.6986938475864812, 0.13506404025339747, 0.8699113210934677, 0.17998825086299972, 0.08100266103409126, 0.5680854273658025, 0.9505584583079332, 0.8737708720845653, 0.14915904608672273, 0.8015100384900984, 0.30095035324324465, 0.7826438950461334, 0.01, 0.06354718263298995, 0.7116243881353951]
Training loss = 0.01896373281876246
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7975
Training loss = 0.016222065488497417
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01697303036848704
step = 2, Training Accuracy: 0.76
Training loss = 0.018250117897987365
step = 3, Training Accuracy: 0.77
Training loss = 0.0186865371465683
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.01743086616198222
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.01817119707663854
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.017356716195742288
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.017778594891230265
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.018083514869213103
step = 9, Training Accuracy: 0.75
Training loss = 0.01721388320128123
step = 10, Training Accuracy: 0.77
Training loss = 0.01882500410079956
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.017706677317619324
step = 12, Training Accuracy: 0.77
Training loss = 0.0159323779741923
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01666419247786204
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.7925
params:  [0.26148409062047695, 0.36574001089727204, 0.17991854634175927, 0.5195956953814499, 0.3058629663699136, 0.7836924918799875, 0.22289015580708102, 0.3420716737644789, 0.629678260601662, 0.8205279358180072, 0.8835052349536012, 0.22746681561808058, 0.6573890654481161, 0.19475256389481754, 0.8051023289950332, 0.01, 0.01, 0.7282378852239194]
Training loss = 0.01820043722788493
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.79625
Training loss = 0.018047269880771637
step = 1, Training Accuracy: 0.77
Training loss = 0.0165415753920873
step = 2, Training Accuracy: 0.81
Training loss = 0.01759002516667048
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017307898898919424
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.01865920086701711
step = 5, Training Accuracy: 0.78
Training loss = 0.016842165489991505
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.018371167480945586
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.018984718024730684
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.018013056914011637
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.016852556765079497
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016662246386210125
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.017415488461653392
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01541669045885404
step = 13, Training Accuracy: 0.77
Training loss = 0.018055546482404074
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.795
params:  [0.560969298086997, 0.01, 0.4122129318814912, 0.5934021697146302, 0.41214795421116845, 0.6688351683379418, 0.06676827967569676, 0.4517040068422998, 0.808723044540979, 0.9186894605586269, 0.9570741449278746, 0.08315418662928553, 0.9322282692752377, 0.12352129639540765, 0.9458724712074296, 0.01, 0.10503063681224042, 0.4594189021114281]
Training loss = 0.017077619731426238
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79875
Training loss = 0.017345301806926727
step = 1, Training Accuracy: 0.78
Training loss = 0.01674572318792343
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.014803555011749268
step = 3, Training Accuracy: 0.82
Training loss = 0.016825897494951884
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.01577851265668869
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.016112741430600486
step = 6, Training Accuracy: 0.8
Training loss = 0.017713538308938345
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.017820290823777517
step = 8, Training Accuracy: 0.8
Training loss = 0.014956831832726796
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015798767109711963
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.017638336519400277
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.015842701693375906
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.01579157536228498
step = 13, Training Accuracy: 0.78
Training loss = 0.015641334156195324
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.795
params:  [0.3201046837039557, 0.11625841848674315, 0.418623158221997, 0.5962743426612134, 0.11371292121665966, 0.7439491578005242, 0.49359421970694484, 0.2991126874527967, 0.7357053995144076, 0.7359206885709222, 0.6335863132969587, 0.3609735197098829, 0.7064978449270323, 0.16544941872278102, 0.6550771113581357, 0.01, 0.01, 0.6323627286429971]
Training loss = 0.017324700554211935
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7975
Training loss = 0.018676077524820964
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01720017264286677
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016728829542795816
step = 3, Training Accuracy: 0.78
Training loss = 0.016414853831132253
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01834004799524943
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.017770416935284934
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.016798419455687205
step = 7, Training Accuracy: 0.77
Training loss = 0.019183608492215475
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.017672830720742543
step = 9, Training Accuracy: 0.77
Training loss = 0.018875263035297393
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.017393298049767813
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.017070413182179132
step = 12, Training Accuracy: 0.76
Training loss = 0.015453410844008128
step = 13, Training Accuracy: 0.79
Training loss = 0.018763343095779417
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.79625
params:  [0.47153790949345675, 0.02085553091957009, 0.53542703693025, 0.5080334879068426, 0.3930002340108474, 0.5130646928533836, 0.1119570548202509, 0.3376110987488919, 0.5655885168390806, 0.7913747436746232, 0.7087913957277108, 0.32775464767999324, 0.99, 0.08990955141460624, 0.5520752756372447, 0.01, 0.08741011307173502, 0.5718469625639049]
Training loss = 0.014121970733006795
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.8
Training loss = 0.015533678233623505
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.015378866493701935
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.015197454293568929
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.015618086357911428
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015565315186977386
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.0166281325618426
step = 6, Training Accuracy: 0.81
Training loss = 0.016082778672377267
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.0158757147192955
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.015369912683963776
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014386392831802368
step = 10, Training Accuracy: 0.81
Training loss = 0.014317857027053833
step = 11, Training Accuracy: 0.81
Training loss = 0.01517515242099762
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.013667673170566559
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01589580367008845
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7925
params:  [0.5839443234997522, 0.01, 0.10539375670259499, 0.5112867444176675, 0.01, 0.545077520568531, 0.23674168455769623, 0.2907047142794804, 0.5153016833716852, 0.99, 0.99, 0.09526301631181738, 0.7768482858167949, 0.2558692149714406, 0.7765773743194321, 0.17504721933430284, 0.137238197997798, 0.7595568294681037]
Training loss = 0.01740279297033946
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79125
Training loss = 0.01759964366753896
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017070657312870025
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.015987997154394785
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016381192406018576
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.0168317112326622
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01600451409816742
step = 6, Training Accuracy: 0.78
Training loss = 0.017164059976736704
step = 7, Training Accuracy: 0.78
Training loss = 0.016780943075815836
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.016083411077658337
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.015910145541032154
step = 10, Training Accuracy: 0.79
Training loss = 0.01723217169443766
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01846963236729304
step = 12, Training Accuracy: 0.78
Training loss = 0.0171117502450943
step = 13, Training Accuracy: 0.78
Training loss = 0.015589093963305156
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7875
params:  [0.3118827172701418, 0.0243020452119976, 0.4842116045766752, 0.6525806276401915, 0.20049073217113947, 0.5676077142730852, 0.4071710285742568, 0.13038356224712716, 0.6726992260494947, 0.99, 0.9593480155675489, 0.01, 0.8082003576958342, 0.1581459118475067, 0.935246736411439, 0.23306312522933362, 0.3129201941361871, 0.6275459237068047]
Training loss = 0.015945177475611368
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.7875
Training loss = 0.014348682016134262
step = 1, Training Accuracy: 0.81
Training loss = 0.015469757417837779
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.016189375519752504
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01647976686557134
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.014848840435345968
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.015647631188233695
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017047427793343863
step = 7, Training Accuracy: 0.75
Training loss = 0.016467284758885703
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.015618887444337209
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.016298732658227285
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015194092889626821
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.016524277528127036
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.015435615678628285
step = 13, Training Accuracy: 0.8
Training loss = 0.01535446415344874
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.7875
[[0.2968145325750992, 0.08033736455705759, 0.21839258676390597, 0.4922206411577652, 0.41529680130576074, 0.8326001997504132, 0.3601450133538541, 0.192511736724276, 0.6423428983738237, 0.75495255924487, 0.854949864686769, 0.17680465262788597, 0.6273141990366851, 0.193838232322698, 0.9672128500374526, 0.04169587408346416, 0.24644886965607984, 0.6444713789813102], [0.3500221613377124, 0.01, 0.46523395367158993, 0.6986938475864812, 0.13506404025339747, 0.8699113210934677, 0.17998825086299972, 0.08100266103409126, 0.5680854273658025, 0.9505584583079332, 0.8737708720845653, 0.14915904608672273, 0.8015100384900984, 0.30095035324324465, 0.7826438950461334, 0.01, 0.06354718263298995, 0.7116243881353951], [0.26148409062047695, 0.36574001089727204, 0.17991854634175927, 0.5195956953814499, 0.3058629663699136, 0.7836924918799875, 0.22289015580708102, 0.3420716737644789, 0.629678260601662, 0.8205279358180072, 0.8835052349536012, 0.22746681561808058, 0.6573890654481161, 0.19475256389481754, 0.8051023289950332, 0.01, 0.01, 0.7282378852239194], [0.560969298086997, 0.01, 0.4122129318814912, 0.5934021697146302, 0.41214795421116845, 0.6688351683379418, 0.06676827967569676, 0.4517040068422998, 0.808723044540979, 0.9186894605586269, 0.9570741449278746, 0.08315418662928553, 0.9322282692752377, 0.12352129639540765, 0.9458724712074296, 0.01, 0.10503063681224042, 0.4594189021114281], [0.3201046837039557, 0.11625841848674315, 0.418623158221997, 0.5962743426612134, 0.11371292121665966, 0.7439491578005242, 0.49359421970694484, 0.2991126874527967, 0.7357053995144076, 0.7359206885709222, 0.6335863132969587, 0.3609735197098829, 0.7064978449270323, 0.16544941872278102, 0.6550771113581357, 0.01, 0.01, 0.6323627286429971], [0.47153790949345675, 0.02085553091957009, 0.53542703693025, 0.5080334879068426, 0.3930002340108474, 0.5130646928533836, 0.1119570548202509, 0.3376110987488919, 0.5655885168390806, 0.7913747436746232, 0.7087913957277108, 0.32775464767999324, 0.99, 0.08990955141460624, 0.5520752756372447, 0.01, 0.08741011307173502, 0.5718469625639049], [0.5839443234997522, 0.01, 0.10539375670259499, 0.5112867444176675, 0.01, 0.545077520568531, 0.23674168455769623, 0.2907047142794804, 0.5153016833716852, 0.99, 0.99, 0.09526301631181738, 0.7768482858167949, 0.2558692149714406, 0.7765773743194321, 0.17504721933430284, 0.137238197997798, 0.7595568294681037], [0.3118827172701418, 0.0243020452119976, 0.4842116045766752, 0.6525806276401915, 0.20049073217113947, 0.5676077142730852, 0.4071710285742568, 0.13038356224712716, 0.6726992260494947, 0.99, 0.9593480155675489, 0.01, 0.8082003576958342, 0.1581459118475067, 0.935246736411439, 0.23306312522933362, 0.3129201941361871, 0.6275459237068047]]
24 	8     	0.792656	0.00321359	0.7875 	0.79625
params:  [0.3352410779610771, 0.15530457178666418, 0.24815091698434827, 0.47489303580110837, 0.30784751695596646, 0.7108502028999135, 0.28061622434411204, 0.10102275365780161, 0.7579797626265834, 0.8829426661145792, 0.6501479560218812, 0.286080270157056, 0.5930010925733045, 0.05874818215868431, 0.6531995998165661, 0.08346099315196166, 0.01, 0.7294329999186552]
Training loss = 0.016276403466860455
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79375
Training loss = 0.015616526305675506
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.015854376753171286
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01687672883272171
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01662632872660955
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.016913935045401254
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.01569338063398997
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.015927172005176543
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.016620207726955414
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.01767321695884069
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014105219940344492
step = 10, Training Accuracy: 0.8466666666666667
Training loss = 0.01788456618785858
step = 11, Training Accuracy: 0.77
Training loss = 0.015923812985420227
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01571914444367091
step = 13, Training Accuracy: 0.83
Training loss = 0.015764462848504385
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79125
params:  [0.40657754917717503, 0.15939685739456305, 0.3232927783589001, 0.4827549939733721, 0.07245535364635525, 0.8103587001647461, 0.36421161079449715, 0.3559059716213206, 0.728052158675661, 0.8301109996086244, 0.99, 0.2927573583838805, 0.7599657836958198, 0.01, 0.8531956721036466, 0.01, 0.2870843620595612, 0.6330280160971682]
Training loss = 0.01703569124142329
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79125
Training loss = 0.01668363869190216
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.017509533564249675
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.0170587291320165
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.016438653866449993
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.016080926408370336
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.016753195027510326
step = 6, Training Accuracy: 0.75
Training loss = 0.01668068548043569
step = 7, Training Accuracy: 0.79
Training loss = 0.017076286574204763
step = 8, Training Accuracy: 0.74
Training loss = 0.018015811642011006
step = 9, Training Accuracy: 0.75
Training loss = 0.01571645051240921
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.016714627246061962
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.017251156816879908
step = 12, Training Accuracy: 0.78
Training loss = 0.017820280392964682
step = 13, Training Accuracy: 0.76
Training loss = 0.017143939882516862
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7925
params:  [0.3943102180420352, 0.3486495441494507, 0.4341121106207561, 0.6310369610655564, 0.171573990158548, 0.7799906829285792, 0.4686862290138256, 0.33467686140405256, 0.6867079426825353, 0.6868302966439669, 0.9214471518886526, 0.34690153909728655, 0.6699331373758973, 0.20686350409848508, 0.9274282194749194, 0.2388743296879322, 0.04642860003746861, 0.8275733912361865]
Training loss = 0.01938086042801539
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79
Training loss = 0.017372416456540425
step = 1, Training Accuracy: 0.77
Training loss = 0.019854485591252646
step = 2, Training Accuracy: 0.75
Training loss = 0.01919457346200943
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.018951547145843507
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.021099474728107453
step = 5, Training Accuracy: 0.76
Training loss = 0.018827078739802043
step = 6, Training Accuracy: 0.78
Training loss = 0.019117825031280518
step = 7, Training Accuracy: 0.75
Training loss = 0.019479118088881173
step = 8, Training Accuracy: 0.78
Training loss = 0.02054088274637858
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.01935481290022532
step = 10, Training Accuracy: 0.7233333333333334
Training loss = 0.019548288583755492
step = 11, Training Accuracy: 0.74
Training loss = 0.018150351444880166
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.020458445052305856
step = 13, Training Accuracy: 0.75
Training loss = 0.020734845598538717
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.79
params:  [0.4248009340075937, 0.01, 0.19989794284616824, 0.6320092445082565, 0.2622860089279699, 0.7998804455603313, 0.4508616988170242, 0.5982353304893426, 0.6760886797518818, 0.7453482134864768, 0.6361043442913626, 0.3050396970848306, 0.5133375996331493, 0.04030945023136734, 0.7250309530853107, 0.04083328581125496, 0.18070600722668922, 0.5766318500758365]
Training loss = 0.013415046334266663
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.7925
Training loss = 0.013622784664233526
step = 1, Training Accuracy: 0.83
Training loss = 0.01386114239692688
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.014519312083721161
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.012693299502134324
step = 4, Training Accuracy: 0.85
Training loss = 0.014161578714847564
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.013504653970400493
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.01351574460665385
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.014322555263837179
step = 8, Training Accuracy: 0.82
Training loss = 0.013784871449073156
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.012852543542782466
step = 10, Training Accuracy: 0.8433333333333334
Training loss = 0.015053398211797079
step = 11, Training Accuracy: 0.82
Training loss = 0.014097822109858195
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.014076547622680664
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.013433850506941477
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.79625
params:  [0.28990485847092223, 0.2818493925474804, 0.373748636589365, 0.6092288363723797, 0.12410720035658715, 0.8084206683382064, 0.440428099037452, 0.4289120249256223, 0.414873689003068, 0.5941737134780851, 0.7374239453216042, 0.35899742640831256, 0.6448839957171848, 0.03849365298298854, 0.8823938868179693, 0.01, 0.14476041571596338, 0.6409660995455775]
Training loss = 0.018125728170077005
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79875
Training loss = 0.01771148363749186
step = 1, Training Accuracy: 0.76
Training loss = 0.016559876799583435
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.01746159772078196
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.018042877515157065
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.018600457906723024
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.01882510195175807
step = 6, Training Accuracy: 0.75
Training loss = 0.017931921382745106
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.017834128042062122
step = 8, Training Accuracy: 0.75
Training loss = 0.019067011972268424
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.01938778211673101
step = 10, Training Accuracy: 0.72
Training loss = 0.02021885971228282
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.016674274901549022
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.019685553610324858
step = 13, Training Accuracy: 0.7266666666666667
Training loss = 0.01679436892271042
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.7925
params:  [0.25665690805504165, 0.15249043664476944, 0.16236751266608976, 0.4265001744225025, 0.21346771795343467, 0.6572286098027628, 0.42913674081641284, 0.0997875457617928, 0.6236387719700915, 0.8403645926141383, 0.7478880477330395, 0.2809905921874036, 0.7679362340912018, 0.27764770874407796, 0.7862368458639332, 0.17275856540359863, 0.01, 0.3708858341025051]
Training loss = 0.01575092633565267
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79375
Training loss = 0.015814402103424073
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.016839062968889873
step = 2, Training Accuracy: 0.79
Training loss = 0.01534062405427297
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.017428024013837178
step = 4, Training Accuracy: 0.77
Training loss = 0.016897892753283184
step = 5, Training Accuracy: 0.79
Training loss = 0.015190906723340352
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.015623693068822225
step = 7, Training Accuracy: 0.79
Training loss = 0.015685418248176576
step = 8, Training Accuracy: 0.76
Training loss = 0.016094584266344705
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015584347446759543
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.01529439131418864
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016085707346598307
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.016596065660317738
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.016043256322542828
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7925
params:  [0.4651716595084734, 0.01, 0.2960251818502768, 0.45313196327255834, 0.12439443355836607, 0.89461099689026, 0.5776253055971828, 0.211242499752554, 0.650027811428685, 0.99, 0.8969821136582709, 0.31375921901441367, 0.625767475784518, 0.22431565985370597, 0.804791717543557, 0.2630201826624201, 0.07876540699638379, 0.567817471287007]
Training loss = 0.017435241540273032
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.795
Training loss = 0.017023648768663406
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.017653873165448506
step = 2, Training Accuracy: 0.79
Training loss = 0.016406915187835693
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.017286678949991863
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.017691658437252046
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.015971802771091462
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.016962911784648895
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.01812781035900116
step = 8, Training Accuracy: 0.81
Training loss = 0.018357247014840445
step = 9, Training Accuracy: 0.79
Training loss = 0.01736981064081192
step = 10, Training Accuracy: 0.8
Training loss = 0.01738259434700012
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.016992058753967285
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016214337050914764
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.017097412049770354
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.795
params:  [0.30204631019587164, 0.14031390355650966, 0.4405400824635944, 0.7552727758143198, 0.39491894138905526, 0.951569457936833, 0.3482367319774378, 0.2853511324116203, 0.6652565638623009, 0.5836568917525612, 0.7205081231139946, 0.48308668317811937, 0.69713228314352, 0.3939967882509099, 0.7246909096968962, 0.05002839784100143, 0.026504030101912095, 0.5791517299404727]
Training loss = 0.020967034697532652
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.79125
Training loss = 0.01886193037033081
step = 1, Training Accuracy: 0.72
Training loss = 0.019537609616915384
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.01852083295583725
step = 3, Training Accuracy: 0.72
Training loss = 0.017944466571013132
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.018951609432697296
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.018625340163707732
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.019757890204588572
step = 7, Training Accuracy: 0.72
Training loss = 0.020092694063981374
step = 8, Training Accuracy: 0.73
Training loss = 0.01931956837574641
step = 9, Training Accuracy: 0.75
Training loss = 0.01700642595688502
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.018092158436775207
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.019198265671730042
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.018104328711827596
step = 13, Training Accuracy: 0.78
Training loss = 0.019365619122982024
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.7875
[[0.3352410779610771, 0.15530457178666418, 0.24815091698434827, 0.47489303580110837, 0.30784751695596646, 0.7108502028999135, 0.28061622434411204, 0.10102275365780161, 0.7579797626265834, 0.8829426661145792, 0.6501479560218812, 0.286080270157056, 0.5930010925733045, 0.05874818215868431, 0.6531995998165661, 0.08346099315196166, 0.01, 0.7294329999186552], [0.40657754917717503, 0.15939685739456305, 0.3232927783589001, 0.4827549939733721, 0.07245535364635525, 0.8103587001647461, 0.36421161079449715, 0.3559059716213206, 0.728052158675661, 0.8301109996086244, 0.99, 0.2927573583838805, 0.7599657836958198, 0.01, 0.8531956721036466, 0.01, 0.2870843620595612, 0.6330280160971682], [0.3943102180420352, 0.3486495441494507, 0.4341121106207561, 0.6310369610655564, 0.171573990158548, 0.7799906829285792, 0.4686862290138256, 0.33467686140405256, 0.6867079426825353, 0.6868302966439669, 0.9214471518886526, 0.34690153909728655, 0.6699331373758973, 0.20686350409848508, 0.9274282194749194, 0.2388743296879322, 0.04642860003746861, 0.8275733912361865], [0.4248009340075937, 0.01, 0.19989794284616824, 0.6320092445082565, 0.2622860089279699, 0.7998804455603313, 0.4508616988170242, 0.5982353304893426, 0.6760886797518818, 0.7453482134864768, 0.6361043442913626, 0.3050396970848306, 0.5133375996331493, 0.04030945023136734, 0.7250309530853107, 0.04083328581125496, 0.18070600722668922, 0.5766318500758365], [0.28990485847092223, 0.2818493925474804, 0.373748636589365, 0.6092288363723797, 0.12410720035658715, 0.8084206683382064, 0.440428099037452, 0.4289120249256223, 0.414873689003068, 0.5941737134780851, 0.7374239453216042, 0.35899742640831256, 0.6448839957171848, 0.03849365298298854, 0.8823938868179693, 0.01, 0.14476041571596338, 0.6409660995455775], [0.25665690805504165, 0.15249043664476944, 0.16236751266608976, 0.4265001744225025, 0.21346771795343467, 0.6572286098027628, 0.42913674081641284, 0.0997875457617928, 0.6236387719700915, 0.8403645926141383, 0.7478880477330395, 0.2809905921874036, 0.7679362340912018, 0.27764770874407796, 0.7862368458639332, 0.17275856540359863, 0.01, 0.3708858341025051], [0.4651716595084734, 0.01, 0.2960251818502768, 0.45313196327255834, 0.12439443355836607, 0.89461099689026, 0.5776253055971828, 0.211242499752554, 0.650027811428685, 0.99, 0.8969821136582709, 0.31375921901441367, 0.625767475784518, 0.22431565985370597, 0.804791717543557, 0.2630201826624201, 0.07876540699638379, 0.567817471287007], [0.30204631019587164, 0.14031390355650966, 0.4405400824635944, 0.7552727758143198, 0.39491894138905526, 0.951569457936833, 0.3482367319774378, 0.2853511324116203, 0.6652565638623009, 0.5836568917525612, 0.7205081231139946, 0.48308668317811937, 0.69713228314352, 0.3939967882509099, 0.7246909096968962, 0.05002839784100143, 0.026504030101912095, 0.5791517299404727]]
25 	8     	0.792188	0.00255792	0.7875 	0.79625
params:  [0.48726590327342534, 0.01, 0.192526528647816, 0.5831201229124009, 0.13868505270511916, 0.7006540825216572, 0.34114692389620294, 0.6601631612411848, 0.8726113173828463, 0.99, 0.7718429311488885, 0.22428776706404835, 0.5354016841975763, 0.2081890969105796, 0.74009023648854, 0.1971962148166988, 0.297633148771304, 0.6999891698436899]
Training loss = 0.014962573647499084
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79375
Training loss = 0.01670743356148402
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01502748926480611
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.015392110645771027
step = 3, Training Accuracy: 0.76
Training loss = 0.0161867822209994
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015999804238478344
step = 5, Training Accuracy: 0.79
Training loss = 0.015669127305348713
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.017398151655991873
step = 7, Training Accuracy: 0.78
Training loss = 0.01682441622018814
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01585040936867396
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.01496195117632548
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.01380283311009407
step = 11, Training Accuracy: 0.8
Training loss = 0.015622303585211435
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015551288624604543
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.01513579159975052
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.79
params:  [0.303931407752608, 0.01, 0.421099474500656, 0.32259767924305816, 0.32745406218901685, 0.9210206970517404, 0.38590906320534507, 0.38748990068350536, 0.6579245687644353, 0.7633115275465174, 0.8521416149790585, 0.14967070469663948, 0.6951805525864707, 0.0993226003172839, 0.84602815886743, 0.04018787170488525, 0.0920413877500494, 0.4720917413329564]
Training loss = 0.01651559333006541
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.79125
Training loss = 0.014576587975025177
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.015356396138668061
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.015151204218467077
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.015612316230932871
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015427432258923849
step = 5, Training Accuracy: 0.79
Training loss = 0.015666745007038116
step = 6, Training Accuracy: 0.78
Training loss = 0.015608716309070587
step = 7, Training Accuracy: 0.79
Training loss = 0.015223095417022705
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015587236483891804
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.0157366344332695
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.016116138497988382
step = 11, Training Accuracy: 0.78
Training loss = 0.014419540266195933
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.01452493538459142
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.016475195785363515
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7975
params:  [0.5862315053364926, 0.19403808194480276, 0.3503262890786891, 0.5444893828889595, 0.33691328376564106, 0.5640765322481034, 0.44511383048573067, 0.48397401717636057, 0.47379200228873986, 0.5713898064445859, 0.6440288902278485, 0.09882321066640584, 0.36726942060764456, 0.19657405068570852, 0.6688531411403436, 0.01, 0.16079840334108356, 0.5491892959479674]
Training loss = 0.018679183622201282
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7975
Training loss = 0.017219888567924498
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.017270686129728954
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01772402713696162
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.017655822436014813
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01801474322875341
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.016590295433998106
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.016157264510790508
step = 7, Training Accuracy: 0.79
Training loss = 0.017080275615056355
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.017019385397434236
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016961480379104613
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.018425535758336387
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.016300185720125836
step = 12, Training Accuracy: 0.81
Training loss = 0.018139464259147645
step = 13, Training Accuracy: 0.79
Training loss = 0.019117765227953592
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7925
params:  [0.6046873476577949, 0.01, 0.3540815579896862, 0.49599849565226617, 0.09687746483084554, 0.657437974733423, 0.5664197181367672, 0.2079110272764994, 0.830836524705882, 0.8860237131383741, 0.6824983305950928, 0.2174569069166537, 0.4467208448162756, 0.029183856563545836, 0.7765035692047304, 0.01, 0.3744098510588023, 0.7110976180174464]
Training loss = 0.019476744333902996
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.8
Training loss = 0.01928060581286748
step = 1, Training Accuracy: 0.73
Training loss = 0.020258610745271046
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.018560116589069368
step = 3, Training Accuracy: 0.72
Training loss = 0.020184050103028615
step = 4, Training Accuracy: 0.7
Training loss = 0.018966735303401948
step = 5, Training Accuracy: 0.72
Training loss = 0.01816103368997574
step = 6, Training Accuracy: 0.75
Training loss = 0.019225427110989887
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.017384229997793835
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.018712854484717052
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.018090912302335102
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.019404292106628418
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.01831530431906382
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.017869833409786224
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01791411449511846
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.79125
params:  [0.6270020338121947, 0.14386985565408786, 0.23876130204850823, 0.6694715739097152, 0.34944677594862167, 0.7999340742395687, 0.4459571067253936, 0.528704041411836, 0.804579076361793, 0.8576358651821299, 0.6634005805190911, 0.3176697475223431, 0.7345834240942634, 0.01, 0.6490415118140003, 0.14109733052062962, 0.02079955995808508, 0.5566005039021841]
Training loss = 0.015709419548511506
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.7925
Training loss = 0.017300014396508533
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016005455255508422
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.016995804607868193
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.01741586705048879
step = 4, Training Accuracy: 0.78
Training loss = 0.014545133809248606
step = 5, Training Accuracy: 0.81
Training loss = 0.015732867817083995
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015286528666814169
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.01518398274978002
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.015496340294679006
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015189061711231867
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.015842514932155608
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.015622900029023489
step = 12, Training Accuracy: 0.8
Training loss = 0.015964668889840442
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.01491284062465032
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.795
params:  [0.5909964152618048, 0.06005588822749315, 0.2102075203304699, 0.5884784578368905, 0.20240557311140045, 0.8387807947320598, 0.6156943452392365, 0.20984324193504067, 0.5907210222092902, 0.9216988950546339, 0.8914617715911819, 0.16427829062296312, 0.5791924868840144, 0.13942548668275112, 0.8743489659209821, 0.09208048070017995, 0.2854801386659195, 0.7423944479347925]
Training loss = 0.01708580960830053
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7925
Training loss = 0.015333419938882192
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01719398081302643
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018365364174048104
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01521255557735761
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.014928046961625417
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.018391444583733877
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.017083608855803806
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01807271291812261
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.01812065621217092
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.017291841407616932
step = 10, Training Accuracy: 0.79
Training loss = 0.017670566936333974
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.018339996337890626
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.015661233365535737
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.0162163320183754
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.795
params:  [0.5456151128921058, 0.07680043101480291, 0.01, 0.5989893464264804, 0.01379238000254443, 0.9376335117300634, 0.5323343562556337, 0.5853898817596838, 0.5313715875509688, 0.8727510977021757, 0.7695576384038364, 0.33166652181377193, 0.653561210353451, 0.01, 0.7404649175332716, 0.17256916509387776, 0.15465804059315988, 0.42990493242746264]
Training loss = 0.015324552754561106
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79125
Training loss = 0.01737263818581899
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.016326800684134165
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.017688836654027304
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.014729466984669367
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.015688324471314748
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.016301097969214123
step = 6, Training Accuracy: 0.79
Training loss = 0.017317362626393638
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016037154297033947
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.017132349014282226
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.016521614491939545
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.01572496215502421
step = 11, Training Accuracy: 0.8
Training loss = 0.01924393723408381
step = 12, Training Accuracy: 0.77
Training loss = 0.01759765326976776
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.017377447883288065
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.78875
params:  [0.36663547547070197, 0.01, 0.36069206633655393, 0.5339422748657258, 0.3426717480483545, 0.8253591257968065, 0.5375254250671055, 0.3086894322070994, 0.5533438886435013, 0.9022895576481682, 0.655818891090512, 0.41178493064877875, 0.6065945993082645, 0.08220570025110692, 0.716555354864012, 0.17689218271830956, 0.01, 0.5721927827428724]
Training loss = 0.014359942773977915
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79375
Training loss = 0.01473877360423406
step = 1, Training Accuracy: 0.8
Training loss = 0.01593228538831075
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.01302886128425598
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.015228834052880605
step = 4, Training Accuracy: 0.8
Training loss = 0.013982195556163788
step = 5, Training Accuracy: 0.81
Training loss = 0.014100407014290491
step = 6, Training Accuracy: 0.82
Training loss = 0.014205173750718435
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.014884084661801656
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.014310935735702515
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014372614920139313
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.014469177722930908
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.015414191981156667
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.013947511812051137
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014155542552471161
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7975
[[0.48726590327342534, 0.01, 0.192526528647816, 0.5831201229124009, 0.13868505270511916, 0.7006540825216572, 0.34114692389620294, 0.6601631612411848, 0.8726113173828463, 0.99, 0.7718429311488885, 0.22428776706404835, 0.5354016841975763, 0.2081890969105796, 0.74009023648854, 0.1971962148166988, 0.297633148771304, 0.6999891698436899], [0.303931407752608, 0.01, 0.421099474500656, 0.32259767924305816, 0.32745406218901685, 0.9210206970517404, 0.38590906320534507, 0.38748990068350536, 0.6579245687644353, 0.7633115275465174, 0.8521416149790585, 0.14967070469663948, 0.6951805525864707, 0.0993226003172839, 0.84602815886743, 0.04018787170488525, 0.0920413877500494, 0.4720917413329564], [0.5862315053364926, 0.19403808194480276, 0.3503262890786891, 0.5444893828889595, 0.33691328376564106, 0.5640765322481034, 0.44511383048573067, 0.48397401717636057, 0.47379200228873986, 0.5713898064445859, 0.6440288902278485, 0.09882321066640584, 0.36726942060764456, 0.19657405068570852, 0.6688531411403436, 0.01, 0.16079840334108356, 0.5491892959479674], [0.6046873476577949, 0.01, 0.3540815579896862, 0.49599849565226617, 0.09687746483084554, 0.657437974733423, 0.5664197181367672, 0.2079110272764994, 0.830836524705882, 0.8860237131383741, 0.6824983305950928, 0.2174569069166537, 0.4467208448162756, 0.029183856563545836, 0.7765035692047304, 0.01, 0.3744098510588023, 0.7110976180174464], [0.6270020338121947, 0.14386985565408786, 0.23876130204850823, 0.6694715739097152, 0.34944677594862167, 0.7999340742395687, 0.4459571067253936, 0.528704041411836, 0.804579076361793, 0.8576358651821299, 0.6634005805190911, 0.3176697475223431, 0.7345834240942634, 0.01, 0.6490415118140003, 0.14109733052062962, 0.02079955995808508, 0.5566005039021841], [0.5909964152618048, 0.06005588822749315, 0.2102075203304699, 0.5884784578368905, 0.20240557311140045, 0.8387807947320598, 0.6156943452392365, 0.20984324193504067, 0.5907210222092902, 0.9216988950546339, 0.8914617715911819, 0.16427829062296312, 0.5791924868840144, 0.13942548668275112, 0.8743489659209821, 0.09208048070017995, 0.2854801386659195, 0.7423944479347925], [0.5456151128921058, 0.07680043101480291, 0.01, 0.5989893464264804, 0.01379238000254443, 0.9376335117300634, 0.5323343562556337, 0.5853898817596838, 0.5313715875509688, 0.8727510977021757, 0.7695576384038364, 0.33166652181377193, 0.653561210353451, 0.01, 0.7404649175332716, 0.17256916509387776, 0.15465804059315988, 0.42990493242746264], [0.36663547547070197, 0.01, 0.36069206633655393, 0.5339422748657258, 0.3426717480483545, 0.8253591257968065, 0.5375254250671055, 0.3086894322070994, 0.5533438886435013, 0.9022895576481682, 0.655818891090512, 0.41178493064877875, 0.6065945993082645, 0.08220570025110692, 0.716555354864012, 0.17689218271830956, 0.01, 0.5721927827428724]]
26 	8     	0.793438	0.00310934	0.78875	0.7975 
params:  [0.3381836741235338, 0.01, 0.253613951251856, 0.5385458852959936, 0.5184793860425705, 0.8491315428622582, 0.40057008292193375, 0.5493400964247447, 0.36589744542638863, 0.99, 0.753397622775752, 0.10238280660280827, 0.5865363279622813, 0.2631623962271652, 0.8460246908785529, 0.01, 0.060910773463184936, 0.4910579054672687]
Training loss = 0.019356654187043507
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79875
Training loss = 0.017361432711283365
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.015045517832040787
step = 2, Training Accuracy: 0.8
Training loss = 0.01749076207478841
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01671186864376068
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.016098873217900593
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.01725419779618581
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.01691718488931656
step = 7, Training Accuracy: 0.79
Training loss = 0.015142319202423095
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.018136012951532998
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.016561475694179536
step = 10, Training Accuracy: 0.79
Training loss = 0.0176179305712382
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.016456157068411508
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.0181759641567866
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.017244205971558887
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79375
params:  [0.18980994626621506, 0.06402653353744826, 0.20746469500773154, 0.5392270340512682, 0.2068669022390514, 0.9056384965765149, 0.5733453153381614, 0.36549716270227167, 0.7484604883396635, 0.8222697699312442, 0.6121852958655235, 0.19455129892590142, 0.7410422798199316, 0.2072381728175519, 0.7785460033327403, 0.04720856852783321, 0.01, 0.42275671312725405]
Training loss = 0.018374222914377847
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79375
Training loss = 0.017649579346179962
step = 1, Training Accuracy: 0.75
Training loss = 0.01803957998752594
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01884065826733907
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.018727536002794903
step = 4, Training Accuracy: 0.75
Training loss = 0.016914442578951517
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.018123418887456257
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.015965280135472614
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.017158880233764648
step = 8, Training Accuracy: 0.75
Training loss = 0.019769371847311656
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.018421473801136016
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.018955608606338503
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.017833769818147025
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.01770875076452891
step = 13, Training Accuracy: 0.76
Training loss = 0.01791391024986903
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.7925
params:  [0.47988988027282375, 0.01, 0.06420308922480422, 0.6959699175533132, 0.42135616412821, 0.8698431887489827, 0.30386437145321954, 0.3930757555979412, 0.5553095087284693, 0.8484086055325153, 0.99, 0.22296180777733238, 0.5063711356952817, 0.04302182893022678, 0.99, 0.01, 0.01, 0.596274619835311]
Training loss = 0.016529366970062256
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7975
Training loss = 0.019483893613020577
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017718960444132487
step = 2, Training Accuracy: 0.78
Training loss = 0.017058631678422292
step = 3, Training Accuracy: 0.76
Training loss = 0.017547016541163127
step = 4, Training Accuracy: 0.76
Training loss = 0.018089536229769387
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.016395612756411236
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017042663991451264
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01832843542098999
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.018053882618745166
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01701644649108251
step = 10, Training Accuracy: 0.74
Training loss = 0.01640253355105718
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.017326975961526235
step = 12, Training Accuracy: 0.76
Training loss = 0.018869518240292867
step = 13, Training Accuracy: 0.75
Training loss = 0.017222729126612345
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.795
params:  [0.38589213303608133, 0.1376776521335986, 0.18642443155477378, 0.38278206096386735, 0.26702628782707155, 0.8164258580020516, 0.4250162666205029, 0.32160502684012165, 0.4002462477298494, 0.8061200247399163, 0.8883006096360547, 0.225167453493812, 0.8295754563549265, 0.05083945144936815, 0.6704595798386033, 0.04993637996607387, 0.15330619784104926, 0.4823603641350922]
Training loss = 0.015039419730504353
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7975
Training loss = 0.015519972145557403
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.017132498919963837
step = 2, Training Accuracy: 0.77
Training loss = 0.015424880981445312
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.016375903685887653
step = 4, Training Accuracy: 0.78
Training loss = 0.01604484558105469
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01595090369383494
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.016939520140488943
step = 7, Training Accuracy: 0.81
Training loss = 0.015786602795124054
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01637059191862742
step = 9, Training Accuracy: 0.79
Training loss = 0.01482337196667989
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016416202982266745
step = 11, Training Accuracy: 0.78
Training loss = 0.016905656059583028
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01625474085410436
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.018520665367444355
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7925
params:  [0.4475112127148366, 0.01203203139725385, 0.4231280432594598, 0.6293517725220725, 0.3698332371250828, 0.99, 0.6260561355965557, 0.29175128832984926, 0.7802242884706172, 0.7599555005394842, 0.6290782371157617, 0.42055571751748916, 0.6427162889970369, 0.01, 0.7912362800848883, 0.04606763961410599, 0.01, 0.564545784063485]
Training loss = 0.016225598653157553
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.78625
Training loss = 0.016263890812794366
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017312645415465035
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.01647277797261874
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016057284772396086
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.015539307097593944
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.01656360020240148
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.016534898777802787
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.016415730814139048
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01663353905081749
step = 9, Training Accuracy: 0.81
Training loss = 0.018554695149262748
step = 10, Training Accuracy: 0.76
Training loss = 0.017644447088241578
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01658109317223231
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017069036861260732
step = 13, Training Accuracy: 0.77
Training loss = 0.018663711150487265
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.785
params:  [0.38846319731904366, 0.03528928696565184, 0.5510928256418153, 0.5634005842419982, 0.49688041009381245, 0.8619641975888744, 0.2535652212112641, 0.5969477243112586, 0.49567932529749603, 0.8880276720792244, 0.5927617644617544, 0.3311708200111948, 0.6645568830738577, 0.2216117521661388, 0.7144759356900399, 0.27533081372625257, 0.027333931638336514, 0.6182641731352515]
Training loss = 0.016360143919785818
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.79375
Training loss = 0.017657449543476103
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.016135421693325044
step = 2, Training Accuracy: 0.78
Training loss = 0.01643660674492518
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01634965757528941
step = 4, Training Accuracy: 0.78
Training loss = 0.015527560139695803
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.01824014296134313
step = 6, Training Accuracy: 0.78
Training loss = 0.01847662796576818
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.016102902591228485
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.016911387840906778
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.017174801131089528
step = 10, Training Accuracy: 0.77
Training loss = 0.017160597344239553
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.01685833474000295
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017180247008800508
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.014772537648677826
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79375
params:  [0.24384504573825216, 0.0986973475667911, 0.44612881938128535, 0.4256066443384727, 0.42673707243968617, 0.8923459190098607, 0.6212563613056291, 0.5247884881829223, 0.7316338705298526, 0.9749694354720317, 0.7407729467251322, 0.4036987722116874, 0.7158022024617077, 0.09217749997062259, 0.6651702378895571, 0.2317940055225775, 0.16617948642095248, 0.4607147322695734]
Training loss = 0.01777114540338516
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.79375
Training loss = 0.01688460330168406
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.016986007591088613
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.017344818015893302
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.019002522925535836
step = 4, Training Accuracy: 0.78
Training loss = 0.017262900372346242
step = 5, Training Accuracy: 0.78
Training loss = 0.018300495942433676
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.018828596472740173
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.016792170306046805
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.017451231877009074
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.01706419636805852
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01664796143770218
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.017647082110246022
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.01788418213526408
step = 13, Training Accuracy: 0.79
Training loss = 0.01922687073548635
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79875
params:  [0.3134643678650344, 0.030619210553633765, 0.7782425717786734, 0.2204242546129979, 0.3325314665567296, 0.8454045722973219, 0.39232697516651643, 0.40096558454085723, 0.6898790733906626, 0.7154263235115044, 0.632852092524237, 0.31563029489418803, 0.537448090058974, 0.1316535210672475, 0.8543079691161073, 0.01, 0.304218580038793, 0.4393847496666655]
Training loss = 0.016060264507929484
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79875
Training loss = 0.01480778177579244
step = 1, Training Accuracy: 0.79
Training loss = 0.016287231743335726
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.01623782008886337
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01655244211355845
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.017277783552805584
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01600746363401413
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.016440013845761617
step = 7, Training Accuracy: 0.78
Training loss = 0.016704848011334737
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015616556505362193
step = 9, Training Accuracy: 0.78
Training loss = 0.015701182782649994
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.017004174093405405
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01677859862645467
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.016114768584569294
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.016737207969029746
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.8
[[0.3381836741235338, 0.01, 0.253613951251856, 0.5385458852959936, 0.5184793860425705, 0.8491315428622582, 0.40057008292193375, 0.5493400964247447, 0.36589744542638863, 0.99, 0.753397622775752, 0.10238280660280827, 0.5865363279622813, 0.2631623962271652, 0.8460246908785529, 0.01, 0.060910773463184936, 0.4910579054672687], [0.18980994626621506, 0.06402653353744826, 0.20746469500773154, 0.5392270340512682, 0.2068669022390514, 0.9056384965765149, 0.5733453153381614, 0.36549716270227167, 0.7484604883396635, 0.8222697699312442, 0.6121852958655235, 0.19455129892590142, 0.7410422798199316, 0.2072381728175519, 0.7785460033327403, 0.04720856852783321, 0.01, 0.42275671312725405], [0.47988988027282375, 0.01, 0.06420308922480422, 0.6959699175533132, 0.42135616412821, 0.8698431887489827, 0.30386437145321954, 0.3930757555979412, 0.5553095087284693, 0.8484086055325153, 0.99, 0.22296180777733238, 0.5063711356952817, 0.04302182893022678, 0.99, 0.01, 0.01, 0.596274619835311], [0.38589213303608133, 0.1376776521335986, 0.18642443155477378, 0.38278206096386735, 0.26702628782707155, 0.8164258580020516, 0.4250162666205029, 0.32160502684012165, 0.4002462477298494, 0.8061200247399163, 0.8883006096360547, 0.225167453493812, 0.8295754563549265, 0.05083945144936815, 0.6704595798386033, 0.04993637996607387, 0.15330619784104926, 0.4823603641350922], [0.4475112127148366, 0.01203203139725385, 0.4231280432594598, 0.6293517725220725, 0.3698332371250828, 0.99, 0.6260561355965557, 0.29175128832984926, 0.7802242884706172, 0.7599555005394842, 0.6290782371157617, 0.42055571751748916, 0.6427162889970369, 0.01, 0.7912362800848883, 0.04606763961410599, 0.01, 0.564545784063485], [0.38846319731904366, 0.03528928696565184, 0.5510928256418153, 0.5634005842419982, 0.49688041009381245, 0.8619641975888744, 0.2535652212112641, 0.5969477243112586, 0.49567932529749603, 0.8880276720792244, 0.5927617644617544, 0.3311708200111948, 0.6645568830738577, 0.2216117521661388, 0.7144759356900399, 0.27533081372625257, 0.027333931638336514, 0.6182641731352515], [0.24384504573825216, 0.0986973475667911, 0.44612881938128535, 0.4256066443384727, 0.42673707243968617, 0.8923459190098607, 0.6212563613056291, 0.5247884881829223, 0.7316338705298526, 0.9749694354720317, 0.7407729467251322, 0.4036987722116874, 0.7158022024617077, 0.09217749997062259, 0.6651702378895571, 0.2317940055225775, 0.16617948642095248, 0.4607147322695734], [0.3134643678650344, 0.030619210553633765, 0.7782425717786734, 0.2204242546129979, 0.3325314665567296, 0.8454045722973219, 0.39232697516651643, 0.40096558454085723, 0.6898790733906626, 0.7154263235115044, 0.632852092524237, 0.31563029489418803, 0.537448090058974, 0.1316535210672475, 0.8543079691161073, 0.01, 0.304218580038793, 0.4393847496666655]]
27 	8     	0.793906	0.00425907	0.785  	0.8    
params:  [0.49386744390034076, 0.01, 0.7882988539143281, 0.3854856009630941, 0.4905806303546185, 0.99, 0.39232025087196665, 0.6222917022100569, 0.8041914034242172, 0.831452184321161, 0.723344589665616, 0.27817336256146136, 0.6945794370618595, 0.07995460927008047, 0.7237707330193002, 0.06197643197245002, 0.1177062364689301, 0.5770408389464762]
Training loss = 0.014671066403388977
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.795
Training loss = 0.016258827845255532
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.015595603187878927
step = 2, Training Accuracy: 0.8
Training loss = 0.016396127939224243
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.01610190510749817
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.015456078052520751
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.015078963041305541
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01530539095401764
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.015066887438297271
step = 8, Training Accuracy: 0.79
Training loss = 0.015836369693279267
step = 9, Training Accuracy: 0.8
Training loss = 0.015229881505171457
step = 10, Training Accuracy: 0.81
Training loss = 0.01472497264544169
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.01605009913444519
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015380267103513081
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.01475264181693395
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.7975
params:  [0.349719467617707, 0.01, 0.4923444952156928, 0.3843619107273395, 0.4975100241264238, 0.8962208006550529, 0.4410999877811446, 0.44136195846098686, 0.6811282015235882, 0.99, 0.7571214027377043, 0.36187555368647917, 0.5562108112918166, 0.15619683588648697, 0.99, 0.10728025294553464, 0.33176045739074705, 0.4011938159400151]
Training loss = 0.01607548415660858
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.79375
Training loss = 0.015462452471256256
step = 1, Training Accuracy: 0.84
Training loss = 0.015372679034868875
step = 2, Training Accuracy: 0.82
Training loss = 0.016074786484241484
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.016192333300908406
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.0158462393283844
step = 5, Training Accuracy: 0.8
Training loss = 0.015694206555684407
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.015611897408962249
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.014561383624871572
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.016090371906757356
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016959451834360758
step = 10, Training Accuracy: 0.79
Training loss = 0.013882065216700237
step = 11, Training Accuracy: 0.82
Training loss = 0.015251653293768565
step = 12, Training Accuracy: 0.82
Training loss = 0.016459291179974873
step = 13, Training Accuracy: 0.8
Training loss = 0.015385435223579406
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79375
params:  [0.09553227198683686, 0.01, 0.5210741241715899, 0.48325493975447353, 0.4458772331319074, 0.7670754596750572, 0.4530252838970181, 0.44895145233266204, 0.6540045606864748, 0.7077059846008894, 0.5117526462466878, 0.1241923603563804, 0.6288343917428993, 0.09082235951404032, 0.8756673808731918, 0.01, 0.45947111130102714, 0.5195094033876433]
Training loss = 0.019570379257202147
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7975
Training loss = 0.018574794034163157
step = 1, Training Accuracy: 0.73
Training loss = 0.01961989124615987
step = 2, Training Accuracy: 0.74
Training loss = 0.018732241491476696
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.017503953377405804
step = 4, Training Accuracy: 0.76
Training loss = 0.01808242589235306
step = 5, Training Accuracy: 0.78
Training loss = 0.01800817201534907
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.018219380676746368
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.018598908185958864
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.017856738865375518
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.017845062017440797
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.018146004180113473
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.019429023762543997
step = 12, Training Accuracy: 0.7266666666666667
Training loss = 0.016202583114306134
step = 13, Training Accuracy: 0.76
Training loss = 0.018744492928187053
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.795
params:  [0.466061707721116, 0.11324849557815356, 0.48420966500527785, 0.3129242234899708, 0.44878748566937887, 0.7320154607718772, 0.5432214938286642, 0.627059380072288, 0.9291144403411347, 0.8680189853714944, 0.7121892417705556, 0.44037618466422235, 0.6251320040802888, 0.012748950646292653, 0.6999590303325833, 0.12760075924406322, 0.30925027023297863, 0.5077042706334831]
Training loss = 0.01852662126223246
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.795
Training loss = 0.016982626765966416
step = 1, Training Accuracy: 0.79
Training loss = 0.015793957809607188
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.01796798805395762
step = 3, Training Accuracy: 0.78
Training loss = 0.017842340767383575
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01732836663722992
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.016822014451026917
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016465833882490795
step = 7, Training Accuracy: 0.78
Training loss = 0.016453859806060792
step = 8, Training Accuracy: 0.81
Training loss = 0.01603442450364431
step = 9, Training Accuracy: 0.78
Training loss = 0.017547791798909505
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.016170462171236674
step = 11, Training Accuracy: 0.76
Training loss = 0.016055230597654978
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.016358810762564343
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.017675090630849204
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7875
params:  [0.472606474020845, 0.03057887231851184, 0.5206309004713692, 0.31734176118758467, 0.3907405717204615, 0.7125449218742552, 0.393266008223666, 0.4523185937741365, 0.7760412812809672, 0.8490277546109143, 0.8120525583299139, 0.29936616316588616, 0.3911410985462088, 0.01, 0.8122042867013821, 0.02857067342751988, 0.2602734857414461, 0.4526454301969627]
Training loss = 0.015935103197892507
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.79875
Training loss = 0.015358802179495494
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.015282251536846161
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.015326211452484131
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.014877779086430868
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015985515316327414
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.015900061229864756
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.014600289265314738
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.016276550988356273
step = 8, Training Accuracy: 0.79
Training loss = 0.015125905871391296
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.015749630232652027
step = 10, Training Accuracy: 0.79
Training loss = 0.015019452373186748
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015893950859705606
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.015798661510149636
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.014962842961152394
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.79125
params:  [0.14453221156260893, 0.1288770482767584, 0.6259096043459668, 0.4962633789303076, 0.31412187318837104, 0.8955334231979858, 0.4210672810413345, 0.5774152198901903, 0.7449178383671913, 0.7605292725272392, 0.4821759067967567, 0.3039765077842985, 0.6878083455089058, 0.01, 0.8864055527800749, 0.11489162208870786, 0.3269817150396753, 0.3813277059922038]
Training loss = 0.016005512475967407
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79125
Training loss = 0.017525368531545002
step = 1, Training Accuracy: 0.76
Training loss = 0.015488014866908392
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015992615620295206
step = 3, Training Accuracy: 0.79
Training loss = 0.019086122314135232
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.017521249850591024
step = 5, Training Accuracy: 0.73
Training loss = 0.0157343993584315
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.017026053468386333
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.015422549893458684
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016255582869052886
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.016919214129447937
step = 10, Training Accuracy: 0.76
Training loss = 0.016709040999412537
step = 11, Training Accuracy: 0.76
Training loss = 0.01748421182235082
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.016013456682364147
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.016354309221108754
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7875
params:  [0.3720254344319692, 0.01, 0.6277700786807147, 0.39921826072411837, 0.3547256421866181, 0.8695083050490684, 0.42677722012492053, 0.205873020253016, 0.4697106433543953, 0.99, 0.936251204365694, 0.3065457228185037, 0.7160876345073477, 0.17577402730645844, 0.6646688259192217, 0.11403154588490881, 0.1143671932960177, 0.3087560331584401]
Training loss = 0.017917256752649942
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.78875
Training loss = 0.018087774713834128
step = 1, Training Accuracy: 0.76
Training loss = 0.01755806157986323
step = 2, Training Accuracy: 0.78
Training loss = 0.016354862054189047
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.016738078892230987
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.017116919457912445
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.018278319040934243
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01691187928120295
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016409364938735963
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.017413142224152883
step = 9, Training Accuracy: 0.8
Training loss = 0.017268520494302112
step = 10, Training Accuracy: 0.7366666666666667
Training loss = 0.017458466192086537
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01721561958392461
step = 12, Training Accuracy: 0.76
Training loss = 0.01749191294113795
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.017052344679832458
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.785
params:  [0.10636077892830509, 0.01, 0.6046901291851987, 0.3057124678880086, 0.46961671711188124, 0.99, 0.5055633946178981, 0.7422792895486923, 0.8436368621671433, 0.7979445296793052, 0.7728990271137165, 0.38208216394053823, 0.7484007702473074, 0.24220071202785387, 0.99, 0.14151300255676272, 0.39218971081877735, 0.19414796260074013]
Training loss = 0.0187957368294398
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.78625
Training loss = 0.017643675804138184
step = 1, Training Accuracy: 0.77
Training loss = 0.016909161905447643
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.017714885274569193
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017611243029435477
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01707637369632721
step = 5, Training Accuracy: 0.75
Training loss = 0.018567118545373282
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.01841462790966034
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.016854981978734335
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.01737584968407949
step = 9, Training Accuracy: 0.76
Training loss = 0.0172858460744222
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.016753758192062377
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01856151521205902
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.015922287503878276
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.018189273178577423
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.785
[[0.49386744390034076, 0.01, 0.7882988539143281, 0.3854856009630941, 0.4905806303546185, 0.99, 0.39232025087196665, 0.6222917022100569, 0.8041914034242172, 0.831452184321161, 0.723344589665616, 0.27817336256146136, 0.6945794370618595, 0.07995460927008047, 0.7237707330193002, 0.06197643197245002, 0.1177062364689301, 0.5770408389464762], [0.349719467617707, 0.01, 0.4923444952156928, 0.3843619107273395, 0.4975100241264238, 0.8962208006550529, 0.4410999877811446, 0.44136195846098686, 0.6811282015235882, 0.99, 0.7571214027377043, 0.36187555368647917, 0.5562108112918166, 0.15619683588648697, 0.99, 0.10728025294553464, 0.33176045739074705, 0.4011938159400151], [0.09553227198683686, 0.01, 0.5210741241715899, 0.48325493975447353, 0.4458772331319074, 0.7670754596750572, 0.4530252838970181, 0.44895145233266204, 0.6540045606864748, 0.7077059846008894, 0.5117526462466878, 0.1241923603563804, 0.6288343917428993, 0.09082235951404032, 0.8756673808731918, 0.01, 0.45947111130102714, 0.5195094033876433], [0.466061707721116, 0.11324849557815356, 0.48420966500527785, 0.3129242234899708, 0.44878748566937887, 0.7320154607718772, 0.5432214938286642, 0.627059380072288, 0.9291144403411347, 0.8680189853714944, 0.7121892417705556, 0.44037618466422235, 0.6251320040802888, 0.012748950646292653, 0.6999590303325833, 0.12760075924406322, 0.30925027023297863, 0.5077042706334831], [0.472606474020845, 0.03057887231851184, 0.5206309004713692, 0.31734176118758467, 0.3907405717204615, 0.7125449218742552, 0.393266008223666, 0.4523185937741365, 0.7760412812809672, 0.8490277546109143, 0.8120525583299139, 0.29936616316588616, 0.3911410985462088, 0.01, 0.8122042867013821, 0.02857067342751988, 0.2602734857414461, 0.4526454301969627], [0.14453221156260893, 0.1288770482767584, 0.6259096043459668, 0.4962633789303076, 0.31412187318837104, 0.8955334231979858, 0.4210672810413345, 0.5774152198901903, 0.7449178383671913, 0.7605292725272392, 0.4821759067967567, 0.3039765077842985, 0.6878083455089058, 0.01, 0.8864055527800749, 0.11489162208870786, 0.3269817150396753, 0.3813277059922038], [0.3720254344319692, 0.01, 0.6277700786807147, 0.39921826072411837, 0.3547256421866181, 0.8695083050490684, 0.42677722012492053, 0.205873020253016, 0.4697106433543953, 0.99, 0.936251204365694, 0.3065457228185037, 0.7160876345073477, 0.17577402730645844, 0.6646688259192217, 0.11403154588490881, 0.1143671932960177, 0.3087560331584401], [0.10636077892830509, 0.01, 0.6046901291851987, 0.3057124678880086, 0.46961671711188124, 0.99, 0.5055633946178981, 0.7422792895486923, 0.8436368621671433, 0.7979445296793052, 0.7728990271137165, 0.38208216394053823, 0.7484007702473074, 0.24220071202785387, 0.99, 0.14151300255676272, 0.39218971081877735, 0.19414796260074013]]
28 	8     	0.790312	0.00445244	0.785  	0.7975 
params:  [0.3299713988845219, 0.01, 0.7008821458600217, 0.4332148437908956, 0.27576186396543534, 0.9189045057669253, 0.5961234725242723, 0.5262491031193504, 0.5343312470454789, 0.9369546290940697, 0.7558439713977769, 0.2925999711823133, 0.6166843681901648, 0.03561102558540115, 0.6866256266461868, 0.01, 0.36764972361358017, 0.6490332156611376]
Training loss = 0.015435938040415447
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.79375
Training loss = 0.01694796810547511
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01785430630048116
step = 2, Training Accuracy: 0.79
Training loss = 0.015653054416179656
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017636965612570443
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.01765760362148285
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.01815664251645406
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017966574331124623
step = 7, Training Accuracy: 0.78
Training loss = 0.015145182112852732
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01606889655192693
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01851463089386622
step = 10, Training Accuracy: 0.76
Training loss = 0.01695902367432912
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.016981481512387594
step = 12, Training Accuracy: 0.79
Training loss = 0.016290133545796077
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.01556000938018163
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
params:  [0.4778516414638606, 0.01, 0.6319402454139125, 0.3892392878487844, 0.4984976511673925, 0.99, 0.42535014592587383, 0.4553269857379874, 0.7233919692256274, 0.8282391167726011, 0.8385984134419122, 0.19209338061487735, 0.5293526484413388, 0.09349652269558621, 0.8776897538737026, 0.01, 0.32064050975077596, 0.5832012440319011]
Training loss = 0.014545782307783762
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7875
Training loss = 0.015903086165587107
step = 1, Training Accuracy: 0.79
Training loss = 0.016187081336975096
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01670579393704732
step = 3, Training Accuracy: 0.8
Training loss = 0.015175495942433675
step = 4, Training Accuracy: 0.8
Training loss = 0.016707056760787965
step = 5, Training Accuracy: 0.78
Training loss = 0.015723395546277365
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.01810201535622279
step = 7, Training Accuracy: 0.76
Training loss = 0.01606012632449468
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.01656388203303019
step = 9, Training Accuracy: 0.79
Training loss = 0.01570842559138934
step = 10, Training Accuracy: 0.81
Training loss = 0.015342502892017364
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017202827433745068
step = 12, Training Accuracy: 0.79
Training loss = 0.015376433779795964
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.015745583971341452
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.79
params:  [0.22042758334923618, 0.01, 0.5791949545515827, 0.5050926105568834, 0.4148719567208694, 0.9104557100338351, 0.3066420556208992, 0.5386859326053597, 0.6965857700186251, 0.9686502865364955, 0.6852839633562651, 0.1952545699460751, 0.5331499902290804, 0.07944368234242162, 0.9504355170066462, 0.08205083635741053, 0.1727998599588797, 0.4452183071369564]
Training loss = 0.01565839409828186
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.79
Training loss = 0.014757125427325567
step = 1, Training Accuracy: 0.79
Training loss = 0.01618005712827047
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014755425949891408
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.015332035919030507
step = 4, Training Accuracy: 0.79
Training loss = 0.016758931477864582
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.015672883093357085
step = 6, Training Accuracy: 0.8
Training loss = 0.014852989117304484
step = 7, Training Accuracy: 0.79
Training loss = 0.015311537583669026
step = 8, Training Accuracy: 0.8
Training loss = 0.014079782664775848
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.016318192581335704
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.014241859217484791
step = 11, Training Accuracy: 0.81
Training loss = 0.015015510121981303
step = 12, Training Accuracy: 0.8
Training loss = 0.015502075751622518
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.015238024493058522
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.7925
params:  [0.3315344426185061, 0.01, 0.6042902257929943, 0.6398617097012271, 0.6760945526710833, 0.99, 0.3070917447406032, 0.6403452875294829, 0.835424465070682, 0.8668859888424573, 0.491195024510103, 0.2860367573665448, 0.5324560848460586, 0.1504900731701191, 0.8727462871677614, 0.1842707780538263, 0.4654049673236072, 0.5198900623583145]
Training loss = 0.015794072449207306
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78875
Training loss = 0.015837384363015492
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.016623372832934063
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015965631008148195
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01557510753472646
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016215452949206035
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.016165346205234528
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.01567957818508148
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016072544505198797
step = 8, Training Accuracy: 0.78
Training loss = 0.016002208590507508
step = 9, Training Accuracy: 0.8
Training loss = 0.015413340131441753
step = 10, Training Accuracy: 0.82
Training loss = 0.016461634039878846
step = 11, Training Accuracy: 0.79
Training loss = 0.017271024882793428
step = 12, Training Accuracy: 0.77
Training loss = 0.01663503274321556
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.016699071526527404
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7925
params:  [0.2797354773208183, 0.01, 0.41242213776398234, 0.4910032177362846, 0.5216509554848578, 0.9817272950795132, 0.5744010724593274, 0.5533149892256496, 0.6869505921753062, 0.9799699212572035, 0.664081345311729, 0.09398741907021985, 0.36329400802164485, 0.052161107538301596, 0.6514377283577886, 0.20480770934334985, 0.24562464951418053, 0.41990947190080086]
Training loss = 0.016769818166891735
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.79625
Training loss = 0.015950300792853037
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.017410246431827547
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.019694752196470898
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017758369743824005
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.018308249016602832
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.01725779096285502
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.017060211698214214
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.017710415919621785
step = 8, Training Accuracy: 0.75
Training loss = 0.017954457104206085
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01761944572130839
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017442163030306497
step = 11, Training Accuracy: 0.78
Training loss = 0.01706578900416692
step = 12, Training Accuracy: 0.74
Training loss = 0.01744771361351013
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.017642068068186443
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.795
params:  [0.524990362640129, 0.01, 0.8069715364116314, 0.5153267636032566, 0.5818512568853689, 0.9491151817871117, 0.32451592457122314, 0.6918446433331338, 0.99, 0.6834093484968745, 0.885683827175551, 0.1814307197147247, 0.4643288556031159, 0.1354930168736686, 0.5993867013950702, 0.040000909731864376, 0.2231687898646342, 0.449389898377938]
Training loss = 0.016019876599311828
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.79
Training loss = 0.015033283829689026
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.014861120680967966
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.015562146504720053
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015196331938107809
step = 4, Training Accuracy: 0.8
Training loss = 0.015959436694780987
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01533869614203771
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015467802633841832
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.015004008313020071
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.016949890951315563
step = 9, Training Accuracy: 0.77
Training loss = 0.015828138788541158
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01441694051027298
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.015228591859340668
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.01553983857234319
step = 13, Training Accuracy: 0.78
Training loss = 0.014826107323169708
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.79375
params:  [0.4587601510543098, 0.15971078040784478, 0.5896741303817568, 0.23674391276146864, 0.5807522280629233, 0.8485491322616213, 0.5663840126910606, 0.6092971122940337, 0.7261512427798935, 0.6605563949319947, 0.4529205719252595, 0.22915462255429203, 0.7243465630974496, 0.11065141214002722, 0.8064068896462265, 0.06179802283578875, 0.12477615154249522, 0.28025414109305574]
Training loss = 0.016630265414714813
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7925
Training loss = 0.01720325420300166
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015619391202926635
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.01653303821881612
step = 3, Training Accuracy: 0.79
Training loss = 0.015600298444430033
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01639351785182953
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.01760595550139745
step = 6, Training Accuracy: 0.8
Training loss = 0.018153703212738036
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.016185412406921385
step = 8, Training Accuracy: 0.8
Training loss = 0.017137552400430044
step = 9, Training Accuracy: 0.82
Training loss = 0.016632841577132542
step = 10, Training Accuracy: 0.81
Training loss = 0.017613076269626618
step = 11, Training Accuracy: 0.78
Training loss = 0.0162573903799057
step = 12, Training Accuracy: 0.78
Training loss = 0.016992986301581064
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.016182776192824044
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78625
params:  [0.5711229233278683, 0.17544283526911497, 0.7179678576989372, 0.5587774368518823, 0.357982660228169, 0.99, 0.2773256918209598, 0.6858865526867404, 0.7386794076392189, 0.7471901980256209, 0.4434611543734658, 0.19155004174896761, 0.7221237619377842, 0.1657060636326672, 0.8366443551828939, 0.1379864145460702, 0.4692072188635089, 0.6602190751151892]
Training loss = 0.01750438302755356
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.78875
Training loss = 0.01732179115215937
step = 1, Training Accuracy: 0.76
Training loss = 0.016054571668306986
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.016786306301752728
step = 3, Training Accuracy: 0.78
Training loss = 0.014611027936140697
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.017932897011439006
step = 5, Training Accuracy: 0.75
Training loss = 0.01749433626731237
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.01636347105105718
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.015166465640068055
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.01647047946850459
step = 9, Training Accuracy: 0.77
Training loss = 0.01612349917491277
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.015569620231787363
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.017177285651365917
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.018386247257391613
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.016557721495628355
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7925
[[0.3299713988845219, 0.01, 0.7008821458600217, 0.4332148437908956, 0.27576186396543534, 0.9189045057669253, 0.5961234725242723, 0.5262491031193504, 0.5343312470454789, 0.9369546290940697, 0.7558439713977769, 0.2925999711823133, 0.6166843681901648, 0.03561102558540115, 0.6866256266461868, 0.01, 0.36764972361358017, 0.6490332156611376], [0.4778516414638606, 0.01, 0.6319402454139125, 0.3892392878487844, 0.4984976511673925, 0.99, 0.42535014592587383, 0.4553269857379874, 0.7233919692256274, 0.8282391167726011, 0.8385984134419122, 0.19209338061487735, 0.5293526484413388, 0.09349652269558621, 0.8776897538737026, 0.01, 0.32064050975077596, 0.5832012440319011], [0.22042758334923618, 0.01, 0.5791949545515827, 0.5050926105568834, 0.4148719567208694, 0.9104557100338351, 0.3066420556208992, 0.5386859326053597, 0.6965857700186251, 0.9686502865364955, 0.6852839633562651, 0.1952545699460751, 0.5331499902290804, 0.07944368234242162, 0.9504355170066462, 0.08205083635741053, 0.1727998599588797, 0.4452183071369564], [0.3315344426185061, 0.01, 0.6042902257929943, 0.6398617097012271, 0.6760945526710833, 0.99, 0.3070917447406032, 0.6403452875294829, 0.835424465070682, 0.8668859888424573, 0.491195024510103, 0.2860367573665448, 0.5324560848460586, 0.1504900731701191, 0.8727462871677614, 0.1842707780538263, 0.4654049673236072, 0.5198900623583145], [0.2797354773208183, 0.01, 0.41242213776398234, 0.4910032177362846, 0.5216509554848578, 0.9817272950795132, 0.5744010724593274, 0.5533149892256496, 0.6869505921753062, 0.9799699212572035, 0.664081345311729, 0.09398741907021985, 0.36329400802164485, 0.052161107538301596, 0.6514377283577886, 0.20480770934334985, 0.24562464951418053, 0.41990947190080086], [0.524990362640129, 0.01, 0.8069715364116314, 0.5153267636032566, 0.5818512568853689, 0.9491151817871117, 0.32451592457122314, 0.6918446433331338, 0.99, 0.6834093484968745, 0.885683827175551, 0.1814307197147247, 0.4643288556031159, 0.1354930168736686, 0.5993867013950702, 0.040000909731864376, 0.2231687898646342, 0.449389898377938], [0.4587601510543098, 0.15971078040784478, 0.5896741303817568, 0.23674391276146864, 0.5807522280629233, 0.8485491322616213, 0.5663840126910606, 0.6092971122940337, 0.7261512427798935, 0.6605563949319947, 0.4529205719252595, 0.22915462255429203, 0.7243465630974496, 0.11065141214002722, 0.8064068896462265, 0.06179802283578875, 0.12477615154249522, 0.28025414109305574], [0.5711229233278683, 0.17544283526911497, 0.7179678576989372, 0.5587774368518823, 0.357982660228169, 0.99, 0.2773256918209598, 0.6858865526867404, 0.7386794076392189, 0.7471901980256209, 0.4434611543734658, 0.19155004174896761, 0.7221237619377842, 0.1657060636326672, 0.8366443551828939, 0.1379864145460702, 0.4692072188635089, 0.6602190751151892]]
29 	8     	0.791406	0.00268368	0.78625	0.795  
hof:  [0.5353033095050068, 0.043746958676444875, 0.2795225890768406, 0.43508087925787475, 0.36616149836212464, 0.7873030875584444, 0.2121421393105074, 0.36117326697126245, 0.7406028456960619, 0.9185499393705596, 0.8365141270623834, 0.3082614196942166, 0.8580511190380584, 0.1790805591275004, 0.6839848682402356, 0.0632730442967086, 0.01, 0.6254884125174087]
new pl params Compose([
  Resize(always_apply=False, p=1, height=256, width=256, interpolation=1),
  MotionBlur(always_apply=False, p=0.5353033095050068, blur_limit=(3, 7)),
  Transpose(always_apply=False, p=0.2795225890768406),
  Flip(always_apply=False, p=0.43508087925787475),
  RandomRotate90(always_apply=False, p=0.36616149836212464),
  OneOf([
    InvertImg(always_apply=False, p=0.2121421393105074),
    Posterize(always_apply=False, p=0.36117326697126245, num_bits=(5, 5)),
    CLAHE(always_apply=False, p=0.9185499393705596, clip_limit=(1, 6.692930445863755), tile_grid_size=(9, 9)),
    Equalize(always_apply=False, p=0.8580511190380584, mode='cv', by_channels=True),
    ISONoise(always_apply=False, p=0.1790805591275004, intensity=(0.01, 0.6254884125174087), color_shift=(0.0632730442967086, 0.6839848682402356)),
  ], p=0.7873030875584444),
  Resize(always_apply=False, p=1, height=128, width=128, interpolation=1),
  ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None),
], p=1, bbox_params=None, keypoint_params=None, additional_targets={})
individual:  [0.5353033095050068, 0.043746958676444875, 0.2795225890768406, 0.43508087925787475, 0.36616149836212464, 0.7873030875584444, 0.2121421393105074, 0.36117326697126245, 0.7406028456960619, 0.9185499393705596, 0.8365141270623834, 0.3082614196942166, 0.8580511190380584, 0.1790805591275004, 0.6839848682402356, 0.0632730442967086, 0.01, 0.6254884125174087]  value:  (0.805,)
gen	nevals	avg     	std       	min    	max    
0  	8     	0.552969	0.0274142 	0.52875	0.61375
1  	8     	0.683906	0.0358787 	0.60375	0.725  
2  	8     	0.710312	0.0153698 	0.695  	0.73625
3  	8     	0.730156	0.0182478 	0.70375	0.7625 
4  	8     	0.742813	0.0128353 	0.72375	0.76375
5  	8     	0.751875	0.0141973 	0.7275 	0.7725 
6  	8     	0.762969	0.0114554 	0.74625	0.78125
7  	8     	0.769375	0.0104396 	0.74375	0.77875
8  	8     	0.760625	0.0107347 	0.73375	0.77   
9  	8     	0.764375	0.0104021 	0.74875	0.7775 
10 	8     	0.775156	0.00377272	0.77   	0.78125
11 	8     	0.782656	0.00738023	0.77   	0.7925 
12 	8     	0.779375	0.00707107	0.76875	0.79   
13 	8     	0.782656	0.00528254	0.77625	0.79375
14 	8     	0.780469	0.00599275	0.77   	0.78875
15 	8     	0.78625 	0.00484123	0.78   	0.795  
16 	8     	0.789062	0.00779999	0.7725 	0.8    
17 	8     	0.78375 	0.00511585	0.77625	0.79125
18 	8     	0.790312	0.00645749	0.7775 	0.79875
19 	8     	0.794375	0.00441942	0.78625	0.80125
20 	8     	0.791562	0.00255792	0.7875 	0.79625
21 	8     	0.787187	0.00568544	0.77875	0.79625
22 	8     	0.791406	0.00382414	0.785  	0.795  
23 	8     	0.795312	0.005511  	0.78875	0.805  
24 	8     	0.792656	0.00321359	0.7875 	0.79625
25 	8     	0.792188	0.00255792	0.7875 	0.79625
26 	8     	0.793438	0.00310934	0.78875	0.7975 
27 	8     	0.793906	0.00425907	0.785  	0.8    
28 	8     	0.790312	0.00445244	0.785  	0.7975 
29 	8     	0.791406	0.00268368	0.78625	0.795  
