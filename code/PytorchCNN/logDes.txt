(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.2
(0, 1) <class 'int'> 1
(0.0, 1.0) <class 'float'> 0.5
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 30
(5, 50) <class 'int'> 20
(0.0, 1.0) <class 'float'> 0.5
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 20
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0, 5) <class 'float'> 1
(25, 75) <class 'float'> 50
(25, 75) <class 'float'> 50
(0, 4) <class 'int'> 1
params:  [0.6490142459033698, 0.2322671098540393, 0.9396946306764662, 0.01, 0.01, 0.99, 0.1961506638996729, 0.16464707456104152, 0.03808020580164573, 0.01, 0.5725886814698102, 0.19361440726225626, 0.19430802548959464, 0.49610134640912273, 0.3591576842195144, 0.43023041874587264, 0.9737638446522174, 0.12975891291524586, 0.4297539875829992, 0.9569089569224076, 0.39430656143020776, 0.4585207096486446, 0.5202584614063771, 0.01]
Training loss = 0.045898592472076415
step = 0, Training Accuracy: 0.31
Validation Accuracy: 0.33
Training loss = 0.047466355164845785
step = 1, Training Accuracy: 0.3566666666666667
Training loss = 0.03912363827228546
step = 2, Training Accuracy: 0.39
Training loss = 0.03740453163782755
step = 3, Training Accuracy: 0.46
Training loss = 0.035157377521197
step = 4, Training Accuracy: 0.5033333333333333
Training loss = 0.036059194405873615
step = 5, Training Accuracy: 0.45
Training loss = 0.0339287132024765
step = 6, Training Accuracy: 0.49333333333333335
Training loss = 0.034715977907180784
step = 7, Training Accuracy: 0.44333333333333336
Training loss = 0.032524977723757426
step = 8, Training Accuracy: 0.5033333333333333
Training loss = 0.03298800488313039
step = 9, Training Accuracy: 0.5166666666666667
Training loss = 0.0331840044260025
step = 10, Training Accuracy: 0.47
Training loss = 0.03353728850682577
step = 11, Training Accuracy: 0.49666666666666665
Training loss = 0.03148691594600678
step = 12, Training Accuracy: 0.53
Training loss = 0.033278128306070964
step = 13, Training Accuracy: 0.5066666666666667
Training loss = 0.03072515328725179
step = 14, Training Accuracy: 0.5266666666666666
Validation Accuracy: 0.50875
params:  [0.3366851826424452, 0.08404673748158739, 0.05644340288977179, 0.10966889132321338, 0.16530551528352785, 0.99, 0.7215399739986231, 0.39239170409407037, 0.15709974088602646, 0.01, 0.5626590785014266, 0.01, 0.58009680696429, 0.016020054646563198, 0.4959508325786198, 0.7556834553526812, 0.31948801633118096, 0.11249187506201698, 0.3198083930243585, 0.6127094055037016, 0.01, 0.5332767769129598, 0.3618083687120638, 0.5671366678656746]
Training loss = 0.03284296830495199
step = 0, Training Accuracy: 0.52
Validation Accuracy: 0.5175
Training loss = 0.03113898038864136
step = 1, Training Accuracy: 0.5733333333333334
Training loss = 0.030540722608566283
step = 2, Training Accuracy: 0.5733333333333334
Training loss = 0.03187391380469004
step = 3, Training Accuracy: 0.55
Training loss = 0.02948544462521871
step = 4, Training Accuracy: 0.65
Training loss = 0.028402098218599955
step = 5, Training Accuracy: 0.62
Training loss = 0.026982145309448244
step = 6, Training Accuracy: 0.65
Training loss = 0.02704208711783091
step = 7, Training Accuracy: 0.6766666666666666
Training loss = 0.026449689467748005
step = 8, Training Accuracy: 0.66
Training loss = 0.02755418340365092
step = 9, Training Accuracy: 0.67
Training loss = 0.024497809012730916
step = 10, Training Accuracy: 0.6866666666666666
Training loss = 0.027525234818458557
step = 11, Training Accuracy: 0.6466666666666666
Training loss = 0.027577664256095886
step = 12, Training Accuracy: 0.6366666666666667
Training loss = 0.02760821541150411
step = 13, Training Accuracy: 0.6566666666666666
Training loss = 0.02670118272304535
step = 14, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.56875
params:  [0.6030854868705384, 0.10646407361846272, 0.6084908075142903, 0.5010598693676073, 0.17839696352589984, 0.99, 0.7437577467182594, 0.01, 0.22365506335374713, 0.2776356403341882, 0.356247728646413, 0.625996871470041, 0.4327123627544025, 0.24056962057796893, 0.24823474303320847, 0.47938403573485955, 0.8092998567487852, 0.3835028866522604, 0.29692339990821237, 0.38447531587510503, 0.2972251908184385, 0.01, 0.6084186816525242, 0.7114109699397908]
Training loss = 0.03176299194494883
step = 0, Training Accuracy: 0.5966666666666667
Validation Accuracy: 0.565
Training loss = 0.02989375332991282
step = 1, Training Accuracy: 0.6033333333333334
Training loss = 0.03073536475499471
step = 2, Training Accuracy: 0.5866666666666667
Training loss = 0.028911153276761373
step = 3, Training Accuracy: 0.61
Training loss = 0.028380921681722005
step = 4, Training Accuracy: 0.6
Training loss = 0.02789959788322449
step = 5, Training Accuracy: 0.6233333333333333
Training loss = 0.02797641654809316
step = 6, Training Accuracy: 0.6033333333333334
Training loss = 0.027580503622690836
step = 7, Training Accuracy: 0.62
Training loss = 0.02666061758995056
step = 8, Training Accuracy: 0.6466666666666666
Training loss = 0.028263410329818727
step = 9, Training Accuracy: 0.61
Training loss = 0.025410646200180055
step = 10, Training Accuracy: 0.66
Training loss = 0.02529781848192215
step = 11, Training Accuracy: 0.6566666666666666
Training loss = 0.027484010060628256
step = 12, Training Accuracy: 0.63
Training loss = 0.026230367223421733
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.026141402920087178
step = 14, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.5375
params:  [0.48925218826701455, 0.20170135602066952, 0.28938407183679427, 0.4905934971598668, 0.22912326480441214, 0.99, 0.3410719388698884, 0.43195866623123863, 0.8301761908661778, 0.18280622025797236, 0.25745191913204374, 0.17785226785123912, 0.7767015467557881, 0.44046710478685724, 0.43409843364874645, 0.01, 0.5275282329606507, 0.11029779486023979, 0.5261141204714513, 0.7465707513125671, 0.01, 0.9693930967442018, 0.3823675540603527, 0.01]
Training loss = 0.032737785975138343
step = 0, Training Accuracy: 0.5733333333333334
Validation Accuracy: 0.515
Training loss = 0.03145172377427419
step = 1, Training Accuracy: 0.56
Training loss = 0.029722024202346802
step = 2, Training Accuracy: 0.5933333333333334
Training loss = 0.029353340268135072
step = 3, Training Accuracy: 0.59
Training loss = 0.028568919698397317
step = 4, Training Accuracy: 0.6233333333333333
Training loss = 0.027073587775230407
step = 5, Training Accuracy: 0.66
Training loss = 0.029069196581840515
step = 6, Training Accuracy: 0.5966666666666667
Training loss = 0.027231234510739645
step = 7, Training Accuracy: 0.65
Training loss = 0.029156054854393005
step = 8, Training Accuracy: 0.6133333333333333
Training loss = 0.02680741747220357
step = 9, Training Accuracy: 0.63
Training loss = 0.026398823261260987
step = 10, Training Accuracy: 0.6633333333333333
Training loss = 0.028077513178189597
step = 11, Training Accuracy: 0.6066666666666667
Training loss = 0.02648748775323232
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02597013771533966
step = 13, Training Accuracy: 0.6233333333333333
Training loss = 0.027616848945617677
step = 14, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.58125
params:  [0.5888360831193729, 0.01, 0.489586469088427, 0.2904642027000838, 0.14229171056566325, 0.99, 0.5180690629823079, 0.32537917069856825, 0.01, 0.31099955860348316, 0.5772651172168293, 0.385706677182885, 0.8991891036964923, 0.4545485903776948, 0.45161428650019725, 0.01, 0.3971856450419692, 0.0738064031703923, 0.07538877738487576, 0.42962385998745595, 0.20153403699273828, 0.5783165816539668, 0.8428468443545061, 0.47557990980603226]
Training loss = 0.027975884278615314
step = 0, Training Accuracy: 0.6266666666666667
Validation Accuracy: 0.5425
Training loss = 0.02990558644135793
step = 1, Training Accuracy: 0.62
Training loss = 0.02705331067244212
step = 2, Training Accuracy: 0.6566666666666666
Training loss = 0.02692298630873362
step = 3, Training Accuracy: 0.6433333333333333
Training loss = 0.025613959630330405
step = 4, Training Accuracy: 0.6666666666666666
Training loss = 0.025047114888827007
step = 5, Training Accuracy: 0.6766666666666666
Training loss = 0.027111131151517233
step = 6, Training Accuracy: 0.6366666666666667
Training loss = 0.028434348305066425
step = 7, Training Accuracy: 0.6133333333333333
Training loss = 0.027369920214017233
step = 8, Training Accuracy: 0.63
Training loss = 0.024773352841536204
step = 9, Training Accuracy: 0.6833333333333333
Training loss = 0.024410068194071453
step = 10, Training Accuracy: 0.6566666666666666
Training loss = 0.024498682618141174
step = 11, Training Accuracy: 0.6933333333333334
Training loss = 0.024444636503855387
step = 12, Training Accuracy: 0.6666666666666666
Training loss = 0.025589659015337628
step = 13, Training Accuracy: 0.7
Training loss = 0.02172317703564962
step = 14, Training Accuracy: 0.72
Validation Accuracy: 0.5425
params:  [0.737309584112914, 0.6921428262847285, 0.5682379803812388, 0.01, 0.44405516521090094, 0.9033815451382974, 0.26502401229912886, 0.7983136548385952, 0.27972828528541466, 0.47541106252388776, 0.18130888588216854, 0.3539022257751415, 0.01, 0.18229063709847357, 0.5298954095262923, 0.030110681119168453, 0.2028391024607935, 0.8571366877429936, 0.6760571281400811, 0.07944468116231573, 0.6208382932808297, 0.22718376356157832, 0.01775502963163178, 0.30539015755969123]
Training loss = 0.035644917885462445
step = 0, Training Accuracy: 0.58
Validation Accuracy: 0.5
Training loss = 0.03113774100939433
step = 1, Training Accuracy: 0.5033333333333333
Training loss = 0.03166322708129883
step = 2, Training Accuracy: 0.5033333333333333
Training loss = 0.030937630931536356
step = 3, Training Accuracy: 0.55
Training loss = 0.03135738531748454
step = 4, Training Accuracy: 0.5533333333333333
Training loss = 0.03131312350432078
step = 5, Training Accuracy: 0.5466666666666666
Training loss = 0.031018190383911133
step = 6, Training Accuracy: 0.56
Training loss = 0.031855487028757734
step = 7, Training Accuracy: 0.51
Training loss = 0.02994584878285726
step = 8, Training Accuracy: 0.5133333333333333
Training loss = 0.029224496682484946
step = 9, Training Accuracy: 0.5566666666666666
Training loss = 0.02804992159207662
step = 10, Training Accuracy: 0.6066666666666667
Training loss = 0.027501303553581238
step = 11, Training Accuracy: 0.5766666666666667
Training loss = 0.02942297319571177
step = 12, Training Accuracy: 0.6233333333333333
Training loss = 0.02952246387799581
step = 13, Training Accuracy: 0.59
Training loss = 0.029101282755533853
step = 14, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.57875
params:  [0.577964838274527, 0.4238342780809495, 0.7890128387732965, 0.01, 0.5475786737022212, 0.99, 0.20759549893180357, 0.5302994159234822, 0.19816450639476102, 0.4754832096068696, 0.99, 0.11902790792542298, 0.4212550753229377, 0.4030094424816344, 0.29599258352645275, 0.3039344628490927, 0.575147855103763, 0.2890954019699558, 0.6565824696850693, 0.1038630160747171, 0.01, 0.7345468615331932, 0.746618047998347, 0.8190378947961842]
Training loss = 0.0317136679093043
step = 0, Training Accuracy: 0.5033333333333333
Validation Accuracy: 0.56375
Training loss = 0.03035571316878001
step = 1, Training Accuracy: 0.5733333333333334
Training loss = 0.02938752035299937
step = 2, Training Accuracy: 0.5833333333333334
Training loss = 0.028407865166664124
step = 3, Training Accuracy: 0.62
Training loss = 0.02724802056948344
step = 4, Training Accuracy: 0.6233333333333333
Training loss = 0.026081543366114297
step = 5, Training Accuracy: 0.63
Training loss = 0.02483079512914022
step = 6, Training Accuracy: 0.6566666666666666
Training loss = 0.026433887481689452
step = 7, Training Accuracy: 0.65
Training loss = 0.02525916278362274
step = 8, Training Accuracy: 0.6566666666666666
Training loss = 0.02592871030171712
step = 9, Training Accuracy: 0.6466666666666666
Training loss = 0.025626844763755798
step = 10, Training Accuracy: 0.66
Training loss = 0.02464507261912028
step = 11, Training Accuracy: 0.6666666666666666
Training loss = 0.024002980987230936
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02386875092983246
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.02503633201122284
step = 14, Training Accuracy: 0.6533333333333333
Validation Accuracy: 0.53875
params:  [0.42638356519913884, 0.01, 0.24596188457947854, 0.17815132620293817, 0.34197128737206345, 0.99, 0.43296116440224475, 0.4780750579062889, 0.2342878061372219, 0.07618606640844855, 0.6877002043295019, 0.99, 0.2539362833619465, 0.7693935564805283, 0.5039005675633721, 0.4481549747108071, 0.5830072397990057, 0.30234559244499315, 0.4768694871757687, 0.2552569145103685, 0.01, 0.27387915069275315, 0.3660455143798937, 0.5069196382970417]
Training loss = 0.029008965889612832
step = 0, Training Accuracy: 0.57
Validation Accuracy: 0.5625
Training loss = 0.026798320412635804
step = 1, Training Accuracy: 0.6633333333333333
Training loss = 0.02645419160525004
step = 2, Training Accuracy: 0.6466666666666666
Training loss = 0.022933250466982524
step = 3, Training Accuracy: 0.6666666666666666
Training loss = 0.022690226435661317
step = 4, Training Accuracy: 0.7
Training loss = 0.022657379110654196
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.021854332486788433
step = 6, Training Accuracy: 0.73
Training loss = 0.02105681836605072
step = 7, Training Accuracy: 0.73
Training loss = 0.02073252389828364
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.01992986261844635
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.018890755673249562
step = 10, Training Accuracy: 0.77
Training loss = 0.019107294380664826
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.019073285659154258
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.01612787902355194
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.016974758406480155
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.56625
[[0.6490142459033698, 0.2322671098540393, 0.9396946306764662, 0.01, 0.01, 0.99, 0.1961506638996729, 0.16464707456104152, 0.03808020580164573, 0.01, 0.5725886814698102, 0.19361440726225626, 0.19430802548959464, 0.49610134640912273, 0.3591576842195144, 0.43023041874587264, 0.9737638446522174, 0.12975891291524586, 0.4297539875829992, 0.9569089569224076, 0.39430656143020776, 0.4585207096486446, 0.5202584614063771, 0.01], [0.3366851826424452, 0.08404673748158739, 0.05644340288977179, 0.10966889132321338, 0.16530551528352785, 0.99, 0.7215399739986231, 0.39239170409407037, 0.15709974088602646, 0.01, 0.5626590785014266, 0.01, 0.58009680696429, 0.016020054646563198, 0.4959508325786198, 0.7556834553526812, 0.31948801633118096, 0.11249187506201698, 0.3198083930243585, 0.6127094055037016, 0.01, 0.5332767769129598, 0.3618083687120638, 0.5671366678656746], [0.6030854868705384, 0.10646407361846272, 0.6084908075142903, 0.5010598693676073, 0.17839696352589984, 0.99, 0.7437577467182594, 0.01, 0.22365506335374713, 0.2776356403341882, 0.356247728646413, 0.625996871470041, 0.4327123627544025, 0.24056962057796893, 0.24823474303320847, 0.47938403573485955, 0.8092998567487852, 0.3835028866522604, 0.29692339990821237, 0.38447531587510503, 0.2972251908184385, 0.01, 0.6084186816525242, 0.7114109699397908], [0.48925218826701455, 0.20170135602066952, 0.28938407183679427, 0.4905934971598668, 0.22912326480441214, 0.99, 0.3410719388698884, 0.43195866623123863, 0.8301761908661778, 0.18280622025797236, 0.25745191913204374, 0.17785226785123912, 0.7767015467557881, 0.44046710478685724, 0.43409843364874645, 0.01, 0.5275282329606507, 0.11029779486023979, 0.5261141204714513, 0.7465707513125671, 0.01, 0.9693930967442018, 0.3823675540603527, 0.01], [0.5888360831193729, 0.01, 0.489586469088427, 0.2904642027000838, 0.14229171056566325, 0.99, 0.5180690629823079, 0.32537917069856825, 0.01, 0.31099955860348316, 0.5772651172168293, 0.385706677182885, 0.8991891036964923, 0.4545485903776948, 0.45161428650019725, 0.01, 0.3971856450419692, 0.0738064031703923, 0.07538877738487576, 0.42962385998745595, 0.20153403699273828, 0.5783165816539668, 0.8428468443545061, 0.47557990980603226], [0.737309584112914, 0.6921428262847285, 0.5682379803812388, 0.01, 0.44405516521090094, 0.9033815451382974, 0.26502401229912886, 0.7983136548385952, 0.27972828528541466, 0.47541106252388776, 0.18130888588216854, 0.3539022257751415, 0.01, 0.18229063709847357, 0.5298954095262923, 0.030110681119168453, 0.2028391024607935, 0.8571366877429936, 0.6760571281400811, 0.07944468116231573, 0.6208382932808297, 0.22718376356157832, 0.01775502963163178, 0.30539015755969123], [0.577964838274527, 0.4238342780809495, 0.7890128387732965, 0.01, 0.5475786737022212, 0.99, 0.20759549893180357, 0.5302994159234822, 0.19816450639476102, 0.4754832096068696, 0.99, 0.11902790792542298, 0.4212550753229377, 0.4030094424816344, 0.29599258352645275, 0.3039344628490927, 0.575147855103763, 0.2890954019699558, 0.6565824696850693, 0.1038630160747171, 0.01, 0.7345468615331932, 0.746618047998347, 0.8190378947961842], [0.42638356519913884, 0.01, 0.24596188457947854, 0.17815132620293817, 0.34197128737206345, 0.99, 0.43296116440224475, 0.4780750579062889, 0.2342878061372219, 0.07618606640844855, 0.6877002043295019, 0.99, 0.2539362833619465, 0.7693935564805283, 0.5039005675633721, 0.4481549747108071, 0.5830072397990057, 0.30234559244499315, 0.4768694871757687, 0.2552569145103685, 0.01, 0.27387915069275315, 0.3660455143798937, 0.5069196382970417]]
gen	nevals	avg     	std      	min    	max    
0  	8     	0.552812	0.0234417	0.50875	0.58125
params:  [0.3875372296400714, 0.8808713402482942, 0.6061902921316666, 0.01, 0.5714228731588762, 0.99, 0.1640714506419956, 0.15307427755287695, 0.47035317538303234, 0.4342092351598245, 0.6320510813853413, 0.16123883226012292, 0.20739529823049024, 0.5450485800736401, 0.7762052994487461, 0.4315398708050562, 0.36845271951538466, 0.01, 0.6943471290182199, 0.48596245794374937, 0.0834893495760707, 0.6664066435832162, 0.01, 0.1823249137394996]
Training loss = 0.03815633873144786
step = 0, Training Accuracy: 0.5266666666666666
Validation Accuracy: 0.54
Training loss = 0.03169457236925761
step = 1, Training Accuracy: 0.5233333333333333
Training loss = 0.03137213965257009
step = 2, Training Accuracy: 0.54
Training loss = 0.031578993201255796
step = 3, Training Accuracy: 0.5566666666666666
Training loss = 0.02893336077531179
step = 4, Training Accuracy: 0.6466666666666666
Training loss = 0.0297244131565094
step = 5, Training Accuracy: 0.5933333333333334
Training loss = 0.027939447164535523
step = 6, Training Accuracy: 0.6066666666666667
Training loss = 0.02821562608083089
step = 7, Training Accuracy: 0.6133333333333333
Training loss = 0.027789012392361957
step = 8, Training Accuracy: 0.6466666666666666
Training loss = 0.025965009133021036
step = 9, Training Accuracy: 0.66
Training loss = 0.026794519225756326
step = 10, Training Accuracy: 0.6033333333333334
Training loss = 0.027769567767779033
step = 11, Training Accuracy: 0.63
Training loss = 0.025666308800379435
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02696514924367269
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.025308965444564818
step = 14, Training Accuracy: 0.64
Validation Accuracy: 0.6825
params:  [0.07478685499650861, 0.252924506574827, 0.43755541094706585, 0.604389104345743, 0.3822660052191336, 0.8938079050456419, 0.3602909996963123, 0.5829155718896891, 0.5764315334783299, 0.01, 0.01, 0.7044677573117988, 0.3869665136598494, 0.6677639243102514, 0.6573816708260628, 0.01, 0.6014013893608787, 0.4644832154336942, 0.5282486231021576, 0.37743065526776265, 0.7901161329980468, 0.7376781422649552, 0.09416928024509627, 0.07035466363564473]
Training loss = 0.023180408676465352
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.67125
Training loss = 0.023258353074391683
step = 1, Training Accuracy: 0.68
Training loss = 0.021614503661791483
step = 2, Training Accuracy: 0.73
Training loss = 0.021124677658081056
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017832745015621185
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018544452985127767
step = 5, Training Accuracy: 0.76
Training loss = 0.01711382865905762
step = 6, Training Accuracy: 0.78
Training loss = 0.0180039248863856
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.016267653604348502
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016699555118878683
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.017451998591423035
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.017181341846783955
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.0154824098944664
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014558062752087911
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.01644073655207952
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.66875
params:  [0.6162483875843768, 0.6770099387116351, 0.01, 0.16203426768537454, 0.6723721951777457, 0.99, 0.37307607528516334, 0.0846317592681819, 0.9630857171403453, 0.40658287815489846, 0.21088963267244976, 0.6196740785164202, 0.09499632809763908, 0.4842364011682022, 0.2252517264246148, 0.01, 0.24577982396834233, 0.1497834076185565, 0.693902181454802, 0.9513849514808495, 0.01, 0.7111567413103104, 0.06315744484635294, 0.780852290958317]
Training loss = 0.0325898536046346
step = 0, Training Accuracy: 0.6166666666666667
Validation Accuracy: 0.64
Training loss = 0.026079741319020588
step = 1, Training Accuracy: 0.6266666666666667
Training loss = 0.025754919449488323
step = 2, Training Accuracy: 0.6133333333333333
Training loss = 0.02450831711292267
step = 3, Training Accuracy: 0.66
Training loss = 0.02301895300547282
step = 4, Training Accuracy: 0.68
Training loss = 0.024086095492045084
step = 5, Training Accuracy: 0.6633333333333333
Training loss = 0.022459491988023123
step = 6, Training Accuracy: 0.6766666666666666
Training loss = 0.024345756967862446
step = 7, Training Accuracy: 0.6866666666666666
Training loss = 0.022966145078341167
step = 8, Training Accuracy: 0.6666666666666666
Training loss = 0.022752702037493387
step = 9, Training Accuracy: 0.6933333333333334
Training loss = 0.021073333223660787
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.01890113830566406
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.022284358541170755
step = 12, Training Accuracy: 0.7066666666666667
Training loss = 0.020077778498331707
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.020530932048956552
step = 14, Training Accuracy: 0.71
Validation Accuracy: 0.69625
params:  [0.39164866188931735, 0.8480738432522765, 0.01, 0.01, 0.15913427771482322, 0.9731780324970323, 0.4299717911835894, 0.99, 0.5422638298834728, 0.01, 0.48109087549373863, 0.9041640089319387, 0.709630944025758, 0.31456713228816646, 0.43490605673082305, 0.19556848216838107, 0.5653708028415657, 0.4924363813571906, 0.18069443495791093, 0.5500029189434243, 0.18076683060455842, 0.6718377976070952, 0.2537435666865145, 0.301492108192733]
Training loss = 0.027722104986508688
step = 0, Training Accuracy: 0.63
Validation Accuracy: 0.71375
Training loss = 0.025724398295084636
step = 1, Training Accuracy: 0.62
Training loss = 0.02492790182431539
step = 2, Training Accuracy: 0.66
Training loss = 0.024190515081087747
step = 3, Training Accuracy: 0.6766666666666666
Training loss = 0.02258734921614329
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.02463821311791738
step = 5, Training Accuracy: 0.66
Training loss = 0.023828031818072002
step = 6, Training Accuracy: 0.6966666666666667
Training loss = 0.02333840529123942
step = 7, Training Accuracy: 0.6833333333333333
Training loss = 0.022957268754641214
step = 8, Training Accuracy: 0.6766666666666666
Training loss = 0.02591217001279195
step = 9, Training Accuracy: 0.6666666666666666
Training loss = 0.021577643950780233
step = 10, Training Accuracy: 0.7033333333333334
Training loss = 0.022802743713061013
step = 11, Training Accuracy: 0.69
Training loss = 0.021118055284023284
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.022783576448758443
step = 13, Training Accuracy: 0.68
Training loss = 0.02089191069205602
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7175
params:  [0.8345455407569855, 0.01, 0.4304994044812208, 0.01, 0.686815124097547, 0.99, 0.7005406407840213, 0.8382358970087966, 0.41331883901116195, 0.6172765739755691, 0.16997532372516208, 0.3287994532232975, 0.4642087121411554, 0.4106193042128521, 0.7403152130884567, 0.04462350715617844, 0.4355852319563824, 0.6167201364775888, 0.15408793317779862, 0.12178169609916745, 0.01, 0.19605860473909087, 0.4365553896941836, 0.3318664558546164]
Training loss = 0.021952570378780366
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.72375
Training loss = 0.020903850396474202
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.019501688480377196
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01619899074236552
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.018043246964613596
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015632657806078594
step = 5, Training Accuracy: 0.81
Training loss = 0.014294800013303757
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.016893459061781566
step = 7, Training Accuracy: 0.79
Training loss = 0.014050961335500082
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.012058903177579244
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.0110894180337588
step = 10, Training Accuracy: 0.8766666666666667
Training loss = 0.010434118260939916
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.012817566593488058
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.011881677508354187
step = 13, Training Accuracy: 0.86
Training loss = 0.010397953043381373
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.6975
params:  [0.5376158560593507, 0.01, 0.23495386881838137, 0.4938112139670019, 0.4845251046023977, 0.45644797996606135, 0.21319479371315297, 0.5536731655057956, 0.99, 0.01, 0.3648050061047645, 0.01, 0.92926055085538, 0.13287837667203758, 0.3473828857360762, 0.363218904605192, 0.8308268953804003, 0.42830612738483925, 0.5979549494399151, 0.19524762711435178, 0.031241356041850882, 0.7663889649262006, 0.4129440923341202, 0.34645392170292544]
Training loss = 0.030576943556467694
step = 0, Training Accuracy: 0.67
Validation Accuracy: 0.6525
Training loss = 0.021464624802271525
step = 1, Training Accuracy: 0.72
Training loss = 0.019348941246668496
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.017984566191832224
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.016538275082906086
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015624408225218454
step = 5, Training Accuracy: 0.8
Training loss = 0.014999527831872304
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014788163999716442
step = 7, Training Accuracy: 0.81
Training loss = 0.01657671630382538
step = 8, Training Accuracy: 0.79
Training loss = 0.013072742223739624
step = 9, Training Accuracy: 0.84
Training loss = 0.012986107369263967
step = 10, Training Accuracy: 0.84
Training loss = 0.015996687014897665
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01481305052836736
step = 12, Training Accuracy: 0.8
Training loss = 0.010610744257767995
step = 13, Training Accuracy: 0.89
Training loss = 0.009678267488876978
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.70625
params:  [0.22486014999950632, 0.45400719556090424, 0.4129215218176502, 0.7086627604712854, 0.25139254477722334, 0.99, 0.7165498072164862, 0.5071686173425632, 0.548172072184024, 0.28806726461842486, 0.01, 0.11755971536900557, 0.3386798175885486, 0.38631329547671034, 0.6483849275659298, 0.1257221549654359, 0.3045495848450878, 0.30146681821920773, 0.6309667469361145, 0.6190505165492646, 0.17418160727472984, 0.609702729354431, 0.0760894425513193, 0.01]
Training loss = 0.030561961034933725
step = 0, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.66375
Training loss = 0.023103190461794536
step = 1, Training Accuracy: 0.69
Training loss = 0.02211476743221283
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.021020556092262267
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.021148596107959747
step = 4, Training Accuracy: 0.73
Training loss = 0.019909839630126953
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.019411437213420868
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.020673089623451234
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.020649115840593976
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.0183912135163943
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.017335462868213653
step = 10, Training Accuracy: 0.75
Training loss = 0.017473679582277933
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017932413220405577
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01813709020614624
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01958875556786855
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.70375
params:  [0.2789990591277706, 0.01, 0.47752820549775876, 0.01, 0.1868534489028691, 0.4346287240383121, 0.2043143029038008, 0.4966008767312856, 0.9695140440414456, 0.569093695240624, 0.01, 0.27268853751296784, 0.08022619399434644, 0.038463152322992133, 0.3227381793968458, 0.04360024226725445, 0.1640253421807117, 0.5414848687864701, 0.49477941526667424, 0.40322499336916795, 0.05390952871297211, 0.3695288037268693, 0.99, 0.319087514136825]
Training loss = 0.025191994309425356
step = 0, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.6925
Training loss = 0.022462921341260274
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.02333177626132965
step = 2, Training Accuracy: 0.7
Training loss = 0.022421752313772837
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.022933517396450043
step = 4, Training Accuracy: 0.71
Training loss = 0.019948612848917642
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.022252174615859984
step = 6, Training Accuracy: 0.75
Training loss = 0.02171858976284663
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.020496767858664194
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.01755358338356018
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.018575983544190725
step = 10, Training Accuracy: 0.76
Training loss = 0.018014410436153414
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.016454241772492727
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.016383458276589713
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01438061128060023
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.6825
[[0.3875372296400714, 0.8808713402482942, 0.6061902921316666, 0.01, 0.5714228731588762, 0.99, 0.1640714506419956, 0.15307427755287695, 0.47035317538303234, 0.4342092351598245, 0.6320510813853413, 0.16123883226012292, 0.20739529823049024, 0.5450485800736401, 0.7762052994487461, 0.4315398708050562, 0.36845271951538466, 0.01, 0.6943471290182199, 0.48596245794374937, 0.0834893495760707, 0.6664066435832162, 0.01, 0.1823249137394996], [0.07478685499650861, 0.252924506574827, 0.43755541094706585, 0.604389104345743, 0.3822660052191336, 0.8938079050456419, 0.3602909996963123, 0.5829155718896891, 0.5764315334783299, 0.01, 0.01, 0.7044677573117988, 0.3869665136598494, 0.6677639243102514, 0.6573816708260628, 0.01, 0.6014013893608787, 0.4644832154336942, 0.5282486231021576, 0.37743065526776265, 0.7901161329980468, 0.7376781422649552, 0.09416928024509627, 0.07035466363564473], [0.6162483875843768, 0.6770099387116351, 0.01, 0.16203426768537454, 0.6723721951777457, 0.99, 0.37307607528516334, 0.0846317592681819, 0.9630857171403453, 0.40658287815489846, 0.21088963267244976, 0.6196740785164202, 0.09499632809763908, 0.4842364011682022, 0.2252517264246148, 0.01, 0.24577982396834233, 0.1497834076185565, 0.693902181454802, 0.9513849514808495, 0.01, 0.7111567413103104, 0.06315744484635294, 0.780852290958317], [0.39164866188931735, 0.8480738432522765, 0.01, 0.01, 0.15913427771482322, 0.9731780324970323, 0.4299717911835894, 0.99, 0.5422638298834728, 0.01, 0.48109087549373863, 0.9041640089319387, 0.709630944025758, 0.31456713228816646, 0.43490605673082305, 0.19556848216838107, 0.5653708028415657, 0.4924363813571906, 0.18069443495791093, 0.5500029189434243, 0.18076683060455842, 0.6718377976070952, 0.2537435666865145, 0.301492108192733], [0.8345455407569855, 0.01, 0.4304994044812208, 0.01, 0.686815124097547, 0.99, 0.7005406407840213, 0.8382358970087966, 0.41331883901116195, 0.6172765739755691, 0.16997532372516208, 0.3287994532232975, 0.4642087121411554, 0.4106193042128521, 0.7403152130884567, 0.04462350715617844, 0.4355852319563824, 0.6167201364775888, 0.15408793317779862, 0.12178169609916745, 0.01, 0.19605860473909087, 0.4365553896941836, 0.3318664558546164], [0.5376158560593507, 0.01, 0.23495386881838137, 0.4938112139670019, 0.4845251046023977, 0.45644797996606135, 0.21319479371315297, 0.5536731655057956, 0.99, 0.01, 0.3648050061047645, 0.01, 0.92926055085538, 0.13287837667203758, 0.3473828857360762, 0.363218904605192, 0.8308268953804003, 0.42830612738483925, 0.5979549494399151, 0.19524762711435178, 0.031241356041850882, 0.7663889649262006, 0.4129440923341202, 0.34645392170292544], [0.22486014999950632, 0.45400719556090424, 0.4129215218176502, 0.7086627604712854, 0.25139254477722334, 0.99, 0.7165498072164862, 0.5071686173425632, 0.548172072184024, 0.28806726461842486, 0.01, 0.11755971536900557, 0.3386798175885486, 0.38631329547671034, 0.6483849275659298, 0.1257221549654359, 0.3045495848450878, 0.30146681821920773, 0.6309667469361145, 0.6190505165492646, 0.17418160727472984, 0.609702729354431, 0.0760894425513193, 0.01], [0.2789990591277706, 0.01, 0.47752820549775876, 0.01, 0.1868534489028691, 0.4346287240383121, 0.2043143029038008, 0.4966008767312856, 0.9695140440414456, 0.569093695240624, 0.01, 0.27268853751296784, 0.08022619399434644, 0.038463152322992133, 0.3227381793968458, 0.04360024226725445, 0.1640253421807117, 0.5414848687864701, 0.49477941526667424, 0.40322499336916795, 0.05390952871297211, 0.3695288037268693, 0.99, 0.319087514136825]]
1  	8     	0.694375	0.0146442	0.66875	0.7175 
params:  [0.48406257704730216, 0.7942266357750813, 0.05023837102331745, 0.21029662460700035, 0.3243205633869997, 0.7177406540145179, 0.01, 0.7888377391028076, 0.4985632652791722, 0.01, 0.99, 0.5979192247018121, 0.6626779277026332, 0.5630903730784813, 0.7139135570410813, 0.01, 0.2830437307642066, 0.5934067222347044, 0.4675582496780958, 0.499302117964885, 0.01, 0.8124830084237402, 0.30540817664174214, 0.03252228080899788]
Training loss = 0.02544658988714218
step = 0, Training Accuracy: 0.6633333333333333
Validation Accuracy: 0.6075
Training loss = 0.024484635988871257
step = 1, Training Accuracy: 0.67
Training loss = 0.024364683032035827
step = 2, Training Accuracy: 0.63
Training loss = 0.02322454333305359
step = 3, Training Accuracy: 0.6866666666666666
Training loss = 0.02059748907883962
step = 4, Training Accuracy: 0.72
Training loss = 0.02282526751359304
step = 5, Training Accuracy: 0.6833333333333333
Training loss = 0.020904189646244048
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.020322750409444174
step = 7, Training Accuracy: 0.7033333333333334
Training loss = 0.02028469890356064
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.02005692372719447
step = 9, Training Accuracy: 0.7133333333333334
Training loss = 0.018915570080280303
step = 10, Training Accuracy: 0.7266666666666667
Training loss = 0.01830071896314621
step = 11, Training Accuracy: 0.76
Training loss = 0.018205276429653167
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01938084642092387
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018999569614728293
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.72
params:  [0.27624537679821093, 0.8679496455187833, 0.2779987507817357, 0.1657137246700604, 0.5958314347406912, 0.8689424063623578, 0.1355936742618027, 0.6874964961723096, 0.46563730130428366, 0.24817889380767358, 0.08287175021757887, 0.362831526198564, 0.4477087584366628, 0.11481752938707215, 0.6851486735726507, 0.034186224764878426, 0.99, 0.4709865532207981, 0.4165851625059514, 0.9707089318713855, 0.01, 0.6348456714999093, 0.21040361209965558, 0.4778436271855737]
Training loss = 0.024403871695200603
step = 0, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.715
Training loss = 0.023947257002194723
step = 1, Training Accuracy: 0.7
Training loss = 0.021352716286977134
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.02373022516568502
step = 3, Training Accuracy: 0.6666666666666666
Training loss = 0.02225538750489553
step = 4, Training Accuracy: 0.71
Training loss = 0.01966734210650126
step = 5, Training Accuracy: 0.72
Training loss = 0.019490311443805693
step = 6, Training Accuracy: 0.73
Training loss = 0.01739149570465088
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.018938909769058227
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.018415227035681405
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.018479041357835135
step = 10, Training Accuracy: 0.73
Training loss = 0.019999168713887534
step = 11, Training Accuracy: 0.7266666666666667
Training loss = 0.019883075853188832
step = 12, Training Accuracy: 0.71
Training loss = 0.020434515178203584
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.01978646566470464
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.72375
params:  [0.684270571731397, 0.2677047218669207, 0.05531736608911132, 0.01, 0.2347320813808775, 0.5268352103308749, 0.12427586986045308, 0.99, 0.6789371981763527, 0.01, 0.6253163281187344, 0.01, 0.3803145167476729, 0.20699069978715384, 0.0886804962310076, 0.018135719528223487, 0.08624729968573286, 0.7466539373216896, 0.4018071442395745, 0.26753711284285137, 0.01, 0.6464446483638834, 0.6631471030413282, 0.4881981263322704]
Training loss = 0.022192967037359873
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.74125
Training loss = 0.022272746364275616
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.018589525520801543
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.019184690217177072
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.018800744613011677
step = 4, Training Accuracy: 0.73
Training loss = 0.01982341766357422
step = 5, Training Accuracy: 0.74
Training loss = 0.01681985209385554
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.01843582699696223
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.01805978884299596
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.019184127549330392
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.015744752486546835
step = 10, Training Accuracy: 0.8
Training loss = 0.015049432615439097
step = 11, Training Accuracy: 0.83
Training loss = 0.017158154249191284
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017684252858161928
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01677479495604833
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.7
params:  [0.37211370538618804, 0.453164497193789, 0.3016171514009313, 0.1593066411937284, 0.01, 0.99, 0.2013937771274171, 0.7687751423582535, 0.7539285788394859, 0.01, 0.12905024281951513, 0.4745072535910194, 0.99, 0.2697370163411116, 0.2754150335524528, 0.422876065026403, 0.7589646426793183, 0.01, 0.7743642312030377, 0.7009988224704509, 0.27790372398643814, 0.99, 0.19064752400108514, 0.24038553030571908]
Training loss = 0.024029947916666666
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.73625
Training loss = 0.019321105281511944
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018124070167541504
step = 2, Training Accuracy: 0.77
Training loss = 0.01760870188474655
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.018215839664141337
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01651535282532374
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.01462356095512708
step = 6, Training Accuracy: 0.8
Training loss = 0.01653599927822749
step = 7, Training Accuracy: 0.79
Training loss = 0.019580902258555095
step = 8, Training Accuracy: 0.77
Training loss = 0.0167899888753891
step = 9, Training Accuracy: 0.76
Training loss = 0.018221596479415892
step = 10, Training Accuracy: 0.77
Training loss = 0.015478086670239767
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01594220022360484
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.012574859460194905
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.016670405666033426
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.74625
params:  [0.09690509429568772, 0.5004936995371027, 0.32830247644838606, 0.3073738365765965, 0.01, 0.9111697890383643, 0.7054146103677308, 0.5515975618668854, 0.7552214281589806, 0.6763322316903928, 0.3965790329166321, 0.28202001588134495, 0.6088962326262801, 0.01, 0.496956016139157, 0.5779099502353762, 0.7064635792156448, 0.6555792271030952, 0.34844736926074216, 0.754109616978335, 0.24079064076937956, 0.5539621260398307, 0.6744687866855529, 0.6382169233554436]
Training loss = 0.02570606052875519
step = 0, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.72
Training loss = 0.023974916140238445
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.020973910689353944
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.02183466056982676
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.02089383820692698
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.020513527790705362
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.02065721482038498
step = 6, Training Accuracy: 0.77
Training loss = 0.019935757915178935
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.02009423146645228
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.02125871568918228
step = 9, Training Accuracy: 0.73
Training loss = 0.02112533320983251
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.019127290646235147
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018763874173164368
step = 12, Training Accuracy: 0.76
Training loss = 0.018959041635195413
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.020123662153879802
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.76625
params:  [0.059316391622907416, 0.5671144065741983, 0.36199744474514006, 0.3757224560777295, 0.031323025982508024, 0.6093517485525679, 0.6284808839413366, 0.99, 0.2671587437759867, 0.01, 0.3435702234983135, 0.44509254871372994, 0.8266059142524831, 0.28756209637386115, 0.5699400150066771, 0.2428639354010323, 0.43755485461596166, 0.5389670151180805, 0.011766248369977084, 0.16648193920184634, 0.4269245562538324, 0.99, 0.3183499663159376, 0.252775154159703]
Training loss = 0.02176673859357834
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.69375
Training loss = 0.01960341552893321
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.019746893346309663
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01751613090435664
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016517471969127655
step = 4, Training Accuracy: 0.78
Training loss = 0.017923409938812255
step = 5, Training Accuracy: 0.78
Training loss = 0.015271675984064738
step = 6, Training Accuracy: 0.79
Training loss = 0.016263671418031058
step = 7, Training Accuracy: 0.76
Training loss = 0.015397342244784036
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.014410049219926198
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015038302938143412
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01423843890428543
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.012014397333065668
step = 12, Training Accuracy: 0.8633333333333333
Training loss = 0.012549324929714202
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.012254007359345754
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.72
params:  [0.6165295925197137, 0.43718549408402674, 0.016089415450639744, 0.31164273328560865, 0.3815024149354754, 0.9364696773443864, 0.444887731919591, 0.8863924357651377, 0.25168442301751465, 0.01, 0.6225009962782917, 0.7605189832006819, 0.5913141210550861, 0.24099641279805745, 0.46991629397391654, 0.8871701778659499, 0.40096339791341307, 0.16449926339870796, 0.7251844331676616, 0.4011091672751968, 0.2140098979835554, 0.7947873090210655, 0.147568521968669, 0.13853448091360604]
Training loss = 0.02894733667373657
step = 0, Training Accuracy: 0.6566666666666666
Validation Accuracy: 0.64
Training loss = 0.02244731883207957
step = 1, Training Accuracy: 0.68
Training loss = 0.022477246820926666
step = 2, Training Accuracy: 0.69
Training loss = 0.01897142897049586
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.018326317171255747
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.019977332452932994
step = 5, Training Accuracy: 0.7166666666666667
Training loss = 0.020574266612529753
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.018155890107154845
step = 7, Training Accuracy: 0.76
Training loss = 0.017557602127393088
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.019034361044565837
step = 9, Training Accuracy: 0.76
Training loss = 0.018258136212825776
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.015903526643911998
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015600089927514394
step = 12, Training Accuracy: 0.78
Training loss = 0.02031472831964493
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.01806586782137553
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.74625
params:  [0.5233845577982604, 0.4675131903063711, 0.11695154789278978, 0.023971740595056606, 0.0719463281725464, 0.6166134145768811, 0.4266993605233484, 0.7130663364072863, 0.5977283718061422, 0.15685974732029817, 0.4289279022553224, 0.40058360364508866, 0.37712995851014736, 0.036492666068687674, 0.2481246606858963, 0.1503209846224639, 0.08167601167214145, 0.3076649514328853, 0.01, 0.455781319523161, 0.08565648553762159, 0.707202997123683, 0.41606082946483763, 0.5527647612154872]
Training loss = 0.02150829037030538
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.765
Training loss = 0.01887739936510722
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.019137873351573943
step = 2, Training Accuracy: 0.77
Training loss = 0.016519117256005605
step = 3, Training Accuracy: 0.8
Training loss = 0.016631618936856586
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01735447843869527
step = 5, Training Accuracy: 0.78
Training loss = 0.016418516238530478
step = 6, Training Accuracy: 0.77
Training loss = 0.016634572247664133
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01564529259999593
step = 8, Training Accuracy: 0.8
Training loss = 0.015069486697514851
step = 9, Training Accuracy: 0.84
Training loss = 0.014828988711039225
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.014669201970100403
step = 11, Training Accuracy: 0.82
Training loss = 0.014927377899487813
step = 12, Training Accuracy: 0.83
Training loss = 0.01486103634039561
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01384747823079427
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.76625
[[0.48406257704730216, 0.7942266357750813, 0.05023837102331745, 0.21029662460700035, 0.3243205633869997, 0.7177406540145179, 0.01, 0.7888377391028076, 0.4985632652791722, 0.01, 0.99, 0.5979192247018121, 0.6626779277026332, 0.5630903730784813, 0.7139135570410813, 0.01, 0.2830437307642066, 0.5934067222347044, 0.4675582496780958, 0.499302117964885, 0.01, 0.8124830084237402, 0.30540817664174214, 0.03252228080899788], [0.27624537679821093, 0.8679496455187833, 0.2779987507817357, 0.1657137246700604, 0.5958314347406912, 0.8689424063623578, 0.1355936742618027, 0.6874964961723096, 0.46563730130428366, 0.24817889380767358, 0.08287175021757887, 0.362831526198564, 0.4477087584366628, 0.11481752938707215, 0.6851486735726507, 0.034186224764878426, 0.99, 0.4709865532207981, 0.4165851625059514, 0.9707089318713855, 0.01, 0.6348456714999093, 0.21040361209965558, 0.4778436271855737], [0.684270571731397, 0.2677047218669207, 0.05531736608911132, 0.01, 0.2347320813808775, 0.5268352103308749, 0.12427586986045308, 0.99, 0.6789371981763527, 0.01, 0.6253163281187344, 0.01, 0.3803145167476729, 0.20699069978715384, 0.0886804962310076, 0.018135719528223487, 0.08624729968573286, 0.7466539373216896, 0.4018071442395745, 0.26753711284285137, 0.01, 0.6464446483638834, 0.6631471030413282, 0.4881981263322704], [0.37211370538618804, 0.453164497193789, 0.3016171514009313, 0.1593066411937284, 0.01, 0.99, 0.2013937771274171, 0.7687751423582535, 0.7539285788394859, 0.01, 0.12905024281951513, 0.4745072535910194, 0.99, 0.2697370163411116, 0.2754150335524528, 0.422876065026403, 0.7589646426793183, 0.01, 0.7743642312030377, 0.7009988224704509, 0.27790372398643814, 0.99, 0.19064752400108514, 0.24038553030571908], [0.09690509429568772, 0.5004936995371027, 0.32830247644838606, 0.3073738365765965, 0.01, 0.9111697890383643, 0.7054146103677308, 0.5515975618668854, 0.7552214281589806, 0.6763322316903928, 0.3965790329166321, 0.28202001588134495, 0.6088962326262801, 0.01, 0.496956016139157, 0.5779099502353762, 0.7064635792156448, 0.6555792271030952, 0.34844736926074216, 0.754109616978335, 0.24079064076937956, 0.5539621260398307, 0.6744687866855529, 0.6382169233554436], [0.059316391622907416, 0.5671144065741983, 0.36199744474514006, 0.3757224560777295, 0.031323025982508024, 0.6093517485525679, 0.6284808839413366, 0.99, 0.2671587437759867, 0.01, 0.3435702234983135, 0.44509254871372994, 0.8266059142524831, 0.28756209637386115, 0.5699400150066771, 0.2428639354010323, 0.43755485461596166, 0.5389670151180805, 0.011766248369977084, 0.16648193920184634, 0.4269245562538324, 0.99, 0.3183499663159376, 0.252775154159703], [0.6165295925197137, 0.43718549408402674, 0.016089415450639744, 0.31164273328560865, 0.3815024149354754, 0.9364696773443864, 0.444887731919591, 0.8863924357651377, 0.25168442301751465, 0.01, 0.6225009962782917, 0.7605189832006819, 0.5913141210550861, 0.24099641279805745, 0.46991629397391654, 0.8871701778659499, 0.40096339791341307, 0.16449926339870796, 0.7251844331676616, 0.4011091672751968, 0.2140098979835554, 0.7947873090210655, 0.147568521968669, 0.13853448091360604], [0.5233845577982604, 0.4675131903063711, 0.11695154789278978, 0.023971740595056606, 0.0719463281725464, 0.6166134145768811, 0.4266993605233484, 0.7130663364072863, 0.5977283718061422, 0.15685974732029817, 0.4289279022553224, 0.40058360364508866, 0.37712995851014736, 0.036492666068687674, 0.2481246606858963, 0.1503209846224639, 0.08167601167214145, 0.3076649514328853, 0.01, 0.455781319523161, 0.08565648553762159, 0.707202997123683, 0.41606082946483763, 0.5527647612154872]]
2  	8     	0.736094	0.0223558	0.7    	0.76625
params:  [0.01, 0.49099864663122983, 0.5694691376448697, 0.36676650118623777, 0.01, 0.3800736546684577, 0.6117336135092296, 0.7113222808189507, 0.9007315160470983, 0.19100376718959108, 0.6839632109019338, 0.33848235781270286, 0.40288429259227854, 0.01, 0.11152872760906873, 0.5784635879199014, 0.3576891456982781, 0.7309713605234873, 0.16175833539433834, 0.5919475585200975, 0.07132584299873407, 0.48457504798258333, 0.47779167852425575, 0.34927666762882803]
Training loss = 0.02340312918027242
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.7575
Training loss = 0.02191474914550781
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.01947570065657298
step = 2, Training Accuracy: 0.74
Training loss = 0.0209946741660436
step = 3, Training Accuracy: 0.72
Training loss = 0.02106289972861608
step = 4, Training Accuracy: 0.73
Training loss = 0.018915376762549084
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.0188522141178449
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018610091507434846
step = 7, Training Accuracy: 0.78
Training loss = 0.016883128782113392
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01911436786254247
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.018319495916366578
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01851516177256902
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.017600518465042115
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.016718116998672487
step = 13, Training Accuracy: 0.78
Training loss = 0.017059786319732664
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.75125
params:  [0.36739375821775583, 0.15231281084656217, 0.4424027359708502, 0.19432695511904996, 0.01, 0.99, 0.7510591333710058, 0.4054445749655134, 0.9067512815073054, 0.2659634610088857, 0.46848362800176446, 0.6809455542743065, 0.8976310430536687, 0.01, 0.3859967096295573, 0.5562195566341931, 0.9347402533253305, 0.805334757432707, 0.26933137182314415, 0.7713905687940066, 0.42872797300345244, 0.2871346380400394, 0.4714230376485993, 0.8740721907170028]
Training loss = 0.020902481079101563
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.755
Training loss = 0.02039999544620514
step = 1, Training Accuracy: 0.75
Training loss = 0.01909167488416036
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.019372660120328268
step = 3, Training Accuracy: 0.75
Training loss = 0.017324240605036418
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.019496421019236248
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.016444201568762463
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.017074618339538574
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.017949109673500063
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.015489798386891683
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.014882393777370453
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.013921773036321004
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.014936576088269551
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.014096219440301259
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.013501259485880534
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.75625
params:  [0.01959421198565997, 0.16292653937371088, 0.2752290257466255, 0.3228144507099703, 0.01, 0.99, 0.47142556820467574, 0.47869159168770714, 0.6798427767624654, 0.4370902372925741, 0.3482348812015601, 0.20515416608576692, 0.33217621167245676, 0.16777042337601228, 0.625650154104748, 0.27197151148523857, 0.7665038506126434, 0.49272476541040977, 0.06921900743622333, 0.3607800030607656, 0.1622606418872402, 0.7386932335431841, 0.8333909186300308, 0.01]
Training loss = 0.023944438298543293
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7025
Training loss = 0.01966372976700465
step = 1, Training Accuracy: 0.75
Training loss = 0.02026306966940562
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.018023975789546967
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.016877785623073578
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.017796639303366342
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.014322941501935324
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.015317464172840118
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015318792859713236
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.015569479366143545
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.015201778610547384
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.01329074482123057
step = 11, Training Accuracy: 0.85
Training loss = 0.013206901053587596
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.011818402161200841
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.01246200606226921
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.75
params:  [0.14795445649131872, 0.4310860143845043, 0.4484180679174452, 0.5999343395483163, 0.41658873762976095, 0.5696202372056125, 0.9860108400789933, 0.7511159200518406, 0.99, 0.5164866597224961, 0.14597404773442552, 0.14257115154067443, 0.5027464533336192, 0.22673042068482102, 0.5333008101341463, 0.40970341548284894, 0.44200574978343343, 0.6817502595957361, 0.18687772876238448, 0.7242708753496063, 0.4343679532237374, 0.7506653128776486, 0.3377374594413821, 0.01]
Training loss = 0.02616030305624008
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.75625
Training loss = 0.021235935588677726
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.02091152548789978
step = 2, Training Accuracy: 0.71
Training loss = 0.021235236326853434
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.020024889012177784
step = 4, Training Accuracy: 0.7133333333333334
Training loss = 0.018308149377504985
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.02045232892036438
step = 6, Training Accuracy: 0.7
Training loss = 0.020198785463968912
step = 7, Training Accuracy: 0.7233333333333334
Training loss = 0.01804981768131256
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.020178700884183248
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.018778151869773864
step = 10, Training Accuracy: 0.75
Training loss = 0.01783843517303467
step = 11, Training Accuracy: 0.74
Training loss = 0.02052725632985433
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.017304595410823822
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.017198347548643748
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.745
params:  [0.5188194917208395, 0.30071282947710487, 0.6281604717256973, 0.37305294527920485, 0.37685477128183686, 0.3825807300039009, 0.99, 0.46240137688202704, 0.1914773034765102, 0.6832010832220897, 0.17011253681954328, 0.398017288998913, 0.665796593156221, 0.048017651134037734, 0.7705633347107677, 0.08798731405572757, 0.5109962224199547, 0.01, 0.516273148409399, 0.7748725356044338, 0.01, 0.5651228502321725, 0.42027622819027366, 0.6307213967391216]
Training loss = 0.02500960568586985
step = 0, Training Accuracy: 0.6633333333333333
Validation Accuracy: 0.7575
Training loss = 0.02038950483004252
step = 1, Training Accuracy: 0.7
Training loss = 0.020690822899341585
step = 2, Training Accuracy: 0.6933333333333334
Training loss = 0.02019534895817439
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.018766854107379913
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.01858758717775345
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.018598604400952658
step = 6, Training Accuracy: 0.76
Training loss = 0.018546972970167795
step = 7, Training Accuracy: 0.72
Training loss = 0.0187915833791097
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.017953492403030395
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01639491468667984
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.01776377171278
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01632420192162196
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01644726057847341
step = 13, Training Accuracy: 0.79
Training loss = 0.01662369360526403
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7675
params:  [0.08703771177324382, 0.5663972006349398, 0.42535825144280237, 0.2273446852235681, 0.01, 0.8295712896846248, 0.581708010569758, 0.5643265485472791, 0.7236265043935967, 0.49691223855717137, 0.6087178956780637, 0.6004981889249293, 0.6673650111094562, 0.31488784731245373, 0.036641195328853016, 0.5564465400634557, 0.16988188750789618, 0.39138388509912797, 0.08939017580333011, 0.323550186852402, 0.2716956191648898, 0.5783870075965377, 0.9116954885809974, 0.22872127145517163]
Training loss = 0.02573739449183146
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.74625
Training loss = 0.020158259471257527
step = 1, Training Accuracy: 0.7
Training loss = 0.021281842986742655
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.01824327290058136
step = 3, Training Accuracy: 0.78
Training loss = 0.019430435200532278
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.01913669168949127
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.019769932627677917
step = 6, Training Accuracy: 0.75
Training loss = 0.018484873175621034
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.018961976865927377
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.0200542938709259
step = 9, Training Accuracy: 0.74
Training loss = 0.017351803779602052
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01569366047779719
step = 11, Training Accuracy: 0.81
Training loss = 0.01552530566851298
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.016353820860385896
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.016440131564935047
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.76125
params:  [0.3018432440311667, 0.574724219103867, 0.11769641730464464, 0.01, 0.30857283120372364, 0.43086999841305124, 0.62996196636831, 0.99, 0.7092081268836962, 0.8625642245521834, 0.03295363408190455, 0.18238319998571353, 0.9829330448584097, 0.01, 0.474030306591572, 0.22357162606166994, 0.36527086524202274, 0.5315025788619089, 0.4842842947397439, 0.5139998744111063, 0.01, 0.780186264830504, 0.3983830959663699, 0.8651761650017538]
Training loss = 0.023020386695861816
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.755
Training loss = 0.021743610401948294
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.021073852181434632
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.0218327130873998
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.022118829488754273
step = 4, Training Accuracy: 0.72
Training loss = 0.02149362176656723
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.02018067757288615
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.019240285654862722
step = 7, Training Accuracy: 0.72
Training loss = 0.019107969800631206
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.021079188982645672
step = 9, Training Accuracy: 0.71
Training loss = 0.01785037577152252
step = 10, Training Accuracy: 0.79
Training loss = 0.017865414321422576
step = 11, Training Accuracy: 0.75
Training loss = 0.018817942440509796
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.01740394761164983
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.020434510310490928
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.745
params:  [0.5307358039043946, 0.2983379807720078, 0.5921704488884758, 0.05775297208070898, 0.18271487403724776, 0.9127245229790271, 0.415630476652992, 0.4500938193572402, 0.9868603436119904, 0.4401179316831422, 0.7661985675052295, 0.99, 0.808927656467171, 0.4616797969315375, 0.1653487536813906, 0.16389430288690587, 0.644887199900259, 0.46632131678692634, 0.149406575970357, 0.4169992123149865, 0.01, 0.4609728618124652, 0.33610560475030715, 0.29494171641213984]
Training loss = 0.01880545973777771
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77
Training loss = 0.018737154205640157
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01705388496319453
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018453693290551503
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016618691384792328
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.013544362684090932
step = 5, Training Accuracy: 0.8533333333333334
Training loss = 0.015258720715840658
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014162143270174662
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.015251727898915608
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.014838303327560426
step = 9, Training Accuracy: 0.83
Training loss = 0.014194158414999644
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.014700661698977152
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.012947110583384831
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.015988679975271224
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.013309555848439535
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.735
[[0.01, 0.49099864663122983, 0.5694691376448697, 0.36676650118623777, 0.01, 0.3800736546684577, 0.6117336135092296, 0.7113222808189507, 0.9007315160470983, 0.19100376718959108, 0.6839632109019338, 0.33848235781270286, 0.40288429259227854, 0.01, 0.11152872760906873, 0.5784635879199014, 0.3576891456982781, 0.7309713605234873, 0.16175833539433834, 0.5919475585200975, 0.07132584299873407, 0.48457504798258333, 0.47779167852425575, 0.34927666762882803], [0.36739375821775583, 0.15231281084656217, 0.4424027359708502, 0.19432695511904996, 0.01, 0.99, 0.7510591333710058, 0.4054445749655134, 0.9067512815073054, 0.2659634610088857, 0.46848362800176446, 0.6809455542743065, 0.8976310430536687, 0.01, 0.3859967096295573, 0.5562195566341931, 0.9347402533253305, 0.805334757432707, 0.26933137182314415, 0.7713905687940066, 0.42872797300345244, 0.2871346380400394, 0.4714230376485993, 0.8740721907170028], [0.01959421198565997, 0.16292653937371088, 0.2752290257466255, 0.3228144507099703, 0.01, 0.99, 0.47142556820467574, 0.47869159168770714, 0.6798427767624654, 0.4370902372925741, 0.3482348812015601, 0.20515416608576692, 0.33217621167245676, 0.16777042337601228, 0.625650154104748, 0.27197151148523857, 0.7665038506126434, 0.49272476541040977, 0.06921900743622333, 0.3607800030607656, 0.1622606418872402, 0.7386932335431841, 0.8333909186300308, 0.01], [0.14795445649131872, 0.4310860143845043, 0.4484180679174452, 0.5999343395483163, 0.41658873762976095, 0.5696202372056125, 0.9860108400789933, 0.7511159200518406, 0.99, 0.5164866597224961, 0.14597404773442552, 0.14257115154067443, 0.5027464533336192, 0.22673042068482102, 0.5333008101341463, 0.40970341548284894, 0.44200574978343343, 0.6817502595957361, 0.18687772876238448, 0.7242708753496063, 0.4343679532237374, 0.7506653128776486, 0.3377374594413821, 0.01], [0.5188194917208395, 0.30071282947710487, 0.6281604717256973, 0.37305294527920485, 0.37685477128183686, 0.3825807300039009, 0.99, 0.46240137688202704, 0.1914773034765102, 0.6832010832220897, 0.17011253681954328, 0.398017288998913, 0.665796593156221, 0.048017651134037734, 0.7705633347107677, 0.08798731405572757, 0.5109962224199547, 0.01, 0.516273148409399, 0.7748725356044338, 0.01, 0.5651228502321725, 0.42027622819027366, 0.6307213967391216], [0.08703771177324382, 0.5663972006349398, 0.42535825144280237, 0.2273446852235681, 0.01, 0.8295712896846248, 0.581708010569758, 0.5643265485472791, 0.7236265043935967, 0.49691223855717137, 0.6087178956780637, 0.6004981889249293, 0.6673650111094562, 0.31488784731245373, 0.036641195328853016, 0.5564465400634557, 0.16988188750789618, 0.39138388509912797, 0.08939017580333011, 0.323550186852402, 0.2716956191648898, 0.5783870075965377, 0.9116954885809974, 0.22872127145517163], [0.3018432440311667, 0.574724219103867, 0.11769641730464464, 0.01, 0.30857283120372364, 0.43086999841305124, 0.62996196636831, 0.99, 0.7092081268836962, 0.8625642245521834, 0.03295363408190455, 0.18238319998571353, 0.9829330448584097, 0.01, 0.474030306591572, 0.22357162606166994, 0.36527086524202274, 0.5315025788619089, 0.4842842947397439, 0.5139998744111063, 0.01, 0.780186264830504, 0.3983830959663699, 0.8651761650017538], [0.5307358039043946, 0.2983379807720078, 0.5921704488884758, 0.05775297208070898, 0.18271487403724776, 0.9127245229790271, 0.415630476652992, 0.4500938193572402, 0.9868603436119904, 0.4401179316831422, 0.7661985675052295, 0.99, 0.808927656467171, 0.4616797969315375, 0.1653487536813906, 0.16389430288690587, 0.644887199900259, 0.46632131678692634, 0.149406575970357, 0.4169992123149865, 0.01, 0.4609728618124652, 0.33610560475030715, 0.29494171641213984]]
3  	8     	0.751406	0.00956959	0.735  	0.7675 
params:  [0.2893999251512786, 0.24523104932906276, 0.7680627940672102, 0.3482166960621, 0.05820463635928419, 0.39326050309844857, 0.7722273412820304, 0.2927906630990976, 0.6449807498785003, 0.16524562872141046, 0.38610641492853154, 0.2740681658245411, 0.7749106239758858, 0.3732659888142295, 0.613724782370769, 0.2675997271007921, 0.01, 0.01, 0.1947584899013606, 0.99, 0.6634739103158074, 0.8056560707932225, 0.5192016282667515, 0.6294984647551699]
Training loss = 0.02331503470738729
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.7425
Training loss = 0.020082860589027404
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.019066693981488546
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.01836652199427287
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.0191666250427564
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01648719886938731
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.018157151142756144
step = 6, Training Accuracy: 0.76
Training loss = 0.016711709201335908
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015132373770078023
step = 8, Training Accuracy: 0.8
Training loss = 0.017026530901590984
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.015303548574447632
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.014274822324514389
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.015652200480302175
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014396149863799413
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.014579973071813584
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74625
params:  [0.41298877918323984, 0.4787005167757402, 0.5517094874431819, 0.511791855087905, 0.0560689781574574, 0.4843499654320054, 0.8508741669878424, 0.5618067390300576, 0.40053529072477734, 0.3423190041851708, 0.4254200857920609, 0.625855302552597, 0.6220042605391028, 0.06027921789384401, 0.6336733462730187, 0.23643662353956135, 0.519295881669829, 0.10569413964296567, 0.3628816716184989, 0.9028294381765509, 0.01, 0.6322381330392457, 0.6803899106363895, 0.488830336901363]
Training loss = 0.02173789213101069
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.73125
Training loss = 0.021833436489105226
step = 1, Training Accuracy: 0.77
Training loss = 0.019998721778392792
step = 2, Training Accuracy: 0.7166666666666667
Training loss = 0.02199111372232437
step = 3, Training Accuracy: 0.6966666666666667
Training loss = 0.020427560806274413
step = 4, Training Accuracy: 0.71
Training loss = 0.02150208463271459
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.01810163011153539
step = 6, Training Accuracy: 0.74
Training loss = 0.01824575940767924
step = 7, Training Accuracy: 0.77
Training loss = 0.020466620127360027
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.01812451700369517
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.02093494345744451
step = 10, Training Accuracy: 0.7
Training loss = 0.02043000260988871
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01765620897213618
step = 12, Training Accuracy: 0.78
Training loss = 0.017761395176251728
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.02100143978993098
step = 14, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.72625
params:  [0.6588733033097741, 0.3786989911234114, 0.6553185582424186, 0.38854340487934313, 0.18117572905582402, 0.6402200625817102, 0.8871823757001118, 0.37045439682137093, 0.6552679752090482, 0.8786698500119658, 0.4875680527612952, 0.6605172350572895, 0.7469656992913971, 0.01, 0.6349486296955986, 0.05314684762557331, 0.44672774830103307, 0.34841730092771084, 0.5259739711826394, 0.7510883215035544, 0.27959914115419526, 0.7479095024784188, 0.5374191532939366, 0.9645982053340072]
Training loss = 0.023719754715760547
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.75625
Training loss = 0.01932749589284261
step = 1, Training Accuracy: 0.74
Training loss = 0.02004584530989329
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.018813560207684835
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017825061678886412
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.018095911145210267
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.01698431561390559
step = 6, Training Accuracy: 0.78
Training loss = 0.01882866770029068
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.02006738195816676
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.016502803961435955
step = 9, Training Accuracy: 0.78
Training loss = 0.017429205377896627
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.016694659491380055
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018139394521713256
step = 12, Training Accuracy: 0.79
Training loss = 0.01594186027844747
step = 13, Training Accuracy: 0.77
Training loss = 0.015689890384674072
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.765
params:  [0.5092881988256649, 0.3647831662368041, 0.8781373312722973, 0.3804335116329508, 0.01, 0.38040487655714683, 0.5456297649481952, 0.6045049354751627, 0.5702635478685986, 0.7208017942661306, 0.3026096347060126, 0.5539821579186881, 0.6138025263784564, 0.02523530144294421, 0.37298124084880424, 0.3063427640669843, 0.3076184019632486, 0.3385410000750847, 0.10053429748767734, 0.5304378681907148, 0.1590726751227349, 0.597222834437443, 0.9597086726994417, 0.5341867355498205]
Training loss = 0.021734145085016886
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.75625
Training loss = 0.02127809335788091
step = 1, Training Accuracy: 0.72
Training loss = 0.02024400552113851
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.019323578874270123
step = 3, Training Accuracy: 0.7166666666666667
Training loss = 0.020126067797342936
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.02032672921816508
step = 5, Training Accuracy: 0.74
Training loss = 0.019351640343666078
step = 6, Training Accuracy: 0.73
Training loss = 0.016827059189478556
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.019242702921231588
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.018302545249462128
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.019274495939413706
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.016805126070976256
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017019857863585154
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017513086001078287
step = 13, Training Accuracy: 0.76
Training loss = 0.017945290903250376
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.74
params:  [0.4214324143914839, 0.22113366424252365, 0.3204623908444957, 0.6608895702201059, 0.05111072723715765, 0.4582513914730243, 0.8585501947182631, 0.3690882278716998, 0.36075795559372636, 0.810181502671843, 0.1850770291523616, 0.5237487751974897, 0.30589139361694984, 0.2943112084076364, 0.11938534699831188, 0.3958518825225447, 0.1179960493196116, 0.01, 0.01601446588314548, 0.5823850331997374, 0.1402810684831195, 0.8824719973067239, 0.42413348574225135, 0.5099487329229233]
Training loss = 0.02088995506366094
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.75625
Training loss = 0.018076187918583553
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.018293724556763966
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01835180213054021
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016806703805923463
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.017303730348745983
step = 5, Training Accuracy: 0.79
Training loss = 0.01585869203011195
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.015139041791359583
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.014106030563513438
step = 8, Training Accuracy: 0.82
Training loss = 0.015116378962993622
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015470216969648997
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014757415850957235
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.015402702192465464
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.015373090008894603
step = 13, Training Accuracy: 0.82
Training loss = 0.012970880965391795
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7625
params:  [0.5005025202699872, 0.3994275358688993, 0.9776093546728624, 0.37845498800978594, 0.01, 0.5056763447436994, 0.8560228417912565, 0.22225236784611901, 0.30320967738011817, 0.8066310218452857, 0.5319177177085441, 0.48468412991276044, 0.9285925657563712, 0.04682559003202906, 0.5048585034963915, 0.01, 0.539644470007804, 0.3393669232013857, 0.41250576019611546, 0.39563221121768316, 0.1409978672231792, 0.6803095055936159, 0.2819356280755503, 0.4799722582928841]
Training loss = 0.016563244611024857
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.78125
Training loss = 0.016208526988824207
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.013540215889612834
step = 2, Training Accuracy: 0.83
Training loss = 0.013353275756041208
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.012098604043324789
step = 4, Training Accuracy: 0.87
Training loss = 0.010788887739181519
step = 5, Training Accuracy: 0.8633333333333333
Training loss = 0.010342084020376206
step = 6, Training Accuracy: 0.87
Training loss = 0.010838565280040105
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.012772582521041234
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.01134147067864736
step = 9, Training Accuracy: 0.86
Training loss = 0.008529377231995264
step = 10, Training Accuracy: 0.8866666666666667
Training loss = 0.011688259343306223
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.012546777526537578
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.010694646139939626
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.01102070152759552
step = 14, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.785
params:  [0.24367275066497163, 0.4515999248073289, 0.4168227750842022, 0.4628105090720904, 0.6601728937375315, 0.5098350355270836, 0.8023417838994809, 0.4557860937914204, 0.4586115798105578, 0.4570243869707952, 0.534028355589033, 0.2863564982953998, 0.9046011500504257, 0.07780597403617594, 0.15076934176554146, 0.28513423092525353, 0.3733455020509859, 0.01, 0.02718315309592584, 0.9389389754637945, 0.07573216551672413, 0.7951397091204477, 0.9820096179738216, 0.4774546494214166]
Training loss = 0.02528998593489329
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.7675
Training loss = 0.023040971159934996
step = 1, Training Accuracy: 0.71
Training loss = 0.020350620845953623
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.023386152784029644
step = 3, Training Accuracy: 0.72
Training loss = 0.019412057797114055
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.019644516905148825
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.0201042906443278
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.019919991791248322
step = 7, Training Accuracy: 0.72
Training loss = 0.019787660439809163
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.017402938604354858
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.018526928623517354
step = 10, Training Accuracy: 0.72
Training loss = 0.01782495379447937
step = 11, Training Accuracy: 0.77
Training loss = 0.019381978511810304
step = 12, Training Accuracy: 0.73
Training loss = 0.016961669425169627
step = 13, Training Accuracy: 0.78
Training loss = 0.01836118280887604
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7625
params:  [0.26921817027502726, 0.42581483847399415, 0.99, 0.01, 0.3680694182568413, 0.432103623930323, 0.99, 0.39308830126107247, 0.37631388240517916, 0.7707381503699242, 0.3279938921704025, 0.6918957271104522, 0.7227035850764383, 0.25606468137398636, 0.6858425440771301, 0.4733445361550873, 0.5588599401678845, 0.27380946801628325, 0.3374486113847291, 0.5918408116671903, 0.01, 0.94578132010806, 0.3862842746829197, 0.7330059782575661]
Training loss = 0.019367639223734537
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.755
Training loss = 0.0185741729537646
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017192196349302927
step = 2, Training Accuracy: 0.77
Training loss = 0.01661228507757187
step = 3, Training Accuracy: 0.8
Training loss = 0.01545349195599556
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015854169030984244
step = 5, Training Accuracy: 0.81
Training loss = 0.014510922431945801
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014103329181671143
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.014345326522986094
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013052472869555156
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013763702710469564
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.014390569627285004
step = 11, Training Accuracy: 0.84
Training loss = 0.013352025548617046
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.013167794793844223
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.010606478949387868
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.76375
[[0.2893999251512786, 0.24523104932906276, 0.7680627940672102, 0.3482166960621, 0.05820463635928419, 0.39326050309844857, 0.7722273412820304, 0.2927906630990976, 0.6449807498785003, 0.16524562872141046, 0.38610641492853154, 0.2740681658245411, 0.7749106239758858, 0.3732659888142295, 0.613724782370769, 0.2675997271007921, 0.01, 0.01, 0.1947584899013606, 0.99, 0.6634739103158074, 0.8056560707932225, 0.5192016282667515, 0.6294984647551699], [0.41298877918323984, 0.4787005167757402, 0.5517094874431819, 0.511791855087905, 0.0560689781574574, 0.4843499654320054, 0.8508741669878424, 0.5618067390300576, 0.40053529072477734, 0.3423190041851708, 0.4254200857920609, 0.625855302552597, 0.6220042605391028, 0.06027921789384401, 0.6336733462730187, 0.23643662353956135, 0.519295881669829, 0.10569413964296567, 0.3628816716184989, 0.9028294381765509, 0.01, 0.6322381330392457, 0.6803899106363895, 0.488830336901363], [0.6588733033097741, 0.3786989911234114, 0.6553185582424186, 0.38854340487934313, 0.18117572905582402, 0.6402200625817102, 0.8871823757001118, 0.37045439682137093, 0.6552679752090482, 0.8786698500119658, 0.4875680527612952, 0.6605172350572895, 0.7469656992913971, 0.01, 0.6349486296955986, 0.05314684762557331, 0.44672774830103307, 0.34841730092771084, 0.5259739711826394, 0.7510883215035544, 0.27959914115419526, 0.7479095024784188, 0.5374191532939366, 0.9645982053340072], [0.5092881988256649, 0.3647831662368041, 0.8781373312722973, 0.3804335116329508, 0.01, 0.38040487655714683, 0.5456297649481952, 0.6045049354751627, 0.5702635478685986, 0.7208017942661306, 0.3026096347060126, 0.5539821579186881, 0.6138025263784564, 0.02523530144294421, 0.37298124084880424, 0.3063427640669843, 0.3076184019632486, 0.3385410000750847, 0.10053429748767734, 0.5304378681907148, 0.1590726751227349, 0.597222834437443, 0.9597086726994417, 0.5341867355498205], [0.4214324143914839, 0.22113366424252365, 0.3204623908444957, 0.6608895702201059, 0.05111072723715765, 0.4582513914730243, 0.8585501947182631, 0.3690882278716998, 0.36075795559372636, 0.810181502671843, 0.1850770291523616, 0.5237487751974897, 0.30589139361694984, 0.2943112084076364, 0.11938534699831188, 0.3958518825225447, 0.1179960493196116, 0.01, 0.01601446588314548, 0.5823850331997374, 0.1402810684831195, 0.8824719973067239, 0.42413348574225135, 0.5099487329229233], [0.5005025202699872, 0.3994275358688993, 0.9776093546728624, 0.37845498800978594, 0.01, 0.5056763447436994, 0.8560228417912565, 0.22225236784611901, 0.30320967738011817, 0.8066310218452857, 0.5319177177085441, 0.48468412991276044, 0.9285925657563712, 0.04682559003202906, 0.5048585034963915, 0.01, 0.539644470007804, 0.3393669232013857, 0.41250576019611546, 0.39563221121768316, 0.1409978672231792, 0.6803095055936159, 0.2819356280755503, 0.4799722582928841], [0.24367275066497163, 0.4515999248073289, 0.4168227750842022, 0.4628105090720904, 0.6601728937375315, 0.5098350355270836, 0.8023417838994809, 0.4557860937914204, 0.4586115798105578, 0.4570243869707952, 0.534028355589033, 0.2863564982953998, 0.9046011500504257, 0.07780597403617594, 0.15076934176554146, 0.28513423092525353, 0.3733455020509859, 0.01, 0.02718315309592584, 0.9389389754637945, 0.07573216551672413, 0.7951397091204477, 0.9820096179738216, 0.4774546494214166], [0.26921817027502726, 0.42581483847399415, 0.99, 0.01, 0.3680694182568413, 0.432103623930323, 0.99, 0.39308830126107247, 0.37631388240517916, 0.7707381503699242, 0.3279938921704025, 0.6918957271104522, 0.7227035850764383, 0.25606468137398636, 0.6858425440771301, 0.4733445361550873, 0.5588599401678845, 0.27380946801628325, 0.3374486113847291, 0.5918408116671903, 0.01, 0.94578132010806, 0.3862842746829197, 0.7330059782575661]]
4  	8     	0.756406	0.0169609 	0.72625	0.785  
params:  [0.7916074855838398, 0.38602814625696125, 0.99, 0.144044712332414, 0.237169422475059, 0.20742926803049028, 0.8464217368491984, 0.40491713815233715, 0.3342153357037307, 0.7347340543546141, 0.8219816048595279, 0.6214250452242378, 0.7883776313510765, 0.07433650242087225, 0.48377431994223236, 0.4983261341199515, 0.230246322976504, 0.2204138410318235, 0.46757781554323263, 0.5972828883195694, 0.2361703040599268, 0.6733702825151062, 0.368357152033859, 0.6853745245447549]
Training loss = 0.020695986449718474
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.78
Training loss = 0.018944995005925495
step = 1, Training Accuracy: 0.72
Training loss = 0.018185536762078604
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.017263986070950827
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017368767460187277
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01612358073393504
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.014697886804739635
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014326014816761016
step = 7, Training Accuracy: 0.78
Training loss = 0.014943756461143494
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.01418275997042656
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.013485922912756602
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.015179569025834401
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01632197380065918
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.014614028334617614
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.015987497468789417
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.79625
params:  [0.3907336816022373, 0.38803544960638386, 0.7119539277183898, 0.14850266390947608, 0.1881819170801753, 0.6286145497971435, 0.7808756219802322, 0.14833912017212209, 0.28347732455220076, 0.99, 0.5772427178305628, 0.22421556233518936, 0.7105865558257005, 0.046498271093868504, 0.7497934588773141, 0.5721992172026982, 0.4750760118332465, 0.45837403029733104, 0.34303635445530134, 0.99, 0.2360892153233315, 0.8977005710980557, 0.99, 0.29302155187726736]
Training loss = 0.026707921624183655
step = 0, Training Accuracy: 0.65
Validation Accuracy: 0.77625
Training loss = 0.023902876178423564
step = 1, Training Accuracy: 0.6866666666666666
Training loss = 0.022374174197514852
step = 2, Training Accuracy: 0.6766666666666666
Training loss = 0.023272822201251982
step = 3, Training Accuracy: 0.69
Training loss = 0.0237339190642039
step = 4, Training Accuracy: 0.66
Training loss = 0.022010228633880614
step = 5, Training Accuracy: 0.71
Training loss = 0.020031328996022543
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.022689069012800853
step = 7, Training Accuracy: 0.6733333333333333
Training loss = 0.02275369067986806
step = 8, Training Accuracy: 0.7
Training loss = 0.0219063671429952
step = 9, Training Accuracy: 0.6666666666666666
Training loss = 0.020689846376578013
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.022106141249338785
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.020484504699707032
step = 12, Training Accuracy: 0.73
Training loss = 0.021462578376134238
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.020769051412741345
step = 14, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.77875
params:  [0.6005741993736752, 0.44225684829112466, 0.8067518554989097, 0.6728257166514647, 0.3384568461024038, 0.9265354761128615, 0.7867602731351827, 0.4479411539028619, 0.4067786098989584, 0.39581119905208934, 0.40201614466732716, 0.1774732617181925, 0.47260857676858553, 0.23288292247536818, 0.6611701382712433, 0.01, 0.4416978031995817, 0.15971514509393028, 0.2910486859351994, 0.35391446926537484, 0.3682914756434106, 0.3182923303639871, 0.3626284564191165, 0.6021208537414091]
Training loss = 0.019970393081506093
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7725
Training loss = 0.019257638355096182
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.01794328232606252
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.018501145243644716
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01697658807039261
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016844149629275003
step = 5, Training Accuracy: 0.77
Training loss = 0.016985618472099305
step = 6, Training Accuracy: 0.79
Training loss = 0.01638920545578003
step = 7, Training Accuracy: 0.81
Training loss = 0.01667266885439555
step = 8, Training Accuracy: 0.8
Training loss = 0.015512040654818217
step = 9, Training Accuracy: 0.8
Training loss = 0.014333444734414419
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.012670301993687948
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.013640762915213904
step = 12, Training Accuracy: 0.82
Training loss = 0.015127144157886505
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01357852816581726
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.775
params:  [0.52791612190415, 0.16970864714955095, 0.9253886242010774, 0.3483643444735911, 0.057143137968896786, 0.9854822484255201, 0.4757438212852666, 0.31348395365167603, 0.49665143591179517, 0.5538832743345564, 0.6732416695567295, 0.8601225534795447, 0.8897822802122367, 0.01, 0.6859396365781092, 0.010680967475932696, 0.46770119413344613, 0.01, 0.22778257821695902, 0.2917619344972072, 0.01630136361623616, 0.7880335789759203, 0.3721745131981523, 0.99]
Training loss = 0.015049210786819457
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7775
Training loss = 0.013006732513507207
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.012295004824797312
step = 2, Training Accuracy: 0.83
Training loss = 0.01341354454557101
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.011832882811625799
step = 4, Training Accuracy: 0.85
Training loss = 0.010681102176507314
step = 5, Training Accuracy: 0.8633333333333333
Training loss = 0.008506905138492584
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.00942277063926061
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.009165776669979095
step = 8, Training Accuracy: 0.88
Training loss = 0.009134414121508599
step = 9, Training Accuracy: 0.9
Training loss = 0.007246760527292887
step = 10, Training Accuracy: 0.8933333333333333
Training loss = 0.009909502739707629
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.008673221866289774
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.007787493367989858
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.0075257200996081035
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.77125
params:  [0.01, 0.10469421818614544, 0.8964834820285134, 0.14953302413815442, 0.01, 0.6419379661242508, 0.5143110373972493, 0.3871630691664798, 0.5881616436805839, 0.6141727931631147, 0.1332788399089928, 0.5472212857464209, 0.9434624533436825, 0.4188871813975247, 0.7958297434450881, 0.362770392706522, 0.9759120229782832, 0.26997932141553493, 0.7252181443116514, 0.49005439596718436, 0.2347450500356847, 0.5895242344680494, 0.7040659669332456, 0.37560432457474063]
Training loss = 0.02094295104344686
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.78625
Training loss = 0.01856251319249471
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01553795983393987
step = 2, Training Accuracy: 0.8
Training loss = 0.014402954677740732
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.015219519833723704
step = 4, Training Accuracy: 0.82
Training loss = 0.014911878705024719
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.012831881095965704
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.012625941187143325
step = 7, Training Accuracy: 0.85
Training loss = 0.013193140079577764
step = 8, Training Accuracy: 0.83
Training loss = 0.012034545838832855
step = 9, Training Accuracy: 0.86
Training loss = 0.013546643654505412
step = 10, Training Accuracy: 0.83
Training loss = 0.013103353480497995
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.011844149678945542
step = 12, Training Accuracy: 0.86
Training loss = 0.010155048370361328
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.009747605919837952
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.76625
params:  [0.5799388368247861, 0.7962791942876926, 0.99, 0.1765107543766434, 0.32264984383576234, 0.7836762313768686, 0.99, 0.47397173753760236, 0.5683186201878222, 0.6508335811162105, 0.23263763185789466, 0.41520185717189817, 0.99, 0.01, 0.327123511397432, 0.01, 0.7081489988857399, 0.34053470253360135, 0.5579413988164106, 0.6842704739088283, 0.27714678941997206, 0.7978720697180541, 0.04192695938423846, 0.924259944038603]
Training loss = 0.023997580111026765
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.76625
Training loss = 0.021380024154980977
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.019119847317536673
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.018541809618473053
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017482179900010428
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.019087293843428294
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.018652307788530987
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.01652377814054489
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01689579665660858
step = 8, Training Accuracy: 0.76
Training loss = 0.017999010682106017
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.014603909154733022
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016670292913913726
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.014895467162132263
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016669446229934694
step = 13, Training Accuracy: 0.75
Training loss = 0.013496109396219253
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.75875
params:  [0.6230483436649112, 0.8928958094222186, 0.99, 0.4294218551010121, 0.03963229023083727, 0.4397420457694715, 0.99, 0.2976324895898897, 0.22815870977116676, 0.6490911034517854, 0.6440303657965571, 0.6029682317621954, 0.6449332075661565, 0.23858024078908657, 0.47335096314631153, 0.01, 0.14709612698005242, 0.4472864357742835, 0.4458862921367866, 0.5935842098064463, 0.2143315963255053, 0.99, 0.10539324343953027, 0.99]
Training loss = 0.019528433183828988
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.77375
Training loss = 0.019174981117248534
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018294352690378823
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.016534265677134195
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.01782865862051646
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01529795080423355
step = 5, Training Accuracy: 0.79
Training loss = 0.015825304786364236
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.016312183539072673
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.0180320997039477
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.015675500979026157
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015326113303502401
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.015213830868403116
step = 11, Training Accuracy: 0.8
Training loss = 0.015536454419294993
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015844628314177194
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.013478191792964936
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.755
params:  [0.9534182832809339, 0.30031992210292985, 0.7286289552274436, 0.01, 0.20179682017606826, 0.7692828648546051, 0.6868033013411233, 0.17139616891764942, 0.6123837327278213, 0.841346017826174, 0.4387405441276433, 0.31277855712162583, 0.6716838843122266, 0.24584251590740858, 0.652616946090883, 0.23359825704452988, 0.3228375839547183, 0.2251535904450299, 0.7434874413215171, 0.5064661088177269, 0.5191504039493078, 0.99, 0.44978003337573225, 0.5918954555414513]
Training loss = 0.019017807692289352
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7575
Training loss = 0.016545238693555196
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.017996662358442942
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.016261758208274843
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.015373453199863434
step = 4, Training Accuracy: 0.81
Training loss = 0.014728094140688578
step = 5, Training Accuracy: 0.8366666666666667
Training loss = 0.015384015341599782
step = 6, Training Accuracy: 0.78
Training loss = 0.014985894362131755
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.013819343745708465
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.01480098009109497
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.013715112805366516
step = 10, Training Accuracy: 0.8433333333333334
Training loss = 0.012740053832530976
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.012663446515798569
step = 12, Training Accuracy: 0.85
Training loss = 0.0142523393034935
step = 13, Training Accuracy: 0.81
Training loss = 0.011773818135261536
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.75625
[[0.7916074855838398, 0.38602814625696125, 0.99, 0.144044712332414, 0.237169422475059, 0.20742926803049028, 0.8464217368491984, 0.40491713815233715, 0.3342153357037307, 0.7347340543546141, 0.8219816048595279, 0.6214250452242378, 0.7883776313510765, 0.07433650242087225, 0.48377431994223236, 0.4983261341199515, 0.230246322976504, 0.2204138410318235, 0.46757781554323263, 0.5972828883195694, 0.2361703040599268, 0.6733702825151062, 0.368357152033859, 0.6853745245447549], [0.3907336816022373, 0.38803544960638386, 0.7119539277183898, 0.14850266390947608, 0.1881819170801753, 0.6286145497971435, 0.7808756219802322, 0.14833912017212209, 0.28347732455220076, 0.99, 0.5772427178305628, 0.22421556233518936, 0.7105865558257005, 0.046498271093868504, 0.7497934588773141, 0.5721992172026982, 0.4750760118332465, 0.45837403029733104, 0.34303635445530134, 0.99, 0.2360892153233315, 0.8977005710980557, 0.99, 0.29302155187726736], [0.6005741993736752, 0.44225684829112466, 0.8067518554989097, 0.6728257166514647, 0.3384568461024038, 0.9265354761128615, 0.7867602731351827, 0.4479411539028619, 0.4067786098989584, 0.39581119905208934, 0.40201614466732716, 0.1774732617181925, 0.47260857676858553, 0.23288292247536818, 0.6611701382712433, 0.01, 0.4416978031995817, 0.15971514509393028, 0.2910486859351994, 0.35391446926537484, 0.3682914756434106, 0.3182923303639871, 0.3626284564191165, 0.6021208537414091], [0.52791612190415, 0.16970864714955095, 0.9253886242010774, 0.3483643444735911, 0.057143137968896786, 0.9854822484255201, 0.4757438212852666, 0.31348395365167603, 0.49665143591179517, 0.5538832743345564, 0.6732416695567295, 0.8601225534795447, 0.8897822802122367, 0.01, 0.6859396365781092, 0.010680967475932696, 0.46770119413344613, 0.01, 0.22778257821695902, 0.2917619344972072, 0.01630136361623616, 0.7880335789759203, 0.3721745131981523, 0.99], [0.01, 0.10469421818614544, 0.8964834820285134, 0.14953302413815442, 0.01, 0.6419379661242508, 0.5143110373972493, 0.3871630691664798, 0.5881616436805839, 0.6141727931631147, 0.1332788399089928, 0.5472212857464209, 0.9434624533436825, 0.4188871813975247, 0.7958297434450881, 0.362770392706522, 0.9759120229782832, 0.26997932141553493, 0.7252181443116514, 0.49005439596718436, 0.2347450500356847, 0.5895242344680494, 0.7040659669332456, 0.37560432457474063], [0.5799388368247861, 0.7962791942876926, 0.99, 0.1765107543766434, 0.32264984383576234, 0.7836762313768686, 0.99, 0.47397173753760236, 0.5683186201878222, 0.6508335811162105, 0.23263763185789466, 0.41520185717189817, 0.99, 0.01, 0.327123511397432, 0.01, 0.7081489988857399, 0.34053470253360135, 0.5579413988164106, 0.6842704739088283, 0.27714678941997206, 0.7978720697180541, 0.04192695938423846, 0.924259944038603], [0.6230483436649112, 0.8928958094222186, 0.99, 0.4294218551010121, 0.03963229023083727, 0.4397420457694715, 0.99, 0.2976324895898897, 0.22815870977116676, 0.6490911034517854, 0.6440303657965571, 0.6029682317621954, 0.6449332075661565, 0.23858024078908657, 0.47335096314631153, 0.01, 0.14709612698005242, 0.4472864357742835, 0.4458862921367866, 0.5935842098064463, 0.2143315963255053, 0.99, 0.10539324343953027, 0.99], [0.9534182832809339, 0.30031992210292985, 0.7286289552274436, 0.01, 0.20179682017606826, 0.7692828648546051, 0.6868033013411233, 0.17139616891764942, 0.6123837327278213, 0.841346017826174, 0.4387405441276433, 0.31277855712162583, 0.6716838843122266, 0.24584251590740858, 0.652616946090883, 0.23359825704452988, 0.3228375839547183, 0.2251535904450299, 0.7434874413215171, 0.5064661088177269, 0.5191504039493078, 0.99, 0.44978003337573225, 0.5918954555414513]]
5  	8     	0.769688	0.0129716 	0.755  	0.79625
params:  [0.9684768046023005, 0.35019589244466975, 0.99, 0.46970915130059127, 0.01, 0.7594772438249406, 0.7868265434955967, 0.18357937530439258, 0.036062466115807434, 0.8197825718811874, 0.8090436502127936, 0.6574753084818027, 0.8504622137720127, 0.279313154186265, 0.7057005442549577, 0.49643767293287283, 0.7627401916812664, 0.3050567660559063, 0.5130791924703471, 0.42479389715854, 0.5604011674755187, 0.7354032486842541, 0.6742375137833663, 0.3726238210112915]
Training loss = 0.016242037216822307
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.76375
Training loss = 0.014662186702092489
step = 1, Training Accuracy: 0.8
Training loss = 0.014467731366554897
step = 2, Training Accuracy: 0.78
Training loss = 0.014921369055906932
step = 3, Training Accuracy: 0.79
Training loss = 0.013828245500723521
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014316689670085907
step = 5, Training Accuracy: 0.82
Training loss = 0.013766118387381236
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.013172745953003566
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.011724811891714731
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.011963493625322978
step = 9, Training Accuracy: 0.87
Training loss = 0.009876107027133306
step = 10, Training Accuracy: 0.89
Training loss = 0.010410723139842351
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.01015550598502159
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.010982496291399002
step = 13, Training Accuracy: 0.85
Training loss = 0.011011256476243337
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.77125
params:  [0.46117777037599456, 0.061092958086063354, 0.9740262462919572, 0.5095503576995686, 0.31687266323958324, 0.02921188947257608, 0.99, 0.37410009865248595, 0.5141419249851656, 0.29861730406086423, 0.6235750574122942, 0.22723054927870728, 0.99, 0.01, 0.4291091552613039, 0.27045508705697097, 0.4497067553977525, 0.11984056539266999, 0.05456302071255886, 0.6946923339893121, 0.5646427348549382, 0.23135701267843867, 0.3079503380087113, 0.4420535474638764]
Training loss = 0.017620379229386647
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.78125
Training loss = 0.017028082311153412
step = 1, Training Accuracy: 0.78
Training loss = 0.014555018693208695
step = 2, Training Accuracy: 0.82
Training loss = 0.013850343227386475
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.013078892081975936
step = 4, Training Accuracy: 0.83
Training loss = 0.013210819959640503
step = 5, Training Accuracy: 0.8366666666666667
Training loss = 0.012494140962759654
step = 6, Training Accuracy: 0.86
Training loss = 0.010176687215765317
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.011578109413385392
step = 8, Training Accuracy: 0.86
Training loss = 0.010775761008262634
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011118482251962027
step = 10, Training Accuracy: 0.8666666666666667
Training loss = 0.009772123595078785
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.010481860488653183
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.00879206729431947
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.007852033078670502
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.76625
params:  [0.23933046501740618, 0.5929488159217874, 0.9341462542019245, 0.07660056110476024, 0.26029461815916427, 0.6514343975092772, 0.99, 0.28489672065649496, 0.2865950061557726, 0.17940308892478452, 0.6531736579150199, 0.45517356345128107, 0.9387139665264246, 0.04520149394149076, 0.1273749642725716, 0.6464680456497678, 0.15091587382454538, 0.01, 0.3471935783071706, 0.6336283869118123, 0.2339951188752471, 0.9392097200751283, 0.44046666489106034, 0.7194992890506915]
Training loss = 0.023155154983202617
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7575
Training loss = 0.020871627430121106
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.01580404669046402
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.017040360967318216
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.016169861306746802
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015400787095228831
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015343270798524222
step = 6, Training Accuracy: 0.79
Training loss = 0.015254410256942113
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.013547245760758718
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.01671779841184616
step = 9, Training Accuracy: 0.77
Training loss = 0.016077946027119955
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.015474194089571635
step = 11, Training Accuracy: 0.83
Training loss = 0.015384802222251892
step = 12, Training Accuracy: 0.78
Training loss = 0.014825468063354491
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01357130616903305
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7975
params:  [0.621007714246598, 0.2226694821930075, 0.5867810785302423, 0.029101482034321308, 0.07552228096315189, 0.7702009927985131, 0.99, 0.30297593326652844, 0.6861257571605697, 0.8164692234291816, 0.6550580862175687, 0.5142548379829418, 0.99, 0.01, 0.9305255486152111, 0.13938594204063726, 0.17500654021419446, 0.3326967337890617, 0.13599391367700486, 0.6639128859727107, 0.05606486499710553, 0.6396419202756315, 0.7027004482980439, 0.8256372377094466]
Training loss = 0.017791763246059418
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79
Training loss = 0.0194421116511027
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018807581464449566
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.014816827674706778
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.015112726191679637
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015164359509944915
step = 5, Training Accuracy: 0.81
Training loss = 0.013341688712437948
step = 6, Training Accuracy: 0.8
Training loss = 0.014639330605665843
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013322835365931193
step = 8, Training Accuracy: 0.84
Training loss = 0.014861342112223307
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.011692136079072952
step = 10, Training Accuracy: 0.85
Training loss = 0.01491654674212138
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.013582933843135834
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.011652198433876038
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.01512428472439448
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.77625
params:  [0.3738509883282739, 0.12407590606439584, 0.99, 0.2755218066179716, 0.06901894610522247, 0.4547233117184694, 0.8764771423340856, 0.5096272752224679, 0.30097033605349616, 0.40682382771642317, 0.15096779775105384, 0.28413690820174997, 0.99, 0.21187713396597035, 0.9742249133561781, 0.4476777932044547, 0.4405057501618948, 0.10461467442405187, 0.4617436750090014, 0.7803916758424752, 0.01, 0.5228564801703749, 0.5875023690789174, 0.3094748821578214]
Training loss = 0.014487006068229676
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.765
Training loss = 0.017340394059816997
step = 1, Training Accuracy: 0.83
Training loss = 0.01446701392531395
step = 2, Training Accuracy: 0.82
Training loss = 0.016583480536937714
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.014952750702699025
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.012811096409956615
step = 5, Training Accuracy: 0.8533333333333334
Training loss = 0.013899368842442831
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.013704092601935069
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.011759557624657949
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.012490311712026596
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013085408806800843
step = 10, Training Accuracy: 0.85
Training loss = 0.012689312249422073
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.011317270845174789
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.013901233623425166
step = 13, Training Accuracy: 0.88
Training loss = 0.010442926486333211
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.765
params:  [0.35612451513289556, 0.6056918640981384, 0.99, 0.01, 0.09076264754354826, 0.7365278673992834, 0.7784129173350597, 0.36596971670144124, 0.014929051538094817, 0.4753919629565422, 0.7134862882473592, 0.11007515311375465, 0.7365530988755145, 0.1186045272679965, 0.4669990568626182, 0.03243151852045334, 0.38663101627664936, 0.24281432847849913, 0.5575403734204489, 0.7269815297009625, 0.3881048818776515, 0.9018103364605541, 0.253363340430922, 0.6470570030585506]
Training loss = 0.020820664366086324
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.78
Training loss = 0.02369762976964315
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.019883613288402557
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.017608851492404938
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.015556922753651937
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.014304320613543193
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.015350803236166636
step = 6, Training Accuracy: 0.82
Training loss = 0.014192279328902562
step = 7, Training Accuracy: 0.8433333333333334
Training loss = 0.014086291193962097
step = 8, Training Accuracy: 0.82
Training loss = 0.01721289793650309
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.015227317611376445
step = 10, Training Accuracy: 0.8
Training loss = 0.01297032505273819
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.014612017770608266
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.0138113734126091
step = 13, Training Accuracy: 0.83
Training loss = 0.013801331222057343
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.76625
params:  [0.5996134458080523, 0.26923662831719153, 0.99, 0.14594956814664312, 0.1317605249839255, 0.3336213671016043, 0.7908355418376127, 0.516845198231817, 0.16444553620985475, 0.8526329980900053, 0.823714080306599, 0.5800938835437807, 0.771656457738903, 0.16425842929803408, 0.8938264591104456, 0.4747883269289792, 0.7038980894859246, 0.32309182817632753, 0.5840064908894225, 0.566030763812335, 0.343080297902213, 0.6826234270714228, 0.7539887230132103, 0.6478573002488747]
Training loss = 0.02018560270468394
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7925
Training loss = 0.020526799360911053
step = 1, Training Accuracy: 0.71
Training loss = 0.017357378005981444
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.016133098204930623
step = 3, Training Accuracy: 0.79
Training loss = 0.01780184845129649
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014333003759384155
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.014425830245018005
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01557662417491277
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01419696494936943
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.014838709980249404
step = 9, Training Accuracy: 0.81
Training loss = 0.016575009226799012
step = 10, Training Accuracy: 0.78
Training loss = 0.01648967166741689
step = 11, Training Accuracy: 0.8
Training loss = 0.015183437069257101
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013179265360037485
step = 13, Training Accuracy: 0.84
Training loss = 0.013448446492354075
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.79
params:  [0.4327582652295935, 0.6944688725787914, 0.9549380326508092, 0.06113678902772879, 0.12075504894355095, 0.6056462359526753, 0.848170632940027, 0.7124389640909587, 0.43028347459095084, 0.4451204009385259, 0.99, 0.36324804406374017, 0.7330807979678737, 0.21871150175995413, 0.9543563390628156, 0.3172418264647181, 0.5196398122975052, 0.4365835772195294, 0.6798304080956923, 0.797295008169918, 0.22166627059797214, 0.18302548270480845, 0.11805374593229145, 0.44306901703134993]
Training loss = 0.01898370881875356
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.79
Training loss = 0.017857874929904937
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.013632883826891581
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.014939303000768026
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015198701719443004
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.014939271807670594
step = 5, Training Accuracy: 0.82
Training loss = 0.014891550441582998
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.013160016139348347
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.015236128767331441
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013607127616802852
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.013524057269096374
step = 10, Training Accuracy: 0.8533333333333334
Training loss = 0.012227926750977834
step = 11, Training Accuracy: 0.85
Training loss = 0.012806868255138397
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.014043056964874267
step = 13, Training Accuracy: 0.82
Training loss = 0.013051543136437733
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.79125
[[0.9684768046023005, 0.35019589244466975, 0.99, 0.46970915130059127, 0.01, 0.7594772438249406, 0.7868265434955967, 0.18357937530439258, 0.036062466115807434, 0.8197825718811874, 0.8090436502127936, 0.6574753084818027, 0.8504622137720127, 0.279313154186265, 0.7057005442549577, 0.49643767293287283, 0.7627401916812664, 0.3050567660559063, 0.5130791924703471, 0.42479389715854, 0.5604011674755187, 0.7354032486842541, 0.6742375137833663, 0.3726238210112915], [0.46117777037599456, 0.061092958086063354, 0.9740262462919572, 0.5095503576995686, 0.31687266323958324, 0.02921188947257608, 0.99, 0.37410009865248595, 0.5141419249851656, 0.29861730406086423, 0.6235750574122942, 0.22723054927870728, 0.99, 0.01, 0.4291091552613039, 0.27045508705697097, 0.4497067553977525, 0.11984056539266999, 0.05456302071255886, 0.6946923339893121, 0.5646427348549382, 0.23135701267843867, 0.3079503380087113, 0.4420535474638764], [0.23933046501740618, 0.5929488159217874, 0.9341462542019245, 0.07660056110476024, 0.26029461815916427, 0.6514343975092772, 0.99, 0.28489672065649496, 0.2865950061557726, 0.17940308892478452, 0.6531736579150199, 0.45517356345128107, 0.9387139665264246, 0.04520149394149076, 0.1273749642725716, 0.6464680456497678, 0.15091587382454538, 0.01, 0.3471935783071706, 0.6336283869118123, 0.2339951188752471, 0.9392097200751283, 0.44046666489106034, 0.7194992890506915], [0.621007714246598, 0.2226694821930075, 0.5867810785302423, 0.029101482034321308, 0.07552228096315189, 0.7702009927985131, 0.99, 0.30297593326652844, 0.6861257571605697, 0.8164692234291816, 0.6550580862175687, 0.5142548379829418, 0.99, 0.01, 0.9305255486152111, 0.13938594204063726, 0.17500654021419446, 0.3326967337890617, 0.13599391367700486, 0.6639128859727107, 0.05606486499710553, 0.6396419202756315, 0.7027004482980439, 0.8256372377094466], [0.3738509883282739, 0.12407590606439584, 0.99, 0.2755218066179716, 0.06901894610522247, 0.4547233117184694, 0.8764771423340856, 0.5096272752224679, 0.30097033605349616, 0.40682382771642317, 0.15096779775105384, 0.28413690820174997, 0.99, 0.21187713396597035, 0.9742249133561781, 0.4476777932044547, 0.4405057501618948, 0.10461467442405187, 0.4617436750090014, 0.7803916758424752, 0.01, 0.5228564801703749, 0.5875023690789174, 0.3094748821578214], [0.35612451513289556, 0.6056918640981384, 0.99, 0.01, 0.09076264754354826, 0.7365278673992834, 0.7784129173350597, 0.36596971670144124, 0.014929051538094817, 0.4753919629565422, 0.7134862882473592, 0.11007515311375465, 0.7365530988755145, 0.1186045272679965, 0.4669990568626182, 0.03243151852045334, 0.38663101627664936, 0.24281432847849913, 0.5575403734204489, 0.7269815297009625, 0.3881048818776515, 0.9018103364605541, 0.253363340430922, 0.6470570030585506], [0.5996134458080523, 0.26923662831719153, 0.99, 0.14594956814664312, 0.1317605249839255, 0.3336213671016043, 0.7908355418376127, 0.516845198231817, 0.16444553620985475, 0.8526329980900053, 0.823714080306599, 0.5800938835437807, 0.771656457738903, 0.16425842929803408, 0.8938264591104456, 0.4747883269289792, 0.7038980894859246, 0.32309182817632753, 0.5840064908894225, 0.566030763812335, 0.343080297902213, 0.6826234270714228, 0.7539887230132103, 0.6478573002488747], [0.4327582652295935, 0.6944688725787914, 0.9549380326508092, 0.06113678902772879, 0.12075504894355095, 0.6056462359526753, 0.848170632940027, 0.7124389640909587, 0.43028347459095084, 0.4451204009385259, 0.99, 0.36324804406374017, 0.7330807979678737, 0.21871150175995413, 0.9543563390628156, 0.3172418264647181, 0.5196398122975052, 0.4365835772195294, 0.6798304080956923, 0.797295008169918, 0.22166627059797214, 0.18302548270480845, 0.11805374593229145, 0.44306901703134993]]
6  	8     	0.777969	0.0122145 	0.765  	0.7975 
params:  [0.4547655952874812, 0.28165898860798266, 0.9313497090353844, 0.09877739858393775, 0.24967746486496376, 0.6916135867405594, 0.99, 0.4257214368754503, 0.19390022902356963, 0.4087688945547061, 0.6642954384403978, 0.1675478210603933, 0.9104932877364046, 0.08072778117207509, 0.6188051243787557, 0.3807347582284515, 0.01, 0.13443597441535338, 0.04224432738021644, 0.5474947406862557, 0.1310079010276153, 0.9145625456850546, 0.1915213004747974, 0.900541543870349]
Training loss = 0.01379122351606687
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.77875
Training loss = 0.014162208139896392
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.012976870437463124
step = 2, Training Accuracy: 0.84
Training loss = 0.012810276051362356
step = 3, Training Accuracy: 0.8
Training loss = 0.013579551378885906
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.011799459209044775
step = 5, Training Accuracy: 0.83
Training loss = 0.008943885341286659
step = 6, Training Accuracy: 0.9
Training loss = 0.012194774945576985
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.010041142205397287
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.009446107844511668
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.010089970231056213
step = 10, Training Accuracy: 0.87
Training loss = 0.009514467294017474
step = 11, Training Accuracy: 0.87
Training loss = 0.010501348425944647
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.008822708129882813
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.010199526449044545
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.7675
params:  [0.4491086820559641, 0.5110112673570865, 0.8731044710710582, 0.18218755082068294, 0.3650997462797822, 0.21605293979567114, 0.99, 0.5389306222703668, 0.233419236545455, 0.17544182845835327, 0.5731093655427737, 0.5015190586068116, 0.99, 0.01, 0.49726851364050745, 0.694593240228059, 0.39460950335574463, 0.1463446569283618, 0.23974226312300628, 0.5161554718255529, 0.434586626846959, 0.99, 0.42466523379905513, 0.33664723473173774]
Training loss = 0.023154229323069254
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.7925
Training loss = 0.02188187559445699
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.01827932059764862
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.01733618090550105
step = 3, Training Accuracy: 0.77
Training loss = 0.01694865455230077
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.016872756481170655
step = 5, Training Accuracy: 0.79
Training loss = 0.016325881083806355
step = 6, Training Accuracy: 0.79
Training loss = 0.016583640178044638
step = 7, Training Accuracy: 0.8
Training loss = 0.018188485701878865
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01585794101158778
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.014650087257226309
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01708432654539744
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01583507239818573
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015647535820802052
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01460818295677503
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.80375
params:  [0.46673565977876436, 0.4524581956952515, 0.99, 0.18407950443766083, 0.01, 0.7988272990473315, 0.7725031874201638, 0.44508254846009104, 0.39547024291776, 0.01, 0.5763782370379694, 0.4210971943806549, 0.6462304696377787, 0.2248099558221034, 0.09715266481129547, 0.5964360926924384, 0.39891572745218307, 0.22704627741918626, 0.649080395792835, 0.7603421000437551, 0.24518981723775254, 0.7398003436107724, 0.6720523499236201, 0.5438036977087074]
Training loss = 0.020732980767885843
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.7975
Training loss = 0.018994742035865785
step = 1, Training Accuracy: 0.76
Training loss = 0.017923609614372255
step = 2, Training Accuracy: 0.76
Training loss = 0.018372803926467896
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01698396702607473
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01710997074842453
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01614625573158264
step = 6, Training Accuracy: 0.81
Training loss = 0.01653067777554194
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01638575777411461
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01618027885754903
step = 9, Training Accuracy: 0.79
Training loss = 0.014936095376809438
step = 10, Training Accuracy: 0.78
Training loss = 0.015941848754882814
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.014680629670619965
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.015189614395300548
step = 13, Training Accuracy: 0.81
Training loss = 0.013835170815388362
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78625
params:  [0.5276525019826608, 0.39720278997649616, 0.9043898479492474, 0.01, 0.01, 0.5761523622871182, 0.8963461011337522, 0.38611137170199117, 0.29201210299753866, 0.40827945667374743, 0.6401064156845556, 0.3677593771010182, 0.59998766268035, 0.01, 0.42760742933362667, 0.23458347443070654, 0.4253623917021472, 0.01, 0.5943349780281952, 0.4585632599330561, 0.17284405623672772, 0.7670717273372735, 0.01, 0.47476174770263724]
Training loss = 0.01889764944712321
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.79125
Training loss = 0.016488847434520722
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.01400173266728719
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014636397163073222
step = 3, Training Accuracy: 0.81
Training loss = 0.013307460397481919
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.013208609024683635
step = 5, Training Accuracy: 0.83
Training loss = 0.013079474866390228
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01139972562591235
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.012432815382877986
step = 8, Training Accuracy: 0.86
Training loss = 0.012438202202320099
step = 9, Training Accuracy: 0.86
Training loss = 0.012206736902395885
step = 10, Training Accuracy: 0.85
Training loss = 0.009976968864599863
step = 11, Training Accuracy: 0.88
Training loss = 0.01151965618133545
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.009887592196464538
step = 13, Training Accuracy: 0.88
Training loss = 0.009387685457865396
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.76375
params:  [0.5543142221000357, 0.21038090032019152, 0.99, 0.14840117843510553, 0.5588480728367612, 0.8188663732033883, 0.99, 0.40305956025570033, 0.20246762762894918, 0.6530789933361466, 0.8645799006645253, 0.3115976266840341, 0.5890457413106946, 0.36639414943621085, 0.3108058327285407, 0.41881553978163677, 0.7828279742162638, 0.0505288652801687, 0.4199492823240118, 0.6411730388248528, 0.17888864521405046, 0.3857840244300751, 0.7987487736612409, 0.6701104621524522]
Training loss = 0.02044792006413142
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.75875
Training loss = 0.0183017232020696
step = 1, Training Accuracy: 0.75
Training loss = 0.01836209903160731
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.015264309446016948
step = 3, Training Accuracy: 0.83
Training loss = 0.01493773361047109
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.015815945168336232
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.013885550796985627
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014239748865365982
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.014176063189903896
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.014423090070486068
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.012715701758861542
step = 10, Training Accuracy: 0.8466666666666667
Training loss = 0.014264083007971445
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.014595569372177123
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.015043148795763652
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.011695516308148702
step = 14, Training Accuracy: 0.86
Validation Accuracy: 0.785
params:  [0.29200000421851197, 0.44082630119215194, 0.99, 0.377436481900627, 0.01, 0.4756540281344428, 0.99, 0.6207305527978217, 0.1717973598964842, 0.22164370679755768, 0.57127821173485, 0.4144368678282941, 0.99, 0.10621141675908706, 0.4085763460497661, 0.7241081134422214, 0.14078599821697332, 0.07571478114505453, 0.49514934275992417, 0.947219257659554, 0.3337262965489811, 0.5186326276567831, 0.1320978530989807, 0.7024560932796862]
Training loss = 0.018955986201763152
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78375
Training loss = 0.017223569949467978
step = 1, Training Accuracy: 0.78
Training loss = 0.016133105953534444
step = 2, Training Accuracy: 0.77
Training loss = 0.016180777549743654
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.01642706722021103
step = 4, Training Accuracy: 0.8
Training loss = 0.014700215061505635
step = 5, Training Accuracy: 0.8
Training loss = 0.013965920011202494
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.015066576699415843
step = 7, Training Accuracy: 0.8
Training loss = 0.01433837076028188
step = 8, Training Accuracy: 0.83
Training loss = 0.01397234171628952
step = 9, Training Accuracy: 0.81
Training loss = 0.013928058048089345
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.014178647100925446
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.013575561741987863
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.01327253133058548
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.013161695996920268
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.78375
params:  [0.18353204062917858, 0.49961998021881854, 0.99, 0.01, 0.38561903462695835, 0.6743765487568376, 0.8885116241813884, 0.6447987957940126, 0.5673030903873751, 0.4141904488383375, 0.8063958087659858, 0.4547343219188101, 0.5352591290246742, 0.01, 0.6987095158753405, 0.5018914803029472, 0.01, 0.48362169881800393, 0.2413714941493689, 0.6297770192895416, 0.10664356615056014, 0.47434609755306956, 0.6047398035823759, 0.12927544401059554]
Training loss = 0.022843373318513233
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.795
Training loss = 0.020604473451773325
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.02166710873444875
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.01822776238123576
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017117601831754047
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016942047874132792
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.018836026390393574
step = 6, Training Accuracy: 0.76
Training loss = 0.016717272301514943
step = 7, Training Accuracy: 0.77
Training loss = 0.016516083975632985
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.018686496714750925
step = 9, Training Accuracy: 0.78
Training loss = 0.01945199797550837
step = 10, Training Accuracy: 0.75
Training loss = 0.016796501974264782
step = 11, Training Accuracy: 0.79
Training loss = 0.016732786695162455
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.015975997547308603
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01668260246515274
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.76375
params:  [0.2713528804985672, 0.40535520617918286, 0.9436885083410991, 0.2892170944681718, 0.1664506612878857, 0.5017143969969905, 0.818235647049337, 0.42548533075098016, 0.5821488312462618, 0.20842324017454017, 0.7733403720400048, 0.665373665566005, 0.8558660702358184, 0.01, 0.44589168943146146, 0.99, 0.48881132406166505, 0.20456930709029206, 0.7014295395878809, 0.6952520584476618, 0.01, 0.5287705532796587, 0.4152925259570498, 0.5718244270594699]
Training loss = 0.01473094493150711
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77375
Training loss = 0.014129673937956493
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01607565442721049
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01430628091096878
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.014243340194225311
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.01375398168961207
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.014052124420801799
step = 6, Training Accuracy: 0.8466666666666667
Training loss = 0.013418951431910197
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.012747701853513718
step = 8, Training Accuracy: 0.86
Training loss = 0.012253623803456624
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.012224597831567128
step = 10, Training Accuracy: 0.8633333333333333
Training loss = 0.01017113909125328
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.013591089447339376
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.013174985547860464
step = 13, Training Accuracy: 0.83
Training loss = 0.011120969280600548
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.77375
[[0.4547655952874812, 0.28165898860798266, 0.9313497090353844, 0.09877739858393775, 0.24967746486496376, 0.6916135867405594, 0.99, 0.4257214368754503, 0.19390022902356963, 0.4087688945547061, 0.6642954384403978, 0.1675478210603933, 0.9104932877364046, 0.08072778117207509, 0.6188051243787557, 0.3807347582284515, 0.01, 0.13443597441535338, 0.04224432738021644, 0.5474947406862557, 0.1310079010276153, 0.9145625456850546, 0.1915213004747974, 0.900541543870349], [0.4491086820559641, 0.5110112673570865, 0.8731044710710582, 0.18218755082068294, 0.3650997462797822, 0.21605293979567114, 0.99, 0.5389306222703668, 0.233419236545455, 0.17544182845835327, 0.5731093655427737, 0.5015190586068116, 0.99, 0.01, 0.49726851364050745, 0.694593240228059, 0.39460950335574463, 0.1463446569283618, 0.23974226312300628, 0.5161554718255529, 0.434586626846959, 0.99, 0.42466523379905513, 0.33664723473173774], [0.46673565977876436, 0.4524581956952515, 0.99, 0.18407950443766083, 0.01, 0.7988272990473315, 0.7725031874201638, 0.44508254846009104, 0.39547024291776, 0.01, 0.5763782370379694, 0.4210971943806549, 0.6462304696377787, 0.2248099558221034, 0.09715266481129547, 0.5964360926924384, 0.39891572745218307, 0.22704627741918626, 0.649080395792835, 0.7603421000437551, 0.24518981723775254, 0.7398003436107724, 0.6720523499236201, 0.5438036977087074], [0.5276525019826608, 0.39720278997649616, 0.9043898479492474, 0.01, 0.01, 0.5761523622871182, 0.8963461011337522, 0.38611137170199117, 0.29201210299753866, 0.40827945667374743, 0.6401064156845556, 0.3677593771010182, 0.59998766268035, 0.01, 0.42760742933362667, 0.23458347443070654, 0.4253623917021472, 0.01, 0.5943349780281952, 0.4585632599330561, 0.17284405623672772, 0.7670717273372735, 0.01, 0.47476174770263724], [0.5543142221000357, 0.21038090032019152, 0.99, 0.14840117843510553, 0.5588480728367612, 0.8188663732033883, 0.99, 0.40305956025570033, 0.20246762762894918, 0.6530789933361466, 0.8645799006645253, 0.3115976266840341, 0.5890457413106946, 0.36639414943621085, 0.3108058327285407, 0.41881553978163677, 0.7828279742162638, 0.0505288652801687, 0.4199492823240118, 0.6411730388248528, 0.17888864521405046, 0.3857840244300751, 0.7987487736612409, 0.6701104621524522], [0.29200000421851197, 0.44082630119215194, 0.99, 0.377436481900627, 0.01, 0.4756540281344428, 0.99, 0.6207305527978217, 0.1717973598964842, 0.22164370679755768, 0.57127821173485, 0.4144368678282941, 0.99, 0.10621141675908706, 0.4085763460497661, 0.7241081134422214, 0.14078599821697332, 0.07571478114505453, 0.49514934275992417, 0.947219257659554, 0.3337262965489811, 0.5186326276567831, 0.1320978530989807, 0.7024560932796862], [0.18353204062917858, 0.49961998021881854, 0.99, 0.01, 0.38561903462695835, 0.6743765487568376, 0.8885116241813884, 0.6447987957940126, 0.5673030903873751, 0.4141904488383375, 0.8063958087659858, 0.4547343219188101, 0.5352591290246742, 0.01, 0.6987095158753405, 0.5018914803029472, 0.01, 0.48362169881800393, 0.2413714941493689, 0.6297770192895416, 0.10664356615056014, 0.47434609755306956, 0.6047398035823759, 0.12927544401059554], [0.2713528804985672, 0.40535520617918286, 0.9436885083410991, 0.2892170944681718, 0.1664506612878857, 0.5017143969969905, 0.818235647049337, 0.42548533075098016, 0.5821488312462618, 0.20842324017454017, 0.7733403720400048, 0.665373665566005, 0.8558660702358184, 0.01, 0.44589168943146146, 0.99, 0.48881132406166505, 0.20456930709029206, 0.7014295395878809, 0.6952520584476618, 0.01, 0.5287705532796587, 0.4152925259570498, 0.5718244270594699]]
7  	8     	0.778438	0.0129716 	0.76375	0.80375
params:  [0.28457267294017535, 0.20114009099116528, 0.9604582130098598, 0.5123039430653643, 0.11488824275718662, 0.18733338136453292, 0.99, 0.3566845257457399, 0.11318712703099038, 0.3939778332523538, 0.5852021522176671, 0.18219226739815442, 0.7712341970321477, 0.2752046135025769, 0.5288699216370365, 0.44314210792385944, 0.36387006089759494, 0.01, 0.5982882587548759, 0.6777974659907813, 0.31348953703713506, 0.99, 0.369311329270704, 0.8556800715285493]
Training loss = 0.01765624980131785
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.795
Training loss = 0.015515685280164082
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014102106789747874
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.012537972728411357
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.01612712780634562
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.011923911223808925
step = 5, Training Accuracy: 0.83
Training loss = 0.0122504693766435
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.011773279458284379
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.0117172006269296
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.012702715297540029
step = 9, Training Accuracy: 0.85
Training loss = 0.011545506169398626
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.013397945314645767
step = 11, Training Accuracy: 0.8533333333333334
Training loss = 0.010553881724675496
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.010929281264543534
step = 13, Training Accuracy: 0.86
Training loss = 0.01126670092344284
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7775
params:  [0.29120268880429323, 0.33372203510831394, 0.99, 0.2196785976294362, 0.31086459216205337, 0.29641826534349336, 0.7546494622861715, 0.2890807202413891, 0.3362439454661382, 0.2598027835476085, 0.3810505838825539, 0.5661619459045124, 0.7484116844450137, 0.3038859850328516, 0.017484441518455796, 0.5182946571229916, 0.16637640920138141, 0.01, 0.3192515150858133, 0.7003751733194347, 0.17706909795705927, 0.83159810271397, 0.37080465814695046, 0.417507435059877]
Training loss = 0.02278105398019155
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78125
Training loss = 0.015300143162409465
step = 1, Training Accuracy: 0.79
Training loss = 0.01728373517592748
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.014868540664513905
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.01609909415245056
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01517866830031077
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014432955781618755
step = 6, Training Accuracy: 0.84
Training loss = 0.014746755063533783
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.012945671677589416
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.01368624081214269
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.012300459891557694
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.012504324813683828
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.012296334207057953
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.011384086310863495
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.011872353901465734
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.7775
params:  [0.5581570596735128, 0.309851394750672, 0.7248489594571608, 0.11335005173441637, 0.14771361602282057, 0.4848766314419811, 0.99, 0.606794886650927, 0.25645797484983673, 0.1681022979102711, 0.6385092180101445, 0.5711635485209315, 0.8849959238326373, 0.0467099870702915, 0.3140766395667605, 0.5561444793019443, 0.6135331441628281, 0.01, 0.40590866337031445, 0.5390017998769068, 0.3921969668869095, 0.7844965299117167, 0.20632345177749845, 0.7663391152634471]
Training loss = 0.01822891136010488
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.77875
Training loss = 0.015341934065024059
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01394003560145696
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.015529889663060506
step = 3, Training Accuracy: 0.81
Training loss = 0.016828916072845458
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.013846887449423473
step = 5, Training Accuracy: 0.84
Training loss = 0.012333112756411234
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.012277063578367234
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013074973275264105
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.012781311770280203
step = 9, Training Accuracy: 0.84
Training loss = 0.012413252741098404
step = 10, Training Accuracy: 0.83
Training loss = 0.013130078564087549
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.012179123759269715
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.01144905373454094
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.01125746543208758
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.77375
params:  [0.43901514534549363, 0.6385465448896344, 0.9187280237295198, 0.3324561703507152, 0.2951051593895955, 0.25907360355764997, 0.7687544470366016, 0.48181127868819384, 0.1278491481009097, 0.13269483599236181, 0.7410964565707742, 0.3597576878242615, 0.8546825793778422, 0.15337481174554765, 0.23068337280947177, 0.8602726732830367, 0.18308299589299548, 0.323229118326556, 0.01, 0.7679132758733599, 0.8066944471920883, 0.9215979073625513, 0.5264925518740831, 0.5403952240546577]
Training loss = 0.019591544369856516
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.78
Training loss = 0.01782830834388733
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.017054469982783
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01532950649658839
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.015011696914831797
step = 4, Training Accuracy: 0.82
Training loss = 0.016506858070691428
step = 5, Training Accuracy: 0.8
Training loss = 0.01495513454079628
step = 6, Training Accuracy: 0.79
Training loss = 0.016438225309054057
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01524292101462682
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.01343870500723521
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.015598443448543548
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.015250480274359384
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.016759587426980336
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.013169976472854615
step = 13, Training Accuracy: 0.85
Training loss = 0.014553934335708618
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.78625
params:  [0.20776085144270867, 0.4325449905447493, 0.99, 0.36709552243335264, 0.6779522201243415, 0.1607887322471488, 0.837926880145398, 0.3619056808577181, 0.24056178783511523, 0.060561530263494895, 0.7892485558007686, 0.9114589488060545, 0.8592995876212443, 0.01, 0.17268143505610337, 0.5160757975757709, 0.2712020418299057, 0.13638075684458678, 0.3239700077863652, 0.7163708156084339, 0.17560801110259688, 0.8038863607063715, 0.2934442176516371, 0.1839756982029625]
Training loss = 0.017623898088932038
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.77875
Training loss = 0.015614743828773498
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.015808645486831665
step = 2, Training Accuracy: 0.81
Training loss = 0.014703076829512914
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.013040971060593922
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.012727532188097636
step = 5, Training Accuracy: 0.81
Training loss = 0.014173972904682159
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.013231875399748485
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.013811865399281184
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.010846577510237694
step = 9, Training Accuracy: 0.87
Training loss = 0.011429222176472346
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.014007037530342738
step = 11, Training Accuracy: 0.84
Training loss = 0.011735204756259918
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.012494467447201412
step = 13, Training Accuracy: 0.84
Training loss = 0.012486594617366792
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.79
params:  [0.4709266029799916, 0.41247075785096793, 0.99, 0.36915801669292914, 0.4822135066509379, 0.20657013346108666, 0.770443966028939, 0.5401975630017882, 0.033106757726599184, 0.269945023652172, 0.16643674968605754, 0.9073888507803236, 0.7571154815785572, 0.01, 0.2133032806991309, 0.6406435679490323, 0.9067690574083956, 0.01, 0.41776529777389443, 0.8475703138560315, 0.6211338773911, 0.9024860348293464, 0.7716279015455143, 0.3009026670126222]
Training loss = 0.02079411079486211
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7775
Training loss = 0.02175645778576533
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01718005875746409
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.018916023472944896
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017135914067427316
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.016820933322111764
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.018749116361141203
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01595276653766632
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.0145699542760849
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.019556885461012523
step = 9, Training Accuracy: 0.76
Training loss = 0.016207365492979686
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.01612371375163396
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01685306578874588
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.014450006633996964
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.014872681299845378
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78
params:  [0.533103367085194, 0.3310709472783946, 0.9571978031774377, 0.01179388084943575, 0.33563662486255663, 0.24736420944029774, 0.8859023888450599, 0.7527470033482015, 0.01, 0.01, 0.2231247713501081, 0.1950889000534427, 0.8967897048635746, 0.01, 0.37040765600364994, 0.5804315951985008, 0.21621376566626038, 0.19404271827186625, 0.2019483584712045, 0.801637245474134, 0.36886479700789454, 0.754033327835232, 0.5106572675031197, 0.1582012825491234]
Training loss = 0.01891984244187673
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7875
Training loss = 0.017926410734653474
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01652328610420227
step = 2, Training Accuracy: 0.82
Training loss = 0.01550380140542984
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015419169465700785
step = 4, Training Accuracy: 0.8
Training loss = 0.015920024812221528
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.014176768263181051
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014111312727133432
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.015940007468064624
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015899772743384045
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.01477530986070633
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01506898432970047
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014895750284194946
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014440366377433142
step = 13, Training Accuracy: 0.81
Training loss = 0.014771329760551453
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.78875
params:  [0.6481717672107895, 0.532389246060544, 0.7945167939283208, 0.42082445374356603, 0.2789626258292812, 0.37149624773457496, 0.99, 0.7123322462679181, 0.2805751755877536, 0.47974899976930685, 0.38985249513648174, 0.6349158573894661, 0.817789930853306, 0.01, 0.5178004094762821, 0.5392316801761949, 0.3910823517374738, 0.2795296590972245, 0.2761169148934396, 0.39732674718462546, 0.18096200446392222, 0.7122478002189786, 0.3885216796221148, 0.8417973445659934]
Training loss = 0.02092738340298335
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78375
Training loss = 0.02025765776634216
step = 1, Training Accuracy: 0.73
Training loss = 0.017125840187072753
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.01770555118719737
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01859499583641688
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.016453084846337635
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.01661634922027588
step = 6, Training Accuracy: 0.81
Training loss = 0.015935746332009632
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015427667001883189
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015334972639878592
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.019535004695256552
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.01540460040171941
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.01551887720823288
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.015915908614794413
step = 13, Training Accuracy: 0.8
Training loss = 0.016129129727681477
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.76625
[[0.28457267294017535, 0.20114009099116528, 0.9604582130098598, 0.5123039430653643, 0.11488824275718662, 0.18733338136453292, 0.99, 0.3566845257457399, 0.11318712703099038, 0.3939778332523538, 0.5852021522176671, 0.18219226739815442, 0.7712341970321477, 0.2752046135025769, 0.5288699216370365, 0.44314210792385944, 0.36387006089759494, 0.01, 0.5982882587548759, 0.6777974659907813, 0.31348953703713506, 0.99, 0.369311329270704, 0.8556800715285493], [0.29120268880429323, 0.33372203510831394, 0.99, 0.2196785976294362, 0.31086459216205337, 0.29641826534349336, 0.7546494622861715, 0.2890807202413891, 0.3362439454661382, 0.2598027835476085, 0.3810505838825539, 0.5661619459045124, 0.7484116844450137, 0.3038859850328516, 0.017484441518455796, 0.5182946571229916, 0.16637640920138141, 0.01, 0.3192515150858133, 0.7003751733194347, 0.17706909795705927, 0.83159810271397, 0.37080465814695046, 0.417507435059877], [0.5581570596735128, 0.309851394750672, 0.7248489594571608, 0.11335005173441637, 0.14771361602282057, 0.4848766314419811, 0.99, 0.606794886650927, 0.25645797484983673, 0.1681022979102711, 0.6385092180101445, 0.5711635485209315, 0.8849959238326373, 0.0467099870702915, 0.3140766395667605, 0.5561444793019443, 0.6135331441628281, 0.01, 0.40590866337031445, 0.5390017998769068, 0.3921969668869095, 0.7844965299117167, 0.20632345177749845, 0.7663391152634471], [0.43901514534549363, 0.6385465448896344, 0.9187280237295198, 0.3324561703507152, 0.2951051593895955, 0.25907360355764997, 0.7687544470366016, 0.48181127868819384, 0.1278491481009097, 0.13269483599236181, 0.7410964565707742, 0.3597576878242615, 0.8546825793778422, 0.15337481174554765, 0.23068337280947177, 0.8602726732830367, 0.18308299589299548, 0.323229118326556, 0.01, 0.7679132758733599, 0.8066944471920883, 0.9215979073625513, 0.5264925518740831, 0.5403952240546577], [0.20776085144270867, 0.4325449905447493, 0.99, 0.36709552243335264, 0.6779522201243415, 0.1607887322471488, 0.837926880145398, 0.3619056808577181, 0.24056178783511523, 0.060561530263494895, 0.7892485558007686, 0.9114589488060545, 0.8592995876212443, 0.01, 0.17268143505610337, 0.5160757975757709, 0.2712020418299057, 0.13638075684458678, 0.3239700077863652, 0.7163708156084339, 0.17560801110259688, 0.8038863607063715, 0.2934442176516371, 0.1839756982029625], [0.4709266029799916, 0.41247075785096793, 0.99, 0.36915801669292914, 0.4822135066509379, 0.20657013346108666, 0.770443966028939, 0.5401975630017882, 0.033106757726599184, 0.269945023652172, 0.16643674968605754, 0.9073888507803236, 0.7571154815785572, 0.01, 0.2133032806991309, 0.6406435679490323, 0.9067690574083956, 0.01, 0.41776529777389443, 0.8475703138560315, 0.6211338773911, 0.9024860348293464, 0.7716279015455143, 0.3009026670126222], [0.533103367085194, 0.3310709472783946, 0.9571978031774377, 0.01179388084943575, 0.33563662486255663, 0.24736420944029774, 0.8859023888450599, 0.7527470033482015, 0.01, 0.01, 0.2231247713501081, 0.1950889000534427, 0.8967897048635746, 0.01, 0.37040765600364994, 0.5804315951985008, 0.21621376566626038, 0.19404271827186625, 0.2019483584712045, 0.801637245474134, 0.36886479700789454, 0.754033327835232, 0.5106572675031197, 0.1582012825491234], [0.6481717672107895, 0.532389246060544, 0.7945167939283208, 0.42082445374356603, 0.2789626258292812, 0.37149624773457496, 0.99, 0.7123322462679181, 0.2805751755877536, 0.47974899976930685, 0.38985249513648174, 0.6349158573894661, 0.817789930853306, 0.01, 0.5178004094762821, 0.5392316801761949, 0.3910823517374738, 0.2795296590972245, 0.2761169148934396, 0.39732674718462546, 0.18096200446392222, 0.7122478002189786, 0.3885216796221148, 0.8417973445659934]]
8  	8     	0.78    	0.0075519 	0.76625	0.79   
params:  [0.5147474995797051, 0.6158043467148657, 0.99, 0.11375276685602634, 0.16684019808839773, 0.4618420036496816, 0.7173034592660411, 0.4035624299461185, 0.01, 0.08324486599054223, 0.6512088801982635, 0.7690773170737468, 0.6626967419053442, 0.06627852028015692, 0.2694832982621482, 0.6757563096374757, 0.01, 0.1366074160292163, 0.3452888005503521, 0.580004648764932, 0.11048790528097532, 0.8973704595225995, 0.274076917395392, 0.2460020604729277]
Training loss = 0.01804211566845576
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7725
Training loss = 0.016935234467188517
step = 1, Training Accuracy: 0.76
Training loss = 0.017939480741818746
step = 2, Training Accuracy: 0.8
Training loss = 0.014138427525758744
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015410240292549133
step = 4, Training Accuracy: 0.79
Training loss = 0.015294747054576873
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.014828461805979411
step = 6, Training Accuracy: 0.81
Training loss = 0.014524586846431096
step = 7, Training Accuracy: 0.81
Training loss = 0.014042051136493683
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.01412018209695816
step = 9, Training Accuracy: 0.81
Training loss = 0.013629465401172637
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01359259953101476
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.013398413360118867
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.012815359234809875
step = 13, Training Accuracy: 0.84
Training loss = 0.013288872440656026
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.77625
params:  [0.10099493938983575, 0.5419324615152877, 0.9595716284999395, 0.3340430018126675, 0.639228437828071, 0.01, 0.9264458921613983, 0.45813094302376167, 0.4663170636030024, 0.047602995143469114, 0.5504767168790898, 0.6198753738061495, 0.99, 0.15771490923884082, 0.18945088431929674, 0.7328277562586375, 0.04058620506945665, 0.06347360819784163, 0.5749539614183691, 0.768259076831453, 0.7367215067497128, 0.99, 0.5693975792085277, 0.8097267868001746]
Training loss = 0.017494870374600093
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
Training loss = 0.018212653199831643
step = 1, Training Accuracy: 0.78
Training loss = 0.01526290198167165
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.016145735681056976
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.018309142291545868
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015362712542215983
step = 5, Training Accuracy: 0.83
Training loss = 0.014140750964482625
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.013758376340071361
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.01635443598031998
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.014544617931048075
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016243972877661387
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014499611755212149
step = 11, Training Accuracy: 0.82
Training loss = 0.014717523753643037
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.013488816618919373
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.01572309816877047
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.77625
params:  [0.4826684116377468, 0.3116011334832542, 0.9701772895501536, 0.07520809557219099, 0.570856541005992, 0.01064200875540966, 0.6198386940475974, 0.5239056865214101, 0.33375756272341683, 0.030267955796381314, 0.6960745317977399, 0.6350126582881194, 0.9032828720168393, 0.01, 0.4124355495419616, 0.23785452900918225, 0.39502020783225084, 0.09298969677007043, 0.7009320090186323, 0.823564728326589, 0.7062637414721586, 0.8890418770912143, 0.01, 0.01]
Training loss = 0.017490060925483705
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.7875
Training loss = 0.016148165961106617
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.015404729545116425
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.015316289365291596
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.013663143863280615
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.0137181356549263
step = 5, Training Accuracy: 0.83
Training loss = 0.013670149942239125
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.013801986475785573
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.011713204483191172
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013471583922704061
step = 9, Training Accuracy: 0.83
Training loss = 0.013269193768501282
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.013482721149921417
step = 11, Training Accuracy: 0.83
Training loss = 0.013552004595597585
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.013279227316379547
step = 13, Training Accuracy: 0.83
Training loss = 0.012667046288649241
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.7825
params:  [0.15854114114506018, 0.7359237582027571, 0.99, 0.25143790993529297, 0.7055038052036823, 0.4100905984310163, 0.7752307692087412, 0.6190305542995816, 0.2925341799891828, 0.01, 0.5722779262424991, 0.6432112114768915, 0.5301711460350504, 0.01, 0.19163055606703816, 0.6929093851965787, 0.46905361035159066, 0.16845721663435687, 0.2451567211881258, 0.7913044423556517, 0.6550157616295498, 0.9526611792907297, 0.494696982523824, 0.16845546292888158]
Training loss = 0.02016667346159617
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7725
Training loss = 0.018147469063599906
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.017588228285312653
step = 2, Training Accuracy: 0.77
Training loss = 0.017675714989503225
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.017594581842422484
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.015901189744472504
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.018296444416046144
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01691307008266449
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016401505966981252
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015615446964899698
step = 9, Training Accuracy: 0.79
Training loss = 0.016846792101860047
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016922267576058705
step = 11, Training Accuracy: 0.78
Training loss = 0.02021426200866699
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01641532003879547
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015959929625193277
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78125
params:  [0.36909878149336367, 0.3629681832945284, 0.99, 0.42233890376340777, 0.5660232216212837, 0.05539345525284767, 0.99, 0.3686431306330654, 0.01, 0.33560016721720953, 0.3917666491946368, 0.5158711133397966, 0.99, 0.030121064478993435, 0.5653543475721554, 0.4073339753812312, 0.2827018041410747, 0.10578961410470647, 0.20865920848642425, 0.7726048520691975, 0.23115998825533246, 0.8418397736556146, 0.40309416349525506, 0.11864560062122967]
Training loss = 0.015764970978101093
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.77875
Training loss = 0.015102974474430084
step = 1, Training Accuracy: 0.8
Training loss = 0.01478933130701383
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.012857513775428136
step = 3, Training Accuracy: 0.84
Training loss = 0.013069774210453033
step = 4, Training Accuracy: 0.85
Training loss = 0.0138731649518013
step = 5, Training Accuracy: 0.84
Training loss = 0.013355972270170848
step = 6, Training Accuracy: 0.81
Training loss = 0.01561188797156016
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.011953158130248387
step = 8, Training Accuracy: 0.85
Training loss = 0.011764404624700546
step = 9, Training Accuracy: 0.84
Training loss = 0.013946625739336013
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.01357716903090477
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01354069655140241
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.011775913933912913
step = 13, Training Accuracy: 0.86
Training loss = 0.01323105812072754
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.7975
params:  [0.32029725031746425, 0.7934373419781213, 0.7016903705470304, 0.2394613501201344, 0.34089932111480326, 0.12727791395597937, 0.8647137342082205, 0.7636484956000953, 0.4973062607756983, 0.08433884287090555, 0.5986536369431465, 0.6292799299645967, 0.99, 0.01, 0.22450894618403425, 0.8745369267042786, 0.3715346157419386, 0.19455867423888523, 0.011781971106637923, 0.9325087699257948, 0.4587006642297325, 0.9229659576965933, 0.41401579360462704, 0.01]
Training loss = 0.017802647898594537
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79375
Training loss = 0.017081676125526427
step = 1, Training Accuracy: 0.77
Training loss = 0.018090411126613616
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015501410365104676
step = 3, Training Accuracy: 0.77
Training loss = 0.015348926385243733
step = 4, Training Accuracy: 0.77
Training loss = 0.01389873613913854
step = 5, Training Accuracy: 0.8333333333333334
Training loss = 0.014519736965497335
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.01785882979631424
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.016253925065199536
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.014684863289197285
step = 9, Training Accuracy: 0.81
Training loss = 0.015223039984703065
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01437737892071406
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.015291060209274293
step = 12, Training Accuracy: 0.78
Training loss = 0.013586740990479787
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014092530806859334
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78625
params:  [0.33640865608549403, 0.5136793664427626, 0.8604874091484783, 0.013858636664153795, 0.28982067946468415, 0.3075856423726707, 0.99, 0.4915206516783137, 0.0974040272627572, 0.18154081964856472, 0.578359067778994, 0.801507583364033, 0.8829889142786603, 0.01, 0.2844667292811583, 0.9209785366286604, 0.01, 0.11924787815473137, 0.3176697379165914, 0.6135837780625633, 0.11626518707340325, 0.777033358669486, 0.39568757432024404, 0.49889411775303916]
Training loss = 0.01886345197757085
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.79125
Training loss = 0.019543562332789102
step = 1, Training Accuracy: 0.76
Training loss = 0.01957521806160609
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01608515650033951
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016484948893388113
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.014670895983775457
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01609315241376559
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.015564559400081635
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015627231498559317
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015156676371892294
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015390331049760182
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016229305962721506
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.015322955250740051
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01494050552447637
step = 13, Training Accuracy: 0.82
Training loss = 0.015682050089041392
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7875
params:  [0.46512991761955175, 0.2829690202819668, 0.99, 0.06206952894452991, 0.6382847474239574, 0.5595407556116152, 0.99, 0.45952497519041635, 0.46666324228096767, 0.12754870101362273, 0.6548357122212316, 0.5065910952385374, 0.6006105419636194, 0.010785112600126451, 0.19184323760557287, 0.7766479137070293, 0.33204045047323794, 0.17023382174840324, 0.062464772850756606, 0.7237852738564854, 0.1611677269513114, 0.5836361291240241, 0.5335871467718964, 0.5024297661037984]
Training loss = 0.01808345710237821
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.79
Training loss = 0.01615981976191203
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.01450153261423111
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.013668930927912394
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.013801469802856445
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.013763480186462403
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.01275166466832161
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.012980164935191472
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.012823950002590815
step = 8, Training Accuracy: 0.85
Training loss = 0.013896139015754065
step = 9, Training Accuracy: 0.82
Training loss = 0.015177534570296606
step = 10, Training Accuracy: 0.83
Training loss = 0.010756840258836746
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.01509124497572581
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.012438368598620096
step = 13, Training Accuracy: 0.84
Training loss = 0.012017514258623123
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.795
[[0.5147474995797051, 0.6158043467148657, 0.99, 0.11375276685602634, 0.16684019808839773, 0.4618420036496816, 0.7173034592660411, 0.4035624299461185, 0.01, 0.08324486599054223, 0.6512088801982635, 0.7690773170737468, 0.6626967419053442, 0.06627852028015692, 0.2694832982621482, 0.6757563096374757, 0.01, 0.1366074160292163, 0.3452888005503521, 0.580004648764932, 0.11048790528097532, 0.8973704595225995, 0.274076917395392, 0.2460020604729277], [0.10099493938983575, 0.5419324615152877, 0.9595716284999395, 0.3340430018126675, 0.639228437828071, 0.01, 0.9264458921613983, 0.45813094302376167, 0.4663170636030024, 0.047602995143469114, 0.5504767168790898, 0.6198753738061495, 0.99, 0.15771490923884082, 0.18945088431929674, 0.7328277562586375, 0.04058620506945665, 0.06347360819784163, 0.5749539614183691, 0.768259076831453, 0.7367215067497128, 0.99, 0.5693975792085277, 0.8097267868001746], [0.4826684116377468, 0.3116011334832542, 0.9701772895501536, 0.07520809557219099, 0.570856541005992, 0.01064200875540966, 0.6198386940475974, 0.5239056865214101, 0.33375756272341683, 0.030267955796381314, 0.6960745317977399, 0.6350126582881194, 0.9032828720168393, 0.01, 0.4124355495419616, 0.23785452900918225, 0.39502020783225084, 0.09298969677007043, 0.7009320090186323, 0.823564728326589, 0.7062637414721586, 0.8890418770912143, 0.01, 0.01], [0.15854114114506018, 0.7359237582027571, 0.99, 0.25143790993529297, 0.7055038052036823, 0.4100905984310163, 0.7752307692087412, 0.6190305542995816, 0.2925341799891828, 0.01, 0.5722779262424991, 0.6432112114768915, 0.5301711460350504, 0.01, 0.19163055606703816, 0.6929093851965787, 0.46905361035159066, 0.16845721663435687, 0.2451567211881258, 0.7913044423556517, 0.6550157616295498, 0.9526611792907297, 0.494696982523824, 0.16845546292888158], [0.36909878149336367, 0.3629681832945284, 0.99, 0.42233890376340777, 0.5660232216212837, 0.05539345525284767, 0.99, 0.3686431306330654, 0.01, 0.33560016721720953, 0.3917666491946368, 0.5158711133397966, 0.99, 0.030121064478993435, 0.5653543475721554, 0.4073339753812312, 0.2827018041410747, 0.10578961410470647, 0.20865920848642425, 0.7726048520691975, 0.23115998825533246, 0.8418397736556146, 0.40309416349525506, 0.11864560062122967], [0.32029725031746425, 0.7934373419781213, 0.7016903705470304, 0.2394613501201344, 0.34089932111480326, 0.12727791395597937, 0.8647137342082205, 0.7636484956000953, 0.4973062607756983, 0.08433884287090555, 0.5986536369431465, 0.6292799299645967, 0.99, 0.01, 0.22450894618403425, 0.8745369267042786, 0.3715346157419386, 0.19455867423888523, 0.011781971106637923, 0.9325087699257948, 0.4587006642297325, 0.9229659576965933, 0.41401579360462704, 0.01], [0.33640865608549403, 0.5136793664427626, 0.8604874091484783, 0.013858636664153795, 0.28982067946468415, 0.3075856423726707, 0.99, 0.4915206516783137, 0.0974040272627572, 0.18154081964856472, 0.578359067778994, 0.801507583364033, 0.8829889142786603, 0.01, 0.2844667292811583, 0.9209785366286604, 0.01, 0.11924787815473137, 0.3176697379165914, 0.6135837780625633, 0.11626518707340325, 0.777033358669486, 0.39568757432024404, 0.49889411775303916], [0.46512991761955175, 0.2829690202819668, 0.99, 0.06206952894452991, 0.6382847474239574, 0.5595407556116152, 0.99, 0.45952497519041635, 0.46666324228096767, 0.12754870101362273, 0.6548357122212316, 0.5065910952385374, 0.6006105419636194, 0.010785112600126451, 0.19184323760557287, 0.7766479137070293, 0.33204045047323794, 0.17023382174840324, 0.062464772850756606, 0.7237852738564854, 0.1611677269513114, 0.5836361291240241, 0.5335871467718964, 0.5024297661037984]]
9  	8     	0.785312	0.00738849	0.77625	0.7975 
params:  [0.5556961562529823, 0.13612937056358343, 0.99, 0.4242054590716958, 0.5766409292791236, 0.5911850471874005, 0.8506170527582769, 0.3704846516994011, 0.274331652123314, 0.3053496714749509, 0.1671258426696937, 0.6768972024852292, 0.99, 0.01, 0.587975529496757, 0.3039837454831234, 0.34736718129625194, 0.2369190211869978, 0.2888201536842664, 0.3560541099408648, 0.6186597515709865, 0.8929886944284531, 0.5465334219191125, 0.0923266536628149]
Training loss = 0.01397747943798701
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.79125
Training loss = 0.013299725999434788
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.013648302753766378
step = 2, Training Accuracy: 0.83
Training loss = 0.012518262962500254
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.011920718650023143
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.012477559943993886
step = 5, Training Accuracy: 0.86
Training loss = 0.010443997929493586
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.01052042841911316
step = 7, Training Accuracy: 0.86
Training loss = 0.013697502464056014
step = 8, Training Accuracy: 0.86
Training loss = 0.010575980246067047
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.010699776510397593
step = 10, Training Accuracy: 0.88
Training loss = 0.011108171790838242
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.010088508228460948
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.009503722041845322
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.010557354589303335
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.785
params:  [0.3149309687976496, 0.1269302304261869, 0.99, 0.020894719578324367, 0.45955298286417345, 0.31490306250077527, 0.8511947080126862, 0.13887401241544195, 0.18442835403362937, 0.23468027826231952, 0.6120959704413186, 0.3996171685238995, 0.9883731059641361, 0.01, 0.9259060884858503, 0.505177053430861, 0.14428477845089183, 0.277135877741559, 0.06693978000227228, 0.6765510395346419, 0.08981110771557864, 0.7769854225266933, 0.5578533760093123, 0.37737628649582267]
Training loss = 0.01784913053115209
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.775
Training loss = 0.01569779376188914
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.015147651632626852
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014868973046541214
step = 3, Training Accuracy: 0.83
Training loss = 0.013846198817094167
step = 4, Training Accuracy: 0.84
Training loss = 0.015388350288073222
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.013194422721862793
step = 6, Training Accuracy: 0.82
Training loss = 0.011583647678295771
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.013044264664252599
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.014674365818500519
step = 9, Training Accuracy: 0.83
Training loss = 0.01230539470911026
step = 10, Training Accuracy: 0.8333333333333334
Training loss = 0.011658628384272257
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.011912532945473989
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.011558864216009776
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.013053350945313772
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7675
params:  [0.5113142239659141, 0.5352373212199945, 0.9742575303567785, 0.19387524145560403, 0.7664686834252246, 0.01, 0.99, 0.3132300204255538, 0.32499397503176664, 0.19704869828535299, 0.45489080930984616, 0.6186071600263195, 0.8909623880369771, 0.01, 0.4220210060871085, 0.5629336811604638, 0.26720750066758436, 0.07475893490323293, 0.01, 0.8042826317009076, 0.01, 0.6629282563400303, 0.4767305955264921, 0.01]
Training loss = 0.01951992650826772
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.75625
Training loss = 0.017885879973570506
step = 1, Training Accuracy: 0.76
Training loss = 0.015463347931702932
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.016678676505883536
step = 3, Training Accuracy: 0.78
Training loss = 0.013545540571212768
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.015427189767360688
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.014304173092047373
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015313851684331893
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.014992615481217702
step = 8, Training Accuracy: 0.79
Training loss = 0.01474547117948532
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014406354029973348
step = 10, Training Accuracy: 0.8
Training loss = 0.016038909554481506
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.014251546760400136
step = 12, Training Accuracy: 0.82
Training loss = 0.013553117364645005
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014036545157432556
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77
params:  [0.3534334331288562, 0.6407358614271794, 0.9683943757253186, 0.5646067332770324, 0.43668014878942846, 0.22590660256790182, 0.8580467249085303, 0.1330845837498409, 0.2932176668220775, 0.4910211295677673, 0.7058156273639155, 0.8776929424305211, 0.7655447605100659, 0.04382375069110338, 0.4682184275223946, 0.5867362565009872, 0.22582635588303482, 0.3708240779305827, 0.41304925698398853, 0.6162013371551744, 0.4578405010439175, 0.6980677872797245, 0.1748854172354405, 0.9214933548214592]
Training loss = 0.01813870479663213
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7675
Training loss = 0.01705916037162145
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.015268045564492544
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.017469556232293446
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.01456884890794754
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.015769758820533754
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.01594411810239156
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.015249587694803874
step = 7, Training Accuracy: 0.8
Training loss = 0.014266999463240306
step = 8, Training Accuracy: 0.79
Training loss = 0.014963070154190064
step = 9, Training Accuracy: 0.79
Training loss = 0.015673433740933735
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014676498969395955
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.014212658007939657
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.014723826348781586
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014807832638422648
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7575
params:  [0.4575334121988722, 0.4838351354434704, 0.931296161682143, 0.3793108366694475, 0.48101572854407715, 0.3301626810609415, 0.99, 0.3866018843268235, 0.34728688384386497, 0.36757813645318604, 0.5401122056171526, 0.4791703641823718, 0.99, 0.01, 0.16472171439524025, 0.7865670606250234, 0.22451871822465494, 0.35001316207393074, 0.17708973834307365, 0.99, 0.3962516794025086, 0.9115830439635491, 0.45529900453321354, 0.390462926899729]
Training loss = 0.015978857378164926
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77125
Training loss = 0.017986376484235126
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.017515187958876292
step = 2, Training Accuracy: 0.76
Training loss = 0.015283792118231455
step = 3, Training Accuracy: 0.8
Training loss = 0.014821920494238536
step = 4, Training Accuracy: 0.81
Training loss = 0.014458736081918082
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01703857531150182
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.017620014746983846
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01614450936516126
step = 8, Training Accuracy: 0.81
Training loss = 0.015753650565942127
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01546844760576884
step = 10, Training Accuracy: 0.79
Training loss = 0.014049787322680156
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.016311503847440085
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.013235669434070587
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.01669265220562617
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.765
params:  [0.557310306346015, 0.5470889656133459, 0.9714736277539096, 0.5701432106387113, 0.6244293772050212, 0.01, 0.8986846454281083, 0.7032869804961462, 0.21670961582945558, 0.3598117576732891, 0.6673823860985955, 0.09292104480517116, 0.8364846583398494, 0.035186705488854206, 0.5066837709919718, 0.7456353714667481, 0.3292511105706074, 0.2906140339797164, 0.45963140844013783, 0.99, 0.01240476118639916, 0.6460147096241099, 0.5636436436895351, 0.3758483309430969]
Training loss = 0.017971933583418528
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.76125
Training loss = 0.017568508287270863
step = 1, Training Accuracy: 0.77
Training loss = 0.01536054015159607
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01594655692577362
step = 3, Training Accuracy: 0.8
Training loss = 0.015492276847362518
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01781728009382884
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.016906216144561767
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.016796221534411113
step = 7, Training Accuracy: 0.79
Training loss = 0.01620148539543152
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.017956091562906902
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.015676257212956745
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.016174809237321217
step = 11, Training Accuracy: 0.78
Training loss = 0.015683077971140543
step = 12, Training Accuracy: 0.79
Training loss = 0.014578721821308135
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.016604013442993164
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.7675
params:  [0.1844599200852512, 0.852651790247681, 0.859480622605146, 0.11415523316810414, 0.8631243411040143, 0.410022286053333, 0.99, 0.3377150296858934, 0.13204889166441197, 0.3587735554227326, 0.6026685811589444, 0.6335056173663361, 0.8815509689255998, 0.01, 0.44842665421643535, 0.6274118956101539, 0.5378481290931028, 0.01, 0.33462414194469803, 0.7300737004209169, 0.11547898112261255, 0.6828526991454041, 0.8845357475311388, 0.35461093350125417]
Training loss = 0.020309617618719737
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.78
Training loss = 0.018450602889060974
step = 1, Training Accuracy: 0.77
Training loss = 0.017368236581484477
step = 2, Training Accuracy: 0.8
Training loss = 0.018103115856647492
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.018522442281246186
step = 4, Training Accuracy: 0.73
Training loss = 0.0179827747742335
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.020228295028209685
step = 6, Training Accuracy: 0.73
Training loss = 0.020623189210891724
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.01851360112428665
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.02025772472222646
step = 9, Training Accuracy: 0.74
Training loss = 0.01788993924856186
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.018997935553391774
step = 11, Training Accuracy: 0.76
Training loss = 0.01706735481818517
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.017580265899499257
step = 13, Training Accuracy: 0.8
Training loss = 0.018193609714508056
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77125
params:  [0.4453455624485602, 0.47387943901222707, 0.99, 0.30206663707695586, 0.6989342703724805, 0.41312305370836133, 0.947684117763256, 0.47330264258221366, 0.38591942901212833, 0.22864507596917605, 0.3260009352419226, 0.99, 0.7198012410304342, 0.01, 0.24197362696007327, 0.7055851467799759, 0.01, 0.01, 0.014826954562661726, 0.6819570406415078, 0.05749676332077791, 0.6710021971856437, 0.35775544190327885, 0.1144754156916305]
Training loss = 0.016818852225939433
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79125
Training loss = 0.014042838513851165
step = 1, Training Accuracy: 0.83
Training loss = 0.016333474119504292
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.016275982062021892
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01581608682870865
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.015084410309791565
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.014742896457513173
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.01568181162079175
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015233595271905263
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015236278474330902
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014374857246875762
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.013131494422753652
step = 11, Training Accuracy: 0.81
Training loss = 0.015131252805391947
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.013458751986424128
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.012696378777424494
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.7925
[[0.5556961562529823, 0.13612937056358343, 0.99, 0.4242054590716958, 0.5766409292791236, 0.5911850471874005, 0.8506170527582769, 0.3704846516994011, 0.274331652123314, 0.3053496714749509, 0.1671258426696937, 0.6768972024852292, 0.99, 0.01, 0.587975529496757, 0.3039837454831234, 0.34736718129625194, 0.2369190211869978, 0.2888201536842664, 0.3560541099408648, 0.6186597515709865, 0.8929886944284531, 0.5465334219191125, 0.0923266536628149], [0.3149309687976496, 0.1269302304261869, 0.99, 0.020894719578324367, 0.45955298286417345, 0.31490306250077527, 0.8511947080126862, 0.13887401241544195, 0.18442835403362937, 0.23468027826231952, 0.6120959704413186, 0.3996171685238995, 0.9883731059641361, 0.01, 0.9259060884858503, 0.505177053430861, 0.14428477845089183, 0.277135877741559, 0.06693978000227228, 0.6765510395346419, 0.08981110771557864, 0.7769854225266933, 0.5578533760093123, 0.37737628649582267], [0.5113142239659141, 0.5352373212199945, 0.9742575303567785, 0.19387524145560403, 0.7664686834252246, 0.01, 0.99, 0.3132300204255538, 0.32499397503176664, 0.19704869828535299, 0.45489080930984616, 0.6186071600263195, 0.8909623880369771, 0.01, 0.4220210060871085, 0.5629336811604638, 0.26720750066758436, 0.07475893490323293, 0.01, 0.8042826317009076, 0.01, 0.6629282563400303, 0.4767305955264921, 0.01], [0.3534334331288562, 0.6407358614271794, 0.9683943757253186, 0.5646067332770324, 0.43668014878942846, 0.22590660256790182, 0.8580467249085303, 0.1330845837498409, 0.2932176668220775, 0.4910211295677673, 0.7058156273639155, 0.8776929424305211, 0.7655447605100659, 0.04382375069110338, 0.4682184275223946, 0.5867362565009872, 0.22582635588303482, 0.3708240779305827, 0.41304925698398853, 0.6162013371551744, 0.4578405010439175, 0.6980677872797245, 0.1748854172354405, 0.9214933548214592], [0.4575334121988722, 0.4838351354434704, 0.931296161682143, 0.3793108366694475, 0.48101572854407715, 0.3301626810609415, 0.99, 0.3866018843268235, 0.34728688384386497, 0.36757813645318604, 0.5401122056171526, 0.4791703641823718, 0.99, 0.01, 0.16472171439524025, 0.7865670606250234, 0.22451871822465494, 0.35001316207393074, 0.17708973834307365, 0.99, 0.3962516794025086, 0.9115830439635491, 0.45529900453321354, 0.390462926899729], [0.557310306346015, 0.5470889656133459, 0.9714736277539096, 0.5701432106387113, 0.6244293772050212, 0.01, 0.8986846454281083, 0.7032869804961462, 0.21670961582945558, 0.3598117576732891, 0.6673823860985955, 0.09292104480517116, 0.8364846583398494, 0.035186705488854206, 0.5066837709919718, 0.7456353714667481, 0.3292511105706074, 0.2906140339797164, 0.45963140844013783, 0.99, 0.01240476118639916, 0.6460147096241099, 0.5636436436895351, 0.3758483309430969], [0.1844599200852512, 0.852651790247681, 0.859480622605146, 0.11415523316810414, 0.8631243411040143, 0.410022286053333, 0.99, 0.3377150296858934, 0.13204889166441197, 0.3587735554227326, 0.6026685811589444, 0.6335056173663361, 0.8815509689255998, 0.01, 0.44842665421643535, 0.6274118956101539, 0.5378481290931028, 0.01, 0.33462414194469803, 0.7300737004209169, 0.11547898112261255, 0.6828526991454041, 0.8845357475311388, 0.35461093350125417], [0.4453455624485602, 0.47387943901222707, 0.99, 0.30206663707695586, 0.6989342703724805, 0.41312305370836133, 0.947684117763256, 0.47330264258221366, 0.38591942901212833, 0.22864507596917605, 0.3260009352419226, 0.99, 0.7198012410304342, 0.01, 0.24197362696007327, 0.7055851467799759, 0.01, 0.01, 0.014826954562661726, 0.6819570406415078, 0.05749676332077791, 0.6710021971856437, 0.35775544190327885, 0.1144754156916305]]
10 	8     	0.772031	0.0105686 	0.7575 	0.7925 
params:  [0.3476398538573186, 0.7635625511081341, 0.99, 0.3421034262399584, 0.678304727964951, 0.46310335996913393, 0.99, 0.6494148584138453, 0.5372438584352777, 0.17207696707210723, 0.40728849875908524, 0.9422458335872642, 0.99, 0.01, 0.44231045484858555, 0.4820353638952741, 0.01, 0.01, 0.01, 0.8439181524996774, 0.46637246910977403, 0.5395828648429695, 0.38815400667991823, 0.13042624765788027]
Training loss = 0.01788129409154256
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79
Training loss = 0.016480285227298736
step = 1, Training Accuracy: 0.81
Training loss = 0.016573452651500703
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01736467033624649
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015971673130989076
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014981899956862131
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.016861192186673483
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.014003201921780905
step = 7, Training Accuracy: 0.81
Training loss = 0.015358373820781708
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.014586534897486368
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015040988822778066
step = 10, Training Accuracy: 0.8366666666666667
Training loss = 0.01334101657072703
step = 11, Training Accuracy: 0.82
Training loss = 0.015630011955897014
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013958036998907726
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.014224556883176169
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78875
params:  [0.5134473685967758, 0.40207952794737833, 0.8924594197507069, 0.380020475268155, 0.888792063535453, 0.3096360092708688, 0.8382439653595518, 0.3628424179467742, 0.2120324703884158, 0.06230086375117186, 0.2009179191281774, 0.6857837505237724, 0.970530841390573, 0.01, 0.12704256781769685, 0.6149334625764673, 0.0772407473838601, 0.2404587953570822, 0.04281636148112988, 0.8598498738797345, 0.44429157272598063, 0.9428181908600994, 0.7420411693289589, 0.21168002093853044]
Training loss = 0.019076844155788423
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.785
Training loss = 0.01902416944503784
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01713675061861674
step = 2, Training Accuracy: 0.76
Training loss = 0.01716607908407847
step = 3, Training Accuracy: 0.78
Training loss = 0.016572287877400716
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.019016606708367665
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.015032394031683605
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015537389914194743
step = 7, Training Accuracy: 0.8
Training loss = 0.016470434566338857
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.014456712106863657
step = 9, Training Accuracy: 0.79
Training loss = 0.01649500072002411
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.014422287543614706
step = 11, Training Accuracy: 0.82
Training loss = 0.01654225339492162
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01601736048857371
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.016476536293824513
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.775
params:  [0.4624206206171246, 0.5093340795645284, 0.99, 0.34380919641934854, 0.9589732338328418, 0.6928994751315519, 0.7820695575215005, 0.4535389619288717, 0.4436263534838386, 0.08325501078655506, 0.39128598747147153, 0.99, 0.99, 0.01, 0.07439029451803808, 0.38699054064733374, 0.01, 0.09333606608314891, 0.05809538778335284, 0.9836990353630699, 0.11133379185625751, 0.9153814697980127, 0.32250291343946574, 0.09658566015783748]
Training loss = 0.01701164275407791
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.785
Training loss = 0.01792596677939097
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.018140613238016766
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.016620061993598938
step = 3, Training Accuracy: 0.77
Training loss = 0.016820198396841686
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016121742725372316
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.016510505974292756
step = 6, Training Accuracy: 0.79
Training loss = 0.014325558841228485
step = 7, Training Accuracy: 0.81
Training loss = 0.016306601762771607
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01592872480551402
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015773902585109075
step = 10, Training Accuracy: 0.79
Training loss = 0.014572215080261231
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01453710546096166
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.0162197016676267
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.014470737477143605
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.7925
params:  [0.27107899048734474, 0.48766302128359873, 0.99, 0.06741208813602712, 0.7268368833546917, 0.05809114699462209, 0.8647803632874453, 0.8029310412832051, 0.6793955905538087, 0.3097388123258162, 0.4401336334932643, 0.7233672684040267, 0.8949871533151745, 0.07476258851327774, 0.14742022869256213, 0.7423366303536112, 0.310211088230408, 0.35135294487863195, 0.06914642071887035, 0.6517988436668238, 0.20961077115267965, 0.7480297542304356, 0.4294420032916085, 0.05924767118503675]
Training loss = 0.015882522066434226
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7925
Training loss = 0.015848615169525147
step = 1, Training Accuracy: 0.8
Training loss = 0.016032585601011912
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.017605859140555066
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.014960224628448487
step = 4, Training Accuracy: 0.79
Training loss = 0.014263238112131755
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.015350500841935476
step = 6, Training Accuracy: 0.79
Training loss = 0.014370671212673188
step = 7, Training Accuracy: 0.79
Training loss = 0.01509452263514201
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01422057956457138
step = 9, Training Accuracy: 0.82
Training loss = 0.015179656197627385
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01345424105723699
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.01556222399075826
step = 12, Training Accuracy: 0.78
Training loss = 0.012702382405598959
step = 13, Training Accuracy: 0.82
Training loss = 0.013143819471200307
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.805
params:  [0.5729755436380659, 0.1351428605482124, 0.8253661046334246, 0.17271318838176838, 0.5454549308960328, 0.45171116582957066, 0.6826601287158507, 0.17249549716082982, 0.3957386326854029, 0.25708019006483257, 0.31549647040229606, 0.7461512346291378, 0.7839987970721688, 0.28065468790701564, 0.41457928816589856, 0.5073976889838305, 0.16563395891748725, 0.23474240106088917, 0.25755741072783134, 0.8197328504566167, 0.01, 0.9053758430479675, 0.4905947842859826, 0.03600474006544416]
Training loss = 0.01637628858288129
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79875
Training loss = 0.016305483182271322
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.017074943284193674
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.01590345650911331
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01525250236193339
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.014797148257493974
step = 5, Training Accuracy: 0.81
Training loss = 0.014148181875546773
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016530254284540812
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014974953730901082
step = 8, Training Accuracy: 0.8
Training loss = 0.013736601968606313
step = 9, Training Accuracy: 0.81
Training loss = 0.014400255481402079
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.013522335588932037
step = 11, Training Accuracy: 0.82
Training loss = 0.014481898794571558
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.012227649788061777
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.014496429562568665
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78125
params:  [0.4674502058580655, 0.37747057157713704, 0.99, 0.4159867198692467, 0.6158281669122367, 0.39564078130118896, 0.8455250826402062, 0.3631433320548772, 0.01685956988090792, 0.29355421862138736, 0.42297489141834266, 0.6461109483227754, 0.8288533226477091, 0.01, 0.4238427755697305, 0.6173619752755914, 0.3705636284434256, 0.019138479410550954, 0.34335164600035867, 0.7200124678670413, 0.3811695988383992, 0.7908363176313356, 0.5817733631468407, 0.01]
Training loss = 0.016706210176150003
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.78125
Training loss = 0.015938410758972166
step = 1, Training Accuracy: 0.81
Training loss = 0.016678094466527304
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014796862900257111
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.015264267126719158
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.015339741905530294
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014121685922145844
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.017543532749017078
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.013773769438266754
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.01382514794667562
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015695486118396124
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015102586845556895
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01558561940987905
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.015754796663920084
step = 13, Training Accuracy: 0.81
Training loss = 0.013753218253453573
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78125
params:  [0.2693210681962551, 0.5133710464229921, 0.9508517895219472, 0.2979403532037341, 0.8053316726121836, 0.2961881514789172, 0.7553991121918336, 0.4120759645914517, 0.34803190892355007, 0.2738808906118336, 0.3625430896006484, 0.755163588282327, 0.7821915054233272, 0.01, 0.38776569195364746, 0.5920169123610975, 0.591043460963014, 0.153486997885656, 0.01, 0.6193472587869896, 0.02351019012182315, 0.804461151005364, 0.32201465797787715, 0.2531034215901762]
Training loss = 0.014672021170457204
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.78
Training loss = 0.01608246773481369
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.015892569919427234
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.014264033635457357
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.014278256793816885
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.013445702145497004
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.013149121056000392
step = 6, Training Accuracy: 0.83
Training loss = 0.01331023226181666
step = 7, Training Accuracy: 0.84
Training loss = 0.012550444602966308
step = 8, Training Accuracy: 0.84
Training loss = 0.013423137764135996
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.012734035203854244
step = 10, Training Accuracy: 0.84
Training loss = 0.012987252771854401
step = 11, Training Accuracy: 0.84
Training loss = 0.012676043013731639
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.012768786152203877
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.01249041199684143
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.78375
params:  [0.7845769207355459, 0.26192026725516815, 0.99, 0.36488525461629745, 0.9420356119977067, 0.4571539542435887, 0.9327839052629845, 0.3545062249612882, 0.24848626466447277, 0.16688086279331615, 0.28138216787589343, 0.7415613159500941, 0.8302757952584285, 0.2993244617659117, 0.24690267019743894, 0.4895694777273546, 0.22291375273856015, 0.01, 0.023247099873066876, 0.8542619553478539, 0.01, 0.5782163952369527, 0.29236371169140707, 0.19028105619591318]
Training loss = 0.014736322164535522
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78875
Training loss = 0.015053413311640422
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.01323359449704488
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.013076219509045283
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.015390710433324179
step = 4, Training Accuracy: 0.79
Training loss = 0.013558136721452077
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.014119839519262314
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.013385067681471506
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.011845893114805222
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.012378437618414562
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.014150786499182383
step = 10, Training Accuracy: 0.8533333333333334
Training loss = 0.012134397327899933
step = 11, Training Accuracy: 0.85
Training loss = 0.01326358844836553
step = 12, Training Accuracy: 0.83
Training loss = 0.012274445195992788
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.011760350118080775
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.79125
[[0.3476398538573186, 0.7635625511081341, 0.99, 0.3421034262399584, 0.678304727964951, 0.46310335996913393, 0.99, 0.6494148584138453, 0.5372438584352777, 0.17207696707210723, 0.40728849875908524, 0.9422458335872642, 0.99, 0.01, 0.44231045484858555, 0.4820353638952741, 0.01, 0.01, 0.01, 0.8439181524996774, 0.46637246910977403, 0.5395828648429695, 0.38815400667991823, 0.13042624765788027], [0.5134473685967758, 0.40207952794737833, 0.8924594197507069, 0.380020475268155, 0.888792063535453, 0.3096360092708688, 0.8382439653595518, 0.3628424179467742, 0.2120324703884158, 0.06230086375117186, 0.2009179191281774, 0.6857837505237724, 0.970530841390573, 0.01, 0.12704256781769685, 0.6149334625764673, 0.0772407473838601, 0.2404587953570822, 0.04281636148112988, 0.8598498738797345, 0.44429157272598063, 0.9428181908600994, 0.7420411693289589, 0.21168002093853044], [0.4624206206171246, 0.5093340795645284, 0.99, 0.34380919641934854, 0.9589732338328418, 0.6928994751315519, 0.7820695575215005, 0.4535389619288717, 0.4436263534838386, 0.08325501078655506, 0.39128598747147153, 0.99, 0.99, 0.01, 0.07439029451803808, 0.38699054064733374, 0.01, 0.09333606608314891, 0.05809538778335284, 0.9836990353630699, 0.11133379185625751, 0.9153814697980127, 0.32250291343946574, 0.09658566015783748], [0.27107899048734474, 0.48766302128359873, 0.99, 0.06741208813602712, 0.7268368833546917, 0.05809114699462209, 0.8647803632874453, 0.8029310412832051, 0.6793955905538087, 0.3097388123258162, 0.4401336334932643, 0.7233672684040267, 0.8949871533151745, 0.07476258851327774, 0.14742022869256213, 0.7423366303536112, 0.310211088230408, 0.35135294487863195, 0.06914642071887035, 0.6517988436668238, 0.20961077115267965, 0.7480297542304356, 0.4294420032916085, 0.05924767118503675], [0.5729755436380659, 0.1351428605482124, 0.8253661046334246, 0.17271318838176838, 0.5454549308960328, 0.45171116582957066, 0.6826601287158507, 0.17249549716082982, 0.3957386326854029, 0.25708019006483257, 0.31549647040229606, 0.7461512346291378, 0.7839987970721688, 0.28065468790701564, 0.41457928816589856, 0.5073976889838305, 0.16563395891748725, 0.23474240106088917, 0.25755741072783134, 0.8197328504566167, 0.01, 0.9053758430479675, 0.4905947842859826, 0.03600474006544416], [0.4674502058580655, 0.37747057157713704, 0.99, 0.4159867198692467, 0.6158281669122367, 0.39564078130118896, 0.8455250826402062, 0.3631433320548772, 0.01685956988090792, 0.29355421862138736, 0.42297489141834266, 0.6461109483227754, 0.8288533226477091, 0.01, 0.4238427755697305, 0.6173619752755914, 0.3705636284434256, 0.019138479410550954, 0.34335164600035867, 0.7200124678670413, 0.3811695988383992, 0.7908363176313356, 0.5817733631468407, 0.01], [0.2693210681962551, 0.5133710464229921, 0.9508517895219472, 0.2979403532037341, 0.8053316726121836, 0.2961881514789172, 0.7553991121918336, 0.4120759645914517, 0.34803190892355007, 0.2738808906118336, 0.3625430896006484, 0.755163588282327, 0.7821915054233272, 0.01, 0.38776569195364746, 0.5920169123610975, 0.591043460963014, 0.153486997885656, 0.01, 0.6193472587869896, 0.02351019012182315, 0.804461151005364, 0.32201465797787715, 0.2531034215901762], [0.7845769207355459, 0.26192026725516815, 0.99, 0.36488525461629745, 0.9420356119977067, 0.4571539542435887, 0.9327839052629845, 0.3545062249612882, 0.24848626466447277, 0.16688086279331615, 0.28138216787589343, 0.7415613159500941, 0.8302757952584285, 0.2993244617659117, 0.24690267019743894, 0.4895694777273546, 0.22291375273856015, 0.01, 0.023247099873066876, 0.8542619553478539, 0.01, 0.5782163952369527, 0.29236371169140707, 0.19028105619591318]]
11 	8     	0.787344	0.00862494	0.775  	0.805  
params:  [0.36211667685290133, 0.5267279532704531, 0.99, 0.2933477385086222, 0.5164709746786307, 0.28754086493089054, 0.7034723170213434, 0.35413448490979904, 0.8041635895035569, 0.3570700266667056, 0.3074774970784885, 0.8921744748617711, 0.9258320105594054, 0.053133904208069974, 0.31506685804424744, 0.3186240065323552, 0.3957691769084905, 0.26050851764475863, 0.01, 0.7049507464115558, 0.4521842901986082, 0.99, 0.5006942683265946, 0.38369330045497674]
Training loss = 0.019014317095279693
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.79375
Training loss = 0.01857822611927986
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017920826574166614
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.0161124449968338
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.01575788656870524
step = 4, Training Accuracy: 0.8
Training loss = 0.01710019459327062
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.01534549574057261
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015262784163157146
step = 7, Training Accuracy: 0.81
Training loss = 0.014940697054068247
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.016886748671531678
step = 9, Training Accuracy: 0.78
Training loss = 0.014969818492730458
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01394448017080625
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01437930812438329
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014927231123050054
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.016021348734696707
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7725
params:  [0.27804523443909757, 0.621115554751849, 0.7655591518106853, 0.03355995827666669, 0.9149641874671048, 0.1160115300788169, 0.7905225350553481, 0.4631428587036052, 0.05228589696995445, 0.33755572523726435, 0.5228448038893158, 0.8044521697030288, 0.8649043778045797, 0.014527634498774492, 0.01, 0.6274949485815212, 0.32066145424532944, 0.24192665347570352, 0.01, 0.9629649042795069, 0.01, 0.7271857929553142, 0.2256072794504513, 0.01]
Training loss = 0.018774861991405486
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.765
Training loss = 0.014996246993541717
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016414776047070822
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.015635500649611157
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.01618650769193967
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.016820682684580486
step = 5, Training Accuracy: 0.8
Training loss = 0.015812698702017465
step = 6, Training Accuracy: 0.79
Training loss = 0.015738595724105835
step = 7, Training Accuracy: 0.83
Training loss = 0.015576914548873902
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015952722430229188
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.014815416137377422
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.014420999189217885
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.014295291205247243
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014436250279347102
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.017107698818047842
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7875
params:  [0.4592700448335963, 0.38291317694371174, 0.99, 0.14287283854209934, 0.8849629410907587, 0.35705180197171366, 0.5921256172501239, 0.7616427270684568, 0.5331252867284806, 0.10762891763558632, 0.5714797343478699, 0.7436454554714507, 0.9386284201492788, 0.08350337920168424, 0.37845433095166275, 0.5593547758823421, 0.4054665787745018, 0.01, 0.01, 0.8157911412149806, 0.01, 0.9119741424998855, 0.15584411682475224, 0.01]
Training loss = 0.014381994505723318
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79375
Training loss = 0.014033007224400838
step = 1, Training Accuracy: 0.84
Training loss = 0.014057152370611826
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.013659268617630005
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.014204845130443573
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.014575505157311758
step = 5, Training Accuracy: 0.81
Training loss = 0.013478279014428456
step = 6, Training Accuracy: 0.82
Training loss = 0.0129903581738472
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.01174227664868037
step = 8, Training Accuracy: 0.84
Training loss = 0.01338286946217219
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.012968136171499889
step = 10, Training Accuracy: 0.8366666666666667
Training loss = 0.012193411588668823
step = 11, Training Accuracy: 0.83
Training loss = 0.01245205208659172
step = 12, Training Accuracy: 0.84
Training loss = 0.011954313268264134
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.012446330214540164
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.775
params:  [0.36434238199579766, 0.5010456199004725, 0.7716658002382882, 0.1697709858550847, 0.952192275186275, 0.01, 0.8414761890727733, 0.8013083738058502, 0.5362328053287047, 0.15510442797705187, 0.4792728264895245, 0.7150436439325751, 0.9843642187004992, 0.012558078596804884, 0.17680688111191428, 0.4786132594960766, 0.17429193042942528, 0.5248142927501218, 0.0865351915378323, 0.6019030303969328, 0.0446957686788754, 0.737895527262398, 0.2139099992664397, 0.10873861721408143]
Training loss = 0.015356238583723704
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77625
Training loss = 0.014669046998023986
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01547589768966039
step = 2, Training Accuracy: 0.83
Training loss = 0.016532564262549083
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.014685416519641876
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.013658608098824819
step = 5, Training Accuracy: 0.8
Training loss = 0.014251219481229782
step = 6, Training Accuracy: 0.79
Training loss = 0.013579404950141906
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.013324010769526164
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.013204578310251236
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.013733916232983272
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01582722713549932
step = 11, Training Accuracy: 0.82
Training loss = 0.013390963276227316
step = 12, Training Accuracy: 0.83
Training loss = 0.012669743796189625
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.01547737181186676
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.7825
params:  [0.3213161735382759, 0.4391590747756056, 0.9353101990070121, 0.46062059534999444, 0.564327109872881, 0.22304031393498958, 0.7373168078617688, 0.5976496569719497, 0.30756832188369837, 0.012619493262431142, 0.3547815354217823, 0.99, 0.7786614960692022, 0.16784737374167025, 0.2970227692828478, 0.6452346570737951, 0.12963555481196049, 0.6752965908841159, 0.01, 0.9308303662603072, 0.33555912926235354, 0.6574783053969627, 0.713029148874847, 0.04724263265255466]
Training loss = 0.020074956516424814
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79
Training loss = 0.018678027490774792
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.016753831803798677
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.020839340289433798
step = 3, Training Accuracy: 0.76
Training loss = 0.017248200327157973
step = 4, Training Accuracy: 0.78
Training loss = 0.01956793963909149
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.017509409189224244
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016113639672597248
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.020010964175065357
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016569195489088695
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015093105733394624
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.016072214742501575
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01595526874065399
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.016300216217835745
step = 13, Training Accuracy: 0.77
Training loss = 0.015602870980898539
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
params:  [0.2477845738980693, 0.5481407265242396, 0.99, 0.19807256937435677, 0.6064839508688604, 0.2812920973432659, 0.9419475359532695, 0.486811487448902, 0.7513659989760133, 0.11219544053731288, 0.23572490604651944, 0.9690758889293183, 0.9687286728412043, 0.01, 0.01, 0.6883139103974127, 0.01, 0.01, 0.20930741642490358, 0.7729898733199526, 0.11041601172336632, 0.6826357573597938, 0.1982876833053789, 0.09475233421977718]
Training loss = 0.014725699822107951
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.78375
Training loss = 0.014760815352201462
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.016778336862723033
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.016134512225786844
step = 3, Training Accuracy: 0.78
Training loss = 0.01458266407251358
step = 4, Training Accuracy: 0.8
Training loss = 0.014983289937178294
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.016160270869731902
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.013491137474775315
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.013079833736022313
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013852993746598562
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.01557057281335195
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.011994945804278056
step = 11, Training Accuracy: 0.85
Training loss = 0.014307523767153421
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013871884147326152
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.013721959789594014
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.76625
params:  [0.1962122767068527, 0.5838391748045492, 0.99, 0.5836953183869164, 0.7185293369098711, 0.38865479227137156, 0.99, 0.7290450085920618, 0.47952801229687825, 0.050157918194749324, 0.668970149167903, 0.7355737443706066, 0.99, 0.0457956517438847, 0.1312010752200457, 0.8325674256461892, 0.2629021089458544, 0.01, 0.01, 0.7881856242230615, 0.026611510820802237, 0.8509415569870356, 0.5011113998836128, 0.055142165578435126]
Training loss = 0.018588429888089498
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.77
Training loss = 0.01876833657423655
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.018087615768114726
step = 2, Training Accuracy: 0.76
Training loss = 0.01629073530435562
step = 3, Training Accuracy: 0.79
Training loss = 0.015126211643218994
step = 4, Training Accuracy: 0.81
Training loss = 0.0158683043718338
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.016718335251013437
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01599064071973165
step = 7, Training Accuracy: 0.81
Training loss = 0.017001635829607644
step = 8, Training Accuracy: 0.78
Training loss = 0.015065805514653524
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01715951810280482
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.014898318449656169
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.01677776505549749
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01495076835155487
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01436880737543106
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.78875
params:  [0.5000212615432286, 0.4171153805109483, 0.9846236146746352, 0.33553835424741063, 0.8061773568790614, 0.04693881469046274, 0.99, 0.7140596650835933, 0.4030663133449014, 0.290722912523385, 0.33151042881688, 0.7044069531154027, 0.99, 0.01, 0.06097102799197611, 0.8529092461315558, 0.10613475626766464, 0.01, 0.0517943993902673, 0.9785538608327822, 0.2822939424765039, 0.40913449291780596, 0.390446811347611, 0.3008391328150583]
Training loss = 0.017829522589842477
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.78375
Training loss = 0.018149604598681132
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016261895100275675
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01671511640151342
step = 3, Training Accuracy: 0.79
Training loss = 0.017744074761867522
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.017086993753910065
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.01365559349457423
step = 6, Training Accuracy: 0.83
Training loss = 0.01629731963078181
step = 7, Training Accuracy: 0.8
Training loss = 0.01602833280960719
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015503329833348593
step = 9, Training Accuracy: 0.81
Training loss = 0.016457227965195973
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015541358590126038
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.014474411507447561
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.015372638901074728
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.01587775468826294
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.795
[[0.36211667685290133, 0.5267279532704531, 0.99, 0.2933477385086222, 0.5164709746786307, 0.28754086493089054, 0.7034723170213434, 0.35413448490979904, 0.8041635895035569, 0.3570700266667056, 0.3074774970784885, 0.8921744748617711, 0.9258320105594054, 0.053133904208069974, 0.31506685804424744, 0.3186240065323552, 0.3957691769084905, 0.26050851764475863, 0.01, 0.7049507464115558, 0.4521842901986082, 0.99, 0.5006942683265946, 0.38369330045497674], [0.27804523443909757, 0.621115554751849, 0.7655591518106853, 0.03355995827666669, 0.9149641874671048, 0.1160115300788169, 0.7905225350553481, 0.4631428587036052, 0.05228589696995445, 0.33755572523726435, 0.5228448038893158, 0.8044521697030288, 0.8649043778045797, 0.014527634498774492, 0.01, 0.6274949485815212, 0.32066145424532944, 0.24192665347570352, 0.01, 0.9629649042795069, 0.01, 0.7271857929553142, 0.2256072794504513, 0.01], [0.4592700448335963, 0.38291317694371174, 0.99, 0.14287283854209934, 0.8849629410907587, 0.35705180197171366, 0.5921256172501239, 0.7616427270684568, 0.5331252867284806, 0.10762891763558632, 0.5714797343478699, 0.7436454554714507, 0.9386284201492788, 0.08350337920168424, 0.37845433095166275, 0.5593547758823421, 0.4054665787745018, 0.01, 0.01, 0.8157911412149806, 0.01, 0.9119741424998855, 0.15584411682475224, 0.01], [0.36434238199579766, 0.5010456199004725, 0.7716658002382882, 0.1697709858550847, 0.952192275186275, 0.01, 0.8414761890727733, 0.8013083738058502, 0.5362328053287047, 0.15510442797705187, 0.4792728264895245, 0.7150436439325751, 0.9843642187004992, 0.012558078596804884, 0.17680688111191428, 0.4786132594960766, 0.17429193042942528, 0.5248142927501218, 0.0865351915378323, 0.6019030303969328, 0.0446957686788754, 0.737895527262398, 0.2139099992664397, 0.10873861721408143], [0.3213161735382759, 0.4391590747756056, 0.9353101990070121, 0.46062059534999444, 0.564327109872881, 0.22304031393498958, 0.7373168078617688, 0.5976496569719497, 0.30756832188369837, 0.012619493262431142, 0.3547815354217823, 0.99, 0.7786614960692022, 0.16784737374167025, 0.2970227692828478, 0.6452346570737951, 0.12963555481196049, 0.6752965908841159, 0.01, 0.9308303662603072, 0.33555912926235354, 0.6574783053969627, 0.713029148874847, 0.04724263265255466], [0.2477845738980693, 0.5481407265242396, 0.99, 0.19807256937435677, 0.6064839508688604, 0.2812920973432659, 0.9419475359532695, 0.486811487448902, 0.7513659989760133, 0.11219544053731288, 0.23572490604651944, 0.9690758889293183, 0.9687286728412043, 0.01, 0.01, 0.6883139103974127, 0.01, 0.01, 0.20930741642490358, 0.7729898733199526, 0.11041601172336632, 0.6826357573597938, 0.1982876833053789, 0.09475233421977718], [0.1962122767068527, 0.5838391748045492, 0.99, 0.5836953183869164, 0.7185293369098711, 0.38865479227137156, 0.99, 0.7290450085920618, 0.47952801229687825, 0.050157918194749324, 0.668970149167903, 0.7355737443706066, 0.99, 0.0457956517438847, 0.1312010752200457, 0.8325674256461892, 0.2629021089458544, 0.01, 0.01, 0.7881856242230615, 0.026611510820802237, 0.8509415569870356, 0.5011113998836128, 0.055142165578435126], [0.5000212615432286, 0.4171153805109483, 0.9846236146746352, 0.33553835424741063, 0.8061773568790614, 0.04693881469046274, 0.99, 0.7140596650835933, 0.4030663133449014, 0.290722912523385, 0.33151042881688, 0.7044069531154027, 0.99, 0.01, 0.06097102799197611, 0.8529092461315558, 0.10613475626766464, 0.01, 0.0517943993902673, 0.9785538608327822, 0.2822939424765039, 0.40913449291780596, 0.390446811347611, 0.3008391328150583]]
12 	8     	0.782031	0.0092055 	0.76625	0.795  
params:  [0.24134312435577182, 0.05148931173668131, 0.99, 0.28895971436922513, 0.6653825983822902, 0.08948895990880426, 0.7085494779194917, 0.7476008717705813, 0.2284662300271732, 0.6115827580614379, 0.3117556667990369, 0.7210662708621992, 0.8456603629132671, 0.28268073110981773, 0.074560887249915, 0.782597898149171, 0.054889544918880664, 0.18886972510818545, 0.15922269400046016, 0.9106976515025643, 0.1610476410124757, 0.462449363691127, 0.6329882226538656, 0.01]
Training loss = 0.01566175421079
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.8025
Training loss = 0.014454813897609711
step = 1, Training Accuracy: 0.82
Training loss = 0.014615541597207387
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.012662172913551331
step = 3, Training Accuracy: 0.84
Training loss = 0.012994161595900854
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.013798105269670487
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.012952664891878764
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.012168196042378744
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.012927516847848892
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.012905510018269221
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011217072755098343
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.013467081586519877
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.012769585847854615
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.012474116086959839
step = 13, Training Accuracy: 0.84
Training loss = 0.012799376447995503
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7975
params:  [0.31035393585949955, 0.41359072726860074, 0.9470208788522307, 0.6830855005631846, 0.8804427433046507, 0.05279622249789406, 0.99, 0.755444539298485, 0.3872097547143182, 0.36904017034140696, 0.47813738460594385, 0.7948120102977217, 0.8251139413479491, 0.22884128566919024, 0.14647296765961376, 0.99, 0.16054216575575647, 0.2249301289184048, 0.01, 0.99, 0.3113655880530619, 0.42103582659595706, 0.5624930458861377, 0.0658906134624643]
Training loss = 0.01788499583800634
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.8025
Training loss = 0.016886790593465168
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018893171151479086
step = 2, Training Accuracy: 0.77
Training loss = 0.017433302501837413
step = 3, Training Accuracy: 0.77
Training loss = 0.01668089598417282
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017615320483843486
step = 5, Training Accuracy: 0.76
Training loss = 0.019034458299477894
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.016185816625754038
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015322375098864237
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01614268347620964
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01600095917781194
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.016162576973438262
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.017978338499863942
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.018873507777849834
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01547004908323288
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.78625
params:  [0.20525159343442265, 0.4068264777526187, 0.99, 0.501810899761109, 0.8470578019326358, 0.4790245917481178, 0.7987491710785455, 0.909578660645112, 0.42333192858813884, 0.4049200475736514, 0.32535718592929264, 0.7256276299293745, 0.6711194131419663, 0.6274721273022262, 0.29800825856965124, 0.554369876660248, 0.017365754470541034, 0.12438266968756358, 0.06971558624208632, 0.8653030544790528, 0.16720694030193348, 0.7432706684458829, 0.2342627883735034, 0.26152105062697706]
Training loss = 0.01633344203233719
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78625
Training loss = 0.016045044362545013
step = 1, Training Accuracy: 0.76
Training loss = 0.017544264098008473
step = 2, Training Accuracy: 0.75
Training loss = 0.015212824741999309
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016971653600533803
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01602844973405202
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01406927540898323
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015547018547852834
step = 7, Training Accuracy: 0.79
Training loss = 0.01591395159562429
step = 8, Training Accuracy: 0.77
Training loss = 0.016190219720204672
step = 9, Training Accuracy: 0.79
Training loss = 0.015311058163642883
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01671711544195811
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015002585649490356
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.015711008807023367
step = 13, Training Accuracy: 0.81
Training loss = 0.015295436183611552
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.77375
params:  [0.35216590728741975, 0.7999868451576585, 0.6837190760417344, 0.24923365085455376, 0.6848245585094273, 0.21495681915209924, 0.99, 0.8320989498782085, 0.33571872078086823, 0.01, 0.5415405193114703, 0.9889470727524279, 0.7711076892121116, 0.06290265367119226, 0.1854376319330227, 0.8841809876066306, 0.19707196126442733, 0.14640115384655678, 0.2657558166399245, 0.8502766029849124, 0.5158752733631714, 0.6286782117738823, 0.8426397413577533, 0.026125619860895732]
Training loss = 0.023758543729782103
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.77625
Training loss = 0.018692492445309957
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01873082717259725
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.018762473165988922
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.02061301718155543
step = 4, Training Accuracy: 0.7133333333333334
Training loss = 0.021499972641468048
step = 5, Training Accuracy: 0.74
Training loss = 0.020342126488685608
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.02023933579524358
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.024087668458620707
step = 8, Training Accuracy: 0.68
Training loss = 0.019060942033926647
step = 9, Training Accuracy: 0.77
Training loss = 0.019675720731417337
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.01916342377662659
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.019124207894007365
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.018456722994645437
step = 13, Training Accuracy: 0.76
Training loss = 0.020093663434187573
step = 14, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.80625
params:  [0.26586560522403635, 0.7304155422446029, 0.9116714312137599, 0.08150153075943606, 0.7158390549678938, 0.01, 0.99, 0.8468242944694345, 0.4962325772888782, 0.10077116771046342, 0.43338569513011993, 0.9442369795561262, 0.9036704941658881, 0.01, 0.01, 0.9012837506531857, 0.01, 0.15407703979294202, 0.3181119386599172, 0.6237828176640738, 0.2633858975365008, 0.6321487937088507, 0.477387834620873, 0.18640070468861947]
Training loss = 0.01722635527451833
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7925
Training loss = 0.01769118070602417
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01756456047296524
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.019237750669320423
step = 3, Training Accuracy: 0.76
Training loss = 0.015852608184019724
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.017333154131968818
step = 5, Training Accuracy: 0.79
Training loss = 0.015979640980561573
step = 6, Training Accuracy: 0.79
Training loss = 0.016083120306332906
step = 7, Training Accuracy: 0.76
Training loss = 0.01788929631312688
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.015181352893511454
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.015880140364170073
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.013990288774172466
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.017046762506167094
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01639565497636795
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015596893181403478
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7825
params:  [0.6256614582664193, 0.31561218155295157, 0.99, 0.24027015314402683, 0.5322174324954468, 0.01, 0.6726042538330377, 0.8028159506673154, 0.1788912990377847, 0.01, 0.7095506268739991, 0.7644842680386371, 0.99, 0.01, 0.0775320083114602, 0.7433422002728352, 0.18898725814770487, 0.3111667692045813, 0.01, 0.9728599652916393, 0.16013709137830026, 0.15382841306477485, 0.618295831172265, 0.23692935361552442]
Training loss = 0.017940632700920105
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7875
Training loss = 0.016958198845386505
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017150620619455974
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016652332345644633
step = 3, Training Accuracy: 0.81
Training loss = 0.01568103571732839
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.017427055438359578
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01698746919631958
step = 6, Training Accuracy: 0.8
Training loss = 0.016686336100101472
step = 7, Training Accuracy: 0.76
Training loss = 0.01699894239505132
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01597649812698364
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01578184207280477
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.015127701709667842
step = 11, Training Accuracy: 0.8
Training loss = 0.018813246190547944
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015706357657909394
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.014697718222935995
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.805
params:  [0.2480388074853509, 0.35082990203222336, 0.7467743864249737, 0.16745243707087717, 0.514575332051655, 0.01, 0.99, 0.5523361409577106, 0.3859888201390228, 0.01, 0.3157812382608078, 0.633365696085793, 0.99, 0.1989602913228665, 0.16578358655002254, 0.9497591420377942, 0.14870423678432176, 0.10515308743057335, 0.11036211121480685, 0.8908264350294514, 0.4896189187531885, 0.36744773769810846, 0.26742735693739705, 0.15159116101545425]
Training loss = 0.015576995213826498
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.81125
Training loss = 0.015253776411215465
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014231573343276977
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.01592137783765793
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.015753221114476523
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014007808367411296
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.015545829037825266
step = 6, Training Accuracy: 0.81
Training loss = 0.013579621861378352
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.012909003595511118
step = 8, Training Accuracy: 0.82
Training loss = 0.014767175118128459
step = 9, Training Accuracy: 0.82
Training loss = 0.013870719770590465
step = 10, Training Accuracy: 0.82
Training loss = 0.013480090498924256
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.013659627040227254
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.0142112664381663
step = 13, Training Accuracy: 0.82
Training loss = 0.013843595087528228
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.79125
params:  [0.3168775717046756, 0.39333579851269723, 0.9088661247351473, 0.42853432544338127, 0.8537649529753241, 0.1556553714999904, 0.99, 0.47611668831799536, 0.741105284678487, 0.01, 0.4484374680205552, 0.8929242007698688, 0.7213570410641933, 0.08547558268021886, 0.04584362837550375, 0.7286927866019243, 0.08668632185340246, 0.04820608145254454, 0.07320498641641508, 0.885474550096277, 0.28722859001974665, 0.6259739747869087, 0.2842531624536692, 0.01]
Training loss = 0.017401492049296696
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7975
Training loss = 0.01897918512423833
step = 1, Training Accuracy: 0.78
Training loss = 0.016844932536284128
step = 2, Training Accuracy: 0.73
Training loss = 0.01839223931233088
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016792326668898263
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01718943327665329
step = 5, Training Accuracy: 0.72
Training loss = 0.015637336174647014
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014733849267164867
step = 7, Training Accuracy: 0.79
Training loss = 0.014498344759146372
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.014145057648420334
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015545854369799297
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.015145880877971649
step = 11, Training Accuracy: 0.81
Training loss = 0.015762416422367097
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01572972963253657
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015165269474188487
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
[[0.24134312435577182, 0.05148931173668131, 0.99, 0.28895971436922513, 0.6653825983822902, 0.08948895990880426, 0.7085494779194917, 0.7476008717705813, 0.2284662300271732, 0.6115827580614379, 0.3117556667990369, 0.7210662708621992, 0.8456603629132671, 0.28268073110981773, 0.074560887249915, 0.782597898149171, 0.054889544918880664, 0.18886972510818545, 0.15922269400046016, 0.9106976515025643, 0.1610476410124757, 0.462449363691127, 0.6329882226538656, 0.01], [0.31035393585949955, 0.41359072726860074, 0.9470208788522307, 0.6830855005631846, 0.8804427433046507, 0.05279622249789406, 0.99, 0.755444539298485, 0.3872097547143182, 0.36904017034140696, 0.47813738460594385, 0.7948120102977217, 0.8251139413479491, 0.22884128566919024, 0.14647296765961376, 0.99, 0.16054216575575647, 0.2249301289184048, 0.01, 0.99, 0.3113655880530619, 0.42103582659595706, 0.5624930458861377, 0.0658906134624643], [0.20525159343442265, 0.4068264777526187, 0.99, 0.501810899761109, 0.8470578019326358, 0.4790245917481178, 0.7987491710785455, 0.909578660645112, 0.42333192858813884, 0.4049200475736514, 0.32535718592929264, 0.7256276299293745, 0.6711194131419663, 0.6274721273022262, 0.29800825856965124, 0.554369876660248, 0.017365754470541034, 0.12438266968756358, 0.06971558624208632, 0.8653030544790528, 0.16720694030193348, 0.7432706684458829, 0.2342627883735034, 0.26152105062697706], [0.35216590728741975, 0.7999868451576585, 0.6837190760417344, 0.24923365085455376, 0.6848245585094273, 0.21495681915209924, 0.99, 0.8320989498782085, 0.33571872078086823, 0.01, 0.5415405193114703, 0.9889470727524279, 0.7711076892121116, 0.06290265367119226, 0.1854376319330227, 0.8841809876066306, 0.19707196126442733, 0.14640115384655678, 0.2657558166399245, 0.8502766029849124, 0.5158752733631714, 0.6286782117738823, 0.8426397413577533, 0.026125619860895732], [0.26586560522403635, 0.7304155422446029, 0.9116714312137599, 0.08150153075943606, 0.7158390549678938, 0.01, 0.99, 0.8468242944694345, 0.4962325772888782, 0.10077116771046342, 0.43338569513011993, 0.9442369795561262, 0.9036704941658881, 0.01, 0.01, 0.9012837506531857, 0.01, 0.15407703979294202, 0.3181119386599172, 0.6237828176640738, 0.2633858975365008, 0.6321487937088507, 0.477387834620873, 0.18640070468861947], [0.6256614582664193, 0.31561218155295157, 0.99, 0.24027015314402683, 0.5322174324954468, 0.01, 0.6726042538330377, 0.8028159506673154, 0.1788912990377847, 0.01, 0.7095506268739991, 0.7644842680386371, 0.99, 0.01, 0.0775320083114602, 0.7433422002728352, 0.18898725814770487, 0.3111667692045813, 0.01, 0.9728599652916393, 0.16013709137830026, 0.15382841306477485, 0.618295831172265, 0.23692935361552442], [0.2480388074853509, 0.35082990203222336, 0.7467743864249737, 0.16745243707087717, 0.514575332051655, 0.01, 0.99, 0.5523361409577106, 0.3859888201390228, 0.01, 0.3157812382608078, 0.633365696085793, 0.99, 0.1989602913228665, 0.16578358655002254, 0.9497591420377942, 0.14870423678432176, 0.10515308743057335, 0.11036211121480685, 0.8908264350294514, 0.4896189187531885, 0.36744773769810846, 0.26742735693739705, 0.15159116101545425], [0.3168775717046756, 0.39333579851269723, 0.9088661247351473, 0.42853432544338127, 0.8537649529753241, 0.1556553714999904, 0.99, 0.47611668831799536, 0.741105284678487, 0.01, 0.4484374680205552, 0.8929242007698688, 0.7213570410641933, 0.08547558268021886, 0.04584362837550375, 0.7286927866019243, 0.08668632185340246, 0.04820608145254454, 0.07320498641641508, 0.885474550096277, 0.28722859001974665, 0.6259739747869087, 0.2842531624536692, 0.01]]
13 	8     	0.791406	0.0104103 	0.77375	0.80625
params:  [0.3132928631005317, 0.2743503160229126, 0.5540518527445745, 0.10396115787704524, 0.8940585491940787, 0.3044583020824243, 0.8153155202386669, 0.99, 0.42764998635564266, 0.1554084923532206, 0.5809351780871369, 0.6992152760490203, 0.6638704837587781, 0.27514439671498125, 0.3323750335457484, 0.9390781312133911, 0.5332148783369915, 0.1408304797827715, 0.01, 0.5617962269326405, 0.38880723980809845, 0.2692112580252226, 0.5751824758712799, 0.48877982678894133]
Training loss = 0.015761996706326803
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.795
Training loss = 0.0164826570947965
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.015798125863075257
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015603942722082138
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.01551021953423818
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.015526719987392426
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.015239365051190059
step = 6, Training Accuracy: 0.8
Training loss = 0.014137594203154247
step = 7, Training Accuracy: 0.81
Training loss = 0.015869033932685853
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.013384025742610295
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.01469786504904429
step = 10, Training Accuracy: 0.83
Training loss = 0.014315697600444158
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.014206462502479554
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014345862368742624
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.013515826463699341
step = 14, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.805
params:  [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01]
Training loss = 0.017867169082164763
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79625
Training loss = 0.017990358422199884
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01628911594549815
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.01793512503306071
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01795981466770172
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015053940912087758
step = 5, Training Accuracy: 0.8
Training loss = 0.015122793465852737
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.01594723512729009
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.019816536605358124
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015349411765734355
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015404236912727355
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016165798505147298
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.015772102375825246
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013997955024242401
step = 13, Training Accuracy: 0.8
Training loss = 0.014435583750406902
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.81125
params:  [0.1976806989715719, 0.4662392255497988, 0.99, 0.39807561797634383, 0.6945163314202992, 0.01, 0.6956962263820807, 0.5394766654416709, 0.18749514880396712, 0.1081631266238128, 0.46019720083757765, 0.8395414088546094, 0.660791921385456, 0.42961055831543915, 0.12563484089013438, 0.7412320184415511, 0.06803418643267534, 0.13141214992852207, 0.4101320098578352, 0.6879514824521615, 0.5888833674735017, 0.3322397841540839, 0.8964959738826647, 0.11169591973165352]
Training loss = 0.01618364453315735
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.8125
Training loss = 0.018269532918930055
step = 1, Training Accuracy: 0.74
Training loss = 0.01665387620528539
step = 2, Training Accuracy: 0.78
Training loss = 0.017920080622037253
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01706535279750824
step = 4, Training Accuracy: 0.75
Training loss = 0.016315860450267793
step = 5, Training Accuracy: 0.8
Training loss = 0.01604593833287557
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.016524546444416047
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01665715823570887
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.015585870146751404
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.017110660473505655
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.0173320139447848
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.016525647441546124
step = 12, Training Accuracy: 0.8
Training loss = 0.014730077932278315
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.015015139281749725
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79875
params:  [0.35507250567515997, 0.5619851477272976, 0.8561921312672975, 0.24553854231765268, 0.5314733850507563, 0.25294280571551614, 0.942838741889036, 0.6412649424997192, 0.04169257857349892, 0.01, 0.657555421490532, 0.795687558311308, 0.7762018424372004, 0.15065354769164122, 0.4394140150615159, 0.9260620384603369, 0.5543046374252536, 0.34468183276981423, 0.4218299315767311, 0.99, 0.3650276254917218, 0.4541677217443269, 0.8398067613255016, 0.01]
Training loss = 0.019514174660046894
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.8025
Training loss = 0.017355788946151734
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.016700288057327272
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01805836985508601
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.01956435590982437
step = 4, Training Accuracy: 0.73
Training loss = 0.018728430966536203
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.018505111237366995
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.016144217848777773
step = 7, Training Accuracy: 0.78
Training loss = 0.016940202713012695
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01733900745709737
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01646474649508794
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.017483234206835428
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.017645624577999116
step = 12, Training Accuracy: 0.77
Training loss = 0.015284659365812938
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01640373428662618
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.80875
params:  [0.3491221542490772, 0.6055883338595063, 0.9309545997226264, 0.01, 0.6979855961633828, 0.1945543974233638, 0.99, 0.5168187748485575, 0.23944793228824568, 0.24485754235462806, 0.3506051700524975, 0.99, 0.6403822227725702, 0.01, 0.20918836067582622, 0.7365601159605009, 0.22442245879811115, 0.5284614819154853, 0.38613376352847445, 0.8160335904685428, 0.6708074523960614, 0.411398897956273, 0.7725369117482057, 0.20231140746894566]
Training loss = 0.02110934376716614
step = 0, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.805
Training loss = 0.019500392476717632
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.020323410431543985
step = 2, Training Accuracy: 0.71
Training loss = 0.018648892641067505
step = 3, Training Accuracy: 0.75
Training loss = 0.019235973755518596
step = 4, Training Accuracy: 0.75
Training loss = 0.019992312788963316
step = 5, Training Accuracy: 0.72
Training loss = 0.018932285110155742
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018897838592529297
step = 7, Training Accuracy: 0.75
Training loss = 0.01798134982585907
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.017657697399457297
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.01837162474791209
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.018217960894107817
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.019647820393244426
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01963750203450521
step = 13, Training Accuracy: 0.7
Training loss = 0.018629181881745657
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79625
params:  [0.4677104910115446, 0.6004604688396039, 0.756713195130059, 0.34240924803112477, 0.42243852969883033, 0.10299657939441012, 0.9586059505720652, 0.7356736242973622, 0.01, 0.27078502210948474, 0.43870337976563767, 0.8218684661709728, 0.99, 0.01, 0.019424698296506143, 0.6533579562929891, 0.12163735596175759, 0.3493307264442117, 0.2887789029415397, 0.99, 0.27763462378322307, 0.38065533979945726, 0.9137093575063248, 0.01]
Training loss = 0.02103258450826009
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.8025
Training loss = 0.01996997445821762
step = 1, Training Accuracy: 0.73
Training loss = 0.01955336978038152
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.018852769633134207
step = 3, Training Accuracy: 0.74
Training loss = 0.019867522219816844
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.019052850306034087
step = 5, Training Accuracy: 0.74
Training loss = 0.01828850696484248
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.019207026958465576
step = 7, Training Accuracy: 0.77
Training loss = 0.017753306726614633
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.018333680331707
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.017607609530289968
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.021228624880313872
step = 11, Training Accuracy: 0.76
Training loss = 0.01927724073330561
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.018992911875247955
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.018262362480163573
step = 14, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7975
params:  [0.2783522270683858, 0.4246737596155347, 0.828875435049343, 0.18568191830495612, 0.43435921909131747, 0.33997486836910085, 0.9186099772564016, 0.9598320620456261, 0.2452656902766348, 0.5258182699077231, 0.4402529261602701, 0.903622062184337, 0.4823398289587698, 0.2209325615744984, 0.22566590201260017, 0.8245892296289503, 0.15983017964606253, 0.07108846586556775, 0.3024417826200434, 0.72647776239974, 0.8229483116069063, 0.3781881220914361, 0.8930166339863121, 0.01]
Training loss = 0.019044423699378966
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.7975
Training loss = 0.017327473958333332
step = 1, Training Accuracy: 0.81
Training loss = 0.01714949031670888
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016045370797316234
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01750259886185328
step = 4, Training Accuracy: 0.76
Training loss = 0.01767084966103236
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01629518578449885
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.018177315990130106
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.017305638194084167
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.016787342131137847
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016839722990989684
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015858050882816315
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01566604564587275
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016918935775756837
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.017058749596277872
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79375
params:  [0.3060668005152578, 0.3430648661300615, 0.7630051964118214, 0.7110105844802587, 0.6328739719971357, 0.07328094797328487, 0.99, 0.6180888629126065, 0.15238597541384222, 0.21915051002925906, 0.2025050574613959, 0.670134912808823, 0.8796918932467961, 0.27477343968682977, 0.2020919060468696, 0.781802852738887, 0.11639272345296894, 0.4243662450542475, 0.16537364175365235, 0.8064232979478053, 0.27927923679068956, 0.3506138963330703, 0.745587087600945, 0.2133728188652097]
Training loss = 0.017083651423454284
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79625
Training loss = 0.017018948793411256
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01932839204867681
step = 2, Training Accuracy: 0.78
Training loss = 0.01600754827260971
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01647634824117025
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.016346232096354166
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01666429966688156
step = 6, Training Accuracy: 0.8
Training loss = 0.016731135249137878
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01620760897795359
step = 8, Training Accuracy: 0.78
Training loss = 0.0177166282137235
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.016482727974653243
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015722772777080535
step = 11, Training Accuracy: 0.8
Training loss = 0.016793337364991504
step = 12, Training Accuracy: 0.77
Training loss = 0.015112150808175405
step = 13, Training Accuracy: 0.8
Training loss = 0.017290295958518984
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.78875
[[0.3132928631005317, 0.2743503160229126, 0.5540518527445745, 0.10396115787704524, 0.8940585491940787, 0.3044583020824243, 0.8153155202386669, 0.99, 0.42764998635564266, 0.1554084923532206, 0.5809351780871369, 0.6992152760490203, 0.6638704837587781, 0.27514439671498125, 0.3323750335457484, 0.9390781312133911, 0.5332148783369915, 0.1408304797827715, 0.01, 0.5617962269326405, 0.38880723980809845, 0.2692112580252226, 0.5751824758712799, 0.48877982678894133], [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01], [0.1976806989715719, 0.4662392255497988, 0.99, 0.39807561797634383, 0.6945163314202992, 0.01, 0.6956962263820807, 0.5394766654416709, 0.18749514880396712, 0.1081631266238128, 0.46019720083757765, 0.8395414088546094, 0.660791921385456, 0.42961055831543915, 0.12563484089013438, 0.7412320184415511, 0.06803418643267534, 0.13141214992852207, 0.4101320098578352, 0.6879514824521615, 0.5888833674735017, 0.3322397841540839, 0.8964959738826647, 0.11169591973165352], [0.35507250567515997, 0.5619851477272976, 0.8561921312672975, 0.24553854231765268, 0.5314733850507563, 0.25294280571551614, 0.942838741889036, 0.6412649424997192, 0.04169257857349892, 0.01, 0.657555421490532, 0.795687558311308, 0.7762018424372004, 0.15065354769164122, 0.4394140150615159, 0.9260620384603369, 0.5543046374252536, 0.34468183276981423, 0.4218299315767311, 0.99, 0.3650276254917218, 0.4541677217443269, 0.8398067613255016, 0.01], [0.3491221542490772, 0.6055883338595063, 0.9309545997226264, 0.01, 0.6979855961633828, 0.1945543974233638, 0.99, 0.5168187748485575, 0.23944793228824568, 0.24485754235462806, 0.3506051700524975, 0.99, 0.6403822227725702, 0.01, 0.20918836067582622, 0.7365601159605009, 0.22442245879811115, 0.5284614819154853, 0.38613376352847445, 0.8160335904685428, 0.6708074523960614, 0.411398897956273, 0.7725369117482057, 0.20231140746894566], [0.4677104910115446, 0.6004604688396039, 0.756713195130059, 0.34240924803112477, 0.42243852969883033, 0.10299657939441012, 0.9586059505720652, 0.7356736242973622, 0.01, 0.27078502210948474, 0.43870337976563767, 0.8218684661709728, 0.99, 0.01, 0.019424698296506143, 0.6533579562929891, 0.12163735596175759, 0.3493307264442117, 0.2887789029415397, 0.99, 0.27763462378322307, 0.38065533979945726, 0.9137093575063248, 0.01], [0.2783522270683858, 0.4246737596155347, 0.828875435049343, 0.18568191830495612, 0.43435921909131747, 0.33997486836910085, 0.9186099772564016, 0.9598320620456261, 0.2452656902766348, 0.5258182699077231, 0.4402529261602701, 0.903622062184337, 0.4823398289587698, 0.2209325615744984, 0.22566590201260017, 0.8245892296289503, 0.15983017964606253, 0.07108846586556775, 0.3024417826200434, 0.72647776239974, 0.8229483116069063, 0.3781881220914361, 0.8930166339863121, 0.01], [0.3060668005152578, 0.3430648661300615, 0.7630051964118214, 0.7110105844802587, 0.6328739719971357, 0.07328094797328487, 0.99, 0.6180888629126065, 0.15238597541384222, 0.21915051002925906, 0.2025050574613959, 0.670134912808823, 0.8796918932467961, 0.27477343968682977, 0.2020919060468696, 0.781802852738887, 0.11639272345296894, 0.4243662450542475, 0.16537364175365235, 0.8064232979478053, 0.27927923679068956, 0.3506138963330703, 0.745587087600945, 0.2133728188652097]]
14 	8     	0.8     	0.00720785	0.78875	0.81125
params:  [0.264195384198241, 0.8327627071486238, 0.99, 0.2919552227102, 0.6617294156948426, 0.292127425445328, 0.99, 0.6568925110676404, 0.2202556844500421, 0.07072939463370512, 0.9745469720960563, 0.8878406395304722, 0.7857200518098447, 0.18541610647302603, 0.5185201940447343, 0.9852845783524649, 0.17204206585093348, 0.01, 0.26846983182060896, 0.7941383220244639, 0.6921127309961128, 0.6408806579666522, 0.36267539400886734, 0.3409442412482649]
Training loss = 0.018576987385749817
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.78375
Training loss = 0.018742579519748687
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.01819673031568527
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.018843311965465546
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.015682372351487478
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.016715002655982972
step = 5, Training Accuracy: 0.75
Training loss = 0.01769608646631241
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.01811826040347417
step = 7, Training Accuracy: 0.75
Training loss = 0.015907400051752726
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015527278482913971
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.016138986547787983
step = 10, Training Accuracy: 0.77
Training loss = 0.01799649159113566
step = 11, Training Accuracy: 0.74
Training loss = 0.01748806893825531
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01842192351818085
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.016136071681976318
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.775
params:  [0.339875574669313, 0.49918526367170485, 0.9103131729312753, 0.2921138294708268, 0.592729889478813, 0.04733421639045679, 0.8708130052521681, 0.6986986182123717, 0.11526296887332793, 0.01, 0.49863821809760206, 0.3041749255729081, 0.9370777873379817, 0.3682141412666046, 0.3497733626235008, 0.6513921387237186, 0.2171177645454672, 0.3367381038467938, 0.14510380180874308, 0.9719311626705511, 0.35012179259604226, 0.4397445006708199, 0.6343259584512478, 0.01]
Training loss = 0.017445944845676423
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7875
Training loss = 0.016197131474812825
step = 1, Training Accuracy: 0.79
Training loss = 0.018133200903733573
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.01677565594514211
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.0171061310172081
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015037091275056202
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.01668030391136805
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.018189760843912762
step = 7, Training Accuracy: 0.77
Training loss = 0.01575687378644943
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.016701995333035787
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.016246014336744944
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01619057923555374
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.01573076511422793
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01661807030439377
step = 13, Training Accuracy: 0.8
Training loss = 0.014762283861637115
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78625
params:  [0.36175465810626667, 0.771650759062442, 0.7363225930573026, 0.22487186137139234, 0.6812426706706778, 0.18405382797239736, 0.9184744593008435, 0.941653922557535, 0.4553939280332371, 0.14331983387947428, 0.38599113930726503, 0.6298885137194282, 0.7769220319503658, 0.01, 0.3005492131866617, 0.9061876460156759, 0.26650072347246456, 0.5004738521650314, 0.2260024318370635, 0.8164852990538298, 0.28638885168840256, 0.40063581554151895, 0.6281572124265404, 0.01]
Training loss = 0.019926685591538748
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7925
Training loss = 0.017727797230084736
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.017428320348262787
step = 2, Training Accuracy: 0.79
Training loss = 0.017518082857131956
step = 3, Training Accuracy: 0.74
Training loss = 0.01927064190308253
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.019027674595514934
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.018847871522108713
step = 6, Training Accuracy: 0.74
Training loss = 0.01974724342425664
step = 7, Training Accuracy: 0.73
Training loss = 0.017322413822015128
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01676650583744049
step = 9, Training Accuracy: 0.77
Training loss = 0.019133825898170472
step = 10, Training Accuracy: 0.74
Training loss = 0.01799640109141668
step = 11, Training Accuracy: 0.75
Training loss = 0.01852283736069997
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.020166855851809183
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.017234105865160623
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
params:  [0.17117094757979062, 0.562749669741782, 0.9107227668498393, 0.42144999224048196, 0.7934752343186503, 0.01, 0.99, 0.8788513593591532, 0.33109563034450157, 0.1485040263917836, 0.6509762766690409, 0.8675417963069617, 0.9396955510232586, 0.2055151334150429, 0.5142524634652317, 0.9457438657247512, 0.1934868126710969, 0.01, 0.01, 0.99, 0.7866551231813665, 0.3867595065441036, 0.40635275022483214, 0.22982045200965512]
Training loss = 0.01683007260163625
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78375
Training loss = 0.015809138615926106
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015279199182987212
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.016987585524717966
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016597538789113363
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.015344666043917338
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015903458893299104
step = 6, Training Accuracy: 0.82
Training loss = 0.01488207459449768
step = 7, Training Accuracy: 0.79
Training loss = 0.01744352827469508
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016049488484859466
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015577459832032521
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015261089106400809
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015134805937608083
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.01640701174736023
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.015440543194611868
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.765
params:  [0.4008643310405859, 0.6970832428896392, 0.6921465124529463, 0.31159731064700824, 0.5183754730430326, 0.20330278629902013, 0.8766073886147915, 0.5119165459923263, 0.31697879640361776, 0.18037506744844686, 0.921059628177124, 0.7831873472983643, 0.6934200336567652, 0.36826278810134483, 0.3504534534051691, 0.7343773879304665, 0.30850300087222926, 0.012558828208181605, 0.29175046251727366, 0.8503370080205277, 0.5675169197573675, 0.15692576949330364, 0.4233546125926121, 0.01]
Training loss = 0.0180417537689209
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.7575
Training loss = 0.016427661776542663
step = 1, Training Accuracy: 0.77
Training loss = 0.016034269481897356
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.0151752707362175
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.013296748300393423
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.014783408095439276
step = 5, Training Accuracy: 0.8
Training loss = 0.016704900761445363
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016763779024283092
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015583472053209941
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.017197786768277486
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.015200260678927104
step = 10, Training Accuracy: 0.79
Training loss = 0.01597342679897944
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01536469320456187
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.013834127485752105
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.014397180924812953
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.79375
params:  [0.31806373231064583, 0.7004146877955908, 0.9241992853399167, 0.15734469722429356, 0.7690065965975826, 0.16744390263309666, 0.99, 0.6294838415134705, 0.15333447614677823, 0.01, 0.7540234242417452, 0.6557357394330954, 0.608272190380877, 0.24722778697680314, 0.15884481714697887, 0.9480568903741388, 0.2797062382995208, 0.01, 0.30161297707495616, 0.99, 0.2259718740141242, 0.6367290542397763, 0.9419613122464084, 0.01]
Training loss = 0.020206294556458792
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.79125
Training loss = 0.020664776861667632
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.02142822861671448
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.01867156396309535
step = 3, Training Accuracy: 0.7
Training loss = 0.020345319410165152
step = 4, Training Accuracy: 0.72
Training loss = 0.018290942311286928
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.01900339702765147
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018766363362471263
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01867619752883911
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.021070469419161478
step = 9, Training Accuracy: 0.71
Training loss = 0.01917880912621816
step = 10, Training Accuracy: 0.71
Training loss = 0.01781214604775111
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.01880502571662267
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.018320874671141307
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.018465299606323243
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79625
params:  [0.5937523206015756, 0.7531293208684413, 0.8681318324338217, 0.37150880876685777, 0.5652679776268087, 0.35252084741669515, 0.9586415399113765, 0.6042245416683718, 0.42199023968538935, 0.054873540749076445, 0.8897170305649649, 0.8986510533425169, 0.5898628229877102, 0.26151670936810983, 0.22200610301845736, 0.9771873744906776, 0.4761126925691952, 0.052647781272982, 0.1804610088006462, 0.910647533136, 0.5547155977239764, 0.38096439803621907, 0.267197748673559, 0.2831251344285397]
Training loss = 0.014412931303183238
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.79
Training loss = 0.015549450516700744
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.01603226105372111
step = 2, Training Accuracy: 0.78
Training loss = 0.01404803141951561
step = 3, Training Accuracy: 0.81
Training loss = 0.01518844445546468
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015562501549720765
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01464643637339274
step = 6, Training Accuracy: 0.83
Training loss = 0.014928218126296997
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014388850728670756
step = 8, Training Accuracy: 0.82
Training loss = 0.016828654011090596
step = 9, Training Accuracy: 0.8
Training loss = 0.015324331323305766
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01631461242834727
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.014591604967912038
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.0150905575354894
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014923920730749766
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.77875
params:  [0.3424718510161171, 0.5908409749173555, 0.99, 0.384163221470892, 0.4985053702552006, 0.12591723023284987, 0.753488950279287, 0.721898391959197, 0.6906904386951185, 0.01, 0.6921688520418043, 0.5572368756827177, 0.9530442228407054, 0.05342257500841674, 0.5299656768878469, 0.7226205290934133, 0.14425545335852244, 0.01, 0.3089766455052878, 0.99, 0.4757151307622566, 0.03412030956825213, 0.660024458981339, 0.01]
Training loss = 0.0171780193845431
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.78625
Training loss = 0.01780468742052714
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01729487955570221
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.018194449692964555
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016905243595441183
step = 4, Training Accuracy: 0.74
Training loss = 0.016877658466498056
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.014868525962034862
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015789087414741516
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.01843883087237676
step = 8, Training Accuracy: 0.75
Training loss = 0.015041351268688838
step = 9, Training Accuracy: 0.79
Training loss = 0.016788917581240338
step = 10, Training Accuracy: 0.77
Training loss = 0.015901770939429603
step = 11, Training Accuracy: 0.77
Training loss = 0.01857567419608434
step = 12, Training Accuracy: 0.73
Training loss = 0.018037529786427815
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.015957110226154328
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7925
[[0.264195384198241, 0.8327627071486238, 0.99, 0.2919552227102, 0.6617294156948426, 0.292127425445328, 0.99, 0.6568925110676404, 0.2202556844500421, 0.07072939463370512, 0.9745469720960563, 0.8878406395304722, 0.7857200518098447, 0.18541610647302603, 0.5185201940447343, 0.9852845783524649, 0.17204206585093348, 0.01, 0.26846983182060896, 0.7941383220244639, 0.6921127309961128, 0.6408806579666522, 0.36267539400886734, 0.3409442412482649], [0.339875574669313, 0.49918526367170485, 0.9103131729312753, 0.2921138294708268, 0.592729889478813, 0.04733421639045679, 0.8708130052521681, 0.6986986182123717, 0.11526296887332793, 0.01, 0.49863821809760206, 0.3041749255729081, 0.9370777873379817, 0.3682141412666046, 0.3497733626235008, 0.6513921387237186, 0.2171177645454672, 0.3367381038467938, 0.14510380180874308, 0.9719311626705511, 0.35012179259604226, 0.4397445006708199, 0.6343259584512478, 0.01], [0.36175465810626667, 0.771650759062442, 0.7363225930573026, 0.22487186137139234, 0.6812426706706778, 0.18405382797239736, 0.9184744593008435, 0.941653922557535, 0.4553939280332371, 0.14331983387947428, 0.38599113930726503, 0.6298885137194282, 0.7769220319503658, 0.01, 0.3005492131866617, 0.9061876460156759, 0.26650072347246456, 0.5004738521650314, 0.2260024318370635, 0.8164852990538298, 0.28638885168840256, 0.40063581554151895, 0.6281572124265404, 0.01], [0.17117094757979062, 0.562749669741782, 0.9107227668498393, 0.42144999224048196, 0.7934752343186503, 0.01, 0.99, 0.8788513593591532, 0.33109563034450157, 0.1485040263917836, 0.6509762766690409, 0.8675417963069617, 0.9396955510232586, 0.2055151334150429, 0.5142524634652317, 0.9457438657247512, 0.1934868126710969, 0.01, 0.01, 0.99, 0.7866551231813665, 0.3867595065441036, 0.40635275022483214, 0.22982045200965512], [0.4008643310405859, 0.6970832428896392, 0.6921465124529463, 0.31159731064700824, 0.5183754730430326, 0.20330278629902013, 0.8766073886147915, 0.5119165459923263, 0.31697879640361776, 0.18037506744844686, 0.921059628177124, 0.7831873472983643, 0.6934200336567652, 0.36826278810134483, 0.3504534534051691, 0.7343773879304665, 0.30850300087222926, 0.012558828208181605, 0.29175046251727366, 0.8503370080205277, 0.5675169197573675, 0.15692576949330364, 0.4233546125926121, 0.01], [0.31806373231064583, 0.7004146877955908, 0.9241992853399167, 0.15734469722429356, 0.7690065965975826, 0.16744390263309666, 0.99, 0.6294838415134705, 0.15333447614677823, 0.01, 0.7540234242417452, 0.6557357394330954, 0.608272190380877, 0.24722778697680314, 0.15884481714697887, 0.9480568903741388, 0.2797062382995208, 0.01, 0.30161297707495616, 0.99, 0.2259718740141242, 0.6367290542397763, 0.9419613122464084, 0.01], [0.5937523206015756, 0.7531293208684413, 0.8681318324338217, 0.37150880876685777, 0.5652679776268087, 0.35252084741669515, 0.9586415399113765, 0.6042245416683718, 0.42199023968538935, 0.054873540749076445, 0.8897170305649649, 0.8986510533425169, 0.5898628229877102, 0.26151670936810983, 0.22200610301845736, 0.9771873744906776, 0.4761126925691952, 0.052647781272982, 0.1804610088006462, 0.910647533136, 0.5547155977239764, 0.38096439803621907, 0.267197748673559, 0.2831251344285397], [0.3424718510161171, 0.5908409749173555, 0.99, 0.384163221470892, 0.4985053702552006, 0.12591723023284987, 0.753488950279287, 0.721898391959197, 0.6906904386951185, 0.01, 0.6921688520418043, 0.5572368756827177, 0.9530442228407054, 0.05342257500841674, 0.5299656768878469, 0.7226205290934133, 0.14425545335852244, 0.01, 0.3089766455052878, 0.99, 0.4757151307622566, 0.03412030956825213, 0.660024458981339, 0.01]]
15 	8     	0.784531	0.0100766 	0.765  	0.79625
params:  [0.5610545058351945, 0.8539966407064861, 0.99, 0.3030147928205421, 0.48377827054354466, 0.3276794247040352, 0.99, 0.6494567528409746, 0.3680194409725897, 0.01, 0.7719599082563088, 0.9220788717371112, 0.6503962586452968, 0.7562174154085752, 0.34898727882013447, 0.5822076891038586, 0.3238762052948222, 0.04860334709272189, 0.3353280503163413, 0.99, 0.40390450749189666, 0.3966972097839411, 0.497019566852702, 0.16282926991646154]
Training loss = 0.01937459111213684
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.795
Training loss = 0.02116418828566869
step = 1, Training Accuracy: 0.74
Training loss = 0.018214304745197297
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.0175395600994428
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.020336245497067768
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.019708975652853646
step = 5, Training Accuracy: 0.7333333333333333
Training loss = 0.01690430223941803
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.017419938345750174
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.016665071050326028
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.01698879400889079
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.019814859529336294
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.017219892342885335
step = 11, Training Accuracy: 0.76
Training loss = 0.01920196493466695
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.019299179514249167
step = 13, Training Accuracy: 0.77
Training loss = 0.018850896259148917
step = 14, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79125
params:  [0.3042559582444279, 0.8095286930356027, 0.819987453744045, 0.19763949313067175, 0.7142484885529347, 0.22306533320094635, 0.8978650318028277, 0.6983608412871425, 0.16204211611640706, 0.01, 0.4988832127599797, 0.8695406888384326, 0.6779975477615935, 0.01, 0.3685605299141223, 0.9716603369443652, 0.1090004702620509, 0.027827713957093332, 0.3366205108291131, 0.99, 0.2779592033787506, 0.4887201768393157, 0.6519307012948318, 0.10690448870000323]
Training loss = 0.01961254169543584
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79125
Training loss = 0.018431598246097566
step = 1, Training Accuracy: 0.75
Training loss = 0.020678964455922443
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.019907843619585038
step = 3, Training Accuracy: 0.74
Training loss = 0.020203705628712973
step = 4, Training Accuracy: 0.74
Training loss = 0.021273398995399474
step = 5, Training Accuracy: 0.7033333333333334
Training loss = 0.019646377464135487
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.01875725855429967
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.019689209858576456
step = 8, Training Accuracy: 0.73
Training loss = 0.0194107523560524
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.01853288600842158
step = 10, Training Accuracy: 0.75
Training loss = 0.020518735845883686
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.021160142024358113
step = 12, Training Accuracy: 0.7033333333333334
Training loss = 0.017975393533706665
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018598088324069978
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7825
params:  [0.47314933163729467, 0.7203542828366791, 0.7409544417475981, 0.18256659331830263, 0.99, 0.09545657391621121, 0.6535876356791452, 0.39697880142325526, 0.6603244688819764, 0.01, 0.4207798616228259, 0.7158453312671764, 0.6394078378446363, 0.09699742948634474, 0.4352019231125906, 0.99, 0.2953676296226519, 0.01, 0.21116243753652555, 0.9541503603662035, 0.09875934560460847, 0.5043282388806595, 0.7329452760571931, 0.04337228509715065]
Training loss = 0.0198449045419693
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.79
Training loss = 0.022792466084162393
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.017689967254797618
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.019792116085688274
step = 3, Training Accuracy: 0.6833333333333333
Training loss = 0.016863332986831667
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.020664159754912058
step = 5, Training Accuracy: 0.74
Training loss = 0.017391356031099956
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.019903693596522012
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01897914171218872
step = 8, Training Accuracy: 0.73
Training loss = 0.01819710006316503
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.01913692682981491
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.019282479484875995
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.018829777439435324
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01910308947165807
step = 13, Training Accuracy: 0.72
Training loss = 0.0193677552541097
step = 14, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.79625
params:  [0.35859367841451717, 0.7730369450112468, 0.6232429977067157, 0.2507267449146662, 0.9410111347966055, 0.04793613584091308, 0.838102332675565, 0.7712241039728002, 0.265740763517706, 0.01, 0.99, 0.99, 0.5553423755890854, 0.25438759991630105, 0.221704145998818, 0.7468788502364185, 0.12454833167783008, 0.01, 0.20278553843746414, 0.99, 0.14815589223215592, 0.3722820980653341, 0.6149523544251443, 0.10464024614030441]
Training loss = 0.019041034281253814
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7975
Training loss = 0.01988024299343427
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.019461736182371774
step = 2, Training Accuracy: 0.7166666666666667
Training loss = 0.020392635464668275
step = 3, Training Accuracy: 0.72
Training loss = 0.01925224890311559
step = 4, Training Accuracy: 0.73
Training loss = 0.020208811561266582
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.018923767606417338
step = 6, Training Accuracy: 0.76
Training loss = 0.017704142034053804
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.019672646721204123
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.0195537864168485
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018376250565052033
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.02076220949490865
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.019405853350957236
step = 12, Training Accuracy: 0.75
Training loss = 0.018167298237482706
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.017782307962576547
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.805
params:  [0.21285052582273287, 0.9399224388538875, 0.8851573786253826, 0.01, 0.7500789676103663, 0.3098054819232607, 0.8485588170726484, 0.5439871911188999, 0.33190693157274603, 0.06565843240414398, 0.5860756671069606, 0.7656441182119855, 0.6598074762484375, 0.05669981985886041, 0.5919329778543079, 0.5044857006817463, 0.4038813106644846, 0.10932757168227285, 0.316191288736666, 0.99, 0.6321006060558252, 0.30755071569589365, 0.8206273977155535, 0.01]
Training loss = 0.021134905815124512
step = 0, Training Accuracy: 0.71
Validation Accuracy: 0.80125
Training loss = 0.020636727462212245
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.02015671302874883
step = 2, Training Accuracy: 0.78
Training loss = 0.0207576193412145
step = 3, Training Accuracy: 0.73
Training loss = 0.02131084054708481
step = 4, Training Accuracy: 0.71
Training loss = 0.021791226466496786
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.020950350761413574
step = 6, Training Accuracy: 0.7133333333333334
Training loss = 0.020389946897824605
step = 7, Training Accuracy: 0.74
Training loss = 0.018692484597365062
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.01948581983645757
step = 9, Training Accuracy: 0.7033333333333334
Training loss = 0.019549943705399832
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.02082836886246999
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.020656070510546368
step = 12, Training Accuracy: 0.73
Training loss = 0.019568811456362408
step = 13, Training Accuracy: 0.72
Training loss = 0.020121814906597136
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.77875
params:  [0.5993134908204669, 0.6958351831987089, 0.9327293777006695, 0.1674695388541812, 0.5925751216145048, 0.7114396510829151, 0.8412548688141457, 0.6771702258783967, 0.3000549990661605, 0.01, 0.8233200553900862, 0.5592806816294913, 0.7045977323936571, 0.11757600641353003, 0.4729475600492056, 0.99, 0.4072259163485398, 0.11190337884440224, 0.2857895936906319, 0.99, 0.19847298890241813, 0.6296950198718974, 0.9894851660234864, 0.2526131950550796]
Training loss = 0.02088941425085068
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7875
Training loss = 0.01950844258069992
step = 1, Training Accuracy: 0.75
Training loss = 0.02180435280005137
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.01889344890912374
step = 3, Training Accuracy: 0.75
Training loss = 0.020649081071217854
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.020164942344029744
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.020500674148400625
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.023510870039463044
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.017928705314795176
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.02039936512708664
step = 9, Training Accuracy: 0.73
Training loss = 0.020154637197653452
step = 10, Training Accuracy: 0.6966666666666667
Training loss = 0.020079501370588938
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.02015850881735484
step = 12, Training Accuracy: 0.7
Training loss = 0.01993024299542109
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.02047860324382782
step = 14, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.78625
params:  [0.21151044856517798, 0.6131551621421084, 0.850392420736518, 0.4744562461440153, 0.6384047919883271, 0.09549679021390055, 0.9198009716592715, 0.8229245169339319, 0.45206742355682816, 0.01, 0.6982787006668588, 0.9526905080475008, 0.5986615098689823, 0.3085632300555338, 0.08918023218839716, 0.99, 0.5012363511877165, 0.08111025416757886, 0.03022371973827409, 0.9254557867397297, 0.3729027610469017, 0.3931324654974469, 0.73205561606619, 0.12886502302107503]
Training loss = 0.01940336267153422
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7875
Training loss = 0.0168409260114034
step = 1, Training Accuracy: 0.78
Training loss = 0.017540582716464997
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.01938162436087926
step = 3, Training Accuracy: 0.76
Training loss = 0.017932205696900686
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.019200954139232635
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.019260029395421346
step = 6, Training Accuracy: 0.75
Training loss = 0.01909636914730072
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.018450568616390228
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01856099287668864
step = 9, Training Accuracy: 0.76
Training loss = 0.020395015875498454
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.018917752802371977
step = 11, Training Accuracy: 0.76
Training loss = 0.017745533883571626
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.015608903467655182
step = 13, Training Accuracy: 0.8
Training loss = 0.017785625060399372
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.78875
params:  [0.5315963955252118, 0.44737734382349825, 0.9689204449682944, 0.2569406973293138, 0.6324763122443605, 0.1881046477484522, 0.7572695859084266, 0.529660216586427, 0.25627361471872584, 0.01, 0.57713149121546, 0.7646297651545487, 0.7396030417770957, 0.01, 0.4563653180207195, 0.7974412004850902, 0.01, 0.10473366814191605, 0.35118540794197295, 0.99, 0.2536703944313087, 0.4442705382397969, 0.6963579270310252, 0.01]
Training loss = 0.019471277793248493
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7925
Training loss = 0.01807679682970047
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017677395443121593
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.017167113522688546
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01797511339187622
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.016809641420841216
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.016948824822902678
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01634730746348699
step = 7, Training Accuracy: 0.76
Training loss = 0.016944424708684284
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.017086218992869058
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.018475892742474874
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.0179330908258756
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.016907282769680024
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017396085262298584
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.016415054400761922
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7925
[[0.5610545058351945, 0.8539966407064861, 0.99, 0.3030147928205421, 0.48377827054354466, 0.3276794247040352, 0.99, 0.6494567528409746, 0.3680194409725897, 0.01, 0.7719599082563088, 0.9220788717371112, 0.6503962586452968, 0.7562174154085752, 0.34898727882013447, 0.5822076891038586, 0.3238762052948222, 0.04860334709272189, 0.3353280503163413, 0.99, 0.40390450749189666, 0.3966972097839411, 0.497019566852702, 0.16282926991646154], [0.3042559582444279, 0.8095286930356027, 0.819987453744045, 0.19763949313067175, 0.7142484885529347, 0.22306533320094635, 0.8978650318028277, 0.6983608412871425, 0.16204211611640706, 0.01, 0.4988832127599797, 0.8695406888384326, 0.6779975477615935, 0.01, 0.3685605299141223, 0.9716603369443652, 0.1090004702620509, 0.027827713957093332, 0.3366205108291131, 0.99, 0.2779592033787506, 0.4887201768393157, 0.6519307012948318, 0.10690448870000323], [0.47314933163729467, 0.7203542828366791, 0.7409544417475981, 0.18256659331830263, 0.99, 0.09545657391621121, 0.6535876356791452, 0.39697880142325526, 0.6603244688819764, 0.01, 0.4207798616228259, 0.7158453312671764, 0.6394078378446363, 0.09699742948634474, 0.4352019231125906, 0.99, 0.2953676296226519, 0.01, 0.21116243753652555, 0.9541503603662035, 0.09875934560460847, 0.5043282388806595, 0.7329452760571931, 0.04337228509715065], [0.35859367841451717, 0.7730369450112468, 0.6232429977067157, 0.2507267449146662, 0.9410111347966055, 0.04793613584091308, 0.838102332675565, 0.7712241039728002, 0.265740763517706, 0.01, 0.99, 0.99, 0.5553423755890854, 0.25438759991630105, 0.221704145998818, 0.7468788502364185, 0.12454833167783008, 0.01, 0.20278553843746414, 0.99, 0.14815589223215592, 0.3722820980653341, 0.6149523544251443, 0.10464024614030441], [0.21285052582273287, 0.9399224388538875, 0.8851573786253826, 0.01, 0.7500789676103663, 0.3098054819232607, 0.8485588170726484, 0.5439871911188999, 0.33190693157274603, 0.06565843240414398, 0.5860756671069606, 0.7656441182119855, 0.6598074762484375, 0.05669981985886041, 0.5919329778543079, 0.5044857006817463, 0.4038813106644846, 0.10932757168227285, 0.316191288736666, 0.99, 0.6321006060558252, 0.30755071569589365, 0.8206273977155535, 0.01], [0.5993134908204669, 0.6958351831987089, 0.9327293777006695, 0.1674695388541812, 0.5925751216145048, 0.7114396510829151, 0.8412548688141457, 0.6771702258783967, 0.3000549990661605, 0.01, 0.8233200553900862, 0.5592806816294913, 0.7045977323936571, 0.11757600641353003, 0.4729475600492056, 0.99, 0.4072259163485398, 0.11190337884440224, 0.2857895936906319, 0.99, 0.19847298890241813, 0.6296950198718974, 0.9894851660234864, 0.2526131950550796], [0.21151044856517798, 0.6131551621421084, 0.850392420736518, 0.4744562461440153, 0.6384047919883271, 0.09549679021390055, 0.9198009716592715, 0.8229245169339319, 0.45206742355682816, 0.01, 0.6982787006668588, 0.9526905080475008, 0.5986615098689823, 0.3085632300555338, 0.08918023218839716, 0.99, 0.5012363511877165, 0.08111025416757886, 0.03022371973827409, 0.9254557867397297, 0.3729027610469017, 0.3931324654974469, 0.73205561606619, 0.12886502302107503], [0.5315963955252118, 0.44737734382349825, 0.9689204449682944, 0.2569406973293138, 0.6324763122443605, 0.1881046477484522, 0.7572695859084266, 0.529660216586427, 0.25627361471872584, 0.01, 0.57713149121546, 0.7646297651545487, 0.7396030417770957, 0.01, 0.4563653180207195, 0.7974412004850902, 0.01, 0.10473366814191605, 0.35118540794197295, 0.99, 0.2536703944313087, 0.4442705382397969, 0.6963579270310252, 0.01]]
16 	8     	0.790156	0.00766581	0.77875	0.805  
params:  [0.4050434000724195, 0.7395382534866359, 0.7982797891225002, 0.27882157663753315, 0.7922609019523763, 0.16908406521376715, 0.8244970951403259, 0.9420009912723561, 0.6838635296162061, 0.03363430379594725, 0.49154438830033453, 0.8154628632154886, 0.6095751565877747, 0.1922162351029152, 0.4021602404728765, 0.9676823882829877, 0.049001338776997735, 0.01, 0.2732907359944889, 0.99, 0.3880175966317032, 0.1742666786605993, 0.4474066292847715, 0.1255890441696016]
Training loss = 0.018673117458820342
step = 0, Training Accuracy: 0.71
Validation Accuracy: 0.7975
Training loss = 0.017888792753219605
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.020006847778956095
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.018083920776844023
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01886503001054128
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.01905974596738815
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.0201048285762469
step = 6, Training Accuracy: 0.71
Training loss = 0.017983926236629488
step = 7, Training Accuracy: 0.74
Training loss = 0.018496620158354442
step = 8, Training Accuracy: 0.7233333333333334
Training loss = 0.02011287212371826
step = 9, Training Accuracy: 0.6866666666666666
Training loss = 0.01864722321430842
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.018793395658334097
step = 11, Training Accuracy: 0.75
Training loss = 0.017693577806154888
step = 12, Training Accuracy: 0.76
Training loss = 0.018797836403052014
step = 13, Training Accuracy: 0.73
Training loss = 0.018987726469834647
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79
params:  [0.3513585102536908, 0.6766010841906598, 0.5646390374267226, 0.2756894803152384, 0.8582628359444553, 0.095552678955099, 0.7283837134846839, 0.4760358957341466, 0.48414527153944564, 0.2181876479882112, 0.9287994975834811, 0.657541810745608, 0.6185640275141489, 0.01, 0.13599225906769194, 0.99, 0.2980312930293909, 0.16525101093303018, 0.5465952933954477, 0.99, 0.01, 0.5618863300077895, 0.7402438907240292, 0.04945102975510278]
Training loss = 0.019828480978806812
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.79625
Training loss = 0.02129662166039149
step = 1, Training Accuracy: 0.6933333333333334
Training loss = 0.019298209349314373
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.018999774058659873
step = 3, Training Accuracy: 0.73
Training loss = 0.017814225057760875
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.02129440704981486
step = 5, Training Accuracy: 0.7
Training loss = 0.01956534375747045
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.01964602728684743
step = 7, Training Accuracy: 0.74
Training loss = 0.019352070291837057
step = 8, Training Accuracy: 0.7233333333333334
Training loss = 0.019002078076203664
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.01923917998870214
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.01913589706023534
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.018406867484251657
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.019641509652137755
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.01895805468161901
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.78875
params:  [0.37140930661374355, 0.5888950961239343, 0.6098514470716188, 0.11872463224131538, 0.8332992589578052, 0.330214001748385, 0.7833113393223782, 0.6529638346212363, 0.5324215193674974, 0.01, 0.9411269330921961, 0.8045298101911839, 0.7535439894814205, 0.01, 0.19561185317793348, 0.9174872479525452, 0.08037518932369525, 0.01, 0.2378180662688769, 0.99, 0.01, 0.40742406141327425, 0.8244981624584098, 0.01]
Training loss = 0.01886295755704244
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.78625
Training loss = 0.019526262680689493
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.01928526391585668
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.0181278798977534
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.01818186491727829
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017539418737093606
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.01864529510339101
step = 6, Training Accuracy: 0.74
Training loss = 0.018984113335609436
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.016431964536507925
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.017343822022279104
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.018743851681550343
step = 10, Training Accuracy: 0.71
Training loss = 0.01889069308837255
step = 11, Training Accuracy: 0.73
Training loss = 0.01864298015832901
step = 12, Training Accuracy: 0.72
Training loss = 0.01894631326198578
step = 13, Training Accuracy: 0.73
Training loss = 0.018531397382418314
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.785
params:  [0.41848937409319265, 0.6604048406250713, 0.815987235925018, 0.24719804624789377, 0.9055678635235563, 0.13247384134018542, 0.99, 0.544135313934916, 0.193326987941463, 0.01, 0.922800218762484, 0.8127328097230703, 0.6748982664632657, 0.011122197495926328, 0.7096122258961506, 0.99, 0.3295487264486364, 0.01, 0.4458883980532203, 0.99, 0.17874528197019152, 0.41379736652760846, 0.33479651084502915, 0.01]
Training loss = 0.019185727735360463
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.78625
Training loss = 0.016230456829071045
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.016745553215344745
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.017075112164020537
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.018197139998277028
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.016062103311220804
step = 5, Training Accuracy: 0.77
Training loss = 0.017996370494365692
step = 6, Training Accuracy: 0.75
Training loss = 0.01853994975487391
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.018388631145159404
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.020068641702334085
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.016508757968743643
step = 10, Training Accuracy: 0.77
Training loss = 0.018363906741142272
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.018351901868979136
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.018742304841677347
step = 13, Training Accuracy: 0.7466666666666667
Training loss = 0.01922122945388158
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.78625
params:  [0.5473006509578959, 0.5414285553893146, 0.7503161869363478, 0.5173865279778828, 0.8906377478191995, 0.01, 0.8439857612892118, 0.5230552818762245, 0.4111912097863894, 0.10022158201274328, 0.8288228472079383, 0.99, 0.3867931531074722, 0.13280326972918255, 0.01, 0.6704471949287938, 0.01, 0.09600728416681367, 0.25687255022576455, 0.99, 0.32384813798105416, 0.4537090177984598, 0.49513407714030844, 0.09916430777123417]
Training loss = 0.014929081797599792
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.78625
Training loss = 0.015995257496833802
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015988241334756216
step = 2, Training Accuracy: 0.78
Training loss = 0.018062268396218616
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.017798714141050973
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.01787594844897588
step = 5, Training Accuracy: 0.77
Training loss = 0.018419845600922902
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.018038773039976756
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.018414962589740753
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016647521456082663
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.014939595237374306
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01670825997988383
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.015953948597113292
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.01495460957288742
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.017192200620969135
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78875
params:  [0.6141251109001586, 0.6893520542584047, 0.8270112772512417, 0.32992919161102013, 0.978238572676269, 0.2703826973745326, 0.9835864033259037, 0.7487717098791983, 0.2765748345679709, 0.01, 0.5951656240284232, 0.929581749654524, 0.7582323493301193, 0.17252405180190794, 0.35741711583987884, 0.9479726229961856, 0.2373804526144408, 0.06405742125237741, 0.23355850721281754, 0.9212300714305608, 0.23823381742455824, 0.5119079319543981, 0.8416026997760213, 0.1764463488952419]
Training loss = 0.018688193162282308
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7875
Training loss = 0.020921644171079
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.018279380202293395
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.020290741523106892
step = 3, Training Accuracy: 0.7033333333333334
Training loss = 0.01761969198783239
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018680723706881206
step = 5, Training Accuracy: 0.75
Training loss = 0.01853863477706909
step = 6, Training Accuracy: 0.78
Training loss = 0.01782478004693985
step = 7, Training Accuracy: 0.74
Training loss = 0.02034684459368388
step = 8, Training Accuracy: 0.72
Training loss = 0.019984788993994394
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.018953620592753094
step = 10, Training Accuracy: 0.7166666666666667
Training loss = 0.017616022129853567
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.018149392604827883
step = 12, Training Accuracy: 0.77
Training loss = 0.017621511320273082
step = 13, Training Accuracy: 0.75
Training loss = 0.015846493740876516
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.78375
params:  [0.01, 0.6973245448406804, 0.99, 0.24158836522026164, 0.8010805659451573, 0.3612021109856191, 0.911061713381149, 0.4430757718027491, 0.29514263540083024, 0.2926367050607494, 0.8655277275227129, 0.7148617462429145, 0.6236388012410885, 0.09153431675402338, 0.46371637530603094, 0.7695224171724915, 0.01, 0.01, 0.3555741893644916, 0.6436337222150709, 0.20151840912984492, 0.4360452942471489, 0.9057612921572138, 0.19685486094571808]
Training loss = 0.015791162649790445
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.78375
Training loss = 0.017329287230968476
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01642170051733653
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.016380745669205984
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.018745357394218443
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017453377346197764
step = 5, Training Accuracy: 0.78
Training loss = 0.017885376115640006
step = 6, Training Accuracy: 0.79
Training loss = 0.016875730057557423
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.017541512846946716
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.016718689302603403
step = 9, Training Accuracy: 0.78
Training loss = 0.01724912623564402
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.01722818156083425
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.018067120909690856
step = 12, Training Accuracy: 0.77
Training loss = 0.017641267279783886
step = 13, Training Accuracy: 0.75
Training loss = 0.017833382387955985
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.78625
params:  [0.30863124166977013, 0.8286547308943657, 0.633561340910137, 0.01, 0.7266632337353733, 0.12327360688240936, 0.8343797447762437, 0.46541721702595407, 0.45797657158604343, 0.07272081327901253, 0.5669727671138142, 0.9589383702583889, 0.46263686714475494, 0.4039097880419943, 0.5092096365581591, 0.919913538471889, 0.01, 0.01, 0.35361662648115405, 0.8137778369541732, 0.13506098683640083, 0.5388357236960089, 0.5476498078928922, 0.24839193912876445]
Training loss = 0.016770191192626953
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7875
Training loss = 0.018703940014044445
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017985279162724813
step = 2, Training Accuracy: 0.78
Training loss = 0.018455981810887655
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.01862685332695643
step = 4, Training Accuracy: 0.75
Training loss = 0.01817327896753947
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.016738521854082745
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.018715347051620482
step = 7, Training Accuracy: 0.75
Training loss = 0.015745437939961752
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01709150493144989
step = 9, Training Accuracy: 0.79
Training loss = 0.01875168134768804
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01968100110689799
step = 11, Training Accuracy: 0.73
Training loss = 0.018268864353497824
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.018309305409590405
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.0170524666706721
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.77625
[[0.4050434000724195, 0.7395382534866359, 0.7982797891225002, 0.27882157663753315, 0.7922609019523763, 0.16908406521376715, 0.8244970951403259, 0.9420009912723561, 0.6838635296162061, 0.03363430379594725, 0.49154438830033453, 0.8154628632154886, 0.6095751565877747, 0.1922162351029152, 0.4021602404728765, 0.9676823882829877, 0.049001338776997735, 0.01, 0.2732907359944889, 0.99, 0.3880175966317032, 0.1742666786605993, 0.4474066292847715, 0.1255890441696016], [0.3513585102536908, 0.6766010841906598, 0.5646390374267226, 0.2756894803152384, 0.8582628359444553, 0.095552678955099, 0.7283837134846839, 0.4760358957341466, 0.48414527153944564, 0.2181876479882112, 0.9287994975834811, 0.657541810745608, 0.6185640275141489, 0.01, 0.13599225906769194, 0.99, 0.2980312930293909, 0.16525101093303018, 0.5465952933954477, 0.99, 0.01, 0.5618863300077895, 0.7402438907240292, 0.04945102975510278], [0.37140930661374355, 0.5888950961239343, 0.6098514470716188, 0.11872463224131538, 0.8332992589578052, 0.330214001748385, 0.7833113393223782, 0.6529638346212363, 0.5324215193674974, 0.01, 0.9411269330921961, 0.8045298101911839, 0.7535439894814205, 0.01, 0.19561185317793348, 0.9174872479525452, 0.08037518932369525, 0.01, 0.2378180662688769, 0.99, 0.01, 0.40742406141327425, 0.8244981624584098, 0.01], [0.41848937409319265, 0.6604048406250713, 0.815987235925018, 0.24719804624789377, 0.9055678635235563, 0.13247384134018542, 0.99, 0.544135313934916, 0.193326987941463, 0.01, 0.922800218762484, 0.8127328097230703, 0.6748982664632657, 0.011122197495926328, 0.7096122258961506, 0.99, 0.3295487264486364, 0.01, 0.4458883980532203, 0.99, 0.17874528197019152, 0.41379736652760846, 0.33479651084502915, 0.01], [0.5473006509578959, 0.5414285553893146, 0.7503161869363478, 0.5173865279778828, 0.8906377478191995, 0.01, 0.8439857612892118, 0.5230552818762245, 0.4111912097863894, 0.10022158201274328, 0.8288228472079383, 0.99, 0.3867931531074722, 0.13280326972918255, 0.01, 0.6704471949287938, 0.01, 0.09600728416681367, 0.25687255022576455, 0.99, 0.32384813798105416, 0.4537090177984598, 0.49513407714030844, 0.09916430777123417], [0.6141251109001586, 0.6893520542584047, 0.8270112772512417, 0.32992919161102013, 0.978238572676269, 0.2703826973745326, 0.9835864033259037, 0.7487717098791983, 0.2765748345679709, 0.01, 0.5951656240284232, 0.929581749654524, 0.7582323493301193, 0.17252405180190794, 0.35741711583987884, 0.9479726229961856, 0.2373804526144408, 0.06405742125237741, 0.23355850721281754, 0.9212300714305608, 0.23823381742455824, 0.5119079319543981, 0.8416026997760213, 0.1764463488952419], [0.01, 0.6973245448406804, 0.99, 0.24158836522026164, 0.8010805659451573, 0.3612021109856191, 0.911061713381149, 0.4430757718027491, 0.29514263540083024, 0.2926367050607494, 0.8655277275227129, 0.7148617462429145, 0.6236388012410885, 0.09153431675402338, 0.46371637530603094, 0.7695224171724915, 0.01, 0.01, 0.3555741893644916, 0.6436337222150709, 0.20151840912984492, 0.4360452942471489, 0.9057612921572138, 0.19685486094571808], [0.30863124166977013, 0.8286547308943657, 0.633561340910137, 0.01, 0.7266632337353733, 0.12327360688240936, 0.8343797447762437, 0.46541721702595407, 0.45797657158604343, 0.07272081327901253, 0.5669727671138142, 0.9589383702583889, 0.46263686714475494, 0.4039097880419943, 0.5092096365581591, 0.919913538471889, 0.01, 0.01, 0.35361662648115405, 0.8137778369541732, 0.13506098683640083, 0.5388357236960089, 0.5476498078928922, 0.24839193912876445]]
17 	8     	0.785625	0.00405046	0.77625	0.79   
params:  [0.2666682002650491, 0.5338229952372255, 0.5903259811300693, 0.17180257655054282, 0.8653430430186893, 0.01, 0.7505539717754854, 0.8034170413167602, 0.5829052294680163, 0.01, 0.7121872476129347, 0.692217293616309, 0.4949620564106375, 0.07450376226784756, 0.4573023828505187, 0.99, 0.368699029779664, 0.10660296889557523, 0.19181134410860398, 0.7889807536161925, 0.051331809824422925, 0.1957126107437193, 0.24601393159701146, 0.0877088914499475]
Training loss = 0.016432828555504483
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.77875
Training loss = 0.014905806680520375
step = 1, Training Accuracy: 0.8
Training loss = 0.01577351878086726
step = 2, Training Accuracy: 0.8
Training loss = 0.014689394434293112
step = 3, Training Accuracy: 0.81
Training loss = 0.0150490998228391
step = 4, Training Accuracy: 0.82
Training loss = 0.014483197728792826
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.014875516494115193
step = 6, Training Accuracy: 0.81
Training loss = 0.01462719942132632
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.014748340845108033
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013924165964126586
step = 9, Training Accuracy: 0.8266666666666667
Training loss = 0.01398533821105957
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.013873462677001952
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.01448002596696218
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.013105878829956055
step = 13, Training Accuracy: 0.84
Training loss = 0.014924352094531058
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.79625
params:  [0.38908910906933364, 0.7866849456809097, 0.7726537337177662, 0.46708823534526756, 0.99, 0.01, 0.6677506218178285, 0.877965577776201, 0.37520031203056126, 0.01, 0.4803592229744863, 0.9647148477525, 0.40311768463197395, 0.18763309759785052, 0.2635951960677621, 0.99, 0.19522210031852408, 0.058192205627409475, 0.10580383952117467, 0.99, 0.07343430577908575, 0.29258520896716733, 0.3984208226914886, 0.01]
Training loss = 0.0165292356411616
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7975
Training loss = 0.018057313760121662
step = 1, Training Accuracy: 0.75
Training loss = 0.019527034759521486
step = 2, Training Accuracy: 0.73
Training loss = 0.019081295132637024
step = 3, Training Accuracy: 0.73
Training loss = 0.018573747078577677
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01792859653631846
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.016228542228539786
step = 6, Training Accuracy: 0.8
Training loss = 0.018768003086249034
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.021328219870726268
step = 8, Training Accuracy: 0.6866666666666666
Training loss = 0.018444847613573075
step = 9, Training Accuracy: 0.7133333333333334
Training loss = 0.017546159823735554
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.017143420378367105
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.02041351576646169
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.01904587745666504
step = 13, Training Accuracy: 0.75
Training loss = 0.01922619839509328
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.79125
params:  [0.39564475578155084, 0.7812376421078684, 0.9502742892975277, 0.31493224239906986, 0.99, 0.11547105579676113, 0.5250338369101906, 0.6748406554678992, 0.5031001375954394, 0.12522632635766126, 0.5153243422538354, 0.7696884441442458, 0.4989099092998163, 0.4137516692586553, 0.592409501076885, 0.9741630776448393, 0.17709033967993756, 0.2407880338499881, 0.5085954271825799, 0.9842896588285942, 0.3470448017112761, 0.18406049022907356, 0.5956175698375749, 0.05649703685583798]
Training loss = 0.021012023289998374
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.7875
Training loss = 0.0219390011827151
step = 1, Training Accuracy: 0.6766666666666666
Training loss = 0.019769338071346284
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.020055267016092938
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.02307591011126836
step = 4, Training Accuracy: 0.69
Training loss = 0.0190061753988266
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.020575515826543173
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.019490744372208914
step = 7, Training Accuracy: 0.7133333333333334
Training loss = 0.02015036066373189
step = 8, Training Accuracy: 0.7333333333333333
Training loss = 0.019746204614639284
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.02165832688411077
step = 10, Training Accuracy: 0.6933333333333334
Training loss = 0.020306708017985026
step = 11, Training Accuracy: 0.7033333333333334
Training loss = 0.01741124709447225
step = 12, Training Accuracy: 0.71
Training loss = 0.0204057643810908
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.019302455683549245
step = 14, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.79
params:  [0.48857971662015964, 0.6670040474571982, 0.6782668756184917, 0.15294323494195938, 0.8837862229910648, 0.025731178800760757, 0.99, 0.609132909304941, 0.5164565175214035, 0.16739218916124546, 0.7828812735410274, 0.8273188115461828, 0.5028699205712186, 0.4231293703497369, 0.17394560355794686, 0.99, 0.24239471214517666, 0.01, 0.5160761377159856, 0.6730498294547311, 0.17970194294342726, 0.15610206466258342, 0.6195132402843353, 0.06378667430620177]
Training loss = 0.016473848124345145
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7975
Training loss = 0.017130762934684754
step = 1, Training Accuracy: 0.77
Training loss = 0.01686787267525991
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.016558844049771627
step = 3, Training Accuracy: 0.8
Training loss = 0.016780972679456076
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.016105720996856688
step = 5, Training Accuracy: 0.8
Training loss = 0.017508139510949452
step = 6, Training Accuracy: 0.78
Training loss = 0.01648526589075724
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.016411560277144115
step = 8, Training Accuracy: 0.76
Training loss = 0.01773251344760259
step = 9, Training Accuracy: 0.78
Training loss = 0.017111913164456684
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.017407571077346803
step = 11, Training Accuracy: 0.76
Training loss = 0.015558546185493469
step = 12, Training Accuracy: 0.78
Training loss = 0.014666839937369029
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.016998745302359262
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.80375
params:  [0.4217381317263781, 0.7676625725442524, 0.7469185849247568, 0.3553862154067522, 0.6517880175666895, 0.33550687905994814, 0.9607683456932816, 0.4911366274800587, 0.4326198497357836, 0.2976220719311263, 0.7052753909406442, 0.8624444684883801, 0.3488308699914452, 0.25201765316792973, 0.30653887901027443, 0.785520243727953, 0.25820432096286244, 0.2185402038207917, 0.2888352313865531, 0.99, 0.34499609529995934, 0.4085046634167531, 0.3127911908347585, 0.27416882483231425]
Training loss = 0.017840209901332855
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.8025
Training loss = 0.01573959529399872
step = 1, Training Accuracy: 0.8
Training loss = 0.016237584153811137
step = 2, Training Accuracy: 0.76
Training loss = 0.01629674732685089
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.015888510048389436
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.015833557943503062
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.015602688988049824
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.016460572183132172
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.014036396692196529
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015455654412508011
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015071228047211964
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.014408075114091238
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.015406577587127686
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014791259964307149
step = 13, Training Accuracy: 0.81
Training loss = 0.013760455350081125
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
params:  [0.58748519345592, 0.6412089746502965, 0.6473870948335366, 0.19649021983741757, 0.99, 0.14681666144026653, 0.9122080325607242, 0.8433749091059175, 0.7644301342658049, 0.01, 0.7146832475781236, 0.99, 0.711253664942971, 0.09321419204943258, 0.23078743848042949, 0.99, 0.01, 0.28941642900487796, 0.2651475368355992, 0.9817471878345442, 0.36174384165472684, 0.2682987569474917, 0.33972507654601447, 0.10627754326428797]
Training loss = 0.01760950724283854
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.77875
Training loss = 0.02007551312446594
step = 1, Training Accuracy: 0.72
Training loss = 0.018486902316411338
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.017991772095362346
step = 3, Training Accuracy: 0.73
Training loss = 0.016743951787551243
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.01737891455491384
step = 5, Training Accuracy: 0.78
Training loss = 0.019234886070092518
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.019383508463700613
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.01786211589972178
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.01744501163562139
step = 9, Training Accuracy: 0.77
Training loss = 0.01812755932410558
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.019315350353717804
step = 11, Training Accuracy: 0.74
Training loss = 0.017492422163486482
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.017983939051628112
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.017463995118935902
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79875
params:  [0.24249432317201847, 0.5733626838949294, 0.5836526044327252, 0.41067817218434177, 0.5071558229214683, 0.3155891135354363, 0.668815466288942, 0.7214120828477834, 0.7007972197002081, 0.19857726722345115, 0.824300541424547, 0.7360883194086681, 0.6674222449791208, 0.32711896453043304, 0.2533028016927462, 0.8882641094884984, 0.18731693433383279, 0.09198821723564965, 0.5143016224612593, 0.9497936594543044, 0.3079810432687888, 0.2897057580475691, 0.2968943061115973, 0.08388417886448284]
Training loss = 0.016720215280850728
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.8
Training loss = 0.01685341070095698
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01572610338528951
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01630483090877533
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.0174144579966863
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.017330422401428222
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.015954395135243733
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.016188823382059733
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016794668734073637
step = 8, Training Accuracy: 0.77
Training loss = 0.01702145834763845
step = 9, Training Accuracy: 0.76
Training loss = 0.016624414920806886
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.015050305426120758
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01690660814444224
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.01615739434957504
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01541415015856425
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.8025
params:  [0.2422387915117434, 0.7379760111888916, 0.9846556347324713, 0.4020744152806558, 0.99, 0.01, 0.99, 0.99, 0.4962234065373444, 0.09564763521000343, 0.7403584057833039, 0.7532381386980317, 0.6788453690432821, 0.01, 0.3764594959198847, 0.7419041974153786, 0.09169107081889732, 0.06338244080187382, 0.5275281564695469, 0.931440049241322, 0.15115762610915348, 0.1831892051269195, 0.36899216736215573, 0.01]
Training loss = 0.0161626332004865
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7975
Training loss = 0.014965398112932841
step = 1, Training Accuracy: 0.79
Training loss = 0.015538570433855058
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.015035471320152283
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.017663082182407377
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.01772822082042694
step = 5, Training Accuracy: 0.77
Training loss = 0.019088111519813537
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.015521300335725149
step = 7, Training Accuracy: 0.83
Training loss = 0.01634530911842982
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.017047183215618135
step = 9, Training Accuracy: 0.81
Training loss = 0.015316893607378005
step = 10, Training Accuracy: 0.79
Training loss = 0.016060204207897187
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01629938930273056
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017087334593137105
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.015098959803581238
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.80625
[[0.2666682002650491, 0.5338229952372255, 0.5903259811300693, 0.17180257655054282, 0.8653430430186893, 0.01, 0.7505539717754854, 0.8034170413167602, 0.5829052294680163, 0.01, 0.7121872476129347, 0.692217293616309, 0.4949620564106375, 0.07450376226784756, 0.4573023828505187, 0.99, 0.368699029779664, 0.10660296889557523, 0.19181134410860398, 0.7889807536161925, 0.051331809824422925, 0.1957126107437193, 0.24601393159701146, 0.0877088914499475], [0.38908910906933364, 0.7866849456809097, 0.7726537337177662, 0.46708823534526756, 0.99, 0.01, 0.6677506218178285, 0.877965577776201, 0.37520031203056126, 0.01, 0.4803592229744863, 0.9647148477525, 0.40311768463197395, 0.18763309759785052, 0.2635951960677621, 0.99, 0.19522210031852408, 0.058192205627409475, 0.10580383952117467, 0.99, 0.07343430577908575, 0.29258520896716733, 0.3984208226914886, 0.01], [0.39564475578155084, 0.7812376421078684, 0.9502742892975277, 0.31493224239906986, 0.99, 0.11547105579676113, 0.5250338369101906, 0.6748406554678992, 0.5031001375954394, 0.12522632635766126, 0.5153243422538354, 0.7696884441442458, 0.4989099092998163, 0.4137516692586553, 0.592409501076885, 0.9741630776448393, 0.17709033967993756, 0.2407880338499881, 0.5085954271825799, 0.9842896588285942, 0.3470448017112761, 0.18406049022907356, 0.5956175698375749, 0.05649703685583798], [0.48857971662015964, 0.6670040474571982, 0.6782668756184917, 0.15294323494195938, 0.8837862229910648, 0.025731178800760757, 0.99, 0.609132909304941, 0.5164565175214035, 0.16739218916124546, 0.7828812735410274, 0.8273188115461828, 0.5028699205712186, 0.4231293703497369, 0.17394560355794686, 0.99, 0.24239471214517666, 0.01, 0.5160761377159856, 0.6730498294547311, 0.17970194294342726, 0.15610206466258342, 0.6195132402843353, 0.06378667430620177], [0.4217381317263781, 0.7676625725442524, 0.7469185849247568, 0.3553862154067522, 0.6517880175666895, 0.33550687905994814, 0.9607683456932816, 0.4911366274800587, 0.4326198497357836, 0.2976220719311263, 0.7052753909406442, 0.8624444684883801, 0.3488308699914452, 0.25201765316792973, 0.30653887901027443, 0.785520243727953, 0.25820432096286244, 0.2185402038207917, 0.2888352313865531, 0.99, 0.34499609529995934, 0.4085046634167531, 0.3127911908347585, 0.27416882483231425], [0.58748519345592, 0.6412089746502965, 0.6473870948335366, 0.19649021983741757, 0.99, 0.14681666144026653, 0.9122080325607242, 0.8433749091059175, 0.7644301342658049, 0.01, 0.7146832475781236, 0.99, 0.711253664942971, 0.09321419204943258, 0.23078743848042949, 0.99, 0.01, 0.28941642900487796, 0.2651475368355992, 0.9817471878345442, 0.36174384165472684, 0.2682987569474917, 0.33972507654601447, 0.10627754326428797], [0.24249432317201847, 0.5733626838949294, 0.5836526044327252, 0.41067817218434177, 0.5071558229214683, 0.3155891135354363, 0.668815466288942, 0.7214120828477834, 0.7007972197002081, 0.19857726722345115, 0.824300541424547, 0.7360883194086681, 0.6674222449791208, 0.32711896453043304, 0.2533028016927462, 0.8882641094884984, 0.18731693433383279, 0.09198821723564965, 0.5143016224612593, 0.9497936594543044, 0.3079810432687888, 0.2897057580475691, 0.2968943061115973, 0.08388417886448284], [0.2422387915117434, 0.7379760111888916, 0.9846556347324713, 0.4020744152806558, 0.99, 0.01, 0.99, 0.99, 0.4962234065373444, 0.09564763521000343, 0.7403584057833039, 0.7532381386980317, 0.6788453690432821, 0.01, 0.3764594959198847, 0.7419041974153786, 0.09169107081889732, 0.06338244080187382, 0.5275281564695469, 0.931440049241322, 0.15115762610915348, 0.1831892051269195, 0.36899216736215573, 0.01]]
18 	8     	0.797187	0.00627339	0.78875	0.80625
params:  [0.4191424265800261, 0.7917479299920948, 0.8653074064016316, 0.16274687626962622, 0.6570149807625272, 0.01, 0.7516354117600993, 0.8246331107893958, 0.4816589150577996, 0.2905838268761649, 0.7717130207020275, 0.8429357792417218, 0.6335488890557494, 0.23107244618784684, 0.25910523584235656, 0.883111001858922, 0.20834937651512758, 0.01, 0.5045696907737671, 0.6377176263417571, 0.2333001513304122, 0.042890338014513835, 0.45547999650908366, 0.01]
Training loss = 0.016134303013483682
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.80625
Training loss = 0.017096169789632163
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01567442774772644
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01652313709259033
step = 3, Training Accuracy: 0.77
Training loss = 0.015051985681056977
step = 4, Training Accuracy: 0.81
Training loss = 0.015777439872423808
step = 5, Training Accuracy: 0.79
Training loss = 0.016501707235972086
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.017953760226567587
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01784753461678823
step = 8, Training Accuracy: 0.75
Training loss = 0.017324261566003165
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.018053609629472098
step = 10, Training Accuracy: 0.75
Training loss = 0.016089277267456056
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015113998154799144
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016767000953356426
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.017809927562872568
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.8025
params:  [0.4017889729624127, 0.8195567536337666, 0.8710113909820852, 0.5962923947817345, 0.8324247634379152, 0.045221036975216594, 0.99, 0.8849226478821561, 0.4788264910694097, 0.21973596077896146, 0.48239219719257115, 0.8649951039014502, 0.530633061171259, 0.11540828472058598, 0.5218534708182887, 0.99, 0.2735138942417885, 0.15720039517173615, 0.6444559767340738, 0.99, 0.01, 0.2425015415088868, 0.4802671093614518, 0.17058036978684643]
Training loss = 0.022824022273222607
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.80125
Training loss = 0.021316475917895636
step = 1, Training Accuracy: 0.73
Training loss = 0.021716246207555134
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.021820815404256184
step = 3, Training Accuracy: 0.7
Training loss = 0.020970008869965872
step = 4, Training Accuracy: 0.7
Training loss = 0.02563052753607432
step = 5, Training Accuracy: 0.6866666666666666
Training loss = 0.01927805205186208
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.021314304172992706
step = 7, Training Accuracy: 0.6933333333333334
Training loss = 0.021012279589970907
step = 8, Training Accuracy: 0.69
Training loss = 0.019879866341749826
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.022639944354693096
step = 10, Training Accuracy: 0.7
Training loss = 0.022346137364705403
step = 11, Training Accuracy: 0.68
Training loss = 0.021207321087519326
step = 12, Training Accuracy: 0.6833333333333333
Training loss = 0.0204285924633344
step = 13, Training Accuracy: 0.73
Training loss = 0.022677530944347383
step = 14, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.8
params:  [0.3478537607700525, 0.3994958715730013, 0.755963037337676, 0.2035015319051485, 0.8995427922311569, 0.17771754599398215, 0.99, 0.5336961519713878, 0.5388444683915766, 0.21748583503985247, 0.6410328649391117, 0.9349417346589506, 0.5339267842566748, 0.33465643357420394, 0.3056813185267141, 0.7730394486578098, 0.01, 0.01, 0.36813401772286725, 0.916491708011085, 0.22672070617002218, 0.5251358296863092, 0.5122582770971261, 0.2145348922726212]
Training loss = 0.015575449069341024
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.80125
Training loss = 0.01519220342238744
step = 1, Training Accuracy: 0.8
Training loss = 0.015147102574507396
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.014151946902275085
step = 3, Training Accuracy: 0.84
Training loss = 0.015063410401344299
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.014494962592919668
step = 5, Training Accuracy: 0.82
Training loss = 0.015045698682467143
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.015109209815661112
step = 7, Training Accuracy: 0.81
Training loss = 0.014516045649846395
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.014527161916097006
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.0157106484969457
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014049848218758902
step = 11, Training Accuracy: 0.83
Training loss = 0.01441487709681193
step = 12, Training Accuracy: 0.83
Training loss = 0.016509074568748474
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.013550232946872711
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.79625
params:  [0.1631883770761854, 0.6776002996483265, 0.99, 0.6863785397143443, 0.896441715403196, 0.12977965751058052, 0.9171111957499719, 0.99, 0.643044949168197, 0.01, 0.7473095365329135, 0.9364883141559396, 0.748549036682807, 0.302580558148454, 0.2821482237562667, 0.8617163329117846, 0.01, 0.30398882490344936, 0.5662054101511667, 0.7493982669353357, 0.036865728257726105, 0.3133278561319889, 0.7119807957027908, 0.16883430601696675]
Training loss = 0.020752970774968463
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.79625
Training loss = 0.018392587701479595
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.020343011518319448
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.019407954514026642
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.019471383889516195
step = 4, Training Accuracy: 0.76
Training loss = 0.01939811706542969
step = 5, Training Accuracy: 0.7266666666666667
Training loss = 0.020098175207773846
step = 6, Training Accuracy: 0.7166666666666667
Training loss = 0.019910265306631723
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.01823924163977305
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.020864278972148896
step = 9, Training Accuracy: 0.72
Training loss = 0.02173150291045507
step = 10, Training Accuracy: 0.7266666666666667
Training loss = 0.018638324538866678
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.01956308513879776
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.02069130222002665
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.01873015989859899
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79
params:  [0.27636222529549204, 0.7371961377452687, 0.8143326078700106, 0.15431526282438066, 0.7862020155326304, 0.01, 0.99, 0.99, 0.45125797971292325, 0.1413678006673195, 0.6595512617879906, 0.6344937872510573, 0.5267130212981369, 0.4039919209842354, 0.172731447953135, 0.8060317686352, 0.32910783607641647, 0.16265339293459252, 0.7301633465974586, 0.8432222569832786, 0.31984517710814697, 0.18086198432305434, 0.48886694498401717, 0.19554952468489478]
Training loss = 0.018515289425849915
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.79875
Training loss = 0.018484249413013458
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01628374328215917
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.017084184885025024
step = 3, Training Accuracy: 0.78
Training loss = 0.01813343326250712
step = 4, Training Accuracy: 0.7433333333333333
Training loss = 0.01726869324843089
step = 5, Training Accuracy: 0.8
Training loss = 0.016392275194327035
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.016633543769518533
step = 7, Training Accuracy: 0.75
Training loss = 0.017047697305679323
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.015312238236268361
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01763749400774638
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01635765125354131
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.01680214047431946
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.016210567951202393
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.015239682892958323
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7925
params:  [0.3016967940782256, 0.545789933390618, 0.5481158046499206, 0.1370269006292088, 0.99, 0.11419108400282191, 0.9193964242495599, 0.8784286481304499, 0.5219681023399063, 0.01, 0.7971172605601541, 0.720791569114498, 0.804801365598778, 0.09342470470275932, 0.20103319992929405, 0.6834127678236022, 0.06598442863154975, 0.4061780667024608, 0.6349261359703741, 0.99, 0.14813998382176935, 0.4374911090625621, 0.49410439216593394, 0.01]
Training loss = 0.0179234645764033
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7975
Training loss = 0.017229731678962707
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.01715680003166199
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.01967601716518402
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017378899455070495
step = 4, Training Accuracy: 0.79
Training loss = 0.01830982983112335
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.01818510522445043
step = 6, Training Accuracy: 0.76
Training loss = 0.018275064925352733
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.018025643825531006
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.016126357813676197
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.017430621683597564
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.01691589206457138
step = 11, Training Accuracy: 0.78
Training loss = 0.017439376215140023
step = 12, Training Accuracy: 0.8
Training loss = 0.018274450997511547
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01945940355459849
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.8
params:  [0.5459571766551912, 0.5927645725373841, 0.9180929596034771, 0.3124982821419851, 0.99, 0.01, 0.99, 0.7706181325792973, 0.6901049050843031, 0.08721436637632163, 0.809098890885393, 0.7869573814849786, 0.7266175081399309, 0.4130073596837656, 0.3269219562331941, 0.626832236370472, 0.16657210627657817, 0.29682471173108105, 0.5602122732373189, 0.882313454341557, 0.29076458296333, 0.1272373953912308, 0.3697643895944101, 0.36586833385318074]
Training loss = 0.017070396939913433
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79625
Training loss = 0.01837665766477585
step = 1, Training Accuracy: 0.77
Training loss = 0.01688019295533498
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.017627677917480468
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.014884251058101655
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.01744727005561193
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.01626132845878601
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016674239138762155
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01682626803716024
step = 8, Training Accuracy: 0.78
Training loss = 0.01670602778593699
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.019269361396630606
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.016746725837389627
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.01747424155473709
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.01573023001352946
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.017377358973026277
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.79
params:  [0.17135919049687384, 0.6825134895157287, 0.9701979486392338, 0.09849907959199591, 0.7413246248442871, 0.12209018560882404, 0.99, 0.7645667824657362, 0.5363769739323193, 0.01, 0.7607554906638988, 0.99, 0.5890308297010312, 0.1668313062331084, 0.34854697211627067, 0.9222635599154488, 0.07886350879875895, 0.1907231985654203, 0.6713014662777173, 0.560069757933654, 0.23637310916578935, 0.44054962004053777, 0.3561526200822495, 0.07970349935589816]
Training loss = 0.015862231055895487
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79125
Training loss = 0.01818868299325307
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.016047038932641346
step = 2, Training Accuracy: 0.79
Training loss = 0.016072234660387038
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01783090760310491
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.016801767150561014
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.015741383532683055
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.015835607548554738
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016103112200895945
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01534012109041214
step = 9, Training Accuracy: 0.81
Training loss = 0.015819845795631407
step = 10, Training Accuracy: 0.78
Training loss = 0.01614816556374232
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016526958743731182
step = 12, Training Accuracy: 0.77
Training loss = 0.01516449789206187
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01659277061621348
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7925
[[0.4191424265800261, 0.7917479299920948, 0.8653074064016316, 0.16274687626962622, 0.6570149807625272, 0.01, 0.7516354117600993, 0.8246331107893958, 0.4816589150577996, 0.2905838268761649, 0.7717130207020275, 0.8429357792417218, 0.6335488890557494, 0.23107244618784684, 0.25910523584235656, 0.883111001858922, 0.20834937651512758, 0.01, 0.5045696907737671, 0.6377176263417571, 0.2333001513304122, 0.042890338014513835, 0.45547999650908366, 0.01], [0.4017889729624127, 0.8195567536337666, 0.8710113909820852, 0.5962923947817345, 0.8324247634379152, 0.045221036975216594, 0.99, 0.8849226478821561, 0.4788264910694097, 0.21973596077896146, 0.48239219719257115, 0.8649951039014502, 0.530633061171259, 0.11540828472058598, 0.5218534708182887, 0.99, 0.2735138942417885, 0.15720039517173615, 0.6444559767340738, 0.99, 0.01, 0.2425015415088868, 0.4802671093614518, 0.17058036978684643], [0.3478537607700525, 0.3994958715730013, 0.755963037337676, 0.2035015319051485, 0.8995427922311569, 0.17771754599398215, 0.99, 0.5336961519713878, 0.5388444683915766, 0.21748583503985247, 0.6410328649391117, 0.9349417346589506, 0.5339267842566748, 0.33465643357420394, 0.3056813185267141, 0.7730394486578098, 0.01, 0.01, 0.36813401772286725, 0.916491708011085, 0.22672070617002218, 0.5251358296863092, 0.5122582770971261, 0.2145348922726212], [0.1631883770761854, 0.6776002996483265, 0.99, 0.6863785397143443, 0.896441715403196, 0.12977965751058052, 0.9171111957499719, 0.99, 0.643044949168197, 0.01, 0.7473095365329135, 0.9364883141559396, 0.748549036682807, 0.302580558148454, 0.2821482237562667, 0.8617163329117846, 0.01, 0.30398882490344936, 0.5662054101511667, 0.7493982669353357, 0.036865728257726105, 0.3133278561319889, 0.7119807957027908, 0.16883430601696675], [0.27636222529549204, 0.7371961377452687, 0.8143326078700106, 0.15431526282438066, 0.7862020155326304, 0.01, 0.99, 0.99, 0.45125797971292325, 0.1413678006673195, 0.6595512617879906, 0.6344937872510573, 0.5267130212981369, 0.4039919209842354, 0.172731447953135, 0.8060317686352, 0.32910783607641647, 0.16265339293459252, 0.7301633465974586, 0.8432222569832786, 0.31984517710814697, 0.18086198432305434, 0.48886694498401717, 0.19554952468489478], [0.3016967940782256, 0.545789933390618, 0.5481158046499206, 0.1370269006292088, 0.99, 0.11419108400282191, 0.9193964242495599, 0.8784286481304499, 0.5219681023399063, 0.01, 0.7971172605601541, 0.720791569114498, 0.804801365598778, 0.09342470470275932, 0.20103319992929405, 0.6834127678236022, 0.06598442863154975, 0.4061780667024608, 0.6349261359703741, 0.99, 0.14813998382176935, 0.4374911090625621, 0.49410439216593394, 0.01], [0.5459571766551912, 0.5927645725373841, 0.9180929596034771, 0.3124982821419851, 0.99, 0.01, 0.99, 0.7706181325792973, 0.6901049050843031, 0.08721436637632163, 0.809098890885393, 0.7869573814849786, 0.7266175081399309, 0.4130073596837656, 0.3269219562331941, 0.626832236370472, 0.16657210627657817, 0.29682471173108105, 0.5602122732373189, 0.882313454341557, 0.29076458296333, 0.1272373953912308, 0.3697643895944101, 0.36586833385318074], [0.17135919049687384, 0.6825134895157287, 0.9701979486392338, 0.09849907959199591, 0.7413246248442871, 0.12209018560882404, 0.99, 0.7645667824657362, 0.5363769739323193, 0.01, 0.7607554906638988, 0.99, 0.5890308297010312, 0.1668313062331084, 0.34854697211627067, 0.9222635599154488, 0.07886350879875895, 0.1907231985654203, 0.6713014662777173, 0.560069757933654, 0.23637310916578935, 0.44054962004053777, 0.3561526200822495, 0.07970349935589816]]
19 	8     	0.795469	0.00459013	0.79   	0.8025 
params:  [0.34932662529122993, 0.7236740264472044, 0.8009812399796019, 0.3656695576689528, 0.7195497550736446, 0.06087050983964089, 0.7543385332112259, 0.900610538463344, 0.7186572926600606, 0.07400971227838932, 0.9216876377177894, 0.99, 0.7139136078183334, 0.01, 0.49170545655974474, 0.7577884657402367, 0.3220074244151893, 0.19231729543474324, 0.7865476083202994, 0.7952882648103153, 0.24597077597902156, 0.01, 0.46839317216284937, 0.01]
Training loss = 0.01585629423459371
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.8025
Training loss = 0.015289989411830901
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.016367884576320647
step = 2, Training Accuracy: 0.8
Training loss = 0.0156723752617836
step = 3, Training Accuracy: 0.79
Training loss = 0.016690791845321656
step = 4, Training Accuracy: 0.79
Training loss = 0.014513211747010549
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.015981338918209076
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.015296444992224375
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.016387626230716705
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.01854684333006541
step = 9, Training Accuracy: 0.7366666666666667
Training loss = 0.0160062671204408
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.01529348353544871
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015659369031588235
step = 12, Training Accuracy: 0.8
Training loss = 0.013582141101360322
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.01504518061876297
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.79875
params:  [0.29701040260611217, 0.5739090715201105, 0.7662740827662955, 0.47682840501579943, 0.99, 0.01, 0.9773452062681647, 0.99, 0.5284873436593741, 0.46869370033657426, 0.4763178781643642, 0.7670643544176067, 0.6727423932180011, 0.4013932317215405, 0.4174754787768124, 0.7592656082195158, 0.24060570781624152, 0.41525309879692085, 0.6492199640643176, 0.7498277051187405, 0.1564651451565196, 0.4158735558981466, 0.4305644070496953, 0.01]
Training loss = 0.018337527612845104
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79625
Training loss = 0.017037704090277352
step = 1, Training Accuracy: 0.74
Training loss = 0.015614572962125142
step = 2, Training Accuracy: 0.8
Training loss = 0.01743806044260661
step = 3, Training Accuracy: 0.81
Training loss = 0.016997503638267516
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.017423604528109232
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.018054459889729816
step = 6, Training Accuracy: 0.77
Training loss = 0.017639521757761636
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.019111078083515167
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01743840366601944
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.017937159339586894
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.016855157812436423
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.01616790423790614
step = 12, Training Accuracy: 0.81
Training loss = 0.01567014475663503
step = 13, Training Accuracy: 0.79
Training loss = 0.01568514436483383
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.8
params:  [0.5044242237217987, 0.7213209280314501, 0.99, 0.31677695016461993, 0.6714300304490656, 0.01, 0.8822249211626669, 0.7420177925629916, 0.47074385502304156, 0.01, 0.5303456500501544, 0.9259455404805284, 0.40329873646714987, 0.31755143625422205, 0.12468239644351045, 0.9630268642902158, 0.23518094697989333, 0.018871936701304645, 0.59312890179589, 0.6959488208964104, 0.01, 0.06813675637048726, 0.37366192776277496, 0.01]
Training loss = 0.018158771296342215
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79625
Training loss = 0.01485910415649414
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01768511494000753
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.014216493169466654
step = 3, Training Accuracy: 0.81
Training loss = 0.016172062853972116
step = 4, Training Accuracy: 0.8
Training loss = 0.017387885948022208
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.015913303842147192
step = 6, Training Accuracy: 0.77
Training loss = 0.016014331777890523
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01734620193640391
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.0174395877122879
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.01632998953262965
step = 10, Training Accuracy: 0.78
Training loss = 0.01742639482021332
step = 11, Training Accuracy: 0.76
Training loss = 0.016266100207964578
step = 12, Training Accuracy: 0.79
Training loss = 0.016218440135320027
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.016310157577196758
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.8025
params:  [0.3741229528972923, 0.7212870696441802, 0.8531863803287718, 0.4187868849139377, 0.5393000243881645, 0.011631873967533332, 0.7500067076587107, 0.7793633198756071, 0.4807791728480441, 0.2059323766213304, 0.5658401412601839, 0.7513574593325286, 0.8163429836666682, 0.02739862667569573, 0.3926139780160487, 0.823420196283038, 0.30999412596198517, 0.01, 0.558068085900394, 0.6794619328525497, 0.18442075675250325, 0.14555871798979822, 0.3098156046640067, 0.01]
Training loss = 0.019467185735702514
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79875
Training loss = 0.017765108048915863
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.0199112140138944
step = 2, Training Accuracy: 0.7166666666666667
Training loss = 0.018843643963336945
step = 3, Training Accuracy: 0.76
Training loss = 0.019120227297147116
step = 4, Training Accuracy: 0.76
Training loss = 0.017643524905045827
step = 5, Training Accuracy: 0.77
Training loss = 0.019350564181804655
step = 6, Training Accuracy: 0.74
Training loss = 0.019803639551003775
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01636831065018972
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.016416938801606495
step = 9, Training Accuracy: 0.78
Training loss = 0.018645243247350056
step = 10, Training Accuracy: 0.72
Training loss = 0.01800974667072296
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.01801128884156545
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017803342044353486
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.01564115395148595
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.80375
params:  [0.5860831119122876, 0.6545965072651031, 0.7599145754137251, 0.32052632039297524, 0.5381319979048806, 0.12631032126981206, 0.7883502439742688, 0.681668436833657, 0.7103516886455808, 0.3058234976401574, 0.7362839589012451, 0.6406818447653712, 0.8245666898581168, 0.01, 0.5405900634594325, 0.9054854112425801, 0.2990653261334365, 0.05845582029742185, 0.7935384617332678, 0.8497631093277731, 0.278804454037727, 0.1061001219632017, 0.577106569952123, 0.01]
Training loss = 0.021758914788564045
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.80125
Training loss = 0.019310033619403838
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.01968115448951721
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.018582610587279003
step = 3, Training Accuracy: 0.76
Training loss = 0.018722045520941418
step = 4, Training Accuracy: 0.75
Training loss = 0.020104786654313405
step = 5, Training Accuracy: 0.73
Training loss = 0.01950796882311503
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.019516919950644175
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01941398451725642
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.01955615679423014
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.019538369476795197
step = 10, Training Accuracy: 0.71
Training loss = 0.019920172293980916
step = 11, Training Accuracy: 0.71
Training loss = 0.01947033127148946
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.020827174286047617
step = 13, Training Accuracy: 0.7433333333333333
Training loss = 0.020630748172601063
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.80375
params:  [0.2909859524521752, 0.6706704571799602, 0.99, 0.11771778468305447, 0.6482826472898562, 0.16454539792092282, 0.6788522336227077, 0.8372904644462973, 0.44001540046500864, 0.1409998020594109, 0.7312677861978365, 0.99, 0.47711019052634246, 0.1669791471662802, 0.4546873666774346, 0.8847484545282818, 0.22549074736605107, 0.13330735307908773, 0.7245181220199667, 0.6975719707081266, 0.17151171313087707, 0.20754856680092926, 0.4354700289763759, 0.07901464773687046]
Training loss = 0.0176359427968661
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.80125
Training loss = 0.018337003688017526
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.018062771260738374
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.016865319112936657
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015539720753828685
step = 4, Training Accuracy: 0.8
Training loss = 0.018117649058500926
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.01629867563645045
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.016019359529018402
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.01724682668844859
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.015528262356917063
step = 9, Training Accuracy: 0.79
Training loss = 0.015451286931832632
step = 10, Training Accuracy: 0.8
Training loss = 0.019996394713719685
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.016201251943906148
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.018958931664625804
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01677845299243927
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.8025
params:  [0.4545788136419586, 0.5871515534373795, 0.9074371676578054, 0.15301158683651842, 0.99, 0.14199898008699524, 0.9312097622868173, 0.9356831105025307, 0.27838641857133867, 0.3643048114285528, 0.7863352095629529, 0.7029744740131303, 0.5311948354980426, 0.019995753721476828, 0.25709016034620746, 0.9003166039036816, 0.3040397456485532, 0.2753546399793377, 0.47962455551571037, 0.6664993770154798, 0.13718428672251665, 0.04831101456427461, 0.41080438747052767, 0.01]
Training loss = 0.016530759632587433
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.79875
Training loss = 0.01851465314626694
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.018444332182407378
step = 2, Training Accuracy: 0.77
Training loss = 0.018386558791001636
step = 3, Training Accuracy: 0.77
Training loss = 0.018056974709033967
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01655395656824112
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.017853046854337057
step = 6, Training Accuracy: 0.77
Training loss = 0.01737778534491857
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.015824966629346213
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.0169973682363828
step = 9, Training Accuracy: 0.78
Training loss = 0.016482767264048258
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.017677086492379507
step = 11, Training Accuracy: 0.76
Training loss = 0.016179832617441815
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016300701101620994
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.017235889931519827
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.795
params:  [0.3938687588566236, 0.6937654053621899, 0.9541638147174529, 0.3033806255468994, 0.687624722186519, 0.01, 0.9856090905947945, 0.7689454900500514, 0.6706626625139563, 0.042417080894029846, 0.7043934615048081, 0.8074487271818565, 0.8016532833428104, 0.032346174224046204, 0.09128583821856764, 0.8936657092457351, 0.1779261180844457, 0.14052300403225587, 0.47814494302099225, 0.5545769670687863, 0.5793823992968961, 0.3431712113326034, 0.5802847291740592, 0.11557062285284403]
Training loss = 0.016003096103668214
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79375
Training loss = 0.014638203680515289
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.015938367495934167
step = 2, Training Accuracy: 0.8
Training loss = 0.016455204337835313
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.01657233456770579
step = 4, Training Accuracy: 0.8
Training loss = 0.016977652410666146
step = 5, Training Accuracy: 0.78
Training loss = 0.017179438769817354
step = 6, Training Accuracy: 0.77
Training loss = 0.01585787643988927
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.017821306784947713
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.01576756586631139
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01867267022530238
step = 10, Training Accuracy: 0.78
Training loss = 0.014853260815143585
step = 11, Training Accuracy: 0.8
Training loss = 0.016811496516068777
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.015052077074845632
step = 13, Training Accuracy: 0.82
Training loss = 0.01769575834274292
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.79625
[[0.34932662529122993, 0.7236740264472044, 0.8009812399796019, 0.3656695576689528, 0.7195497550736446, 0.06087050983964089, 0.7543385332112259, 0.900610538463344, 0.7186572926600606, 0.07400971227838932, 0.9216876377177894, 0.99, 0.7139136078183334, 0.01, 0.49170545655974474, 0.7577884657402367, 0.3220074244151893, 0.19231729543474324, 0.7865476083202994, 0.7952882648103153, 0.24597077597902156, 0.01, 0.46839317216284937, 0.01], [0.29701040260611217, 0.5739090715201105, 0.7662740827662955, 0.47682840501579943, 0.99, 0.01, 0.9773452062681647, 0.99, 0.5284873436593741, 0.46869370033657426, 0.4763178781643642, 0.7670643544176067, 0.6727423932180011, 0.4013932317215405, 0.4174754787768124, 0.7592656082195158, 0.24060570781624152, 0.41525309879692085, 0.6492199640643176, 0.7498277051187405, 0.1564651451565196, 0.4158735558981466, 0.4305644070496953, 0.01], [0.5044242237217987, 0.7213209280314501, 0.99, 0.31677695016461993, 0.6714300304490656, 0.01, 0.8822249211626669, 0.7420177925629916, 0.47074385502304156, 0.01, 0.5303456500501544, 0.9259455404805284, 0.40329873646714987, 0.31755143625422205, 0.12468239644351045, 0.9630268642902158, 0.23518094697989333, 0.018871936701304645, 0.59312890179589, 0.6959488208964104, 0.01, 0.06813675637048726, 0.37366192776277496, 0.01], [0.3741229528972923, 0.7212870696441802, 0.8531863803287718, 0.4187868849139377, 0.5393000243881645, 0.011631873967533332, 0.7500067076587107, 0.7793633198756071, 0.4807791728480441, 0.2059323766213304, 0.5658401412601839, 0.7513574593325286, 0.8163429836666682, 0.02739862667569573, 0.3926139780160487, 0.823420196283038, 0.30999412596198517, 0.01, 0.558068085900394, 0.6794619328525497, 0.18442075675250325, 0.14555871798979822, 0.3098156046640067, 0.01], [0.5860831119122876, 0.6545965072651031, 0.7599145754137251, 0.32052632039297524, 0.5381319979048806, 0.12631032126981206, 0.7883502439742688, 0.681668436833657, 0.7103516886455808, 0.3058234976401574, 0.7362839589012451, 0.6406818447653712, 0.8245666898581168, 0.01, 0.5405900634594325, 0.9054854112425801, 0.2990653261334365, 0.05845582029742185, 0.7935384617332678, 0.8497631093277731, 0.278804454037727, 0.1061001219632017, 0.577106569952123, 0.01], [0.2909859524521752, 0.6706704571799602, 0.99, 0.11771778468305447, 0.6482826472898562, 0.16454539792092282, 0.6788522336227077, 0.8372904644462973, 0.44001540046500864, 0.1409998020594109, 0.7312677861978365, 0.99, 0.47711019052634246, 0.1669791471662802, 0.4546873666774346, 0.8847484545282818, 0.22549074736605107, 0.13330735307908773, 0.7245181220199667, 0.6975719707081266, 0.17151171313087707, 0.20754856680092926, 0.4354700289763759, 0.07901464773687046], [0.4545788136419586, 0.5871515534373795, 0.9074371676578054, 0.15301158683651842, 0.99, 0.14199898008699524, 0.9312097622868173, 0.9356831105025307, 0.27838641857133867, 0.3643048114285528, 0.7863352095629529, 0.7029744740131303, 0.5311948354980426, 0.019995753721476828, 0.25709016034620746, 0.9003166039036816, 0.3040397456485532, 0.2753546399793377, 0.47962455551571037, 0.6664993770154798, 0.13718428672251665, 0.04831101456427461, 0.41080438747052767, 0.01], [0.3938687588566236, 0.6937654053621899, 0.9541638147174529, 0.3033806255468994, 0.687624722186519, 0.01, 0.9856090905947945, 0.7689454900500514, 0.6706626625139563, 0.042417080894029846, 0.7043934615048081, 0.8074487271818565, 0.8016532833428104, 0.032346174224046204, 0.09128583821856764, 0.8936657092457351, 0.1779261180844457, 0.14052300403225587, 0.47814494302099225, 0.5545769670687863, 0.5793823992968961, 0.3431712113326034, 0.5802847291740592, 0.11557062285284403]]
20 	8     	0.800312	0.00317153	0.795  	0.80375
params:  [0.7991874807426519, 0.8497473485628944, 0.9618763645939971, 0.5305628094223889, 0.43090530294122253, 0.24185008065869712, 0.7960400984771838, 0.9248751662136222, 0.5072779705738539, 0.16056930385577695, 0.6077770142144445, 0.7137200868573352, 0.6888833074664782, 0.40395429195917854, 0.20215636377475044, 0.8370836188748284, 0.4108317075947333, 0.021497541367766, 0.6744663850985072, 0.6196647650586893, 0.17573657517048655, 0.01, 0.4601595743819228, 0.01]
Training loss = 0.017428610622882843
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.8
Training loss = 0.01757781535387039
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.020700444877147676
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.017984144687652588
step = 3, Training Accuracy: 0.75
Training loss = 0.01728013147910436
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.01761174390713374
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.019467582205931346
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.018432832956314087
step = 7, Training Accuracy: 0.77
Training loss = 0.01909701993068059
step = 8, Training Accuracy: 0.74
Training loss = 0.017953221797943116
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.017657212018966674
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01692832907040914
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01698663870493571
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.019581639965375264
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.01861377904812495
step = 14, Training Accuracy: 0.75
Validation Accuracy: 0.79625
params:  [0.5834554721999436, 0.5162506109606468, 0.9775335002185055, 0.28194341261070344, 0.5093122251044906, 0.01, 0.8919419602245696, 0.8119484647222912, 0.5956799483530425, 0.42235739037945713, 0.5174877814318367, 0.7316404931389818, 0.6025351239946747, 0.2069259897451532, 0.540569777608656, 0.7759420416362515, 0.3327206634888066, 0.15659129558712837, 0.7735038307719988, 0.5656052634326844, 0.13931579977863723, 0.13577884497389617, 0.5091241784434158, 0.03943205716615767]
Training loss = 0.018102435370286305
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.79375
Training loss = 0.014517393211523693
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.015414659976959228
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.017055129110813142
step = 3, Training Accuracy: 0.77
Training loss = 0.015225506325562795
step = 4, Training Accuracy: 0.8
Training loss = 0.01651396612326304
step = 5, Training Accuracy: 0.76
Training loss = 0.014618303378423054
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.01565680593252182
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.015835845271746318
step = 8, Training Accuracy: 0.8
Training loss = 0.01499709665775299
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.014188180267810822
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.013911750862995783
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.015177841087182363
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015154414375623067
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014475148518880208
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.80375
params:  [0.44192774421637127, 0.6391513676962483, 0.99, 0.29351264013131567, 0.629724124321572, 0.025934891761803733, 0.7155628511389671, 0.859094404886167, 0.8112745069217755, 0.242890038446481, 0.5505969362668012, 0.5173799569064731, 0.7300566291417737, 0.13459689381372314, 0.571576181033197, 0.9105049228020743, 0.3674206496178387, 0.07715678036526696, 0.7142256808837639, 0.7808592834962084, 0.1886060886040037, 0.32917500274478917, 0.09329669797961909, 0.06883061434011659]
Training loss = 0.01969041625658671
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.80125
Training loss = 0.017602252960205077
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.018565295040607454
step = 2, Training Accuracy: 0.74
Training loss = 0.017643765409787494
step = 3, Training Accuracy: 0.74
Training loss = 0.017582931617895762
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.017362086176872252
step = 5, Training Accuracy: 0.75
Training loss = 0.01828233281771342
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.017170795102914173
step = 7, Training Accuracy: 0.76
Training loss = 0.017957348922888437
step = 8, Training Accuracy: 0.75
Training loss = 0.020743926763534547
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.0167411994934082
step = 10, Training Accuracy: 0.77
Training loss = 0.018294078210989634
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.017416258653004963
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.017152551809946695
step = 13, Training Accuracy: 0.75
Training loss = 0.01879775047302246
step = 14, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.795
params:  [0.5488934823166156, 0.6981113262963747, 0.7128604262776704, 0.34974322415226156, 0.3442933040529598, 0.16511292814246187, 0.6200372710400142, 0.7791337633822094, 0.661065354073893, 0.3404393675481001, 0.4097898679166052, 0.4198963106221284, 0.8348077551462854, 0.07209447705012081, 0.5089932333923016, 0.6570000148943816, 0.24741249260836318, 0.01, 0.7824145871495585, 0.6786234900016234, 0.1339138558531923, 0.01, 0.4186458241843598, 0.035276431353715554]
Training loss = 0.016687867840131123
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.80125
Training loss = 0.018828854064146677
step = 1, Training Accuracy: 0.76
Training loss = 0.015793691873550415
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.016279945472876232
step = 3, Training Accuracy: 0.76
Training loss = 0.018245306611061097
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01672018696864446
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.018674943645795188
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.01816498816013336
step = 7, Training Accuracy: 0.76
Training loss = 0.01683069298664729
step = 8, Training Accuracy: 0.77
Training loss = 0.018048082888126375
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.016669232845306397
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.01608314722776413
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.016707193156083426
step = 12, Training Accuracy: 0.79
Training loss = 0.016743422547976176
step = 13, Training Accuracy: 0.78
Training loss = 0.016340651313463846
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.8025
params:  [0.46365699567096386, 0.6418270382729949, 0.7576797556391462, 0.3944256083179277, 0.6971252131688892, 0.38930807163333664, 0.6972561981810802, 0.772266676165358, 0.7069490849739953, 0.0476256254935932, 0.6806211474999728, 0.8490386117898943, 0.657806139356745, 0.08815432708456601, 0.5031346217734203, 0.8980858973527097, 0.4430140913133411, 0.01, 0.6671725577012467, 0.6224608441100381, 0.03464139549334744, 0.2888360490725196, 0.31256032535124445, 0.01]
Training loss = 0.016870581110318503
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.80125
Training loss = 0.01783484806617101
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017214499612649283
step = 2, Training Accuracy: 0.77
Training loss = 0.016672028998533886
step = 3, Training Accuracy: 0.77
Training loss = 0.016806799670060477
step = 4, Training Accuracy: 0.79
Training loss = 0.017244711220264435
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.017243507305781048
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.01716522216796875
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01736946851015091
step = 8, Training Accuracy: 0.77
Training loss = 0.01766016225020091
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015657991766929627
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.016772537032763164
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01670951892932256
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.017227703829606374
step = 13, Training Accuracy: 0.79
Training loss = 0.016264741619427998
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.805
params:  [0.42195939695132356, 0.6573906700840267, 0.7935128347033542, 0.41572324486170664, 0.5556313160127774, 0.030683086629295656, 0.7857762152665935, 0.5998843818950882, 0.6927180899378363, 0.23666832635359475, 0.5885073668781908, 0.7678440231704219, 0.47283359197658104, 0.28649497013293074, 0.2845478050739538, 0.6901061049306638, 0.3810728440222386, 0.2354305336915562, 0.8455078453706392, 0.6197126434281622, 0.15765106654999114, 0.01, 0.12164824280756054, 0.01]
Training loss = 0.01371007094780604
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.81125
Training loss = 0.01414038469394048
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.013098834554354349
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01409062534570694
step = 3, Training Accuracy: 0.78
Training loss = 0.014617033402125041
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.015670655568440755
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.014522589246431987
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.012883303711811702
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.013878215650717417
step = 8, Training Accuracy: 0.8333333333333334
Training loss = 0.012607612361510595
step = 9, Training Accuracy: 0.83
Training loss = 0.015397147238254548
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.01444556454817454
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.014471787412961325
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.011743844350179037
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.014490819275379182
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.81125
params:  [0.4455114138242339, 0.4706892232503452, 0.8011171478134677, 0.2875809900358299, 0.868905760345195, 0.01, 0.7523366538619267, 0.6430789851206238, 0.7757234747019504, 0.311094810436668, 0.5294523044342967, 0.7545799684157217, 0.92635093385272, 0.18800671010604075, 0.4332912111191106, 0.7927836336680927, 0.46350432610022896, 0.01, 0.7608455829499954, 0.7107837733873733, 0.03404015950516798, 0.10914466759770092, 0.37322047908724226, 0.11669740878462888]
Training loss = 0.015527828633785247
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.80875
Training loss = 0.013644286294778188
step = 1, Training Accuracy: 0.83
Training loss = 0.016178651253382366
step = 2, Training Accuracy: 0.8
Training loss = 0.01567752222220103
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01529986987511317
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.015095097720623016
step = 5, Training Accuracy: 0.81
Training loss = 0.015490534603595734
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.014571468234062196
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.014695057272911071
step = 8, Training Accuracy: 0.82
Training loss = 0.016408362487951914
step = 9, Training Accuracy: 0.77
Training loss = 0.015099631249904632
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.01606056233247121
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01584809939066569
step = 12, Training Accuracy: 0.79
Training loss = 0.01450611670811971
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.014788142939408621
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.80375
params:  [0.7200317687476647, 0.9465155986119524, 0.7924000054225422, 0.31233626308896884, 0.5650877622295815, 0.01, 0.8705007158334694, 0.8283709433991059, 0.5787617591226514, 0.3112955860226818, 0.6830848322417701, 0.8676747580423014, 0.8212118366831435, 0.12002351266636493, 0.3942839037910207, 0.8163581398213805, 0.2544601639199228, 0.01, 0.6694715370659768, 0.7253602211421616, 0.1749124049046963, 0.02743968390607157, 0.3415446246288165, 0.07213529526283585]
Training loss = 0.02136170248190562
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.8075
Training loss = 0.021253474752108256
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017483581801255543
step = 2, Training Accuracy: 0.74
Training loss = 0.020462930997212726
step = 3, Training Accuracy: 0.73
Training loss = 0.01912889152765274
step = 4, Training Accuracy: 0.72
Training loss = 0.019933046102523805
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.020473003685474396
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.01991035580635071
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.019903314610322315
step = 8, Training Accuracy: 0.7133333333333334
Training loss = 0.02188148319721222
step = 9, Training Accuracy: 0.7233333333333334
Training loss = 0.018649048109849294
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01810943196217219
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018598307867844898
step = 12, Training Accuracy: 0.74
Training loss = 0.020857190092404682
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.019635692338148752
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.80125
[[0.7991874807426519, 0.8497473485628944, 0.9618763645939971, 0.5305628094223889, 0.43090530294122253, 0.24185008065869712, 0.7960400984771838, 0.9248751662136222, 0.5072779705738539, 0.16056930385577695, 0.6077770142144445, 0.7137200868573352, 0.6888833074664782, 0.40395429195917854, 0.20215636377475044, 0.8370836188748284, 0.4108317075947333, 0.021497541367766, 0.6744663850985072, 0.6196647650586893, 0.17573657517048655, 0.01, 0.4601595743819228, 0.01], [0.5834554721999436, 0.5162506109606468, 0.9775335002185055, 0.28194341261070344, 0.5093122251044906, 0.01, 0.8919419602245696, 0.8119484647222912, 0.5956799483530425, 0.42235739037945713, 0.5174877814318367, 0.7316404931389818, 0.6025351239946747, 0.2069259897451532, 0.540569777608656, 0.7759420416362515, 0.3327206634888066, 0.15659129558712837, 0.7735038307719988, 0.5656052634326844, 0.13931579977863723, 0.13577884497389617, 0.5091241784434158, 0.03943205716615767], [0.44192774421637127, 0.6391513676962483, 0.99, 0.29351264013131567, 0.629724124321572, 0.025934891761803733, 0.7155628511389671, 0.859094404886167, 0.8112745069217755, 0.242890038446481, 0.5505969362668012, 0.5173799569064731, 0.7300566291417737, 0.13459689381372314, 0.571576181033197, 0.9105049228020743, 0.3674206496178387, 0.07715678036526696, 0.7142256808837639, 0.7808592834962084, 0.1886060886040037, 0.32917500274478917, 0.09329669797961909, 0.06883061434011659], [0.5488934823166156, 0.6981113262963747, 0.7128604262776704, 0.34974322415226156, 0.3442933040529598, 0.16511292814246187, 0.6200372710400142, 0.7791337633822094, 0.661065354073893, 0.3404393675481001, 0.4097898679166052, 0.4198963106221284, 0.8348077551462854, 0.07209447705012081, 0.5089932333923016, 0.6570000148943816, 0.24741249260836318, 0.01, 0.7824145871495585, 0.6786234900016234, 0.1339138558531923, 0.01, 0.4186458241843598, 0.035276431353715554], [0.46365699567096386, 0.6418270382729949, 0.7576797556391462, 0.3944256083179277, 0.6971252131688892, 0.38930807163333664, 0.6972561981810802, 0.772266676165358, 0.7069490849739953, 0.0476256254935932, 0.6806211474999728, 0.8490386117898943, 0.657806139356745, 0.08815432708456601, 0.5031346217734203, 0.8980858973527097, 0.4430140913133411, 0.01, 0.6671725577012467, 0.6224608441100381, 0.03464139549334744, 0.2888360490725196, 0.31256032535124445, 0.01], [0.42195939695132356, 0.6573906700840267, 0.7935128347033542, 0.41572324486170664, 0.5556313160127774, 0.030683086629295656, 0.7857762152665935, 0.5998843818950882, 0.6927180899378363, 0.23666832635359475, 0.5885073668781908, 0.7678440231704219, 0.47283359197658104, 0.28649497013293074, 0.2845478050739538, 0.6901061049306638, 0.3810728440222386, 0.2354305336915562, 0.8455078453706392, 0.6197126434281622, 0.15765106654999114, 0.01, 0.12164824280756054, 0.01], [0.4455114138242339, 0.4706892232503452, 0.8011171478134677, 0.2875809900358299, 0.868905760345195, 0.01, 0.7523366538619267, 0.6430789851206238, 0.7757234747019504, 0.311094810436668, 0.5294523044342967, 0.7545799684157217, 0.92635093385272, 0.18800671010604075, 0.4332912111191106, 0.7927836336680927, 0.46350432610022896, 0.01, 0.7608455829499954, 0.7107837733873733, 0.03404015950516798, 0.10914466759770092, 0.37322047908724226, 0.11669740878462888], [0.7200317687476647, 0.9465155986119524, 0.7924000054225422, 0.31233626308896884, 0.5650877622295815, 0.01, 0.8705007158334694, 0.8283709433991059, 0.5787617591226514, 0.3112955860226818, 0.6830848322417701, 0.8676747580423014, 0.8212118366831435, 0.12002351266636493, 0.3942839037910207, 0.8163581398213805, 0.2544601639199228, 0.01, 0.6694715370659768, 0.7253602211421616, 0.1749124049046963, 0.02743968390607157, 0.3415446246288165, 0.07213529526283585]]
21 	8     	0.802344	0.00477778	0.795  	0.81125
params:  [0.5250875656333833, 0.49546191426270325, 0.806215472085073, 0.4573656631420633, 0.667756393954537, 0.01, 0.8400638649491307, 0.6090039525609088, 0.8238248702813233, 0.3910513412197445, 0.577287191582645, 0.5500771591033791, 0.5797226613599957, 0.21623439292985466, 0.5304330478831626, 0.8200424422375826, 0.3842385266271649, 0.255516461183866, 0.9726894951439348, 0.6619349369395799, 0.01, 0.01, 0.27626201112356275, 0.14072309235367622]
Training loss = 0.019167802532513937
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.8025
Training loss = 0.016061418851216633
step = 1, Training Accuracy: 0.78
Training loss = 0.01448882132768631
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.015785651405652364
step = 3, Training Accuracy: 0.79
Training loss = 0.016333081275224686
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01606920878092448
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015709724525610605
step = 6, Training Accuracy: 0.79
Training loss = 0.014282026489575704
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.016929555237293243
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.014180194934209188
step = 9, Training Accuracy: 0.8
Training loss = 0.01500120053688685
step = 10, Training Accuracy: 0.78
Training loss = 0.015215588907400768
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014861061573028564
step = 12, Training Accuracy: 0.81
Training loss = 0.014248980383078258
step = 13, Training Accuracy: 0.83
Training loss = 0.014243439435958863
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.80125
params:  [0.3977452549203779, 0.4405550614880822, 0.9233704627874154, 0.44810307740485084, 0.5993654223715271, 0.18849442820337775, 0.762673403550742, 0.507651683449474, 0.533977896463401, 0.298973268262542, 0.45886264234553775, 0.9085724941531511, 0.6849133271115485, 0.16179074929204804, 0.33664570938317206, 0.706543825429906, 0.3383465519179573, 0.26575805635357375, 0.99, 0.6242807358745931, 0.29515533398373806, 0.14511414588264177, 0.11458903360629319, 0.01]
Training loss = 0.013533800641695659
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.80625
Training loss = 0.014380012849966685
step = 1, Training Accuracy: 0.81
Training loss = 0.014560378491878509
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.014551922281583151
step = 3, Training Accuracy: 0.82
Training loss = 0.014353514313697815
step = 4, Training Accuracy: 0.82
Training loss = 0.013277914921442667
step = 5, Training Accuracy: 0.8533333333333334
Training loss = 0.014212265610694885
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.013687466382980346
step = 7, Training Accuracy: 0.84
Training loss = 0.013477652966976165
step = 8, Training Accuracy: 0.84
Training loss = 0.014446922640005747
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.014370990991592407
step = 10, Training Accuracy: 0.82
Training loss = 0.01485199898481369
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.014138081073760987
step = 12, Training Accuracy: 0.84
Training loss = 0.014366375903288523
step = 13, Training Accuracy: 0.8
Training loss = 0.01467810183763504
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.80625
params:  [0.5973108550118202, 0.572930462800024, 0.8935692686244533, 0.30924583222086277, 0.5953554594324911, 0.20096919198064728, 0.6079384310400167, 0.7629194381684092, 0.5424420671774501, 0.13732907903444008, 0.6901088906765455, 0.6724775370746674, 0.6676437962724161, 0.13942388494479035, 0.3392114665143755, 0.6938301275757892, 0.3555255595572737, 0.18174965008771254, 0.6659653298041546, 0.5798123630407535, 0.1831824398475494, 0.04349534220453939, 0.21627180574647453, 0.01]
Training loss = 0.013545474310715993
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.80875
Training loss = 0.014318540692329407
step = 1, Training Accuracy: 0.82
Training loss = 0.013174488147099813
step = 2, Training Accuracy: 0.83
Training loss = 0.01332185298204422
step = 3, Training Accuracy: 0.84
Training loss = 0.01297158216436704
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.013157730003197988
step = 5, Training Accuracy: 0.8466666666666667
Training loss = 0.01301675130923589
step = 6, Training Accuracy: 0.83
Training loss = 0.013782948851585389
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.012849940955638885
step = 8, Training Accuracy: 0.83
Training loss = 0.014246361156304677
step = 9, Training Accuracy: 0.83
Training loss = 0.0122681658466657
step = 10, Training Accuracy: 0.82
Training loss = 0.012610471496979395
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.01333497017621994
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.013248571455478668
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.012545924087365469
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.79875
params:  [0.518950111630342, 0.5600314651756848, 0.7461739128790252, 0.4343480145861684, 0.3835602280365187, 0.20527138066177092, 0.6574505940705753, 0.8830803256064362, 0.57146577670342, 0.01, 0.6012244762349533, 0.7446287809020946, 0.398023582130604, 0.24616319741315776, 0.5860574342041838, 0.6411950500391257, 0.24796258706420488, 0.3961108340070286, 0.99, 0.6148835669201774, 0.31553604318971534, 0.13990631736530273, 0.3663813936236957, 0.03501228517823493]
Training loss = 0.016723158756891888
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.805
Training loss = 0.016289416551589966
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01602498521407445
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.014676956882079442
step = 3, Training Accuracy: 0.81
Training loss = 0.01659637818733851
step = 4, Training Accuracy: 0.79
Training loss = 0.018394703964392346
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.0164263978600502
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016807859043280284
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016238592267036438
step = 8, Training Accuracy: 0.79
Training loss = 0.016586484014987947
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.017414479355017345
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.017561693688233692
step = 11, Training Accuracy: 0.8
Training loss = 0.016797832548618316
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.015487380425135295
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01787424067656199
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.80375
params:  [0.43843156216436285, 0.6816462140340735, 0.7763354644015552, 0.305356362389251, 0.44919902652326427, 0.01, 0.6650437013669879, 0.6410766781915783, 0.5846599801413181, 0.2503622835272439, 0.703985350498879, 0.9544259544544422, 0.7484466122980769, 0.01, 0.4324829495531812, 0.99, 0.1701661134993175, 0.011993058468775825, 0.5004520887446706, 0.6287874830876394, 0.21792915606874497, 0.01, 0.2489819916038206, 0.06627444520761153]
Training loss = 0.016175411293903985
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.805
Training loss = 0.017442496915658315
step = 1, Training Accuracy: 0.77
Training loss = 0.016362854838371278
step = 2, Training Accuracy: 0.75
Training loss = 0.015795229772726696
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.014574527591466904
step = 4, Training Accuracy: 0.78
Training loss = 0.0173665119210879
step = 5, Training Accuracy: 0.75
Training loss = 0.015717239479223887
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.017302628258864083
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.016772975126902262
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.018148029446601866
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.01523935874303182
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.017008760770161946
step = 11, Training Accuracy: 0.76
Training loss = 0.015178300440311432
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.015725735227266946
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.017119267086187998
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.795
params:  [0.5429480324441822, 0.48330910033553165, 0.7518254316765689, 0.33063154904103814, 0.6890528171420037, 0.10529999522336626, 0.7996024624962562, 0.6788727760663217, 0.44599065411029326, 0.20640442896616926, 0.5932071043351673, 0.8223601794137649, 0.6707577385282282, 0.09888795258952734, 0.5831421968153014, 0.99, 0.21043294097691884, 0.0971581530733095, 0.8551765670575108, 0.4954280023756983, 0.2253199244998037, 0.01, 0.33062238578695735, 0.24373076668113414]
Training loss = 0.019500542283058166
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.805
Training loss = 0.01685269902149836
step = 1, Training Accuracy: 0.76
Training loss = 0.017126669585704805
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.017729042172431944
step = 3, Training Accuracy: 0.77
Training loss = 0.01615417152643204
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01575963705778122
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.016830366253852844
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015640790065129598
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01670892129341761
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015508419970671336
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016673587560653687
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.016721433103084563
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.014853093425432841
step = 12, Training Accuracy: 0.8
Training loss = 0.015977218945821128
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.01700003574291865
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.805
params:  [0.4790147780721955, 0.73015279162329, 0.743133704495912, 0.38003198783368614, 0.7015631689250451, 0.1471136701855157, 0.8084092110704254, 0.8038211144003864, 0.9387886468049857, 0.3683073977048301, 0.5419956938631864, 0.7638989153959272, 0.4455849578282353, 0.10870828077283602, 0.5020153778859946, 0.9668329678243981, 0.3643418700780109, 0.0847147436926446, 0.9842969034769873, 0.8538920285761827, 0.2224313152218209, 0.01, 0.2369132879314274, 0.1355697202355011]
Training loss = 0.01693012555440267
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79625
Training loss = 0.01657945493857066
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.015037203431129456
step = 2, Training Accuracy: 0.82
Training loss = 0.016879357397556305
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01605510085821152
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.016680428485075633
step = 5, Training Accuracy: 0.78
Training loss = 0.0165615913271904
step = 6, Training Accuracy: 0.78
Training loss = 0.01567691961924235
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.017931673725446066
step = 8, Training Accuracy: 0.77
Training loss = 0.01429504374663035
step = 9, Training Accuracy: 0.83
Training loss = 0.015614062746365865
step = 10, Training Accuracy: 0.78
Training loss = 0.014871726930141448
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01761292025446892
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.015346486866474152
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015983978311220805
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.795
params:  [0.6388857776680377, 0.570317846304774, 0.7521819315074115, 0.2494419159686214, 0.7930419572432851, 0.29280707438474685, 0.6304611678920544, 0.648966651503682, 0.5469917769008101, 0.09089314393631706, 0.758185362122247, 0.9236635546530634, 0.6349727908954067, 0.3267194285998566, 0.2891444861471884, 0.6829535394325223, 0.22009165760584007, 0.15920528940907092, 0.7106853713599516, 0.49735903286435834, 0.3453012476400259, 0.01, 0.4087477263217858, 0.09786389894183561]
Training loss = 0.01676463633775711
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.80625
Training loss = 0.016217858493328095
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01600066731373469
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01762836962938309
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.017605503698190052
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.014924839437007904
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01587961385647456
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.01565435528755188
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.016422530313332875
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015305219391981761
step = 9, Training Accuracy: 0.79
Training loss = 0.015944442053635915
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.017096546391646067
step = 11, Training Accuracy: 0.78
Training loss = 0.01646156926949819
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.016135197977224985
step = 13, Training Accuracy: 0.77
Training loss = 0.01596165935198466
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.80625
[[0.5250875656333833, 0.49546191426270325, 0.806215472085073, 0.4573656631420633, 0.667756393954537, 0.01, 0.8400638649491307, 0.6090039525609088, 0.8238248702813233, 0.3910513412197445, 0.577287191582645, 0.5500771591033791, 0.5797226613599957, 0.21623439292985466, 0.5304330478831626, 0.8200424422375826, 0.3842385266271649, 0.255516461183866, 0.9726894951439348, 0.6619349369395799, 0.01, 0.01, 0.27626201112356275, 0.14072309235367622], [0.3977452549203779, 0.4405550614880822, 0.9233704627874154, 0.44810307740485084, 0.5993654223715271, 0.18849442820337775, 0.762673403550742, 0.507651683449474, 0.533977896463401, 0.298973268262542, 0.45886264234553775, 0.9085724941531511, 0.6849133271115485, 0.16179074929204804, 0.33664570938317206, 0.706543825429906, 0.3383465519179573, 0.26575805635357375, 0.99, 0.6242807358745931, 0.29515533398373806, 0.14511414588264177, 0.11458903360629319, 0.01], [0.5973108550118202, 0.572930462800024, 0.8935692686244533, 0.30924583222086277, 0.5953554594324911, 0.20096919198064728, 0.6079384310400167, 0.7629194381684092, 0.5424420671774501, 0.13732907903444008, 0.6901088906765455, 0.6724775370746674, 0.6676437962724161, 0.13942388494479035, 0.3392114665143755, 0.6938301275757892, 0.3555255595572737, 0.18174965008771254, 0.6659653298041546, 0.5798123630407535, 0.1831824398475494, 0.04349534220453939, 0.21627180574647453, 0.01], [0.518950111630342, 0.5600314651756848, 0.7461739128790252, 0.4343480145861684, 0.3835602280365187, 0.20527138066177092, 0.6574505940705753, 0.8830803256064362, 0.57146577670342, 0.01, 0.6012244762349533, 0.7446287809020946, 0.398023582130604, 0.24616319741315776, 0.5860574342041838, 0.6411950500391257, 0.24796258706420488, 0.3961108340070286, 0.99, 0.6148835669201774, 0.31553604318971534, 0.13990631736530273, 0.3663813936236957, 0.03501228517823493], [0.43843156216436285, 0.6816462140340735, 0.7763354644015552, 0.305356362389251, 0.44919902652326427, 0.01, 0.6650437013669879, 0.6410766781915783, 0.5846599801413181, 0.2503622835272439, 0.703985350498879, 0.9544259544544422, 0.7484466122980769, 0.01, 0.4324829495531812, 0.99, 0.1701661134993175, 0.011993058468775825, 0.5004520887446706, 0.6287874830876394, 0.21792915606874497, 0.01, 0.2489819916038206, 0.06627444520761153], [0.5429480324441822, 0.48330910033553165, 0.7518254316765689, 0.33063154904103814, 0.6890528171420037, 0.10529999522336626, 0.7996024624962562, 0.6788727760663217, 0.44599065411029326, 0.20640442896616926, 0.5932071043351673, 0.8223601794137649, 0.6707577385282282, 0.09888795258952734, 0.5831421968153014, 0.99, 0.21043294097691884, 0.0971581530733095, 0.8551765670575108, 0.4954280023756983, 0.2253199244998037, 0.01, 0.33062238578695735, 0.24373076668113414], [0.4790147780721955, 0.73015279162329, 0.743133704495912, 0.38003198783368614, 0.7015631689250451, 0.1471136701855157, 0.8084092110704254, 0.8038211144003864, 0.9387886468049857, 0.3683073977048301, 0.5419956938631864, 0.7638989153959272, 0.4455849578282353, 0.10870828077283602, 0.5020153778859946, 0.9668329678243981, 0.3643418700780109, 0.0847147436926446, 0.9842969034769873, 0.8538920285761827, 0.2224313152218209, 0.01, 0.2369132879314274, 0.1355697202355011], [0.6388857776680377, 0.570317846304774, 0.7521819315074115, 0.2494419159686214, 0.7930419572432851, 0.29280707438474685, 0.6304611678920544, 0.648966651503682, 0.5469917769008101, 0.09089314393631706, 0.758185362122247, 0.9236635546530634, 0.6349727908954067, 0.3267194285998566, 0.2891444861471884, 0.6829535394325223, 0.22009165760584007, 0.15920528940907092, 0.7106853713599516, 0.49735903286435834, 0.3453012476400259, 0.01, 0.4087477263217858, 0.09786389894183561]]
22 	8     	0.801406	0.00439449	0.795  	0.80625
params:  [0.5619716993949446, 0.59776689696876, 0.6970866710032205, 0.06840118641394677, 0.8341659689298446, 0.01, 0.9726286125662659, 0.6218854514049167, 0.5451345154798074, 0.1535773818157031, 0.507840886728668, 0.9587887841113326, 0.5456728702459025, 0.20204584384396154, 0.4450054894891596, 0.9632036636635706, 0.31440290890168116, 0.29838930310591905, 0.8768889136668383, 0.48023178139258055, 0.5354688154685403, 0.26851178729225395, 0.034993598535559944, 0.01]
Training loss = 0.017736042737960814
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.8
Training loss = 0.014332694709300995
step = 1, Training Accuracy: 0.82
Training loss = 0.013850540320078531
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.016135540505250293
step = 3, Training Accuracy: 0.78
Training loss = 0.014796098669370015
step = 4, Training Accuracy: 0.79
Training loss = 0.016415534019470216
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.0162908673286438
step = 6, Training Accuracy: 0.78
Training loss = 0.01627824087937673
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.01633907437324524
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.015799776017665863
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01568080812692642
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.014933716257413228
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.015192986329396566
step = 12, Training Accuracy: 0.79
Training loss = 0.01481297900279363
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01583699882030487
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.80625
params:  [0.48018337604480166, 0.4485100589258682, 0.890478901645139, 0.263489246639957, 0.8631415209084612, 0.1766228581840917, 0.7562391481656315, 0.864203155016616, 0.48689951973439066, 0.2880149225768506, 0.7480438960216709, 0.8007672273656361, 0.7208283908099137, 0.14827798954859048, 0.1048099200951696, 0.5235498180272623, 0.46427699974462067, 0.12614699150942227, 0.7076758794781979, 0.5917262395073469, 0.35664836125095073, 0.01, 0.2853182395522182, 0.23899148029024325]
Training loss = 0.01564882347981135
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.8075
Training loss = 0.015244485636552174
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.0157428702712059
step = 2, Training Accuracy: 0.79
Training loss = 0.013580987354119618
step = 3, Training Accuracy: 0.81
Training loss = 0.01415692776441574
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.015675397018591563
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.015914077957471212
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01690714677174886
step = 7, Training Accuracy: 0.75
Training loss = 0.015699085394541422
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.015436247885227204
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.01448501542210579
step = 10, Training Accuracy: 0.78
Training loss = 0.015643731852372486
step = 11, Training Accuracy: 0.78
Training loss = 0.013394616295893986
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.015164198676745096
step = 13, Training Accuracy: 0.8
Training loss = 0.01370147337516149
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.805
params:  [0.4252196828296, 0.43570697379261586, 0.8262216293604823, 0.4244501155693491, 0.7875499523740435, 0.26117295249099376, 0.8805763173477117, 0.6138954508210192, 0.5573523395839838, 0.08058500316837972, 0.4804914073195517, 0.9626470838441302, 0.4230158652840945, 0.18201968820380987, 0.30430850580179775, 0.5788577143010141, 0.25304368805369026, 0.01, 0.99, 0.47545975544042474, 0.3483680621676245, 0.01, 0.16995872304901513, 0.01]
Training loss = 0.015898878971735638
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.81125
Training loss = 0.017900009552637736
step = 1, Training Accuracy: 0.79
Training loss = 0.01572668880224228
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.01566889613866806
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.01629806329806646
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.015418867568174998
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01733699788649877
step = 6, Training Accuracy: 0.77
Training loss = 0.014373362561066945
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.016369643012682598
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.015180782576402029
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.015216045578320821
step = 10, Training Accuracy: 0.81
Training loss = 0.014345590273539226
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.016769290566444398
step = 12, Training Accuracy: 0.81
Training loss = 0.015561992128690083
step = 13, Training Accuracy: 0.84
Training loss = 0.015284520983695983
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.80875
params:  [0.47266387946032734, 0.5226421932841715, 0.9203912680286425, 0.49512790062320916, 0.6334466623723968, 0.3453857753230391, 0.7306276212362736, 0.6334762005514193, 0.6194193397109101, 0.38082723310545785, 0.5310273580338506, 0.7387052049575441, 0.5857237873082355, 0.11540086412944531, 0.3632116391487418, 0.6869299452802621, 0.34980394833736583, 0.21219141615556866, 0.8051856266822832, 0.21238211988589506, 0.06491858351464685, 0.13010563748961962, 0.3675447722671383, 0.08950826550514285]
Training loss = 0.0148686483502388
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.80875
Training loss = 0.014663320183753967
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.01519580751657486
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.01435629626115163
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01374542385339737
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.014368885954221089
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.014144446353117625
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.013282071352005004
step = 7, Training Accuracy: 0.84
Training loss = 0.015225052038828532
step = 8, Training Accuracy: 0.82
Training loss = 0.014982162316640218
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.013985575437545776
step = 10, Training Accuracy: 0.8433333333333334
Training loss = 0.013724884589513143
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.012532231559356054
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.013694360355536143
step = 13, Training Accuracy: 0.8533333333333334
Training loss = 0.013556447128454845
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.80625
params:  [0.5268754327625916, 0.37043257394092666, 0.562226443980678, 0.323306429998219, 0.6472483127826105, 0.30233099189215284, 0.6535433347220782, 0.6140284686974248, 0.6534451154569585, 0.35919195210293686, 0.7970810756585986, 0.9371976251556485, 0.3151970392073176, 0.18134911113527682, 0.5888357973961877, 0.5698732990450717, 0.29807390852119664, 0.16640853310891332, 0.7600805215907191, 0.46604323741646014, 0.13409128457334596, 0.07309555421438753, 0.0333157655532165, 0.2427718318099692]
Training loss = 0.014664131353298823
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.8025
Training loss = 0.013726827005545299
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.015067060937484106
step = 2, Training Accuracy: 0.82
Training loss = 0.01583954652150472
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015551132261753082
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.015622194707393646
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014513414353132248
step = 6, Training Accuracy: 0.81
Training loss = 0.014884086151917775
step = 7, Training Accuracy: 0.82
Training loss = 0.01453323985139529
step = 8, Training Accuracy: 0.79
Training loss = 0.014519588202238082
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.014476304948329926
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01584337552388509
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014455868601799012
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.01550233523050944
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.014985416928927103
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.80375
params:  [0.6110678486805645, 0.5188741867505586, 0.7844910158651928, 0.37045457588473474, 0.5360136820549599, 0.20825374216944184, 0.5991668140281274, 0.6168469567747665, 0.4872636726215927, 0.10074626074000477, 0.7731494556159396, 0.9864333805034985, 0.6305651108348191, 0.14454169000497585, 0.12760524475202095, 0.7328966378409297, 0.08834980020835495, 0.3683404106813175, 0.9273888063270247, 0.37246401840809124, 0.49029623448889825, 0.14845284406995984, 0.40525404144541044, 0.14213838744822754]
Training loss = 0.015711244444052377
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.8025
Training loss = 0.015455065369606018
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.016425000727176665
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.01836512198050817
step = 3, Training Accuracy: 0.76
Training loss = 0.017371875544389088
step = 4, Training Accuracy: 0.77
Training loss = 0.015724378923575085
step = 5, Training Accuracy: 0.76
Training loss = 0.016165282453099887
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016592407325903575
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016656756599744162
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01828184594710668
step = 9, Training Accuracy: 0.76
Training loss = 0.016961462795734406
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.0156659397482872
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.016899555921554565
step = 12, Training Accuracy: 0.78
Training loss = 0.01604125142097473
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.017637671132882435
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.80125
params:  [0.46854148580297955, 0.581812127090922, 0.9083350838466758, 0.37270702946056683, 0.6980068730384137, 0.27684472865874504, 0.7683009726094953, 0.6015677045031096, 0.4552374281149687, 0.2964715643999965, 0.5500619726476819, 0.7922687302453209, 0.6609506421747738, 0.12014912120357432, 0.35510975142664786, 0.5831799986394787, 0.23977700986248998, 0.3114743393727369, 0.6263908271013698, 0.6688481879704706, 0.3421328505887289, 0.1094130396059341, 0.33217002309416155, 0.01]
Training loss = 0.01554827610651652
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.805
Training loss = 0.015491108496983846
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01846522092819214
step = 2, Training Accuracy: 0.81
Training loss = 0.013682962010304132
step = 3, Training Accuracy: 0.82
Training loss = 0.014015806118647257
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.016024676660696666
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01520149533947309
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015170472065607706
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.014232328832149506
step = 8, Training Accuracy: 0.81
Training loss = 0.013303014685710272
step = 9, Training Accuracy: 0.81
Training loss = 0.015128327012062072
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.014118682344754536
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.014690665105978648
step = 12, Training Accuracy: 0.8
Training loss = 0.014739324450492858
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014683475991090138
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.81
params:  [0.49670618007220285, 0.3610097179505434, 0.8625632155661095, 0.4987406981515509, 0.6415642945201505, 0.2762405757284097, 0.7937948462077544, 0.7294030350834406, 0.47073776201725176, 0.09024096527939315, 0.5921458996974693, 0.621881268437158, 0.6930723666504507, 0.01206459921339928, 0.39502665104692813, 0.7670225740581494, 0.2060442771166348, 0.16165843877898872, 0.99, 0.6606440332937161, 0.1545182754337855, 0.1928025204500004, 0.3015203946592447, 0.01]
Training loss = 0.01521179308493932
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.80125
Training loss = 0.014177955488363902
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014000908136367798
step = 2, Training Accuracy: 0.84
Training loss = 0.01594059576590856
step = 3, Training Accuracy: 0.79
Training loss = 0.015486560463905334
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01460581123828888
step = 5, Training Accuracy: 0.8
Training loss = 0.013818986713886261
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.014191719591617585
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015131247440973918
step = 8, Training Accuracy: 0.79
Training loss = 0.014068638483683268
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015182886918385824
step = 10, Training Accuracy: 0.8
Training loss = 0.014815330505371094
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.013369443068901699
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.014577954610188803
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015492930908997853
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.7975
[[0.5619716993949446, 0.59776689696876, 0.6970866710032205, 0.06840118641394677, 0.8341659689298446, 0.01, 0.9726286125662659, 0.6218854514049167, 0.5451345154798074, 0.1535773818157031, 0.507840886728668, 0.9587887841113326, 0.5456728702459025, 0.20204584384396154, 0.4450054894891596, 0.9632036636635706, 0.31440290890168116, 0.29838930310591905, 0.8768889136668383, 0.48023178139258055, 0.5354688154685403, 0.26851178729225395, 0.034993598535559944, 0.01], [0.48018337604480166, 0.4485100589258682, 0.890478901645139, 0.263489246639957, 0.8631415209084612, 0.1766228581840917, 0.7562391481656315, 0.864203155016616, 0.48689951973439066, 0.2880149225768506, 0.7480438960216709, 0.8007672273656361, 0.7208283908099137, 0.14827798954859048, 0.1048099200951696, 0.5235498180272623, 0.46427699974462067, 0.12614699150942227, 0.7076758794781979, 0.5917262395073469, 0.35664836125095073, 0.01, 0.2853182395522182, 0.23899148029024325], [0.4252196828296, 0.43570697379261586, 0.8262216293604823, 0.4244501155693491, 0.7875499523740435, 0.26117295249099376, 0.8805763173477117, 0.6138954508210192, 0.5573523395839838, 0.08058500316837972, 0.4804914073195517, 0.9626470838441302, 0.4230158652840945, 0.18201968820380987, 0.30430850580179775, 0.5788577143010141, 0.25304368805369026, 0.01, 0.99, 0.47545975544042474, 0.3483680621676245, 0.01, 0.16995872304901513, 0.01], [0.47266387946032734, 0.5226421932841715, 0.9203912680286425, 0.49512790062320916, 0.6334466623723968, 0.3453857753230391, 0.7306276212362736, 0.6334762005514193, 0.6194193397109101, 0.38082723310545785, 0.5310273580338506, 0.7387052049575441, 0.5857237873082355, 0.11540086412944531, 0.3632116391487418, 0.6869299452802621, 0.34980394833736583, 0.21219141615556866, 0.8051856266822832, 0.21238211988589506, 0.06491858351464685, 0.13010563748961962, 0.3675447722671383, 0.08950826550514285], [0.5268754327625916, 0.37043257394092666, 0.562226443980678, 0.323306429998219, 0.6472483127826105, 0.30233099189215284, 0.6535433347220782, 0.6140284686974248, 0.6534451154569585, 0.35919195210293686, 0.7970810756585986, 0.9371976251556485, 0.3151970392073176, 0.18134911113527682, 0.5888357973961877, 0.5698732990450717, 0.29807390852119664, 0.16640853310891332, 0.7600805215907191, 0.46604323741646014, 0.13409128457334596, 0.07309555421438753, 0.0333157655532165, 0.2427718318099692], [0.6110678486805645, 0.5188741867505586, 0.7844910158651928, 0.37045457588473474, 0.5360136820549599, 0.20825374216944184, 0.5991668140281274, 0.6168469567747665, 0.4872636726215927, 0.10074626074000477, 0.7731494556159396, 0.9864333805034985, 0.6305651108348191, 0.14454169000497585, 0.12760524475202095, 0.7328966378409297, 0.08834980020835495, 0.3683404106813175, 0.9273888063270247, 0.37246401840809124, 0.49029623448889825, 0.14845284406995984, 0.40525404144541044, 0.14213838744822754], [0.46854148580297955, 0.581812127090922, 0.9083350838466758, 0.37270702946056683, 0.6980068730384137, 0.27684472865874504, 0.7683009726094953, 0.6015677045031096, 0.4552374281149687, 0.2964715643999965, 0.5500619726476819, 0.7922687302453209, 0.6609506421747738, 0.12014912120357432, 0.35510975142664786, 0.5831799986394787, 0.23977700986248998, 0.3114743393727369, 0.6263908271013698, 0.6688481879704706, 0.3421328505887289, 0.1094130396059341, 0.33217002309416155, 0.01], [0.49670618007220285, 0.3610097179505434, 0.8625632155661095, 0.4987406981515509, 0.6415642945201505, 0.2762405757284097, 0.7937948462077544, 0.7294030350834406, 0.47073776201725176, 0.09024096527939315, 0.5921458996974693, 0.621881268437158, 0.6930723666504507, 0.01206459921339928, 0.39502665104692813, 0.7670225740581494, 0.2060442771166348, 0.16165843877898872, 0.99, 0.6606440332937161, 0.1545182754337855, 0.1928025204500004, 0.3015203946592447, 0.01]]
23 	8     	0.804844	0.00377272	0.7975 	0.81   
params:  [0.34537601152677044, 0.5565267711679861, 0.9830630856436987, 0.34807968198677053, 0.7928263469946804, 0.5095429808434514, 0.9230453648147532, 0.3931560441550355, 0.36629369591111094, 0.25909666235717255, 0.5184638621064723, 0.99, 0.8179779280356572, 0.1979763124115156, 0.5326878092662133, 0.99, 0.1503878860153232, 0.32215396842409016, 0.624510990902415, 0.4203040882686694, 0.19743029953420113, 0.100468379226322, 0.09721155339361642, 0.01]
Training loss = 0.015970720251401265
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.80625
Training loss = 0.01702067956328392
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.014525051414966583
step = 2, Training Accuracy: 0.8
Training loss = 0.017595602869987487
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.017689503331979117
step = 4, Training Accuracy: 0.75
Training loss = 0.014780238370100658
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.01515181064605713
step = 6, Training Accuracy: 0.81
Training loss = 0.015081945061683654
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.016100836396217347
step = 8, Training Accuracy: 0.78
Training loss = 0.017260459462801615
step = 9, Training Accuracy: 0.81
Training loss = 0.016130958795547486
step = 10, Training Accuracy: 0.78
Training loss = 0.01687531848748525
step = 11, Training Accuracy: 0.78
Training loss = 0.014109693765640259
step = 12, Training Accuracy: 0.81
Training loss = 0.015221847792466481
step = 13, Training Accuracy: 0.8
Training loss = 0.014081015288829803
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.8
params:  [0.3474503469614977, 0.7073810769857943, 0.7299570801977117, 0.2818481545194878, 0.7017863570431334, 0.19366776252345574, 0.7539608097105185, 0.35305371661768054, 0.2617386052991659, 0.3809627716518079, 0.5456512033306252, 0.7720607569415255, 0.6014060311824135, 0.2246129283328258, 0.5547620595513598, 0.6665614513226031, 0.13188838669875608, 0.15367063666540975, 0.5238446127079556, 0.37928301513063944, 0.22069078440072318, 0.04673129260040493, 0.17491588747607506, 0.18235901886284817]
Training loss = 0.015624833405017852
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.80625
Training loss = 0.016331096986929576
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.016368201673030852
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.015406020085016887
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01564954991141955
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014175195594628652
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.01578524589538574
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.014523014922936757
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.013856350481510162
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.01515643964211146
step = 9, Training Accuracy: 0.82
Training loss = 0.01584959199031194
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.0184469473361969
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.015121480921904245
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014913614690303802
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.015139705538749694
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79875
params:  [0.42875054608979635, 0.5790351437105025, 0.7718851767334053, 0.1557744282325116, 0.84020719725607, 0.31124564493253404, 0.8112188903991572, 0.631177229705098, 0.47714062711110977, 0.14687559013676343, 0.5596063831400956, 0.880791176495702, 0.7278854062786655, 0.06958841589021715, 0.4867498799493932, 0.6654821950301287, 0.3630497320454359, 0.24335883789152307, 0.8003659100554045, 0.7272616893860471, 0.5336404993874393, 0.037098165235581515, 0.2078748983534235, 0.01]
Training loss = 0.01682713657617569
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.81
Training loss = 0.015949338972568512
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.016322568158308664
step = 2, Training Accuracy: 0.77
Training loss = 0.016020791629950206
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.016808112859725954
step = 4, Training Accuracy: 0.78
Training loss = 0.0155937393506368
step = 5, Training Accuracy: 0.78
Training loss = 0.01609013875325521
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.015871818164984387
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015634612242380778
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.0164974245429039
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015946591993172964
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.016447824935118358
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.014107677439848582
step = 12, Training Accuracy: 0.79
Training loss = 0.01629674941301346
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01615137279033661
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.80375
params:  [0.4699790065903623, 0.532409016872714, 0.7547418138755893, 0.5429568430820796, 0.9424265270938028, 0.4400568271544285, 0.8673502290259527, 0.8843754430334052, 0.5486169470545263, 0.1922416448610928, 0.5834267479091876, 0.7527623692366033, 0.45836582910119245, 0.10625716662690399, 0.29732550040717465, 0.5829268486841169, 0.25181244190016705, 0.24857196595912354, 0.4375632204641797, 0.35746848059207426, 0.2617364099254899, 0.12290338990575363, 0.02148884591508124, 0.01]
Training loss = 0.013806328972180685
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.8025
Training loss = 0.014297788341840109
step = 1, Training Accuracy: 0.84
Training loss = 0.013011231621106466
step = 2, Training Accuracy: 0.84
Training loss = 0.013801888724168142
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.013408137212196986
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.014159530103206634
step = 5, Training Accuracy: 0.82
Training loss = 0.013901182015736898
step = 6, Training Accuracy: 0.83
Training loss = 0.012792279918988545
step = 7, Training Accuracy: 0.82
Training loss = 0.01276487519343694
step = 8, Training Accuracy: 0.8333333333333334
Training loss = 0.01216598962744077
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.014158184826374053
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.013100531299908956
step = 11, Training Accuracy: 0.85
Training loss = 0.014864436189333597
step = 12, Training Accuracy: 0.83
Training loss = 0.013889485647281012
step = 13, Training Accuracy: 0.82
Training loss = 0.012825864255428315
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.8
params:  [0.27157792635479516, 0.5932447508614542, 0.7243640066699744, 0.40025367161091835, 0.8985839377067516, 0.2168698745416679, 0.9006064043977299, 0.7137651539058065, 0.5006630608949809, 0.23800567906396944, 0.4658799362119492, 0.8861143128658041, 0.6367504157706008, 0.05077375911834457, 0.30840674025804143, 0.48165500807482475, 0.33489980742157777, 0.27053275820990086, 0.8244674807766641, 0.35590298183564306, 0.25777124123701645, 0.2010983558847913, 0.14109267128418856, 0.01]
Training loss = 0.015896613399187725
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.80375
Training loss = 0.015274131298065185
step = 1, Training Accuracy: 0.79
Training loss = 0.016691076656182607
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.015433314343293508
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.015341921150684357
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.0158019491036733
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.016799980700016023
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.014524003167947134
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.017479479114214578
step = 8, Training Accuracy: 0.78
Training loss = 0.014888921280701956
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.016487250030040743
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.015149226288000743
step = 11, Training Accuracy: 0.82
Training loss = 0.015726312150557836
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.014502127567927042
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.015559416611989339
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.80375
params:  [0.6332902975199849, 0.5122320270906148, 0.9255659421724054, 0.3270692289528931, 0.6481039247224254, 0.5318744632168337, 0.7827411154237438, 0.4756869065053026, 0.2436152928360249, 0.2810813476573709, 0.7054716862192577, 0.9112952963454894, 0.6167836018448931, 0.19808824420571047, 0.34169308215634, 0.5400084396691296, 0.15071206529005313, 0.1852583421916436, 0.5058682275266346, 0.4613817280007604, 0.35824412198247135, 0.05981646109929102, 0.3612522293601446, 0.1502285519706117]
Training loss = 0.014394631584485371
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.8025
Training loss = 0.016458784540494282
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.015974470774332682
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.015287187298138936
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.016831736962000528
step = 4, Training Accuracy: 0.78
Training loss = 0.015779570241769156
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014885085026423136
step = 6, Training Accuracy: 0.8
Training loss = 0.015449901819229126
step = 7, Training Accuracy: 0.79
Training loss = 0.01674839546283086
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.014694125056266785
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.014336796303590138
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.016985961298147837
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01636270264784495
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.015704400042692822
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015093809266885121
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.80375
params:  [0.4414512166582685, 0.34800282621306905, 0.8078772517583768, 0.4198960906183398, 0.7326841922156178, 0.13640487872498186, 0.99, 0.6711915636693166, 0.6182376820745701, 0.18159958276235083, 0.4687500781575323, 0.9147044915234471, 0.5380639982714006, 0.01, 0.0971753776924239, 0.6170390547328931, 0.3190467558257764, 0.01, 0.7966844670004251, 0.7571835909919122, 0.35838739925065255, 0.3597350771894347, 0.07432755336680691, 0.01]
Training loss = 0.01659647434949875
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.8025
Training loss = 0.01562023679415385
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.0165983651081721
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.017597064673900605
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.01652681291103363
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.015028694321711859
step = 5, Training Accuracy: 0.79
Training loss = 0.015608725249767303
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.016855801741282147
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.015615194042523702
step = 8, Training Accuracy: 0.8
Training loss = 0.017918373147646587
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.01548376699288686
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.015186182955900828
step = 11, Training Accuracy: 0.79
Training loss = 0.015403140187263489
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.0142161296804746
step = 13, Training Accuracy: 0.8
Training loss = 0.0144975795596838
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.80375
params:  [0.5048291081803143, 0.4101121712186249, 0.99, 0.34239015963849595, 0.8061035046026045, 0.32050485922386496, 0.7916670762174349, 0.48080213329756605, 0.5526036228021308, 0.3182322117357089, 0.3485605628430395, 0.6732199381303402, 0.7366055992488734, 0.2511901126762551, 0.32907766340905, 0.4973650398039431, 0.21337683958082393, 0.32888880545982724, 0.9758264485245932, 0.5731688403025355, 0.35219123713611294, 0.01, 0.09372475106136668, 0.01]
Training loss = 0.014070288240909577
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.80125
Training loss = 0.014046628773212434
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.015442250470320384
step = 2, Training Accuracy: 0.78
Training loss = 0.013878457049528758
step = 3, Training Accuracy: 0.84
Training loss = 0.014040800829728445
step = 4, Training Accuracy: 0.82
Training loss = 0.013626823524634043
step = 5, Training Accuracy: 0.8433333333333334
Training loss = 0.016034617225329083
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.011226586004098257
step = 7, Training Accuracy: 0.87
Training loss = 0.014391812384128571
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.013520681063334147
step = 9, Training Accuracy: 0.8566666666666667
Training loss = 0.01314335068066915
step = 10, Training Accuracy: 0.83
Training loss = 0.013326016664505004
step = 11, Training Accuracy: 0.83
Training loss = 0.013545758028825124
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.012563515702883403
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.012732873310645421
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.8025
[[0.34537601152677044, 0.5565267711679861, 0.9830630856436987, 0.34807968198677053, 0.7928263469946804, 0.5095429808434514, 0.9230453648147532, 0.3931560441550355, 0.36629369591111094, 0.25909666235717255, 0.5184638621064723, 0.99, 0.8179779280356572, 0.1979763124115156, 0.5326878092662133, 0.99, 0.1503878860153232, 0.32215396842409016, 0.624510990902415, 0.4203040882686694, 0.19743029953420113, 0.100468379226322, 0.09721155339361642, 0.01], [0.3474503469614977, 0.7073810769857943, 0.7299570801977117, 0.2818481545194878, 0.7017863570431334, 0.19366776252345574, 0.7539608097105185, 0.35305371661768054, 0.2617386052991659, 0.3809627716518079, 0.5456512033306252, 0.7720607569415255, 0.6014060311824135, 0.2246129283328258, 0.5547620595513598, 0.6665614513226031, 0.13188838669875608, 0.15367063666540975, 0.5238446127079556, 0.37928301513063944, 0.22069078440072318, 0.04673129260040493, 0.17491588747607506, 0.18235901886284817], [0.42875054608979635, 0.5790351437105025, 0.7718851767334053, 0.1557744282325116, 0.84020719725607, 0.31124564493253404, 0.8112188903991572, 0.631177229705098, 0.47714062711110977, 0.14687559013676343, 0.5596063831400956, 0.880791176495702, 0.7278854062786655, 0.06958841589021715, 0.4867498799493932, 0.6654821950301287, 0.3630497320454359, 0.24335883789152307, 0.8003659100554045, 0.7272616893860471, 0.5336404993874393, 0.037098165235581515, 0.2078748983534235, 0.01], [0.4699790065903623, 0.532409016872714, 0.7547418138755893, 0.5429568430820796, 0.9424265270938028, 0.4400568271544285, 0.8673502290259527, 0.8843754430334052, 0.5486169470545263, 0.1922416448610928, 0.5834267479091876, 0.7527623692366033, 0.45836582910119245, 0.10625716662690399, 0.29732550040717465, 0.5829268486841169, 0.25181244190016705, 0.24857196595912354, 0.4375632204641797, 0.35746848059207426, 0.2617364099254899, 0.12290338990575363, 0.02148884591508124, 0.01], [0.27157792635479516, 0.5932447508614542, 0.7243640066699744, 0.40025367161091835, 0.8985839377067516, 0.2168698745416679, 0.9006064043977299, 0.7137651539058065, 0.5006630608949809, 0.23800567906396944, 0.4658799362119492, 0.8861143128658041, 0.6367504157706008, 0.05077375911834457, 0.30840674025804143, 0.48165500807482475, 0.33489980742157777, 0.27053275820990086, 0.8244674807766641, 0.35590298183564306, 0.25777124123701645, 0.2010983558847913, 0.14109267128418856, 0.01], [0.6332902975199849, 0.5122320270906148, 0.9255659421724054, 0.3270692289528931, 0.6481039247224254, 0.5318744632168337, 0.7827411154237438, 0.4756869065053026, 0.2436152928360249, 0.2810813476573709, 0.7054716862192577, 0.9112952963454894, 0.6167836018448931, 0.19808824420571047, 0.34169308215634, 0.5400084396691296, 0.15071206529005313, 0.1852583421916436, 0.5058682275266346, 0.4613817280007604, 0.35824412198247135, 0.05981646109929102, 0.3612522293601446, 0.1502285519706117], [0.4414512166582685, 0.34800282621306905, 0.8078772517583768, 0.4198960906183398, 0.7326841922156178, 0.13640487872498186, 0.99, 0.6711915636693166, 0.6182376820745701, 0.18159958276235083, 0.4687500781575323, 0.9147044915234471, 0.5380639982714006, 0.01, 0.0971753776924239, 0.6170390547328931, 0.3190467558257764, 0.01, 0.7966844670004251, 0.7571835909919122, 0.35838739925065255, 0.3597350771894347, 0.07432755336680691, 0.01], [0.5048291081803143, 0.4101121712186249, 0.99, 0.34239015963849595, 0.8061035046026045, 0.32050485922386496, 0.7916670762174349, 0.48080213329756605, 0.5526036228021308, 0.3182322117357089, 0.3485605628430395, 0.6732199381303402, 0.7366055992488734, 0.2511901126762551, 0.32907766340905, 0.4973650398039431, 0.21337683958082393, 0.32888880545982724, 0.9758264485245932, 0.5731688403025355, 0.35219123713611294, 0.01, 0.09372475106136668, 0.01]]
24 	8     	0.802031	0.00197024	0.79875	0.80375
params:  [0.6039077440868573, 0.4540766030244845, 0.8018083477667521, 0.39348342511938084, 0.8382383792431006, 0.43292479836957887, 0.9016933359670858, 0.7122191848480499, 0.5901444778960827, 0.24751952785944245, 0.7304114644313973, 0.7690543582338127, 0.7455998394264616, 0.22136177035265323, 0.5220206131569803, 0.46144413372097215, 0.16059924200351247, 0.28859295171677096, 0.99, 0.48856232120072735, 0.3698891533900361, 0.07190407946592144, 0.27509313464062424, 0.01]
Training loss = 0.015279568334420523
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.80375
Training loss = 0.014743587573369344
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.014837671021620433
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.014664872090021768
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.013489025682210921
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.014758273363113403
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.012688075602054595
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.01576178828875224
step = 7, Training Accuracy: 0.82
Training loss = 0.01418199489514033
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.014264893333117167
step = 9, Training Accuracy: 0.82
Training loss = 0.015188773473103842
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.014889380633831025
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.013616052369276682
step = 12, Training Accuracy: 0.79
Training loss = 0.015705796877543133
step = 13, Training Accuracy: 0.82
Training loss = 0.013161458174387615
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.80375
params:  [0.3455351515669245, 0.5879079624220558, 0.8628302574874771, 0.367530773504815, 0.7331500623697284, 0.19705142248350696, 0.9190555532173463, 0.7580840305261975, 0.3104075687042932, 0.019839622889442965, 0.49366282843575937, 0.99, 0.6422351114720286, 0.01, 0.644940737655063, 0.6584369597202799, 0.3084052346996934, 0.1461564908576768, 0.8827809675174059, 0.5769696286811853, 0.4863350797406699, 0.23800811217886297, 0.20759255734247412, 0.12051594839502769]
Training loss = 0.016116387446721395
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.8075
Training loss = 0.0149508668979009
step = 1, Training Accuracy: 0.77
Training loss = 0.01911855419476827
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01729125251372655
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016072707176208498
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016043382932742437
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.016452421645323435
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.01516942838827769
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.014837194879849752
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01637096772591273
step = 9, Training Accuracy: 0.79
Training loss = 0.017522677977879843
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015217246115207672
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.015305761694908143
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014717460771401724
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.015556824008623758
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.80625
params:  [0.37892177780932457, 0.6403005608642547, 0.5318379837622337, 0.20030643355640126, 0.7730253612110268, 0.2837349929300425, 0.99, 0.7073195388070522, 0.5368478600077553, 0.37177239608401774, 0.6779901788240115, 0.6926646220253504, 0.7194741549441417, 0.3252939475172283, 0.6017007445637863, 0.44773336187785795, 0.4692364806148266, 0.37849905377021176, 0.7770529062571327, 0.44071711924197016, 0.5570470085673888, 0.01, 0.18342397253797169, 0.1704093164662338]
Training loss = 0.013647335122028986
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.8025
Training loss = 0.01601866583029429
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.012910517503817876
step = 2, Training Accuracy: 0.86
Training loss = 0.012522284388542176
step = 3, Training Accuracy: 0.8466666666666667
Training loss = 0.014288474172353745
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.015940930247306823
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.013537019242842991
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.014368834992249807
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.013341699540615082
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.014726584553718567
step = 9, Training Accuracy: 0.81
Training loss = 0.013323324223359426
step = 10, Training Accuracy: 0.8333333333333334
Training loss = 0.014141045411427815
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.015068281094233195
step = 12, Training Accuracy: 0.8
Training loss = 0.014436513384183247
step = 13, Training Accuracy: 0.8
Training loss = 0.014500089784463247
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.80125
params:  [0.41427998801190197, 0.5783771926940919, 0.7435429162796949, 0.14231223872476756, 0.769498314304904, 0.5779369661761422, 0.9851688728442127, 0.6688748384901005, 0.4740023257756589, 0.01, 0.381983697361767, 0.9471753766578759, 0.9278595682492565, 0.1521734193902861, 0.43369904380988705, 0.5440805543054537, 0.4800897036326981, 0.11991420396006428, 0.864563437896169, 0.6680584612291531, 0.46925340269761784, 0.3204770682232376, 0.39513371783745077, 0.01]
Training loss = 0.016478195389111838
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79625
Training loss = 0.014436675012111664
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.015480507413546245
step = 2, Training Accuracy: 0.8
Training loss = 0.01783203254143397
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016806179483731587
step = 4, Training Accuracy: 0.76
Training loss = 0.01726951852440834
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.016822718779246012
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.020855978429317475
step = 7, Training Accuracy: 0.75
Training loss = 0.01557069182395935
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.01851317286491394
step = 9, Training Accuracy: 0.75
Training loss = 0.016465519269307453
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.016189561386903126
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.01863963782787323
step = 12, Training Accuracy: 0.74
Training loss = 0.015588028232256572
step = 13, Training Accuracy: 0.81
Training loss = 0.017018495202064512
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.80625
params:  [0.1742502876933444, 0.7956073853417805, 0.5410244229648726, 0.34740935010225754, 0.8189062422385134, 0.3096009660941347, 0.8901227885538154, 0.7440365613736839, 0.4778447176754591, 0.26176898809700916, 0.8161166654239211, 0.9256242524934577, 0.740366484729125, 0.042283469676970116, 0.4787935437483777, 0.5211768715749997, 0.4258013911971772, 0.38155095117090154, 0.6864108116071579, 0.4239102627376022, 0.31646898261337203, 0.09963604575022388, 0.27077844645913646, 0.1912212381693671]
Training loss = 0.015614517529805501
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.80625
Training loss = 0.017591285904248556
step = 1, Training Accuracy: 0.78
Training loss = 0.015981892943382262
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01641502633690834
step = 3, Training Accuracy: 0.79
Training loss = 0.01878262629111608
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015720640122890473
step = 5, Training Accuracy: 0.81
Training loss = 0.017643485367298126
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.015730863412221272
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.017188068528970084
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.017708622018496194
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.016035901308059694
step = 10, Training Accuracy: 0.78
Training loss = 0.016709819535414377
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.016198686957359314
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01664889484643936
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.018523328999678294
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79625
params:  [0.47618865787989123, 0.33939232699001637, 0.8260685208192073, 0.32978711366024527, 0.7126735131757431, 0.20555653159807058, 0.6581374075267276, 0.5117509676430427, 0.3550659045922706, 0.2655809411753859, 0.6005495245932685, 0.966958235966395, 0.7310149135185151, 0.05845527066959947, 0.1910688667888578, 0.818255762681038, 0.3638819807261428, 0.16734076325483252, 0.8525435560817368, 0.31931215193614565, 0.46127517454122324, 0.04195048038782412, 0.23689728886328132, 0.01]
Training loss = 0.01410994937022527
step = 0, Training Accuracy: 0.85
Validation Accuracy: 0.805
Training loss = 0.013219379136959713
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.013819236755371094
step = 2, Training Accuracy: 0.84
Training loss = 0.014031325280666352
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.013840556343396504
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.012992588877677918
step = 5, Training Accuracy: 0.8466666666666667
Training loss = 0.013464502841234207
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.016345467219750085
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.013213337312142054
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.013691085775693258
step = 9, Training Accuracy: 0.83
Training loss = 0.013608666360378266
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.012846420506636301
step = 11, Training Accuracy: 0.85
Training loss = 0.012965413381656011
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.01433818057179451
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.012726268743475279
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.80625
params:  [0.5650409487346952, 0.5631395009261064, 0.6050534968926928, 0.290455857285152, 0.99, 0.41993779461566016, 0.8228484337106744, 0.5912383810810506, 0.3094015937138714, 0.23488487351740528, 0.47426567474041964, 0.5975147844600646, 0.5453944002972502, 0.01, 0.44737752545951864, 0.5674371867063124, 0.3984199300960899, 0.4835252421534905, 0.8791197384984252, 0.7310930903421555, 0.23284890847996603, 0.10486816497369267, 0.1804857090824209, 0.040490683975541605]
Training loss = 0.01654362976551056
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.80625
Training loss = 0.015220156808694204
step = 1, Training Accuracy: 0.81
Training loss = 0.015286381344000498
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.01675719747940699
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.01600948433081309
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.018229791124661762
step = 5, Training Accuracy: 0.77
Training loss = 0.016344812015692392
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.01742423782745997
step = 7, Training Accuracy: 0.75
Training loss = 0.0159391122063001
step = 8, Training Accuracy: 0.79
Training loss = 0.017047319809595743
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.016666515866915387
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01761630376180013
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015984026690324147
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.016230830947558086
step = 13, Training Accuracy: 0.8
Training loss = 0.016358604629834492
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.80125
params:  [0.4740362412059441, 0.3790251040626005, 0.7440246899305448, 0.32058973202592317, 0.898724229342968, 0.3713610641012696, 0.8869550226778367, 0.7340379224656339, 0.47123330531111507, 0.23379982070219618, 0.25962266589282584, 0.8612052469514411, 0.8033332809121794, 0.01, 0.2071832381706533, 0.7419990043503805, 0.2716298624585086, 0.2146781784217104, 0.7687205319215373, 0.6254402640603062, 0.4930523658718774, 0.16426838026462845, 0.02805817133712385, 0.01]
Training loss = 0.01397597223520279
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.79875
Training loss = 0.013200008273124696
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.013710830211639404
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.015073308249314627
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.014759492675463358
step = 4, Training Accuracy: 0.82
Training loss = 0.014448133806387584
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.014225084086259205
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.014298440019289652
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.015401025315125784
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015081624488035838
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.013352727939685186
step = 10, Training Accuracy: 0.83
Training loss = 0.014277341862519582
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.013988617658615112
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.013667322893937428
step = 13, Training Accuracy: 0.82
Training loss = 0.01476529061794281
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.795
[[0.6039077440868573, 0.4540766030244845, 0.8018083477667521, 0.39348342511938084, 0.8382383792431006, 0.43292479836957887, 0.9016933359670858, 0.7122191848480499, 0.5901444778960827, 0.24751952785944245, 0.7304114644313973, 0.7690543582338127, 0.7455998394264616, 0.22136177035265323, 0.5220206131569803, 0.46144413372097215, 0.16059924200351247, 0.28859295171677096, 0.99, 0.48856232120072735, 0.3698891533900361, 0.07190407946592144, 0.27509313464062424, 0.01], [0.3455351515669245, 0.5879079624220558, 0.8628302574874771, 0.367530773504815, 0.7331500623697284, 0.19705142248350696, 0.9190555532173463, 0.7580840305261975, 0.3104075687042932, 0.019839622889442965, 0.49366282843575937, 0.99, 0.6422351114720286, 0.01, 0.644940737655063, 0.6584369597202799, 0.3084052346996934, 0.1461564908576768, 0.8827809675174059, 0.5769696286811853, 0.4863350797406699, 0.23800811217886297, 0.20759255734247412, 0.12051594839502769], [0.37892177780932457, 0.6403005608642547, 0.5318379837622337, 0.20030643355640126, 0.7730253612110268, 0.2837349929300425, 0.99, 0.7073195388070522, 0.5368478600077553, 0.37177239608401774, 0.6779901788240115, 0.6926646220253504, 0.7194741549441417, 0.3252939475172283, 0.6017007445637863, 0.44773336187785795, 0.4692364806148266, 0.37849905377021176, 0.7770529062571327, 0.44071711924197016, 0.5570470085673888, 0.01, 0.18342397253797169, 0.1704093164662338], [0.41427998801190197, 0.5783771926940919, 0.7435429162796949, 0.14231223872476756, 0.769498314304904, 0.5779369661761422, 0.9851688728442127, 0.6688748384901005, 0.4740023257756589, 0.01, 0.381983697361767, 0.9471753766578759, 0.9278595682492565, 0.1521734193902861, 0.43369904380988705, 0.5440805543054537, 0.4800897036326981, 0.11991420396006428, 0.864563437896169, 0.6680584612291531, 0.46925340269761784, 0.3204770682232376, 0.39513371783745077, 0.01], [0.1742502876933444, 0.7956073853417805, 0.5410244229648726, 0.34740935010225754, 0.8189062422385134, 0.3096009660941347, 0.8901227885538154, 0.7440365613736839, 0.4778447176754591, 0.26176898809700916, 0.8161166654239211, 0.9256242524934577, 0.740366484729125, 0.042283469676970116, 0.4787935437483777, 0.5211768715749997, 0.4258013911971772, 0.38155095117090154, 0.6864108116071579, 0.4239102627376022, 0.31646898261337203, 0.09963604575022388, 0.27077844645913646, 0.1912212381693671], [0.47618865787989123, 0.33939232699001637, 0.8260685208192073, 0.32978711366024527, 0.7126735131757431, 0.20555653159807058, 0.6581374075267276, 0.5117509676430427, 0.3550659045922706, 0.2655809411753859, 0.6005495245932685, 0.966958235966395, 0.7310149135185151, 0.05845527066959947, 0.1910688667888578, 0.818255762681038, 0.3638819807261428, 0.16734076325483252, 0.8525435560817368, 0.31931215193614565, 0.46127517454122324, 0.04195048038782412, 0.23689728886328132, 0.01], [0.5650409487346952, 0.5631395009261064, 0.6050534968926928, 0.290455857285152, 0.99, 0.41993779461566016, 0.8228484337106744, 0.5912383810810506, 0.3094015937138714, 0.23488487351740528, 0.47426567474041964, 0.5975147844600646, 0.5453944002972502, 0.01, 0.44737752545951864, 0.5674371867063124, 0.3984199300960899, 0.4835252421534905, 0.8791197384984252, 0.7310930903421555, 0.23284890847996603, 0.10486816497369267, 0.1804857090824209, 0.040490683975541605], [0.4740362412059441, 0.3790251040626005, 0.7440246899305448, 0.32058973202592317, 0.898724229342968, 0.3713610641012696, 0.8869550226778367, 0.7340379224656339, 0.47123330531111507, 0.23379982070219618, 0.25962266589282584, 0.8612052469514411, 0.8033332809121794, 0.01, 0.2071832381706533, 0.7419990043503805, 0.2716298624585086, 0.2146781784217104, 0.7687205319215373, 0.6254402640603062, 0.4930523658718774, 0.16426838026462845, 0.02805817133712385, 0.01]]
25 	8     	0.802031	0.00418971	0.795  	0.80625
params:  [0.2718464896810534, 0.47057794373406187, 0.6492748026995223, 0.19407745230604706, 0.5902807222890295, 0.2539542971810923, 0.8148726431492344, 0.6584665123820379, 0.44230798033239327, 0.01, 0.4067470829647848, 0.8892436050871058, 0.7426703279588088, 0.1335099809354129, 0.5856693474862963, 0.4058526643830665, 0.23628964848349543, 0.12384146199615163, 0.8929134976936332, 0.5217186736691054, 0.4342555962831632, 0.1912131890004956, 0.33907846473731096, 0.10226615164460236]
Training loss = 0.014242573579152425
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.80375
Training loss = 0.015180281599362692
step = 1, Training Accuracy: 0.82
Training loss = 0.015066709121068318
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.014946199456850688
step = 3, Training Accuracy: 0.79
Training loss = 0.014853648145993551
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.015337100923061371
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.015484188894430796
step = 6, Training Accuracy: 0.79
Training loss = 0.015274377167224884
step = 7, Training Accuracy: 0.81
Training loss = 0.013901741604010264
step = 8, Training Accuracy: 0.8
Training loss = 0.012696521133184433
step = 9, Training Accuracy: 0.82
Training loss = 0.013942475914955139
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01214328279097875
step = 11, Training Accuracy: 0.84
Training loss = 0.014072116911411286
step = 12, Training Accuracy: 0.81
Training loss = 0.013232458829879761
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014405225465695063
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79875
params:  [0.519686839007125, 0.6159751221936034, 0.8479983428254354, 0.25313521564372293, 0.5972714127844331, 0.17177983163378993, 0.7702754163663765, 0.7570166603421826, 0.35827677904399324, 0.01, 0.6225632973972373, 0.9277635459462579, 0.9807349633550512, 0.01, 0.5411551110320045, 0.5928037304264391, 0.3602882617872066, 0.01, 0.8983754472692704, 0.5173017584300368, 0.4563263514648694, 0.20364256928419183, 0.10718406499949415, 0.12198411904877532]
Training loss = 0.014290701150894165
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.80625
Training loss = 0.01433971792459488
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.012452118496100107
step = 2, Training Accuracy: 0.86
Training loss = 0.01316538617014885
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.012977359543244044
step = 4, Training Accuracy: 0.82
Training loss = 0.012627711693445842
step = 5, Training Accuracy: 0.8633333333333333
Training loss = 0.013231234351793925
step = 6, Training Accuracy: 0.84
Training loss = 0.014663055141766867
step = 7, Training Accuracy: 0.83
Training loss = 0.01190831944346428
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.01482669472694397
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.013580055832862854
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.013715876191854477
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.014088782171408335
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.013047405729691187
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.015422614216804504
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.80625
params:  [0.46495556187840664, 0.41376666355565905, 0.9028140415495866, 0.01, 0.728590861357515, 0.45924014843915967, 0.99, 0.685527507400409, 0.31449360643077995, 0.01, 0.5771624757472529, 0.99, 0.7326321161635466, 0.18351914412539477, 0.33981607744404413, 0.803354336264092, 0.25645422398243106, 0.07650177560713885, 0.8658401073631812, 0.6983302712141939, 0.5916088074202736, 0.19572738240697368, 0.1884351529750021, 0.09272070531023552]
Training loss = 0.015960993667443593
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.805
Training loss = 0.0168368927637736
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.015550971031188965
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01725833530227343
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01584942638874054
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.016140126983324687
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01585950642824173
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016184839407602947
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015879661937554676
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01670338710149129
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.014645984570185344
step = 10, Training Accuracy: 0.78
Training loss = 0.017040213346481325
step = 11, Training Accuracy: 0.79
Training loss = 0.0165329372882843
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.013981847415367761
step = 13, Training Accuracy: 0.81
Training loss = 0.014779562552769979
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.8
params:  [0.11329671889082366, 0.6213899665762355, 0.99, 0.2981368951444328, 0.7063149701535032, 0.30016021682982447, 0.8620100144876743, 0.614485068256899, 0.40273055668619384, 0.025908976055766413, 0.48833835876518583, 0.9176104320880736, 0.7577049628757145, 0.13613762323646972, 0.66319782886257, 0.5838334476438367, 0.35546430972110155, 0.01, 0.9871495504153768, 0.6059043504691891, 0.4632196802081826, 0.27358916996356164, 0.2995247789534164, 0.24913514607855963]
Training loss = 0.018236448566118876
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.80625
Training loss = 0.018660928706328073
step = 1, Training Accuracy: 0.78
Training loss = 0.01776871979236603
step = 2, Training Accuracy: 0.8
Training loss = 0.017011313140392302
step = 3, Training Accuracy: 0.79
Training loss = 0.017763611475626627
step = 4, Training Accuracy: 0.77
Training loss = 0.017720426718393963
step = 5, Training Accuracy: 0.78
Training loss = 0.019004776775836944
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.017953561842441557
step = 7, Training Accuracy: 0.77
Training loss = 0.018884719957907994
step = 8, Training Accuracy: 0.8
Training loss = 0.0167458313703537
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.0171513835589091
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.01746903032064438
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.017436773975690205
step = 12, Training Accuracy: 0.7766666666666666
Training loss = 0.017504862447579702
step = 13, Training Accuracy: 0.76
Training loss = 0.01734859377145767
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.805
params:  [0.342464132313797, 0.42718467818290806, 0.7650604717898423, 0.04552402249221582, 0.7759303472881331, 0.09494908980726183, 0.99, 0.9677057309432797, 0.35062929738328236, 0.06855789058622216, 0.4935581188001366, 0.99, 0.7187575887629498, 0.01, 0.6171950759447259, 0.5988379496541446, 0.3003409570812582, 0.01, 0.9368321631263178, 0.6351455863624593, 0.45609030759457114, 0.3909776656651084, 0.06385743641653652, 0.050923510390992716]
Training loss = 0.015299493571122487
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.80625
Training loss = 0.016929291486740113
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01555172880490621
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.014660896162192027
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.014232348998387654
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.013471670399109523
step = 5, Training Accuracy: 0.83
Training loss = 0.015910207033157348
step = 6, Training Accuracy: 0.79
Training loss = 0.015442403356234232
step = 7, Training Accuracy: 0.82
Training loss = 0.014201260358095168
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.012598577539126079
step = 9, Training Accuracy: 0.85
Training loss = 0.01685170421997706
step = 10, Training Accuracy: 0.81
Training loss = 0.014820880889892578
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.01491148014863332
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.014989395439624787
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015199642578760783
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.8
params:  [0.36371070017889034, 0.5803902871098664, 0.8056458983407769, 0.2205608020426914, 0.7011516919057603, 0.2119153536962656, 0.99, 0.6603715435258981, 0.21743803637131118, 0.05952006860849361, 0.3653529095374113, 0.9132240284696002, 0.7603847710611935, 0.01, 0.4107513207913665, 0.4828299865453134, 0.29677715685659883, 0.19545347434171195, 0.821666677444513, 0.8585006228834857, 0.4234730316122779, 0.3061439602453564, 0.32615111261046403, 0.1166464665491079]
Training loss = 0.016731488307317098
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.80875
Training loss = 0.01590277115503947
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016662821471691132
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01730747441450755
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01660066803296407
step = 4, Training Accuracy: 0.77
Training loss = 0.01533389647801717
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01673997312784195
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.016698946158091227
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.019089144269625345
step = 8, Training Accuracy: 0.77
Training loss = 0.01639009714126587
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.01704454650481542
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.015282306472460429
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01563809275627136
step = 12, Training Accuracy: 0.8
Training loss = 0.01817006399234136
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.015965242783228556
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.80625
params:  [0.502612382995466, 0.30829619611095344, 0.7700348154027532, 0.3701564702910165, 0.8638161457929024, 0.5064283858686425, 0.7980071129724203, 0.7670083583171052, 0.17833513338958415, 0.031318059649803234, 0.5082945335024188, 0.7482380781673506, 0.8177757674787212, 0.09056463180712346, 0.634560333369564, 0.6510409224050369, 0.40873292417986457, 0.3004398321590831, 0.9211491253814493, 0.4452658103905831, 0.4117847031339878, 0.21851205137431332, 0.1787196741862927, 0.025334554874593157]
Training loss = 0.014009610414505005
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.80125
Training loss = 0.014400929709275563
step = 1, Training Accuracy: 0.81
Training loss = 0.013201021179556847
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.014665918548901875
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.013919130216042202
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.013930904865264892
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01268625279267629
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.01339219535390536
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.013980052669843038
step = 8, Training Accuracy: 0.83
Training loss = 0.012860519190629324
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.012104662756125132
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.014054951866467795
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.014575809439023335
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.01392955909172694
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.013555995225906371
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.80875
params:  [0.3983556516293023, 0.5901307241919584, 0.7806252088543947, 0.4402684700996347, 0.7502446804022702, 0.2872828779594104, 0.99, 0.7550156444756759, 0.18600569005475323, 0.021026354161826716, 0.627550684427784, 0.9393856143514252, 0.8745953856884165, 0.01, 0.46041606161759463, 0.8646960377800817, 0.4729367124456407, 0.038075889437557256, 0.9466608227379436, 0.6612736431023494, 0.47011034897554244, 0.33339668345053447, 0.24182388551024994, 0.0402632723708429]
Training loss = 0.015761034886042278
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.805
Training loss = 0.015084876120090485
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.014202146331469217
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.0152439484000206
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.013898823807636896
step = 4, Training Accuracy: 0.84
Training loss = 0.015245803991953532
step = 5, Training Accuracy: 0.81
Training loss = 0.013609011471271516
step = 6, Training Accuracy: 0.82
Training loss = 0.015299795269966126
step = 7, Training Accuracy: 0.83
Training loss = 0.014345574577649435
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.014344436824321747
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.014074189364910126
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.015508404076099396
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.013945579528808594
step = 12, Training Accuracy: 0.82
Training loss = 0.013405574063460032
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.014158265093962352
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.80875
[[0.2718464896810534, 0.47057794373406187, 0.6492748026995223, 0.19407745230604706, 0.5902807222890295, 0.2539542971810923, 0.8148726431492344, 0.6584665123820379, 0.44230798033239327, 0.01, 0.4067470829647848, 0.8892436050871058, 0.7426703279588088, 0.1335099809354129, 0.5856693474862963, 0.4058526643830665, 0.23628964848349543, 0.12384146199615163, 0.8929134976936332, 0.5217186736691054, 0.4342555962831632, 0.1912131890004956, 0.33907846473731096, 0.10226615164460236], [0.519686839007125, 0.6159751221936034, 0.8479983428254354, 0.25313521564372293, 0.5972714127844331, 0.17177983163378993, 0.7702754163663765, 0.7570166603421826, 0.35827677904399324, 0.01, 0.6225632973972373, 0.9277635459462579, 0.9807349633550512, 0.01, 0.5411551110320045, 0.5928037304264391, 0.3602882617872066, 0.01, 0.8983754472692704, 0.5173017584300368, 0.4563263514648694, 0.20364256928419183, 0.10718406499949415, 0.12198411904877532], [0.46495556187840664, 0.41376666355565905, 0.9028140415495866, 0.01, 0.728590861357515, 0.45924014843915967, 0.99, 0.685527507400409, 0.31449360643077995, 0.01, 0.5771624757472529, 0.99, 0.7326321161635466, 0.18351914412539477, 0.33981607744404413, 0.803354336264092, 0.25645422398243106, 0.07650177560713885, 0.8658401073631812, 0.6983302712141939, 0.5916088074202736, 0.19572738240697368, 0.1884351529750021, 0.09272070531023552], [0.11329671889082366, 0.6213899665762355, 0.99, 0.2981368951444328, 0.7063149701535032, 0.30016021682982447, 0.8620100144876743, 0.614485068256899, 0.40273055668619384, 0.025908976055766413, 0.48833835876518583, 0.9176104320880736, 0.7577049628757145, 0.13613762323646972, 0.66319782886257, 0.5838334476438367, 0.35546430972110155, 0.01, 0.9871495504153768, 0.6059043504691891, 0.4632196802081826, 0.27358916996356164, 0.2995247789534164, 0.24913514607855963], [0.342464132313797, 0.42718467818290806, 0.7650604717898423, 0.04552402249221582, 0.7759303472881331, 0.09494908980726183, 0.99, 0.9677057309432797, 0.35062929738328236, 0.06855789058622216, 0.4935581188001366, 0.99, 0.7187575887629498, 0.01, 0.6171950759447259, 0.5988379496541446, 0.3003409570812582, 0.01, 0.9368321631263178, 0.6351455863624593, 0.45609030759457114, 0.3909776656651084, 0.06385743641653652, 0.050923510390992716], [0.36371070017889034, 0.5803902871098664, 0.8056458983407769, 0.2205608020426914, 0.7011516919057603, 0.2119153536962656, 0.99, 0.6603715435258981, 0.21743803637131118, 0.05952006860849361, 0.3653529095374113, 0.9132240284696002, 0.7603847710611935, 0.01, 0.4107513207913665, 0.4828299865453134, 0.29677715685659883, 0.19545347434171195, 0.821666677444513, 0.8585006228834857, 0.4234730316122779, 0.3061439602453564, 0.32615111261046403, 0.1166464665491079], [0.502612382995466, 0.30829619611095344, 0.7700348154027532, 0.3701564702910165, 0.8638161457929024, 0.5064283858686425, 0.7980071129724203, 0.7670083583171052, 0.17833513338958415, 0.031318059649803234, 0.5082945335024188, 0.7482380781673506, 0.8177757674787212, 0.09056463180712346, 0.634560333369564, 0.6510409224050369, 0.40873292417986457, 0.3004398321590831, 0.9211491253814493, 0.4452658103905831, 0.4117847031339878, 0.21851205137431332, 0.1787196741862927, 0.025334554874593157], [0.3983556516293023, 0.5901307241919584, 0.7806252088543947, 0.4402684700996347, 0.7502446804022702, 0.2872828779594104, 0.99, 0.7550156444756759, 0.18600569005475323, 0.021026354161826716, 0.627550684427784, 0.9393856143514252, 0.8745953856884165, 0.01, 0.46041606161759463, 0.8646960377800817, 0.4729367124456407, 0.038075889437557256, 0.9466608227379436, 0.6612736431023494, 0.47011034897554244, 0.33339668345053447, 0.24182388551024994, 0.0402632723708429]]
26 	8     	0.804219	0.00379851	0.79875	0.80875
params:  [0.5630879929172425, 0.4452652672028541, 0.7186356230951968, 0.5144528068068642, 0.7874420409307276, 0.3903779857408377, 0.6535674965623836, 0.7758982078283985, 0.31420534215895146, 0.23197106848463617, 0.6001984474723556, 0.865604362986917, 0.858532129147705, 0.01, 0.5791269217631498, 0.856493623518608, 0.5564363204181746, 0.32542620110358045, 0.8181665739466851, 0.3718770032501605, 0.5832383965610504, 0.2176740721922786, 0.08328835531244126, 0.16253412610619902]
Training loss = 0.013298709293206533
step = 0, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.805
Training loss = 0.013738391200701396
step = 1, Training Accuracy: 0.8333333333333334
Training loss = 0.012713263332843781
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.014912556409835815
step = 3, Training Accuracy: 0.81
Training loss = 0.012167720595995584
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.012833712274829547
step = 5, Training Accuracy: 0.82
Training loss = 0.013366835465033849
step = 6, Training Accuracy: 0.83
Training loss = 0.012407887876033783
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.013761516710122426
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.013414765894412994
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.012368796567122142
step = 10, Training Accuracy: 0.8633333333333333
Training loss = 0.013213717887798945
step = 11, Training Accuracy: 0.83
Training loss = 0.012358840852975846
step = 12, Training Accuracy: 0.84
Training loss = 0.015382690032323202
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.014045349756876628
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.80125
params:  [0.565354847290306, 0.45030308508206246, 0.8018476440356352, 0.5028186852035385, 0.787285899850562, 0.26806200606393793, 0.99, 0.9186974329351558, 0.15900248236099596, 0.015735195367905287, 0.5399578417447611, 0.7512536596955575, 0.7934122999193108, 0.12168208693259025, 0.7887981093779415, 0.9696596220011244, 0.4239619877196483, 0.07875697581104443, 0.99, 0.8261437580097537, 0.36657674180397837, 0.29736374398279275, 0.11945998061207598, 0.01]
Training loss = 0.015951557159423826
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.80125
Training loss = 0.0181124343474706
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01752276380856832
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.017211964130401613
step = 3, Training Accuracy: 0.79
Training loss = 0.01697141339381536
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.018750985860824586
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.01662383019924164
step = 6, Training Accuracy: 0.76
Training loss = 0.015430759290854136
step = 7, Training Accuracy: 0.81
Training loss = 0.01694497068723043
step = 8, Training Accuracy: 0.78
Training loss = 0.016970020135243735
step = 9, Training Accuracy: 0.78
Training loss = 0.016664792497952777
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015092054704825083
step = 11, Training Accuracy: 0.83
Training loss = 0.01738611191511154
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.016698392232259114
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.016930506130059562
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.80125
params:  [0.4491187259903079, 0.666372163729175, 0.5557541940063797, 0.2039823677971087, 0.8241226935402199, 0.17654332543191356, 0.7801474184128556, 0.6093950159763949, 0.13357445628738673, 0.01, 0.5212191289454634, 0.7842139432733171, 0.7879983341991569, 0.06180914658635387, 0.5219629553667527, 0.7293898326879587, 0.16992811860171647, 0.19552853021747624, 0.982900027916793, 0.6579045528655576, 0.6070101285266536, 0.3438188736261585, 0.36305640651729076, 0.01]
Training loss = 0.01683578689893087
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.8075
Training loss = 0.01744072248538335
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01955390473206838
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01785946746667226
step = 3, Training Accuracy: 0.76
Training loss = 0.017109344005584715
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.018580191532770792
step = 5, Training Accuracy: 0.75
Training loss = 0.018643465340137482
step = 6, Training Accuracy: 0.73
Training loss = 0.01991093744834264
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.0171007975935936
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.017597505748271944
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.018241571088631947
step = 10, Training Accuracy: 0.76
Training loss = 0.018508839110533398
step = 11, Training Accuracy: 0.75
Training loss = 0.017248236735661823
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017094548543294272
step = 13, Training Accuracy: 0.79
Training loss = 0.016632519761721292
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.81
params:  [0.6439219297798875, 0.43592860282248286, 0.9622246685334388, 0.29953083008604353, 0.633926156346114, 0.4938077107624096, 0.783252396231803, 0.7711998534364066, 0.264019424503207, 0.01, 0.6248476160415107, 0.9115194362224951, 0.8390868662471399, 0.01, 0.6350299268163075, 0.6977479543336444, 0.5085997447969178, 0.08331542374079436, 0.926199802730485, 0.46709496876847445, 0.28167372829777554, 0.26982820015466313, 0.16425516449203578, 0.21937781085323235]
Training loss = 0.013949596385161082
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.805
Training loss = 0.01656817545493444
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.015893270472685496
step = 2, Training Accuracy: 0.8
Training loss = 0.014903228481610615
step = 3, Training Accuracy: 0.81
Training loss = 0.014217272897561391
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014649334649244944
step = 5, Training Accuracy: 0.8566666666666667
Training loss = 0.015227661530176798
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015175721645355224
step = 7, Training Accuracy: 0.81
Training loss = 0.014058263500531515
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013542951345443725
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.014476437966028849
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01524540513753891
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01552289326985677
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.016009933551152548
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01705566684405009
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.8025
params:  [0.3632696284032759, 0.4310168014828966, 0.7851445206696785, 0.11322521619919973, 0.803313880732964, 0.5506461373144778, 0.99, 0.8863715224258446, 0.13213201065518673, 0.01, 0.6744135574670628, 0.8836437141370034, 0.9034104756880844, 0.01, 0.5405575331974881, 0.692918913854857, 0.49006509357306116, 0.23279335802000234, 0.939577145227489, 0.3977093087903797, 0.48893988272672567, 0.19891369372526407, 0.019021131747116837, 0.01]
Training loss = 0.013287310302257539
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.80125
Training loss = 0.01395680844783783
step = 1, Training Accuracy: 0.83
Training loss = 0.01492472638686498
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014949033359686533
step = 3, Training Accuracy: 0.81
Training loss = 0.01481952061255773
step = 4, Training Accuracy: 0.81
Training loss = 0.015549326439698537
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.013964693347613017
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.013904263575871786
step = 7, Training Accuracy: 0.82
Training loss = 0.01288337970773379
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.014318959613641102
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015029205679893494
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.014806461334228516
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.014696086943149566
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.015142338871955872
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.014320424795150756
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.8
params:  [0.600512698787475, 0.2095092850341782, 0.6497998895899257, 0.48930181689029617, 0.7883255817297187, 0.36766504463256405, 0.8035041200185369, 0.97325840723798, 0.10083124436617218, 0.019437072907064198, 0.5901487313794509, 0.675727956715387, 0.880674992333551, 0.01, 0.5932224091228199, 0.6736439929265147, 0.6844069619421022, 0.01, 0.7860999268603525, 0.7540923390055296, 0.3580882474699665, 0.20115571958762063, 0.1253913950300522, 0.10802474101016689]
Training loss = 0.014556055068969726
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.80375
Training loss = 0.01486036201318105
step = 1, Training Accuracy: 0.77
Training loss = 0.015540431638558706
step = 2, Training Accuracy: 0.81
Training loss = 0.017257870535055796
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01528003806869189
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014478573004404704
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.015618090232213338
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.013961268365383148
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.015690074066321055
step = 8, Training Accuracy: 0.81
Training loss = 0.015037176062663396
step = 9, Training Accuracy: 0.79
Training loss = 0.014787130753199259
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.016319237798452377
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.014964792529741923
step = 12, Training Accuracy: 0.79
Training loss = 0.015082821547985077
step = 13, Training Accuracy: 0.8
Training loss = 0.013759562770525615
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.805
params:  [0.4969756314666048, 0.28757039636708087, 0.99, 0.10401675930621213, 0.7246130613588622, 0.4379395428056928, 0.8897806728928764, 0.5889853551139265, 0.31538385178076944, 0.0908334288546454, 0.29966234401781744, 0.7399893566084882, 0.7275694060249136, 0.1165498894945137, 0.6048624453343306, 0.6446870453672057, 0.5057305660075141, 0.2807802474040109, 0.99, 0.6186563540429831, 0.320505120595878, 0.47362213140565246, 0.1759248504008035, 0.23505165115573295]
Training loss = 0.01556861052910487
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.80625
Training loss = 0.015210089683532714
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.014079419573148092
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.013946628471215566
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.015337608953317006
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.014912430147329967
step = 5, Training Accuracy: 0.79
Training loss = 0.015223400791486104
step = 6, Training Accuracy: 0.81
Training loss = 0.013813019245862961
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.014605275690555572
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01306050548950831
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.014115640223026275
step = 10, Training Accuracy: 0.8
Training loss = 0.017122401495774587
step = 11, Training Accuracy: 0.77
Training loss = 0.015488120218118032
step = 12, Training Accuracy: 0.78
Training loss = 0.014713420470555624
step = 13, Training Accuracy: 0.8
Training loss = 0.01392491747935613
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.80125
params:  [0.490213447984367, 0.28225289857945124, 0.6396006224528512, 0.3599099923471379, 0.6046769650952688, 0.49144102045324267, 0.7868241308129327, 0.6863535898277213, 0.12742393422487663, 0.0655714004958658, 0.6415944694402305, 0.7366784734403486, 0.99, 0.01, 0.49282807744298857, 0.5315824488086904, 0.42772345682635154, 0.25771047054770346, 0.7997216234246093, 0.5114388957809284, 0.46200459446634856, 0.3516102245888203, 0.06713866572637178, 0.01]
Training loss = 0.013520075430472692
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.80125
Training loss = 0.01364257256189982
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.014320933222770692
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.014175056219100953
step = 3, Training Accuracy: 0.8
Training loss = 0.01468371738990148
step = 4, Training Accuracy: 0.83
Training loss = 0.013309234629074733
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.014128477573394775
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.012737777332464855
step = 7, Training Accuracy: 0.84
Training loss = 0.014957184592882792
step = 8, Training Accuracy: 0.82
Training loss = 0.014528319040934245
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.014052699009577433
step = 10, Training Accuracy: 0.84
Training loss = 0.014453200896581014
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.014141292770703634
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.014770865639050801
step = 13, Training Accuracy: 0.82
Training loss = 0.014200831154982248
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.805
[[0.5630879929172425, 0.4452652672028541, 0.7186356230951968, 0.5144528068068642, 0.7874420409307276, 0.3903779857408377, 0.6535674965623836, 0.7758982078283985, 0.31420534215895146, 0.23197106848463617, 0.6001984474723556, 0.865604362986917, 0.858532129147705, 0.01, 0.5791269217631498, 0.856493623518608, 0.5564363204181746, 0.32542620110358045, 0.8181665739466851, 0.3718770032501605, 0.5832383965610504, 0.2176740721922786, 0.08328835531244126, 0.16253412610619902], [0.565354847290306, 0.45030308508206246, 0.8018476440356352, 0.5028186852035385, 0.787285899850562, 0.26806200606393793, 0.99, 0.9186974329351558, 0.15900248236099596, 0.015735195367905287, 0.5399578417447611, 0.7512536596955575, 0.7934122999193108, 0.12168208693259025, 0.7887981093779415, 0.9696596220011244, 0.4239619877196483, 0.07875697581104443, 0.99, 0.8261437580097537, 0.36657674180397837, 0.29736374398279275, 0.11945998061207598, 0.01], [0.4491187259903079, 0.666372163729175, 0.5557541940063797, 0.2039823677971087, 0.8241226935402199, 0.17654332543191356, 0.7801474184128556, 0.6093950159763949, 0.13357445628738673, 0.01, 0.5212191289454634, 0.7842139432733171, 0.7879983341991569, 0.06180914658635387, 0.5219629553667527, 0.7293898326879587, 0.16992811860171647, 0.19552853021747624, 0.982900027916793, 0.6579045528655576, 0.6070101285266536, 0.3438188736261585, 0.36305640651729076, 0.01], [0.6439219297798875, 0.43592860282248286, 0.9622246685334388, 0.29953083008604353, 0.633926156346114, 0.4938077107624096, 0.783252396231803, 0.7711998534364066, 0.264019424503207, 0.01, 0.6248476160415107, 0.9115194362224951, 0.8390868662471399, 0.01, 0.6350299268163075, 0.6977479543336444, 0.5085997447969178, 0.08331542374079436, 0.926199802730485, 0.46709496876847445, 0.28167372829777554, 0.26982820015466313, 0.16425516449203578, 0.21937781085323235], [0.3632696284032759, 0.4310168014828966, 0.7851445206696785, 0.11322521619919973, 0.803313880732964, 0.5506461373144778, 0.99, 0.8863715224258446, 0.13213201065518673, 0.01, 0.6744135574670628, 0.8836437141370034, 0.9034104756880844, 0.01, 0.5405575331974881, 0.692918913854857, 0.49006509357306116, 0.23279335802000234, 0.939577145227489, 0.3977093087903797, 0.48893988272672567, 0.19891369372526407, 0.019021131747116837, 0.01], [0.600512698787475, 0.2095092850341782, 0.6497998895899257, 0.48930181689029617, 0.7883255817297187, 0.36766504463256405, 0.8035041200185369, 0.97325840723798, 0.10083124436617218, 0.019437072907064198, 0.5901487313794509, 0.675727956715387, 0.880674992333551, 0.01, 0.5932224091228199, 0.6736439929265147, 0.6844069619421022, 0.01, 0.7860999268603525, 0.7540923390055296, 0.3580882474699665, 0.20115571958762063, 0.1253913950300522, 0.10802474101016689], [0.4969756314666048, 0.28757039636708087, 0.99, 0.10401675930621213, 0.7246130613588622, 0.4379395428056928, 0.8897806728928764, 0.5889853551139265, 0.31538385178076944, 0.0908334288546454, 0.29966234401781744, 0.7399893566084882, 0.7275694060249136, 0.1165498894945137, 0.6048624453343306, 0.6446870453672057, 0.5057305660075141, 0.2807802474040109, 0.99, 0.6186563540429831, 0.320505120595878, 0.47362213140565246, 0.1759248504008035, 0.23505165115573295], [0.490213447984367, 0.28225289857945124, 0.6396006224528512, 0.3599099923471379, 0.6046769650952688, 0.49144102045324267, 0.7868241308129327, 0.6863535898277213, 0.12742393422487663, 0.0655714004958658, 0.6415944694402305, 0.7366784734403486, 0.99, 0.01, 0.49282807744298857, 0.5315824488086904, 0.42772345682635154, 0.25771047054770346, 0.7997216234246093, 0.5114388957809284, 0.46200459446634856, 0.3516102245888203, 0.06713866572637178, 0.01]]
27 	8     	0.803281	0.00305787	0.8    	0.81   
params:  [0.5127577977343553, 0.4021132799468905, 0.6198965251485231, 0.3206030897489007, 0.9278228879850797, 0.35063746201190915, 0.8886136958468507, 0.6147405935993696, 0.01, 0.17294141935504662, 0.4765853437122002, 0.7625141368618659, 0.7308455943228969, 0.1826436171188397, 0.5640607109157258, 0.685270386611262, 0.16563440641967, 0.19795277949722606, 0.99, 0.41193944450770126, 0.48363132565893746, 0.3095570427126799, 0.19392249648412663, 0.03771548627605657]
Training loss = 0.013345040182272594
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.80625
Training loss = 0.013265660405158997
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.014668335616588592
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.015542817413806915
step = 3, Training Accuracy: 0.82
Training loss = 0.015426652133464813
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.01488251785437266
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.016300510068734488
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.013132242262363434
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.015194686750570932
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.012867339551448823
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.014395390301942826
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.013785767952601115
step = 11, Training Accuracy: 0.82
Training loss = 0.015317122538884481
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.013898015469312668
step = 13, Training Accuracy: 0.82
Training loss = 0.013526936372121176
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.805
params:  [0.46834539877294795, 0.639063863404876, 0.501286741252233, 0.3213598729636171, 0.7275222596182905, 0.3931594397165287, 0.8264734421038925, 0.8052152966756982, 0.1521987542204361, 0.3289352079851967, 0.6842937103750051, 0.6548294948811563, 0.7113515975166607, 0.01, 0.5777224337203796, 0.4502621168191959, 0.3424158874689634, 0.09917974504836487, 0.7344087094784875, 0.7246143533659328, 0.5769133112143053, 0.18878771683531073, 0.07759525278489879, 0.01]
Training loss = 0.014040948152542114
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.8125
Training loss = 0.016367544531822206
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.0160720431804657
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.016688028573989867
step = 3, Training Accuracy: 0.79
Training loss = 0.01673684110244115
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015311255554358165
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.01600244661172231
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.017264496187369027
step = 7, Training Accuracy: 0.78
Training loss = 0.013607940971851348
step = 8, Training Accuracy: 0.83
Training loss = 0.016576344867547353
step = 9, Training Accuracy: 0.8
Training loss = 0.015930075496435166
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.01589923560619354
step = 11, Training Accuracy: 0.82
Training loss = 0.01729180951913198
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.016870946089426676
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.01648062328497569
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.8075
params:  [0.5218608561416551, 0.48234212039484803, 0.5914974255483973, 0.27814302674831726, 0.7707475003461708, 0.3065681078653898, 0.8297260462679632, 0.7825553682996796, 0.2186071198751279, 0.01, 0.46220066808166915, 0.7694368286710946, 0.7764433080804249, 0.013824591608583254, 0.5688664722395352, 0.7373873393923979, 0.32254038317189415, 0.3279446687947053, 0.8515956681594579, 0.8136275315612639, 0.5056671590712857, 0.35848911530194344, 0.3733113508653806, 0.15567489457823425]
Training loss = 0.01673669308423996
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.80875
Training loss = 0.017245707909266154
step = 1, Training Accuracy: 0.79
Training loss = 0.015804702540238698
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.016819441219170887
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.015552694896856943
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.017372167905171713
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.016942120095094045
step = 6, Training Accuracy: 0.8
Training loss = 0.014460718035697937
step = 7, Training Accuracy: 0.8
Training loss = 0.014578749686479568
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015992262760798136
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.016678672631581626
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.01597707172234853
step = 11, Training Accuracy: 0.81
Training loss = 0.016775127351284027
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.017386352916558583
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.015277413427829742
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.80625
params:  [0.47997837317914505, 0.32851430820127225, 0.7094959920065774, 0.2930018152522515, 0.8574925632761358, 0.44221020875747885, 0.7649395532875559, 0.7233138798739761, 0.16847904507728273, 0.01, 0.4229463263318297, 0.7437617123671703, 0.7481201092483037, 0.01, 0.4865756169867464, 0.7765043178836876, 0.4485787101210553, 0.06456056757225631, 0.9161296000147963, 0.698036281114659, 0.35999214937116586, 0.1876693158265057, 0.34372846357717357, 0.01]
Training loss = 0.014021378854910532
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.80125
Training loss = 0.015587042172749838
step = 1, Training Accuracy: 0.79
Training loss = 0.0156310898065567
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.01378366952141126
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.013948019246260325
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.01375000203649203
step = 5, Training Accuracy: 0.8
Training loss = 0.014244034588336944
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.013415278792381287
step = 7, Training Accuracy: 0.84
Training loss = 0.014365693032741546
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013598724603652954
step = 9, Training Accuracy: 0.8266666666666667
Training loss = 0.014478672345479329
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01473488837480545
step = 11, Training Accuracy: 0.81
Training loss = 0.013049720724423727
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.014955106675624847
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.015004914104938507
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.8075
params:  [0.43002655385042804, 0.46469425183511, 0.6611681167940107, 0.2033396725261849, 0.6934911773590122, 0.22766914521683948, 0.809246277376789, 0.7928648334433093, 0.01, 0.01, 0.37038679903929556, 0.5516519879211439, 0.7221440892505933, 0.01, 0.6008169476879005, 0.6138249783680924, 0.4725409417615283, 0.22443234334797274, 0.9585790601434641, 0.6345484777726339, 0.5773490359107725, 0.34908376771456384, 0.13484968956842905, 0.01810499075132787]
Training loss = 0.013540728092193604
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.80375
Training loss = 0.014385072588920593
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.015643907189369203
step = 2, Training Accuracy: 0.81
Training loss = 0.014017522682746251
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.01362343966960907
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.016069151113430657
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.013641757865746815
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.015671775291363398
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01422828475634257
step = 8, Training Accuracy: 0.84
Training loss = 0.013459263145923614
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013980491658051809
step = 10, Training Accuracy: 0.82
Training loss = 0.014521037042140961
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.014495554566383361
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.012741547822952271
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.013249561339616776
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.8025
params:  [0.4341849094157603, 0.4491723868045949, 0.5925455516061099, 0.09913374313537388, 0.6607216383721538, 0.3253723854654714, 0.8084220134523473, 0.8929548072005147, 0.1242606006443376, 0.01, 0.5711175984996016, 0.6770540446344382, 0.9426802567536079, 0.13931634253890648, 0.36158979103430705, 0.8015343010681383, 0.3860802992102393, 0.14195716735949987, 0.7531973339403242, 0.586971542849329, 0.4210202811508498, 0.5289612310021944, 0.3456908059636553, 0.01]
Training loss = 0.01589355876048406
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.80375
Training loss = 0.01521239419778188
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.01450391431649526
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.015177725851535796
step = 3, Training Accuracy: 0.81
Training loss = 0.013894959986209869
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.014987022479375204
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.012971038470665613
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01445040042201678
step = 7, Training Accuracy: 0.83
Training loss = 0.017383159299691517
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.014981907904148101
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.017359154025713603
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.012756631374359131
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.01562502731879552
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.014636646012465159
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.013450237313906351
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.80375
params:  [0.44374726458425, 0.38584092096014866, 0.6560766093977455, 0.2936615384918015, 0.6138356373279871, 0.3653759342497989, 0.7600011682831607, 0.4476453640497817, 0.0631817009753356, 0.01, 0.6508715239158263, 0.5538950976975596, 0.8763322815072319, 0.01, 0.6046104236445906, 0.6742676874268282, 0.3761458202061554, 0.2930991990303116, 0.9570864285251326, 0.5242457639481692, 0.5237892041723838, 0.18670852912338376, 0.25534463769043975, 0.07517751634032944]
Training loss = 0.015052774548530578
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.79875
Training loss = 0.0145161634683609
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.013943829039732614
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.014780847827593485
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.01571029265721639
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.014783829251925151
step = 5, Training Accuracy: 0.81
Training loss = 0.014288602968056997
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014069991012414296
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.015413523018360138
step = 8, Training Accuracy: 0.83
Training loss = 0.012195340444644293
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.013633839140335719
step = 10, Training Accuracy: 0.83
Training loss = 0.014402905056873958
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.014022642026344935
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.013642865419387817
step = 13, Training Accuracy: 0.83
Training loss = 0.015074950853983561
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.79625
params:  [0.5124740192434599, 0.4387574153782563, 0.7058895027748846, 0.3874377491710701, 0.8241120875173514, 0.2562635320164442, 0.768734839846016, 0.7277149574758326, 0.01, 0.09549517182395241, 0.48089386689128677, 0.6477903452369373, 0.8461738113486138, 0.11168427558310992, 0.727444850102519, 0.8599932252996628, 0.36829123936261243, 0.2762879253035544, 0.8538478889963259, 0.6993752199195491, 0.5572844662068632, 0.11835463640127195, 0.3244622018601443, 0.01]
Training loss = 0.016529516180356342
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.79875
Training loss = 0.017295515735944112
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.016693981687227886
step = 2, Training Accuracy: 0.79
Training loss = 0.013471512198448181
step = 3, Training Accuracy: 0.81
Training loss = 0.01467116912206014
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.015383914411067963
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.016436410049597422
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.01453744446237882
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.014056553443272909
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.016457772254943846
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.017235950032869974
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015186235407988231
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.014593459367752075
step = 12, Training Accuracy: 0.83
Training loss = 0.014068245192368826
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.014923549095789591
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.8025
[[0.5127577977343553, 0.4021132799468905, 0.6198965251485231, 0.3206030897489007, 0.9278228879850797, 0.35063746201190915, 0.8886136958468507, 0.6147405935993696, 0.01, 0.17294141935504662, 0.4765853437122002, 0.7625141368618659, 0.7308455943228969, 0.1826436171188397, 0.5640607109157258, 0.685270386611262, 0.16563440641967, 0.19795277949722606, 0.99, 0.41193944450770126, 0.48363132565893746, 0.3095570427126799, 0.19392249648412663, 0.03771548627605657], [0.46834539877294795, 0.639063863404876, 0.501286741252233, 0.3213598729636171, 0.7275222596182905, 0.3931594397165287, 0.8264734421038925, 0.8052152966756982, 0.1521987542204361, 0.3289352079851967, 0.6842937103750051, 0.6548294948811563, 0.7113515975166607, 0.01, 0.5777224337203796, 0.4502621168191959, 0.3424158874689634, 0.09917974504836487, 0.7344087094784875, 0.7246143533659328, 0.5769133112143053, 0.18878771683531073, 0.07759525278489879, 0.01], [0.5218608561416551, 0.48234212039484803, 0.5914974255483973, 0.27814302674831726, 0.7707475003461708, 0.3065681078653898, 0.8297260462679632, 0.7825553682996796, 0.2186071198751279, 0.01, 0.46220066808166915, 0.7694368286710946, 0.7764433080804249, 0.013824591608583254, 0.5688664722395352, 0.7373873393923979, 0.32254038317189415, 0.3279446687947053, 0.8515956681594579, 0.8136275315612639, 0.5056671590712857, 0.35848911530194344, 0.3733113508653806, 0.15567489457823425], [0.47997837317914505, 0.32851430820127225, 0.7094959920065774, 0.2930018152522515, 0.8574925632761358, 0.44221020875747885, 0.7649395532875559, 0.7233138798739761, 0.16847904507728273, 0.01, 0.4229463263318297, 0.7437617123671703, 0.7481201092483037, 0.01, 0.4865756169867464, 0.7765043178836876, 0.4485787101210553, 0.06456056757225631, 0.9161296000147963, 0.698036281114659, 0.35999214937116586, 0.1876693158265057, 0.34372846357717357, 0.01], [0.43002655385042804, 0.46469425183511, 0.6611681167940107, 0.2033396725261849, 0.6934911773590122, 0.22766914521683948, 0.809246277376789, 0.7928648334433093, 0.01, 0.01, 0.37038679903929556, 0.5516519879211439, 0.7221440892505933, 0.01, 0.6008169476879005, 0.6138249783680924, 0.4725409417615283, 0.22443234334797274, 0.9585790601434641, 0.6345484777726339, 0.5773490359107725, 0.34908376771456384, 0.13484968956842905, 0.01810499075132787], [0.4341849094157603, 0.4491723868045949, 0.5925455516061099, 0.09913374313537388, 0.6607216383721538, 0.3253723854654714, 0.8084220134523473, 0.8929548072005147, 0.1242606006443376, 0.01, 0.5711175984996016, 0.6770540446344382, 0.9426802567536079, 0.13931634253890648, 0.36158979103430705, 0.8015343010681383, 0.3860802992102393, 0.14195716735949987, 0.7531973339403242, 0.586971542849329, 0.4210202811508498, 0.5289612310021944, 0.3456908059636553, 0.01], [0.44374726458425, 0.38584092096014866, 0.6560766093977455, 0.2936615384918015, 0.6138356373279871, 0.3653759342497989, 0.7600011682831607, 0.4476453640497817, 0.0631817009753356, 0.01, 0.6508715239158263, 0.5538950976975596, 0.8763322815072319, 0.01, 0.6046104236445906, 0.6742676874268282, 0.3761458202061554, 0.2930991990303116, 0.9570864285251326, 0.5242457639481692, 0.5237892041723838, 0.18670852912338376, 0.25534463769043975, 0.07517751634032944], [0.5124740192434599, 0.4387574153782563, 0.7058895027748846, 0.3874377491710701, 0.8241120875173514, 0.2562635320164442, 0.768734839846016, 0.7277149574758326, 0.01, 0.09549517182395241, 0.48089386689128677, 0.6477903452369373, 0.8461738113486138, 0.11168427558310992, 0.727444850102519, 0.8599932252996628, 0.36829123936261243, 0.2762879253035544, 0.8538478889963259, 0.6993752199195491, 0.5572844662068632, 0.11835463640127195, 0.3244622018601443, 0.01]]
28 	8     	0.803906	0.00344814	0.79625	0.8075 
params:  [0.5546523721148862, 0.5000750536832541, 0.7065614284367795, 0.39702468080892406, 0.8946803283410245, 0.3338441316959273, 0.949175962923221, 0.8804375300910845, 0.2929189017808652, 0.09954040539769624, 0.5819719361371499, 0.7192662741820127, 0.7214514291499907, 0.0656411305073197, 0.5986787563996084, 0.6390631509244111, 0.4282440108349748, 0.30247625894818486, 0.8546643907700272, 0.6344524310670316, 0.470469519231373, 0.01, 0.2010809435844546, 0.17736545051301283]
Training loss = 0.01612372318903605
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.80375
Training loss = 0.015434710284074147
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014907108147939047
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.01679455240567525
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.015471516946951548
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.01731403559446335
step = 5, Training Accuracy: 0.79
Training loss = 0.0151091135541598
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01567481368780136
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.014043919146060943
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.016356225311756133
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.016451042592525483
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016514902611573536
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01461788256963094
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.01599906712770462
step = 13, Training Accuracy: 0.8
Training loss = 0.013046301404635112
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.805
params:  [0.37468357409473907, 0.3857681918956203, 0.3878637787952045, 0.4774589557718874, 0.8717005066903605, 0.21280711038358502, 0.7255694055127455, 0.5860790938229512, 0.1542425468565311, 0.3776292942034386, 0.5142484885307772, 0.6495013803599626, 0.647903815872119, 0.12692632179758212, 0.583602817379516, 0.38739800852752804, 0.2574137742476026, 0.08036719952802139, 0.7220414341903935, 0.7703909210567866, 0.5307161287777915, 0.3260951098969819, 0.18150623300157737, 0.18096638111370705]
Training loss = 0.013944324056307475
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.80375
Training loss = 0.014215890169143677
step = 1, Training Accuracy: 0.81
Training loss = 0.013030823369820912
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.014117959986130397
step = 3, Training Accuracy: 0.83
Training loss = 0.014270784755547841
step = 4, Training Accuracy: 0.8
Training loss = 0.014438745379447938
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.01414319525162379
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.013726661453644434
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.013922790338595708
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015372575720151266
step = 9, Training Accuracy: 0.78
Training loss = 0.014289514025052389
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.014253330628077189
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.013730849027633668
step = 12, Training Accuracy: 0.83
Training loss = 0.013605427791674932
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.015268530050913493
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.8025
params:  [0.46364638138557146, 0.485547069086724, 0.47856454968908624, 0.51652536296428, 0.7325577302251723, 0.3806307788497081, 0.8731188962599753, 0.880320868806161, 0.1858212613999971, 0.01, 0.6479092809395941, 0.6372141732481208, 0.6321262130651126, 0.029295653829845128, 0.46459547093895026, 0.757215174907472, 0.499945136743395, 0.27943961353461033, 0.99, 0.99, 0.3639131633440935, 0.01, 0.12049779142423842, 0.15725637603482834]
Training loss = 0.017592345476150514
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.8025
Training loss = 0.016052435239156088
step = 1, Training Accuracy: 0.8
Training loss = 0.0174730525414149
step = 2, Training Accuracy: 0.74
Training loss = 0.017842246691385906
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.015988724231719972
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017341493169466655
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.017327512204647063
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.016262050767739615
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.017180489897727965
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015369269549846649
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.017178475260734558
step = 10, Training Accuracy: 0.76
Training loss = 0.01725508858760198
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.017917961676915485
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.01688157399495443
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.0180346742272377
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79875
params:  [0.5048872050318922, 0.5293406704530292, 0.5265694371070532, 0.4878077385218471, 0.8148198144331218, 0.5275271452068334, 0.7034133789954402, 0.9386902382539979, 0.2337847963145616, 0.18790873826335228, 0.46322894960925115, 0.6314575445860835, 0.6818021396360439, 0.010249362205742957, 0.682027600212164, 0.3791189744068952, 0.4919466841996081, 0.1620323917333207, 0.8782923681350278, 0.6530316772604019, 0.5692714111277839, 0.3274306453219797, 0.09320481060117912, 0.01]
Training loss = 0.016387330492337544
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.8025
Training loss = 0.01605156312386195
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.014200555036465327
step = 2, Training Accuracy: 0.81
Training loss = 0.01441734234491984
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.015951286256313323
step = 4, Training Accuracy: 0.81
Training loss = 0.014663855830828349
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.015281995336214702
step = 6, Training Accuracy: 0.8
Training loss = 0.015241644978523254
step = 7, Training Accuracy: 0.8
Training loss = 0.01569888412952423
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.015167722702026367
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.015696396629015605
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.017219474812348683
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.015636516710122426
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.01603668918212255
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.016493882735570273
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.81
params:  [0.4585827150140854, 0.4470093273986745, 0.5373395405042881, 0.2585177278036488, 0.6423421965138018, 0.48122324505489367, 0.7401362843274903, 0.7319491420327071, 0.15642253775929002, 0.18969646964884426, 0.555032230971319, 0.6677305399659134, 0.7027228850839825, 0.03506154288117532, 0.7228715578669628, 0.6525568204336831, 0.565897030843856, 0.08580544879462627, 0.7882491093004937, 0.7232931234933306, 0.43658973284473734, 0.1049743036485031, 0.06372027660562593, 0.11869782074087634]
Training loss = 0.015008995433648427
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.80875
Training loss = 0.014846204916636149
step = 1, Training Accuracy: 0.8
Training loss = 0.013000332514444986
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.013966463108857474
step = 3, Training Accuracy: 0.82
Training loss = 0.014086826841036479
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.016001820961634317
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.013779954810937245
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015899611214796702
step = 7, Training Accuracy: 0.78
Training loss = 0.016094887753327687
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.014243180652459463
step = 9, Training Accuracy: 0.82
Training loss = 0.016274272898832955
step = 10, Training Accuracy: 0.77
Training loss = 0.014334648152192434
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.012677666445573171
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.014964276055494944
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.01624375581741333
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.8
params:  [0.3781587059242885, 0.40584120733100676, 0.5743790160985861, 0.30430425286505086, 0.7219787038372321, 0.3086749976808766, 0.81802820653688, 0.8809070231644275, 0.1048771553653815, 0.23616478580199207, 0.489981457774908, 0.7079339646998511, 0.5256151427028273, 0.01550341145664673, 0.530522920298424, 0.6319918839870075, 0.2525733151145798, 0.01, 0.771558013497326, 0.7327026973212787, 0.3804860239342807, 0.3891383176424092, 0.2134294041311848, 0.16838530638151245]
Training loss = 0.014254948099454243
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.8075
Training loss = 0.01519711196422577
step = 1, Training Accuracy: 0.8
Training loss = 0.014157675802707673
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.016316628754138945
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.015646087328592937
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.014045824507872263
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015567801594734192
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.014365321050087611
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013419581850369771
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.014597005943457286
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.014389205276966095
step = 10, Training Accuracy: 0.82
Training loss = 0.01636348158121109
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.014855294326941172
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.014974738558133444
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014378403822580973
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.80375
params:  [0.6030196536838577, 0.5657408891790118, 0.607373494523803, 0.3446581498844961, 0.6228778181394805, 0.42137032650506806, 0.8020921471396874, 0.6893841667892225, 0.273718337621415, 0.32357491469647004, 0.6695152494944783, 0.5600269531747029, 0.757416343224398, 0.32587816101009276, 0.39880771300771245, 0.5948716926656384, 0.44183310176464463, 0.1654539094723187, 0.8448037523738648, 0.7315177625440934, 0.36519532512573066, 0.21342810910497192, 0.04583052108706459, 0.01]
Training loss = 0.01531485637029012
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.805
Training loss = 0.016753898561000825
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.015990032653013866
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01532420833905538
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.014208666880925496
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.01749078353246053
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.013421075344085693
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.0164881760875384
step = 7, Training Accuracy: 0.79
Training loss = 0.015745596786340077
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.013245249837636948
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.016063753763834634
step = 10, Training Accuracy: 0.79
Training loss = 0.014455414513746897
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.015502831439177195
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.01588957617680232
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.014449799259503682
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.81
params:  [0.5733761650190438, 0.6728761070876601, 0.6124709625905195, 0.3191267352317908, 0.7893201475051707, 0.27293748428587206, 0.99, 0.7327875062009989, 0.17445015015122228, 0.2570264137391175, 0.6745638129462175, 0.9212561559150962, 0.6333647483247097, 0.01, 0.335708918944174, 0.6363220975099682, 0.5058163858418878, 0.2881724099083007, 0.7761481356619784, 0.8884764954912115, 0.27277636304778874, 0.22898395676331457, 0.1893690572368335, 0.04436671491525822]
Training loss = 0.015839818318684896
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.80375
Training loss = 0.016191416879494984
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.01676776150862376
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.015491774628559747
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.01642876277367274
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01598601520061493
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.015953225791454317
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016641747454802194
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.016063904762268065
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.017763560911019643
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.016180741091569265
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.016632664799690246
step = 11, Training Accuracy: 0.77
Training loss = 0.01614868621031443
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.017478301425774893
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.016505539417266846
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.80375
[[0.5546523721148862, 0.5000750536832541, 0.7065614284367795, 0.39702468080892406, 0.8946803283410245, 0.3338441316959273, 0.949175962923221, 0.8804375300910845, 0.2929189017808652, 0.09954040539769624, 0.5819719361371499, 0.7192662741820127, 0.7214514291499907, 0.0656411305073197, 0.5986787563996084, 0.6390631509244111, 0.4282440108349748, 0.30247625894818486, 0.8546643907700272, 0.6344524310670316, 0.470469519231373, 0.01, 0.2010809435844546, 0.17736545051301283], [0.37468357409473907, 0.3857681918956203, 0.3878637787952045, 0.4774589557718874, 0.8717005066903605, 0.21280711038358502, 0.7255694055127455, 0.5860790938229512, 0.1542425468565311, 0.3776292942034386, 0.5142484885307772, 0.6495013803599626, 0.647903815872119, 0.12692632179758212, 0.583602817379516, 0.38739800852752804, 0.2574137742476026, 0.08036719952802139, 0.7220414341903935, 0.7703909210567866, 0.5307161287777915, 0.3260951098969819, 0.18150623300157737, 0.18096638111370705], [0.46364638138557146, 0.485547069086724, 0.47856454968908624, 0.51652536296428, 0.7325577302251723, 0.3806307788497081, 0.8731188962599753, 0.880320868806161, 0.1858212613999971, 0.01, 0.6479092809395941, 0.6372141732481208, 0.6321262130651126, 0.029295653829845128, 0.46459547093895026, 0.757215174907472, 0.499945136743395, 0.27943961353461033, 0.99, 0.99, 0.3639131633440935, 0.01, 0.12049779142423842, 0.15725637603482834], [0.5048872050318922, 0.5293406704530292, 0.5265694371070532, 0.4878077385218471, 0.8148198144331218, 0.5275271452068334, 0.7034133789954402, 0.9386902382539979, 0.2337847963145616, 0.18790873826335228, 0.46322894960925115, 0.6314575445860835, 0.6818021396360439, 0.010249362205742957, 0.682027600212164, 0.3791189744068952, 0.4919466841996081, 0.1620323917333207, 0.8782923681350278, 0.6530316772604019, 0.5692714111277839, 0.3274306453219797, 0.09320481060117912, 0.01], [0.4585827150140854, 0.4470093273986745, 0.5373395405042881, 0.2585177278036488, 0.6423421965138018, 0.48122324505489367, 0.7401362843274903, 0.7319491420327071, 0.15642253775929002, 0.18969646964884426, 0.555032230971319, 0.6677305399659134, 0.7027228850839825, 0.03506154288117532, 0.7228715578669628, 0.6525568204336831, 0.565897030843856, 0.08580544879462627, 0.7882491093004937, 0.7232931234933306, 0.43658973284473734, 0.1049743036485031, 0.06372027660562593, 0.11869782074087634], [0.3781587059242885, 0.40584120733100676, 0.5743790160985861, 0.30430425286505086, 0.7219787038372321, 0.3086749976808766, 0.81802820653688, 0.8809070231644275, 0.1048771553653815, 0.23616478580199207, 0.489981457774908, 0.7079339646998511, 0.5256151427028273, 0.01550341145664673, 0.530522920298424, 0.6319918839870075, 0.2525733151145798, 0.01, 0.771558013497326, 0.7327026973212787, 0.3804860239342807, 0.3891383176424092, 0.2134294041311848, 0.16838530638151245], [0.6030196536838577, 0.5657408891790118, 0.607373494523803, 0.3446581498844961, 0.6228778181394805, 0.42137032650506806, 0.8020921471396874, 0.6893841667892225, 0.273718337621415, 0.32357491469647004, 0.6695152494944783, 0.5600269531747029, 0.757416343224398, 0.32587816101009276, 0.39880771300771245, 0.5948716926656384, 0.44183310176464463, 0.1654539094723187, 0.8448037523738648, 0.7315177625440934, 0.36519532512573066, 0.21342810910497192, 0.04583052108706459, 0.01], [0.5733761650190438, 0.6728761070876601, 0.6124709625905195, 0.3191267352317908, 0.7893201475051707, 0.27293748428587206, 0.99, 0.7327875062009989, 0.17445015015122228, 0.2570264137391175, 0.6745638129462175, 0.9212561559150962, 0.6333647483247097, 0.01, 0.335708918944174, 0.6363220975099682, 0.5058163858418878, 0.2881724099083007, 0.7761481356619784, 0.8884764954912115, 0.27277636304778874, 0.22898395676331457, 0.1893690572368335, 0.04436671491525822]]
29 	8     	0.804219	0.00384959	0.79875	0.81   
hof:  [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01]
new pl params Compose([
  Resize(always_apply=False, p=1, height=256, width=256, interpolation=1),
  Transpose(always_apply=False, p=0.4546697027176039),
  OneOf([
    RandomBrightnessContrast(always_apply=False, p=0.9204193933550783, brightness_limit=(-0.2179701676469351, 0.2179701676469351), contrast_limit=(-0.745324316140274, 0.745324316140274), brightness_by_max=False),
    HueSaturationValue(always_apply=False, p=0.99, hue_shift_limit=(-40, 40), sat_shift_limit=(-28, 28), val_shift_limit=(-5, 5)),
    RGBShift(always_apply=False, p=0.7030585412151354, r_shift_limit=(-32, 32), g_shift_limit=(-46, 46), b_shift_limit=(-15, 15)),
    RandomBrightness(always_apply=False, p=0.01, limit=(-0.7843266009278341, 0.7843266009278341)),
    RandomContrast(always_apply=False, p=0.13323349436411253, limit=(-0.01, 0.01)),
    ChannelDropout(always_apply=False, p=0.18908445647472388, channel_drop_range=(1, 1), fill_value=0),
  ], p=0.686768414501576),
  ElasticTransform(always_apply=False, p=0.8208260937274724, alpha=2.1211221727961687, sigma=44.52551052599132, alpha_affine=53.039932664294895, interpolation=0, border_mode=4, value=None, mask_value=None, approximate=False),
  Resize(always_apply=False, p=1, height=128, width=128, interpolation=1),
  ToTensor(always_apply=True, p=1.0, num_classes=1, sigmoid=True, normalize=None),
], p=1, bbox_params=None, keypoint_params=None, additional_targets={})
individual:  [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01]  value:  (0.81125,)
gen	nevals	avg     	std       	min    	max    
0  	8     	0.552812	0.0234417 	0.50875	0.58125
1  	8     	0.694375	0.0146442 	0.66875	0.7175 
2  	8     	0.736094	0.0223558 	0.7    	0.76625
3  	8     	0.751406	0.00956959	0.735  	0.7675 
4  	8     	0.756406	0.0169609 	0.72625	0.785  
5  	8     	0.769688	0.0129716 	0.755  	0.79625
6  	8     	0.777969	0.0122145 	0.765  	0.7975 
7  	8     	0.778438	0.0129716 	0.76375	0.80375
8  	8     	0.78    	0.0075519 	0.76625	0.79   
9  	8     	0.785312	0.00738849	0.77625	0.7975 
10 	8     	0.772031	0.0105686 	0.7575 	0.7925 
11 	8     	0.787344	0.00862494	0.775  	0.805  
12 	8     	0.782031	0.0092055 	0.76625	0.795  
13 	8     	0.791406	0.0104103 	0.77375	0.80625
14 	8     	0.8     	0.00720785	0.78875	0.81125
15 	8     	0.784531	0.0100766 	0.765  	0.79625
16 	8     	0.790156	0.00766581	0.77875	0.805  
17 	8     	0.785625	0.00405046	0.77625	0.79   
18 	8     	0.797187	0.00627339	0.78875	0.80625
19 	8     	0.795469	0.00459013	0.79   	0.8025 
20 	8     	0.800312	0.00317153	0.795  	0.80375
21 	8     	0.802344	0.00477778	0.795  	0.81125
22 	8     	0.801406	0.00439449	0.795  	0.80625
23 	8     	0.804844	0.00377272	0.7975 	0.81   
24 	8     	0.802031	0.00197024	0.79875	0.80375
25 	8     	0.802031	0.00418971	0.795  	0.80625
26 	8     	0.804219	0.00379851	0.79875	0.80875
27 	8     	0.803281	0.00305787	0.8    	0.81   
28 	8     	0.803906	0.00344814	0.79625	0.8075 
29 	8     	0.804219	0.00384959	0.79875	0.81   
