(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.2
(0, 1) <class 'int'> 1
(0.0, 1.0) <class 'float'> 0.5
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 30
(5, 50) <class 'int'> 20
(0.0, 1.0) <class 'float'> 0.5
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 20
(5, 50) <class 'int'> 20
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.2
(0.0, 1.0) <class 'float'> 0.5
(0.0, 1.0) <class 'float'> 0.5
(0, 5) <class 'float'> 1
(25, 75) <class 'float'> 50
(25, 75) <class 'float'> 50
(0, 4) <class 'int'> 1
params:  [0.6490142459033698, 0.2322671098540393, 0.9396946306764662, 0.01, 0.01, 0.99, 0.1961506638996729, 0.16464707456104152, 0.03808020580164573, 0.01, 0.5725886814698102, 0.19361440726225626, 0.19430802548959464, 0.49610134640912273, 0.3591576842195144, 0.43023041874587264, 0.9737638446522174, 0.12975891291524586, 0.4297539875829992, 0.9569089569224076, 0.39430656143020776, 0.4585207096486446, 0.5202584614063771, 0.01]
Training loss = 0.045898592472076415
step = 0, Training Accuracy: 0.31
Validation Accuracy: 0.33
Training loss = 0.047466355164845785
step = 1, Training Accuracy: 0.3566666666666667
Training loss = 0.03912363827228546
step = 2, Training Accuracy: 0.39
Training loss = 0.03740453163782755
step = 3, Training Accuracy: 0.46
Training loss = 0.035157377521197
step = 4, Training Accuracy: 0.5033333333333333
Training loss = 0.036059194405873615
step = 5, Training Accuracy: 0.45
Training loss = 0.0339287132024765
step = 6, Training Accuracy: 0.49333333333333335
Training loss = 0.034715977907180784
step = 7, Training Accuracy: 0.44333333333333336
Training loss = 0.032524977723757426
step = 8, Training Accuracy: 0.5033333333333333
Training loss = 0.03298800488313039
step = 9, Training Accuracy: 0.5166666666666667
Training loss = 0.0331840044260025
step = 10, Training Accuracy: 0.47
Training loss = 0.03353728850682577
step = 11, Training Accuracy: 0.49666666666666665
Training loss = 0.03148691594600678
step = 12, Training Accuracy: 0.53
Training loss = 0.033278128306070964
step = 13, Training Accuracy: 0.5066666666666667
Training loss = 0.03072515328725179
step = 14, Training Accuracy: 0.5266666666666666
Validation Accuracy: 0.50875
params:  [0.3366851826424452, 0.08404673748158739, 0.05644340288977179, 0.10966889132321338, 0.16530551528352785, 0.99, 0.7215399739986231, 0.39239170409407037, 0.15709974088602646, 0.01, 0.5626590785014266, 0.01, 0.58009680696429, 0.016020054646563198, 0.4959508325786198, 0.7556834553526812, 0.31948801633118096, 0.11249187506201698, 0.3198083930243585, 0.6127094055037016, 0.01, 0.5332767769129598, 0.3618083687120638, 0.5671366678656746]
Training loss = 0.03284296830495199
step = 0, Training Accuracy: 0.52
Validation Accuracy: 0.5175
Training loss = 0.03113898038864136
step = 1, Training Accuracy: 0.5733333333333334
Training loss = 0.030540722608566283
step = 2, Training Accuracy: 0.5733333333333334
Training loss = 0.03187391380469004
step = 3, Training Accuracy: 0.55
Training loss = 0.02948544462521871
step = 4, Training Accuracy: 0.65
Training loss = 0.028402098218599955
step = 5, Training Accuracy: 0.62
Training loss = 0.026982145309448244
step = 6, Training Accuracy: 0.65
Training loss = 0.02704208711783091
step = 7, Training Accuracy: 0.6766666666666666
Training loss = 0.026449689467748005
step = 8, Training Accuracy: 0.66
Training loss = 0.02755418340365092
step = 9, Training Accuracy: 0.67
Training loss = 0.024497809012730916
step = 10, Training Accuracy: 0.6866666666666666
Training loss = 0.027525234818458557
step = 11, Training Accuracy: 0.6466666666666666
Training loss = 0.027577664256095886
step = 12, Training Accuracy: 0.6366666666666667
Training loss = 0.02760821541150411
step = 13, Training Accuracy: 0.6566666666666666
Training loss = 0.02670118272304535
step = 14, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.56875
params:  [0.6030854868705384, 0.10646407361846272, 0.6084908075142903, 0.5010598693676073, 0.17839696352589984, 0.99, 0.7437577467182594, 0.01, 0.22365506335374713, 0.2776356403341882, 0.356247728646413, 0.625996871470041, 0.4327123627544025, 0.24056962057796893, 0.24823474303320847, 0.47938403573485955, 0.8092998567487852, 0.3835028866522604, 0.29692339990821237, 0.38447531587510503, 0.2972251908184385, 0.01, 0.6084186816525242, 0.7114109699397908]
Training loss = 0.03176299194494883
step = 0, Training Accuracy: 0.5966666666666667
Validation Accuracy: 0.565
Training loss = 0.02989375332991282
step = 1, Training Accuracy: 0.6033333333333334
Training loss = 0.03073536475499471
step = 2, Training Accuracy: 0.5866666666666667
Training loss = 0.028911153276761373
step = 3, Training Accuracy: 0.61
Training loss = 0.028380921681722005
step = 4, Training Accuracy: 0.6
Training loss = 0.02789959788322449
step = 5, Training Accuracy: 0.6233333333333333
Training loss = 0.02797641654809316
step = 6, Training Accuracy: 0.6033333333333334
Training loss = 0.027580503622690836
step = 7, Training Accuracy: 0.62
Training loss = 0.02666061758995056
step = 8, Training Accuracy: 0.6466666666666666
Training loss = 0.028263410329818727
step = 9, Training Accuracy: 0.61
Training loss = 0.025410646200180055
step = 10, Training Accuracy: 0.66
Training loss = 0.02529781848192215
step = 11, Training Accuracy: 0.6566666666666666
Training loss = 0.027484010060628256
step = 12, Training Accuracy: 0.63
Training loss = 0.026230367223421733
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.026141402920087178
step = 14, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.5375
params:  [0.48925218826701455, 0.20170135602066952, 0.28938407183679427, 0.4905934971598668, 0.22912326480441214, 0.99, 0.3410719388698884, 0.43195866623123863, 0.8301761908661778, 0.18280622025797236, 0.25745191913204374, 0.17785226785123912, 0.7767015467557881, 0.44046710478685724, 0.43409843364874645, 0.01, 0.5275282329606507, 0.11029779486023979, 0.5261141204714513, 0.7465707513125671, 0.01, 0.9693930967442018, 0.3823675540603527, 0.01]
Training loss = 0.032737785975138343
step = 0, Training Accuracy: 0.5733333333333334
Validation Accuracy: 0.515
Training loss = 0.03145172377427419
step = 1, Training Accuracy: 0.56
Training loss = 0.029722024202346802
step = 2, Training Accuracy: 0.5933333333333334
Training loss = 0.029353340268135072
step = 3, Training Accuracy: 0.59
Training loss = 0.028568919698397317
step = 4, Training Accuracy: 0.6233333333333333
Training loss = 0.027073587775230407
step = 5, Training Accuracy: 0.66
Training loss = 0.029069196581840515
step = 6, Training Accuracy: 0.5966666666666667
Training loss = 0.027231234510739645
step = 7, Training Accuracy: 0.65
Training loss = 0.029156054854393005
step = 8, Training Accuracy: 0.6133333333333333
Training loss = 0.02680741747220357
step = 9, Training Accuracy: 0.63
Training loss = 0.026398823261260987
step = 10, Training Accuracy: 0.6633333333333333
Training loss = 0.028077513178189597
step = 11, Training Accuracy: 0.6066666666666667
Training loss = 0.02648748775323232
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02597013771533966
step = 13, Training Accuracy: 0.6233333333333333
Training loss = 0.027616848945617677
step = 14, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.58125
params:  [0.5888360831193729, 0.01, 0.489586469088427, 0.2904642027000838, 0.14229171056566325, 0.99, 0.5180690629823079, 0.32537917069856825, 0.01, 0.31099955860348316, 0.5772651172168293, 0.385706677182885, 0.8991891036964923, 0.4545485903776948, 0.45161428650019725, 0.01, 0.3971856450419692, 0.0738064031703923, 0.07538877738487576, 0.42962385998745595, 0.20153403699273828, 0.5783165816539668, 0.8428468443545061, 0.47557990980603226]
Training loss = 0.027975884278615314
step = 0, Training Accuracy: 0.6266666666666667
Validation Accuracy: 0.5425
Training loss = 0.02990558644135793
step = 1, Training Accuracy: 0.62
Training loss = 0.02705331067244212
step = 2, Training Accuracy: 0.6566666666666666
Training loss = 0.02692298630873362
step = 3, Training Accuracy: 0.6433333333333333
Training loss = 0.025613959630330405
step = 4, Training Accuracy: 0.6666666666666666
Training loss = 0.025047114888827007
step = 5, Training Accuracy: 0.6766666666666666
Training loss = 0.027111131151517233
step = 6, Training Accuracy: 0.6366666666666667
Training loss = 0.028434348305066425
step = 7, Training Accuracy: 0.6133333333333333
Training loss = 0.027369920214017233
step = 8, Training Accuracy: 0.63
Training loss = 0.024773352841536204
step = 9, Training Accuracy: 0.6833333333333333
Training loss = 0.024410068194071453
step = 10, Training Accuracy: 0.6566666666666666
Training loss = 0.024498682618141174
step = 11, Training Accuracy: 0.6933333333333334
Training loss = 0.024444636503855387
step = 12, Training Accuracy: 0.6666666666666666
Training loss = 0.025589659015337628
step = 13, Training Accuracy: 0.7
Training loss = 0.02172317703564962
step = 14, Training Accuracy: 0.72
Validation Accuracy: 0.5425
params:  [0.737309584112914, 0.6921428262847285, 0.5682379803812388, 0.01, 0.44405516521090094, 0.9033815451382974, 0.26502401229912886, 0.7983136548385952, 0.27972828528541466, 0.47541106252388776, 0.18130888588216854, 0.3539022257751415, 0.01, 0.18229063709847357, 0.5298954095262923, 0.030110681119168453, 0.2028391024607935, 0.8571366877429936, 0.6760571281400811, 0.07944468116231573, 0.6208382932808297, 0.22718376356157832, 0.01775502963163178, 0.30539015755969123]
Training loss = 0.035644917885462445
step = 0, Training Accuracy: 0.58
Validation Accuracy: 0.5
Training loss = 0.03113774100939433
step = 1, Training Accuracy: 0.5033333333333333
Training loss = 0.03166322708129883
step = 2, Training Accuracy: 0.5033333333333333
Training loss = 0.030937630931536356
step = 3, Training Accuracy: 0.55
Training loss = 0.03135738531748454
step = 4, Training Accuracy: 0.5533333333333333
Training loss = 0.03131312350432078
step = 5, Training Accuracy: 0.5466666666666666
Training loss = 0.031018190383911133
step = 6, Training Accuracy: 0.56
Training loss = 0.031855487028757734
step = 7, Training Accuracy: 0.51
Training loss = 0.02994584878285726
step = 8, Training Accuracy: 0.5133333333333333
Training loss = 0.029224496682484946
step = 9, Training Accuracy: 0.5566666666666666
Training loss = 0.02804992159207662
step = 10, Training Accuracy: 0.6066666666666667
Training loss = 0.027501303553581238
step = 11, Training Accuracy: 0.5766666666666667
Training loss = 0.02942297319571177
step = 12, Training Accuracy: 0.6233333333333333
Training loss = 0.02952246387799581
step = 13, Training Accuracy: 0.59
Training loss = 0.029101282755533853
step = 14, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.57875
params:  [0.577964838274527, 0.4238342780809495, 0.7890128387732965, 0.01, 0.5475786737022212, 0.99, 0.20759549893180357, 0.5302994159234822, 0.19816450639476102, 0.4754832096068696, 0.99, 0.11902790792542298, 0.4212550753229377, 0.4030094424816344, 0.29599258352645275, 0.3039344628490927, 0.575147855103763, 0.2890954019699558, 0.6565824696850693, 0.1038630160747171, 0.01, 0.7345468615331932, 0.746618047998347, 0.8190378947961842]
Training loss = 0.0317136679093043
step = 0, Training Accuracy: 0.5033333333333333
Validation Accuracy: 0.56375
Training loss = 0.03035571316878001
step = 1, Training Accuracy: 0.5733333333333334
Training loss = 0.02938752035299937
step = 2, Training Accuracy: 0.5833333333333334
Training loss = 0.028407865166664124
step = 3, Training Accuracy: 0.62
Training loss = 0.02724802056948344
step = 4, Training Accuracy: 0.6233333333333333
Training loss = 0.026081543366114297
step = 5, Training Accuracy: 0.63
Training loss = 0.02483079512914022
step = 6, Training Accuracy: 0.6566666666666666
Training loss = 0.026433887481689452
step = 7, Training Accuracy: 0.65
Training loss = 0.02525916278362274
step = 8, Training Accuracy: 0.6566666666666666
Training loss = 0.02592871030171712
step = 9, Training Accuracy: 0.6466666666666666
Training loss = 0.025626844763755798
step = 10, Training Accuracy: 0.66
Training loss = 0.02464507261912028
step = 11, Training Accuracy: 0.6666666666666666
Training loss = 0.024002980987230936
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02386875092983246
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.02503633201122284
step = 14, Training Accuracy: 0.6533333333333333
Validation Accuracy: 0.53875
params:  [0.42638356519913884, 0.01, 0.24596188457947854, 0.17815132620293817, 0.34197128737206345, 0.99, 0.43296116440224475, 0.4780750579062889, 0.2342878061372219, 0.07618606640844855, 0.6877002043295019, 0.99, 0.2539362833619465, 0.7693935564805283, 0.5039005675633721, 0.4481549747108071, 0.5830072397990057, 0.30234559244499315, 0.4768694871757687, 0.2552569145103685, 0.01, 0.27387915069275315, 0.3660455143798937, 0.5069196382970417]
Training loss = 0.029008965889612832
step = 0, Training Accuracy: 0.57
Validation Accuracy: 0.5625
Training loss = 0.026798320412635804
step = 1, Training Accuracy: 0.6633333333333333
Training loss = 0.02645419160525004
step = 2, Training Accuracy: 0.6466666666666666
Training loss = 0.022933250466982524
step = 3, Training Accuracy: 0.6666666666666666
Training loss = 0.022690226435661317
step = 4, Training Accuracy: 0.7
Training loss = 0.022657379110654196
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.021854332486788433
step = 6, Training Accuracy: 0.73
Training loss = 0.02105681836605072
step = 7, Training Accuracy: 0.73
Training loss = 0.02073252389828364
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.01992986261844635
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.018890755673249562
step = 10, Training Accuracy: 0.77
Training loss = 0.019107294380664826
step = 11, Training Accuracy: 0.7633333333333333
Training loss = 0.019073285659154258
step = 12, Training Accuracy: 0.7733333333333333
Training loss = 0.01612787902355194
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.016974758406480155
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.56625
[[0.6490142459033698, 0.2322671098540393, 0.9396946306764662, 0.01, 0.01, 0.99, 0.1961506638996729, 0.16464707456104152, 0.03808020580164573, 0.01, 0.5725886814698102, 0.19361440726225626, 0.19430802548959464, 0.49610134640912273, 0.3591576842195144, 0.43023041874587264, 0.9737638446522174, 0.12975891291524586, 0.4297539875829992, 0.9569089569224076, 0.39430656143020776, 0.4585207096486446, 0.5202584614063771, 0.01], [0.3366851826424452, 0.08404673748158739, 0.05644340288977179, 0.10966889132321338, 0.16530551528352785, 0.99, 0.7215399739986231, 0.39239170409407037, 0.15709974088602646, 0.01, 0.5626590785014266, 0.01, 0.58009680696429, 0.016020054646563198, 0.4959508325786198, 0.7556834553526812, 0.31948801633118096, 0.11249187506201698, 0.3198083930243585, 0.6127094055037016, 0.01, 0.5332767769129598, 0.3618083687120638, 0.5671366678656746], [0.6030854868705384, 0.10646407361846272, 0.6084908075142903, 0.5010598693676073, 0.17839696352589984, 0.99, 0.7437577467182594, 0.01, 0.22365506335374713, 0.2776356403341882, 0.356247728646413, 0.625996871470041, 0.4327123627544025, 0.24056962057796893, 0.24823474303320847, 0.47938403573485955, 0.8092998567487852, 0.3835028866522604, 0.29692339990821237, 0.38447531587510503, 0.2972251908184385, 0.01, 0.6084186816525242, 0.7114109699397908], [0.48925218826701455, 0.20170135602066952, 0.28938407183679427, 0.4905934971598668, 0.22912326480441214, 0.99, 0.3410719388698884, 0.43195866623123863, 0.8301761908661778, 0.18280622025797236, 0.25745191913204374, 0.17785226785123912, 0.7767015467557881, 0.44046710478685724, 0.43409843364874645, 0.01, 0.5275282329606507, 0.11029779486023979, 0.5261141204714513, 0.7465707513125671, 0.01, 0.9693930967442018, 0.3823675540603527, 0.01], [0.5888360831193729, 0.01, 0.489586469088427, 0.2904642027000838, 0.14229171056566325, 0.99, 0.5180690629823079, 0.32537917069856825, 0.01, 0.31099955860348316, 0.5772651172168293, 0.385706677182885, 0.8991891036964923, 0.4545485903776948, 0.45161428650019725, 0.01, 0.3971856450419692, 0.0738064031703923, 0.07538877738487576, 0.42962385998745595, 0.20153403699273828, 0.5783165816539668, 0.8428468443545061, 0.47557990980603226], [0.737309584112914, 0.6921428262847285, 0.5682379803812388, 0.01, 0.44405516521090094, 0.9033815451382974, 0.26502401229912886, 0.7983136548385952, 0.27972828528541466, 0.47541106252388776, 0.18130888588216854, 0.3539022257751415, 0.01, 0.18229063709847357, 0.5298954095262923, 0.030110681119168453, 0.2028391024607935, 0.8571366877429936, 0.6760571281400811, 0.07944468116231573, 0.6208382932808297, 0.22718376356157832, 0.01775502963163178, 0.30539015755969123], [0.577964838274527, 0.4238342780809495, 0.7890128387732965, 0.01, 0.5475786737022212, 0.99, 0.20759549893180357, 0.5302994159234822, 0.19816450639476102, 0.4754832096068696, 0.99, 0.11902790792542298, 0.4212550753229377, 0.4030094424816344, 0.29599258352645275, 0.3039344628490927, 0.575147855103763, 0.2890954019699558, 0.6565824696850693, 0.1038630160747171, 0.01, 0.7345468615331932, 0.746618047998347, 0.8190378947961842], [0.42638356519913884, 0.01, 0.24596188457947854, 0.17815132620293817, 0.34197128737206345, 0.99, 0.43296116440224475, 0.4780750579062889, 0.2342878061372219, 0.07618606640844855, 0.6877002043295019, 0.99, 0.2539362833619465, 0.7693935564805283, 0.5039005675633721, 0.4481549747108071, 0.5830072397990057, 0.30234559244499315, 0.4768694871757687, 0.2552569145103685, 0.01, 0.27387915069275315, 0.3660455143798937, 0.5069196382970417]]
gen	nevals	avg     	std      	min    	max    
0  	8     	0.552812	0.0234417	0.50875	0.58125
params:  [0.3875372296400714, 0.8808713402482942, 0.6061902921316666, 0.01, 0.5714228731588762, 0.99, 0.1640714506419956, 0.15307427755287695, 0.47035317538303234, 0.4342092351598245, 0.6320510813853413, 0.16123883226012292, 0.20739529823049024, 0.5450485800736401, 0.7762052994487461, 0.4315398708050562, 0.36845271951538466, 0.01, 0.6943471290182199, 0.48596245794374937, 0.0834893495760707, 0.6664066435832162, 0.01, 0.1823249137394996]
Training loss = 0.03815633873144786
step = 0, Training Accuracy: 0.5266666666666666
Validation Accuracy: 0.54
Training loss = 0.03169457236925761
step = 1, Training Accuracy: 0.5233333333333333
Training loss = 0.03137213965257009
step = 2, Training Accuracy: 0.54
Training loss = 0.031578993201255796
step = 3, Training Accuracy: 0.5566666666666666
Training loss = 0.02893336077531179
step = 4, Training Accuracy: 0.6466666666666666
Training loss = 0.0297244131565094
step = 5, Training Accuracy: 0.5933333333333334
Training loss = 0.027939447164535523
step = 6, Training Accuracy: 0.6066666666666667
Training loss = 0.02821562608083089
step = 7, Training Accuracy: 0.6133333333333333
Training loss = 0.027789012392361957
step = 8, Training Accuracy: 0.6466666666666666
Training loss = 0.025965009133021036
step = 9, Training Accuracy: 0.66
Training loss = 0.026794519225756326
step = 10, Training Accuracy: 0.6033333333333334
Training loss = 0.027769567767779033
step = 11, Training Accuracy: 0.63
Training loss = 0.025666308800379435
step = 12, Training Accuracy: 0.6733333333333333
Training loss = 0.02696514924367269
step = 13, Training Accuracy: 0.6666666666666666
Training loss = 0.025308965444564818
step = 14, Training Accuracy: 0.64
Validation Accuracy: 0.6825
params:  [0.07478685499650861, 0.252924506574827, 0.43755541094706585, 0.604389104345743, 0.3822660052191336, 0.8938079050456419, 0.3602909996963123, 0.5829155718896891, 0.5764315334783299, 0.01, 0.01, 0.7044677573117988, 0.3869665136598494, 0.6677639243102514, 0.6573816708260628, 0.01, 0.6014013893608787, 0.4644832154336942, 0.5282486231021576, 0.37743065526776265, 0.7901161329980468, 0.7376781422649552, 0.09416928024509627, 0.07035466363564473]
Training loss = 0.023180408676465352
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.67125
Training loss = 0.023258353074391683
step = 1, Training Accuracy: 0.68
Training loss = 0.021614503661791483
step = 2, Training Accuracy: 0.73
Training loss = 0.021124677658081056
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017832745015621185
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.018544452985127767
step = 5, Training Accuracy: 0.76
Training loss = 0.01711382865905762
step = 6, Training Accuracy: 0.78
Training loss = 0.0180039248863856
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.016267653604348502
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016699555118878683
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.017451998591423035
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.017181341846783955
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.0154824098944664
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014558062752087911
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.01644073655207952
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.66875
params:  [0.6162483875843768, 0.6770099387116351, 0.01, 0.16203426768537454, 0.6723721951777457, 0.99, 0.37307607528516334, 0.0846317592681819, 0.9630857171403453, 0.40658287815489846, 0.21088963267244976, 0.6196740785164202, 0.09499632809763908, 0.4842364011682022, 0.2252517264246148, 0.01, 0.24577982396834233, 0.1497834076185565, 0.693902181454802, 0.9513849514808495, 0.01, 0.7111567413103104, 0.06315744484635294, 0.780852290958317]
Training loss = 0.0325898536046346
step = 0, Training Accuracy: 0.6166666666666667
Validation Accuracy: 0.64
Training loss = 0.026079741319020588
step = 1, Training Accuracy: 0.6266666666666667
Training loss = 0.025754919449488323
step = 2, Training Accuracy: 0.6133333333333333
Training loss = 0.02450831711292267
step = 3, Training Accuracy: 0.66
Training loss = 0.02301895300547282
step = 4, Training Accuracy: 0.68
Training loss = 0.024086095492045084
step = 5, Training Accuracy: 0.6633333333333333
Training loss = 0.022459491988023123
step = 6, Training Accuracy: 0.6766666666666666
Training loss = 0.024345756967862446
step = 7, Training Accuracy: 0.6866666666666666
Training loss = 0.022966145078341167
step = 8, Training Accuracy: 0.6666666666666666
Training loss = 0.022752702037493387
step = 9, Training Accuracy: 0.6933333333333334
Training loss = 0.021073333223660787
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.01890113830566406
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.022284358541170755
step = 12, Training Accuracy: 0.7066666666666667
Training loss = 0.020077778498331707
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.020530932048956552
step = 14, Training Accuracy: 0.71
Validation Accuracy: 0.69625
params:  [0.39164866188931735, 0.8480738432522765, 0.01, 0.01, 0.15913427771482322, 0.9731780324970323, 0.4299717911835894, 0.99, 0.5422638298834728, 0.01, 0.48109087549373863, 0.9041640089319387, 0.709630944025758, 0.31456713228816646, 0.43490605673082305, 0.19556848216838107, 0.5653708028415657, 0.4924363813571906, 0.18069443495791093, 0.5500029189434243, 0.18076683060455842, 0.6718377976070952, 0.2537435666865145, 0.301492108192733]
Training loss = 0.027722104986508688
step = 0, Training Accuracy: 0.63
Validation Accuracy: 0.71375
Training loss = 0.025724398295084636
step = 1, Training Accuracy: 0.62
Training loss = 0.02492790182431539
step = 2, Training Accuracy: 0.66
Training loss = 0.024190515081087747
step = 3, Training Accuracy: 0.6766666666666666
Training loss = 0.02258734921614329
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.02463821311791738
step = 5, Training Accuracy: 0.66
Training loss = 0.023828031818072002
step = 6, Training Accuracy: 0.6966666666666667
Training loss = 0.02333840529123942
step = 7, Training Accuracy: 0.6833333333333333
Training loss = 0.022957268754641214
step = 8, Training Accuracy: 0.6766666666666666
Training loss = 0.02591217001279195
step = 9, Training Accuracy: 0.6666666666666666
Training loss = 0.021577643950780233
step = 10, Training Accuracy: 0.7033333333333334
Training loss = 0.022802743713061013
step = 11, Training Accuracy: 0.69
Training loss = 0.021118055284023284
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.022783576448758443
step = 13, Training Accuracy: 0.68
Training loss = 0.02089191069205602
step = 14, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7175
params:  [0.8345455407569855, 0.01, 0.4304994044812208, 0.01, 0.686815124097547, 0.99, 0.7005406407840213, 0.8382358970087966, 0.41331883901116195, 0.6172765739755691, 0.16997532372516208, 0.3287994532232975, 0.4642087121411554, 0.4106193042128521, 0.7403152130884567, 0.04462350715617844, 0.4355852319563824, 0.6167201364775888, 0.15408793317779862, 0.12178169609916745, 0.01, 0.19605860473909087, 0.4365553896941836, 0.3318664558546164]
Training loss = 0.021952570378780366
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.72375
Training loss = 0.020903850396474202
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.019501688480377196
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01619899074236552
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.018043246964613596
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015632657806078594
step = 5, Training Accuracy: 0.81
Training loss = 0.014294800013303757
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.016893459061781566
step = 7, Training Accuracy: 0.79
Training loss = 0.014050961335500082
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.012058903177579244
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.0110894180337588
step = 10, Training Accuracy: 0.8766666666666667
Training loss = 0.010434118260939916
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.012817566593488058
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.011881677508354187
step = 13, Training Accuracy: 0.86
Training loss = 0.010397953043381373
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.6975
params:  [0.5376158560593507, 0.01, 0.23495386881838137, 0.4938112139670019, 0.4845251046023977, 0.45644797996606135, 0.21319479371315297, 0.5536731655057956, 0.99, 0.01, 0.3648050061047645, 0.01, 0.92926055085538, 0.13287837667203758, 0.3473828857360762, 0.363218904605192, 0.8308268953804003, 0.42830612738483925, 0.5979549494399151, 0.19524762711435178, 0.031241356041850882, 0.7663889649262006, 0.4129440923341202, 0.34645392170292544]
Training loss = 0.030576943556467694
step = 0, Training Accuracy: 0.67
Validation Accuracy: 0.6525
Training loss = 0.021464624802271525
step = 1, Training Accuracy: 0.72
Training loss = 0.019348941246668496
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.017984566191832224
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.016538275082906086
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015624408225218454
step = 5, Training Accuracy: 0.8
Training loss = 0.014999527831872304
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014788163999716442
step = 7, Training Accuracy: 0.81
Training loss = 0.01657671630382538
step = 8, Training Accuracy: 0.79
Training loss = 0.013072742223739624
step = 9, Training Accuracy: 0.84
Training loss = 0.012986107369263967
step = 10, Training Accuracy: 0.84
Training loss = 0.015996687014897665
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01481305052836736
step = 12, Training Accuracy: 0.8
Training loss = 0.010610744257767995
step = 13, Training Accuracy: 0.89
Training loss = 0.009678267488876978
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.70625
params:  [0.22486014999950632, 0.45400719556090424, 0.4129215218176502, 0.7086627604712854, 0.25139254477722334, 0.99, 0.7165498072164862, 0.5071686173425632, 0.548172072184024, 0.28806726461842486, 0.01, 0.11755971536900557, 0.3386798175885486, 0.38631329547671034, 0.6483849275659298, 0.1257221549654359, 0.3045495848450878, 0.30146681821920773, 0.6309667469361145, 0.6190505165492646, 0.17418160727472984, 0.609702729354431, 0.0760894425513193, 0.01]
Training loss = 0.030561961034933725
step = 0, Training Accuracy: 0.6033333333333334
Validation Accuracy: 0.66375
Training loss = 0.023103190461794536
step = 1, Training Accuracy: 0.69
Training loss = 0.02211476743221283
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.021020556092262267
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.021148596107959747
step = 4, Training Accuracy: 0.73
Training loss = 0.019909839630126953
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.019411437213420868
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.020673089623451234
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.020649115840593976
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.0183912135163943
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.017335462868213653
step = 10, Training Accuracy: 0.75
Training loss = 0.017473679582277933
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017932413220405577
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01813709020614624
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.01958875556786855
step = 14, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.70375
params:  [0.2789990591277706, 0.01, 0.47752820549775876, 0.01, 0.1868534489028691, 0.4346287240383121, 0.2043143029038008, 0.4966008767312856, 0.9695140440414456, 0.569093695240624, 0.01, 0.27268853751296784, 0.08022619399434644, 0.038463152322992133, 0.3227381793968458, 0.04360024226725445, 0.1640253421807117, 0.5414848687864701, 0.49477941526667424, 0.40322499336916795, 0.05390952871297211, 0.3695288037268693, 0.99, 0.319087514136825]
Training loss = 0.025191994309425356
step = 0, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.6925
Training loss = 0.022462921341260274
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.02333177626132965
step = 2, Training Accuracy: 0.7
Training loss = 0.022421752313772837
step = 3, Training Accuracy: 0.7266666666666667
Training loss = 0.022933517396450043
step = 4, Training Accuracy: 0.71
Training loss = 0.019948612848917642
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.022252174615859984
step = 6, Training Accuracy: 0.75
Training loss = 0.02171858976284663
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.020496767858664194
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.01755358338356018
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.018575983544190725
step = 10, Training Accuracy: 0.76
Training loss = 0.018014410436153414
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.016454241772492727
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.016383458276589713
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01438061128060023
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.6825
[[0.3875372296400714, 0.8808713402482942, 0.6061902921316666, 0.01, 0.5714228731588762, 0.99, 0.1640714506419956, 0.15307427755287695, 0.47035317538303234, 0.4342092351598245, 0.6320510813853413, 0.16123883226012292, 0.20739529823049024, 0.5450485800736401, 0.7762052994487461, 0.4315398708050562, 0.36845271951538466, 0.01, 0.6943471290182199, 0.48596245794374937, 0.0834893495760707, 0.6664066435832162, 0.01, 0.1823249137394996], [0.07478685499650861, 0.252924506574827, 0.43755541094706585, 0.604389104345743, 0.3822660052191336, 0.8938079050456419, 0.3602909996963123, 0.5829155718896891, 0.5764315334783299, 0.01, 0.01, 0.7044677573117988, 0.3869665136598494, 0.6677639243102514, 0.6573816708260628, 0.01, 0.6014013893608787, 0.4644832154336942, 0.5282486231021576, 0.37743065526776265, 0.7901161329980468, 0.7376781422649552, 0.09416928024509627, 0.07035466363564473], [0.6162483875843768, 0.6770099387116351, 0.01, 0.16203426768537454, 0.6723721951777457, 0.99, 0.37307607528516334, 0.0846317592681819, 0.9630857171403453, 0.40658287815489846, 0.21088963267244976, 0.6196740785164202, 0.09499632809763908, 0.4842364011682022, 0.2252517264246148, 0.01, 0.24577982396834233, 0.1497834076185565, 0.693902181454802, 0.9513849514808495, 0.01, 0.7111567413103104, 0.06315744484635294, 0.780852290958317], [0.39164866188931735, 0.8480738432522765, 0.01, 0.01, 0.15913427771482322, 0.9731780324970323, 0.4299717911835894, 0.99, 0.5422638298834728, 0.01, 0.48109087549373863, 0.9041640089319387, 0.709630944025758, 0.31456713228816646, 0.43490605673082305, 0.19556848216838107, 0.5653708028415657, 0.4924363813571906, 0.18069443495791093, 0.5500029189434243, 0.18076683060455842, 0.6718377976070952, 0.2537435666865145, 0.301492108192733], [0.8345455407569855, 0.01, 0.4304994044812208, 0.01, 0.686815124097547, 0.99, 0.7005406407840213, 0.8382358970087966, 0.41331883901116195, 0.6172765739755691, 0.16997532372516208, 0.3287994532232975, 0.4642087121411554, 0.4106193042128521, 0.7403152130884567, 0.04462350715617844, 0.4355852319563824, 0.6167201364775888, 0.15408793317779862, 0.12178169609916745, 0.01, 0.19605860473909087, 0.4365553896941836, 0.3318664558546164], [0.5376158560593507, 0.01, 0.23495386881838137, 0.4938112139670019, 0.4845251046023977, 0.45644797996606135, 0.21319479371315297, 0.5536731655057956, 0.99, 0.01, 0.3648050061047645, 0.01, 0.92926055085538, 0.13287837667203758, 0.3473828857360762, 0.363218904605192, 0.8308268953804003, 0.42830612738483925, 0.5979549494399151, 0.19524762711435178, 0.031241356041850882, 0.7663889649262006, 0.4129440923341202, 0.34645392170292544], [0.22486014999950632, 0.45400719556090424, 0.4129215218176502, 0.7086627604712854, 0.25139254477722334, 0.99, 0.7165498072164862, 0.5071686173425632, 0.548172072184024, 0.28806726461842486, 0.01, 0.11755971536900557, 0.3386798175885486, 0.38631329547671034, 0.6483849275659298, 0.1257221549654359, 0.3045495848450878, 0.30146681821920773, 0.6309667469361145, 0.6190505165492646, 0.17418160727472984, 0.609702729354431, 0.0760894425513193, 0.01], [0.2789990591277706, 0.01, 0.47752820549775876, 0.01, 0.1868534489028691, 0.4346287240383121, 0.2043143029038008, 0.4966008767312856, 0.9695140440414456, 0.569093695240624, 0.01, 0.27268853751296784, 0.08022619399434644, 0.038463152322992133, 0.3227381793968458, 0.04360024226725445, 0.1640253421807117, 0.5414848687864701, 0.49477941526667424, 0.40322499336916795, 0.05390952871297211, 0.3695288037268693, 0.99, 0.319087514136825]]
1  	8     	0.694375	0.0146442	0.66875	0.7175 
params:  [0.48406257704730216, 0.7942266357750813, 0.05023837102331745, 0.21029662460700035, 0.3243205633869997, 0.7177406540145179, 0.01, 0.7888377391028076, 0.4985632652791722, 0.01, 0.99, 0.5979192247018121, 0.6626779277026332, 0.5630903730784813, 0.7139135570410813, 0.01, 0.2830437307642066, 0.5934067222347044, 0.4675582496780958, 0.499302117964885, 0.01, 0.8124830084237402, 0.30540817664174214, 0.03252228080899788]
Training loss = 0.02544658988714218
step = 0, Training Accuracy: 0.6633333333333333
Validation Accuracy: 0.6075
Training loss = 0.024484635988871257
step = 1, Training Accuracy: 0.67
Training loss = 0.024364683032035827
step = 2, Training Accuracy: 0.63
Training loss = 0.02322454333305359
step = 3, Training Accuracy: 0.6866666666666666
Training loss = 0.02059748907883962
step = 4, Training Accuracy: 0.72
Training loss = 0.02282526751359304
step = 5, Training Accuracy: 0.6833333333333333
Training loss = 0.020904189646244048
step = 6, Training Accuracy: 0.7566666666666667
Training loss = 0.020322750409444174
step = 7, Training Accuracy: 0.7033333333333334
Training loss = 0.02028469890356064
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.02005692372719447
step = 9, Training Accuracy: 0.7133333333333334
Training loss = 0.018915570080280303
step = 10, Training Accuracy: 0.7266666666666667
Training loss = 0.01830071896314621
step = 11, Training Accuracy: 0.76
Training loss = 0.018205276429653167
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01938084642092387
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.018999569614728293
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.72
params:  [0.27624537679821093, 0.8679496455187833, 0.2779987507817357, 0.1657137246700604, 0.5958314347406912, 0.8689424063623578, 0.1355936742618027, 0.6874964961723096, 0.46563730130428366, 0.24817889380767358, 0.08287175021757887, 0.362831526198564, 0.4477087584366628, 0.11481752938707215, 0.6851486735726507, 0.034186224764878426, 0.99, 0.4709865532207981, 0.4165851625059514, 0.9707089318713855, 0.01, 0.6348456714999093, 0.21040361209965558, 0.4778436271855737]
Training loss = 0.024403871695200603
step = 0, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.715
Training loss = 0.023947257002194723
step = 1, Training Accuracy: 0.7
Training loss = 0.021352716286977134
step = 2, Training Accuracy: 0.7333333333333333
Training loss = 0.02373022516568502
step = 3, Training Accuracy: 0.6666666666666666
Training loss = 0.02225538750489553
step = 4, Training Accuracy: 0.71
Training loss = 0.01966734210650126
step = 5, Training Accuracy: 0.72
Training loss = 0.019490311443805693
step = 6, Training Accuracy: 0.73
Training loss = 0.01739149570465088
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.018938909769058227
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.018415227035681405
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.018479041357835135
step = 10, Training Accuracy: 0.73
Training loss = 0.019999168713887534
step = 11, Training Accuracy: 0.7266666666666667
Training loss = 0.019883075853188832
step = 12, Training Accuracy: 0.71
Training loss = 0.020434515178203584
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.01978646566470464
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.72375
params:  [0.684270571731397, 0.2677047218669207, 0.05531736608911132, 0.01, 0.2347320813808775, 0.5268352103308749, 0.12427586986045308, 0.99, 0.6789371981763527, 0.01, 0.6253163281187344, 0.01, 0.3803145167476729, 0.20699069978715384, 0.0886804962310076, 0.018135719528223487, 0.08624729968573286, 0.7466539373216896, 0.4018071442395745, 0.26753711284285137, 0.01, 0.6464446483638834, 0.6631471030413282, 0.4881981263322704]
Training loss = 0.022192967037359873
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.74125
Training loss = 0.022272746364275616
step = 1, Training Accuracy: 0.6966666666666667
Training loss = 0.018589525520801543
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.019184690217177072
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.018800744613011677
step = 4, Training Accuracy: 0.73
Training loss = 0.01982341766357422
step = 5, Training Accuracy: 0.74
Training loss = 0.01681985209385554
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.01843582699696223
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.01805978884299596
step = 8, Training Accuracy: 0.7366666666666667
Training loss = 0.019184127549330392
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.015744752486546835
step = 10, Training Accuracy: 0.8
Training loss = 0.015049432615439097
step = 11, Training Accuracy: 0.83
Training loss = 0.017158154249191284
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.017684252858161928
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01677479495604833
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.7
params:  [0.37211370538618804, 0.453164497193789, 0.3016171514009313, 0.1593066411937284, 0.01, 0.99, 0.2013937771274171, 0.7687751423582535, 0.7539285788394859, 0.01, 0.12905024281951513, 0.4745072535910194, 0.99, 0.2697370163411116, 0.2754150335524528, 0.422876065026403, 0.7589646426793183, 0.01, 0.7743642312030377, 0.7009988224704509, 0.27790372398643814, 0.99, 0.19064752400108514, 0.24038553030571908]
Training loss = 0.024029947916666666
step = 0, Training Accuracy: 0.68
Validation Accuracy: 0.73625
Training loss = 0.019321105281511944
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018124070167541504
step = 2, Training Accuracy: 0.77
Training loss = 0.01760870188474655
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.018215839664141337
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01651535282532374
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.01462356095512708
step = 6, Training Accuracy: 0.8
Training loss = 0.01653599927822749
step = 7, Training Accuracy: 0.79
Training loss = 0.019580902258555095
step = 8, Training Accuracy: 0.77
Training loss = 0.0167899888753891
step = 9, Training Accuracy: 0.76
Training loss = 0.018221596479415892
step = 10, Training Accuracy: 0.77
Training loss = 0.015478086670239767
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01594220022360484
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.012574859460194905
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.016670405666033426
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.74625
params:  [0.09690509429568772, 0.5004936995371027, 0.32830247644838606, 0.3073738365765965, 0.01, 0.9111697890383643, 0.7054146103677308, 0.5515975618668854, 0.7552214281589806, 0.6763322316903928, 0.3965790329166321, 0.28202001588134495, 0.6088962326262801, 0.01, 0.496956016139157, 0.5779099502353762, 0.7064635792156448, 0.6555792271030952, 0.34844736926074216, 0.754109616978335, 0.24079064076937956, 0.5539621260398307, 0.6744687866855529, 0.6382169233554436]
Training loss = 0.02570606052875519
step = 0, Training Accuracy: 0.6733333333333333
Validation Accuracy: 0.72
Training loss = 0.023974916140238445
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.020973910689353944
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.02183466056982676
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.02089383820692698
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.020513527790705362
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.02065721482038498
step = 6, Training Accuracy: 0.77
Training loss = 0.019935757915178935
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.02009423146645228
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.02125871568918228
step = 9, Training Accuracy: 0.73
Training loss = 0.02112533320983251
step = 10, Training Accuracy: 0.7333333333333333
Training loss = 0.019127290646235147
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018763874173164368
step = 12, Training Accuracy: 0.76
Training loss = 0.018959041635195413
step = 13, Training Accuracy: 0.7633333333333333
Training loss = 0.020123662153879802
step = 14, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.76625
params:  [0.059316391622907416, 0.5671144065741983, 0.36199744474514006, 0.3757224560777295, 0.031323025982508024, 0.6093517485525679, 0.6284808839413366, 0.99, 0.2671587437759867, 0.01, 0.3435702234983135, 0.44509254871372994, 0.8266059142524831, 0.28756209637386115, 0.5699400150066771, 0.2428639354010323, 0.43755485461596166, 0.5389670151180805, 0.011766248369977084, 0.16648193920184634, 0.4269245562538324, 0.99, 0.3183499663159376, 0.252775154159703]
Training loss = 0.02176673859357834
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.69375
Training loss = 0.01960341552893321
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.019746893346309663
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01751613090435664
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016517471969127655
step = 4, Training Accuracy: 0.78
Training loss = 0.017923409938812255
step = 5, Training Accuracy: 0.78
Training loss = 0.015271675984064738
step = 6, Training Accuracy: 0.79
Training loss = 0.016263671418031058
step = 7, Training Accuracy: 0.76
Training loss = 0.015397342244784036
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.014410049219926198
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015038302938143412
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01423843890428543
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.012014397333065668
step = 12, Training Accuracy: 0.8633333333333333
Training loss = 0.012549324929714202
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.012254007359345754
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.72
params:  [0.6165295925197137, 0.43718549408402674, 0.016089415450639744, 0.31164273328560865, 0.3815024149354754, 0.9364696773443864, 0.444887731919591, 0.8863924357651377, 0.25168442301751465, 0.01, 0.6225009962782917, 0.7605189832006819, 0.5913141210550861, 0.24099641279805745, 0.46991629397391654, 0.8871701778659499, 0.40096339791341307, 0.16449926339870796, 0.7251844331676616, 0.4011091672751968, 0.2140098979835554, 0.7947873090210655, 0.147568521968669, 0.13853448091360604]
Training loss = 0.02894733667373657
step = 0, Training Accuracy: 0.6566666666666666
Validation Accuracy: 0.64
Training loss = 0.02244731883207957
step = 1, Training Accuracy: 0.68
Training loss = 0.022477246820926666
step = 2, Training Accuracy: 0.69
Training loss = 0.01897142897049586
step = 3, Training Accuracy: 0.7233333333333334
Training loss = 0.018326317171255747
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.019977332452932994
step = 5, Training Accuracy: 0.7166666666666667
Training loss = 0.020574266612529753
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.018155890107154845
step = 7, Training Accuracy: 0.76
Training loss = 0.017557602127393088
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.019034361044565837
step = 9, Training Accuracy: 0.76
Training loss = 0.018258136212825776
step = 10, Training Accuracy: 0.7433333333333333
Training loss = 0.015903526643911998
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015600089927514394
step = 12, Training Accuracy: 0.78
Training loss = 0.02031472831964493
step = 13, Training Accuracy: 0.7366666666666667
Training loss = 0.01806586782137553
step = 14, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.74625
params:  [0.5233845577982604, 0.4675131903063711, 0.11695154789278978, 0.023971740595056606, 0.0719463281725464, 0.6166134145768811, 0.4266993605233484, 0.7130663364072863, 0.5977283718061422, 0.15685974732029817, 0.4289279022553224, 0.40058360364508866, 0.37712995851014736, 0.036492666068687674, 0.2481246606858963, 0.1503209846224639, 0.08167601167214145, 0.3076649514328853, 0.01, 0.455781319523161, 0.08565648553762159, 0.707202997123683, 0.41606082946483763, 0.5527647612154872]
Training loss = 0.02150829037030538
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.765
Training loss = 0.01887739936510722
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.019137873351573943
step = 2, Training Accuracy: 0.77
Training loss = 0.016519117256005605
step = 3, Training Accuracy: 0.8
Training loss = 0.016631618936856586
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.01735447843869527
step = 5, Training Accuracy: 0.78
Training loss = 0.016418516238530478
step = 6, Training Accuracy: 0.77
Training loss = 0.016634572247664133
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.01564529259999593
step = 8, Training Accuracy: 0.8
Training loss = 0.015069486697514851
step = 9, Training Accuracy: 0.84
Training loss = 0.014828988711039225
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.014669201970100403
step = 11, Training Accuracy: 0.82
Training loss = 0.014927377899487813
step = 12, Training Accuracy: 0.83
Training loss = 0.01486103634039561
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01384747823079427
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.76625
[[0.48406257704730216, 0.7942266357750813, 0.05023837102331745, 0.21029662460700035, 0.3243205633869997, 0.7177406540145179, 0.01, 0.7888377391028076, 0.4985632652791722, 0.01, 0.99, 0.5979192247018121, 0.6626779277026332, 0.5630903730784813, 0.7139135570410813, 0.01, 0.2830437307642066, 0.5934067222347044, 0.4675582496780958, 0.499302117964885, 0.01, 0.8124830084237402, 0.30540817664174214, 0.03252228080899788], [0.27624537679821093, 0.8679496455187833, 0.2779987507817357, 0.1657137246700604, 0.5958314347406912, 0.8689424063623578, 0.1355936742618027, 0.6874964961723096, 0.46563730130428366, 0.24817889380767358, 0.08287175021757887, 0.362831526198564, 0.4477087584366628, 0.11481752938707215, 0.6851486735726507, 0.034186224764878426, 0.99, 0.4709865532207981, 0.4165851625059514, 0.9707089318713855, 0.01, 0.6348456714999093, 0.21040361209965558, 0.4778436271855737], [0.684270571731397, 0.2677047218669207, 0.05531736608911132, 0.01, 0.2347320813808775, 0.5268352103308749, 0.12427586986045308, 0.99, 0.6789371981763527, 0.01, 0.6253163281187344, 0.01, 0.3803145167476729, 0.20699069978715384, 0.0886804962310076, 0.018135719528223487, 0.08624729968573286, 0.7466539373216896, 0.4018071442395745, 0.26753711284285137, 0.01, 0.6464446483638834, 0.6631471030413282, 0.4881981263322704], [0.37211370538618804, 0.453164497193789, 0.3016171514009313, 0.1593066411937284, 0.01, 0.99, 0.2013937771274171, 0.7687751423582535, 0.7539285788394859, 0.01, 0.12905024281951513, 0.4745072535910194, 0.99, 0.2697370163411116, 0.2754150335524528, 0.422876065026403, 0.7589646426793183, 0.01, 0.7743642312030377, 0.7009988224704509, 0.27790372398643814, 0.99, 0.19064752400108514, 0.24038553030571908], [0.09690509429568772, 0.5004936995371027, 0.32830247644838606, 0.3073738365765965, 0.01, 0.9111697890383643, 0.7054146103677308, 0.5515975618668854, 0.7552214281589806, 0.6763322316903928, 0.3965790329166321, 0.28202001588134495, 0.6088962326262801, 0.01, 0.496956016139157, 0.5779099502353762, 0.7064635792156448, 0.6555792271030952, 0.34844736926074216, 0.754109616978335, 0.24079064076937956, 0.5539621260398307, 0.6744687866855529, 0.6382169233554436], [0.059316391622907416, 0.5671144065741983, 0.36199744474514006, 0.3757224560777295, 0.031323025982508024, 0.6093517485525679, 0.6284808839413366, 0.99, 0.2671587437759867, 0.01, 0.3435702234983135, 0.44509254871372994, 0.8266059142524831, 0.28756209637386115, 0.5699400150066771, 0.2428639354010323, 0.43755485461596166, 0.5389670151180805, 0.011766248369977084, 0.16648193920184634, 0.4269245562538324, 0.99, 0.3183499663159376, 0.252775154159703], [0.6165295925197137, 0.43718549408402674, 0.016089415450639744, 0.31164273328560865, 0.3815024149354754, 0.9364696773443864, 0.444887731919591, 0.8863924357651377, 0.25168442301751465, 0.01, 0.6225009962782917, 0.7605189832006819, 0.5913141210550861, 0.24099641279805745, 0.46991629397391654, 0.8871701778659499, 0.40096339791341307, 0.16449926339870796, 0.7251844331676616, 0.4011091672751968, 0.2140098979835554, 0.7947873090210655, 0.147568521968669, 0.13853448091360604], [0.5233845577982604, 0.4675131903063711, 0.11695154789278978, 0.023971740595056606, 0.0719463281725464, 0.6166134145768811, 0.4266993605233484, 0.7130663364072863, 0.5977283718061422, 0.15685974732029817, 0.4289279022553224, 0.40058360364508866, 0.37712995851014736, 0.036492666068687674, 0.2481246606858963, 0.1503209846224639, 0.08167601167214145, 0.3076649514328853, 0.01, 0.455781319523161, 0.08565648553762159, 0.707202997123683, 0.41606082946483763, 0.5527647612154872]]
2  	8     	0.736094	0.0223558	0.7    	0.76625
params:  [0.01, 0.49099864663122983, 0.5694691376448697, 0.36676650118623777, 0.01, 0.3800736546684577, 0.6117336135092296, 0.7113222808189507, 0.9007315160470983, 0.19100376718959108, 0.6839632109019338, 0.33848235781270286, 0.40288429259227854, 0.01, 0.11152872760906873, 0.5784635879199014, 0.3576891456982781, 0.7309713605234873, 0.16175833539433834, 0.5919475585200975, 0.07132584299873407, 0.48457504798258333, 0.47779167852425575, 0.34927666762882803]
Training loss = 0.02340312918027242
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.7575
Training loss = 0.02191474914550781
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.01947570065657298
step = 2, Training Accuracy: 0.74
Training loss = 0.0209946741660436
step = 3, Training Accuracy: 0.72
Training loss = 0.02106289972861608
step = 4, Training Accuracy: 0.73
Training loss = 0.018915376762549084
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.0188522141178449
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018610091507434846
step = 7, Training Accuracy: 0.78
Training loss = 0.016883128782113392
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01911436786254247
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.018319495916366578
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.01851516177256902
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.017600518465042115
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.016718116998672487
step = 13, Training Accuracy: 0.78
Training loss = 0.017059786319732664
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.75125
params:  [0.36739375821775583, 0.15231281084656217, 0.4424027359708502, 0.19432695511904996, 0.01, 0.99, 0.7510591333710058, 0.4054445749655134, 0.9067512815073054, 0.2659634610088857, 0.46848362800176446, 0.6809455542743065, 0.8976310430536687, 0.01, 0.3859967096295573, 0.5562195566341931, 0.9347402533253305, 0.805334757432707, 0.26933137182314415, 0.7713905687940066, 0.42872797300345244, 0.2871346380400394, 0.4714230376485993, 0.8740721907170028]
Training loss = 0.020902481079101563
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.755
Training loss = 0.02039999544620514
step = 1, Training Accuracy: 0.75
Training loss = 0.01909167488416036
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.019372660120328268
step = 3, Training Accuracy: 0.75
Training loss = 0.017324240605036418
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.019496421019236248
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.016444201568762463
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.017074618339538574
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.017949109673500063
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.015489798386891683
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.014882393777370453
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.013921773036321004
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.014936576088269551
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.014096219440301259
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.013501259485880534
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.75625
params:  [0.01959421198565997, 0.16292653937371088, 0.2752290257466255, 0.3228144507099703, 0.01, 0.99, 0.47142556820467574, 0.47869159168770714, 0.6798427767624654, 0.4370902372925741, 0.3482348812015601, 0.20515416608576692, 0.33217621167245676, 0.16777042337601228, 0.625650154104748, 0.27197151148523857, 0.7665038506126434, 0.49272476541040977, 0.06921900743622333, 0.3607800030607656, 0.1622606418872402, 0.7386932335431841, 0.8333909186300308, 0.01]
Training loss = 0.023944438298543293
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7025
Training loss = 0.01966372976700465
step = 1, Training Accuracy: 0.75
Training loss = 0.02026306966940562
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.018023975789546967
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.016877785623073578
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.017796639303366342
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.014322941501935324
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.015317464172840118
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015318792859713236
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.015569479366143545
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.015201778610547384
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.01329074482123057
step = 11, Training Accuracy: 0.85
Training loss = 0.013206901053587596
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.011818402161200841
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.01246200606226921
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.75
params:  [0.14795445649131872, 0.4310860143845043, 0.4484180679174452, 0.5999343395483163, 0.41658873762976095, 0.5696202372056125, 0.9860108400789933, 0.7511159200518406, 0.99, 0.5164866597224961, 0.14597404773442552, 0.14257115154067443, 0.5027464533336192, 0.22673042068482102, 0.5333008101341463, 0.40970341548284894, 0.44200574978343343, 0.6817502595957361, 0.18687772876238448, 0.7242708753496063, 0.4343679532237374, 0.7506653128776486, 0.3377374594413821, 0.01]
Training loss = 0.02616030305624008
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.75625
Training loss = 0.021235935588677726
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.02091152548789978
step = 2, Training Accuracy: 0.71
Training loss = 0.021235236326853434
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.020024889012177784
step = 4, Training Accuracy: 0.7133333333333334
Training loss = 0.018308149377504985
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.02045232892036438
step = 6, Training Accuracy: 0.7
Training loss = 0.020198785463968912
step = 7, Training Accuracy: 0.7233333333333334
Training loss = 0.01804981768131256
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.020178700884183248
step = 9, Training Accuracy: 0.7266666666666667
Training loss = 0.018778151869773864
step = 10, Training Accuracy: 0.75
Training loss = 0.01783843517303467
step = 11, Training Accuracy: 0.74
Training loss = 0.02052725632985433
step = 12, Training Accuracy: 0.7633333333333333
Training loss = 0.017304595410823822
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.017198347548643748
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.745
params:  [0.5188194917208395, 0.30071282947710487, 0.6281604717256973, 0.37305294527920485, 0.37685477128183686, 0.3825807300039009, 0.99, 0.46240137688202704, 0.1914773034765102, 0.6832010832220897, 0.17011253681954328, 0.398017288998913, 0.665796593156221, 0.048017651134037734, 0.7705633347107677, 0.08798731405572757, 0.5109962224199547, 0.01, 0.516273148409399, 0.7748725356044338, 0.01, 0.5651228502321725, 0.42027622819027366, 0.6307213967391216]
Training loss = 0.02500960568586985
step = 0, Training Accuracy: 0.6633333333333333
Validation Accuracy: 0.7575
Training loss = 0.02038950483004252
step = 1, Training Accuracy: 0.7
Training loss = 0.020690822899341585
step = 2, Training Accuracy: 0.6933333333333334
Training loss = 0.02019534895817439
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.018766854107379913
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.01858758717775345
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.018598604400952658
step = 6, Training Accuracy: 0.76
Training loss = 0.018546972970167795
step = 7, Training Accuracy: 0.72
Training loss = 0.0187915833791097
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.017953492403030395
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01639491468667984
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.01776377171278
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01632420192162196
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.01644726057847341
step = 13, Training Accuracy: 0.79
Training loss = 0.01662369360526403
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7675
params:  [0.08703771177324382, 0.5663972006349398, 0.42535825144280237, 0.2273446852235681, 0.01, 0.8295712896846248, 0.581708010569758, 0.5643265485472791, 0.7236265043935967, 0.49691223855717137, 0.6087178956780637, 0.6004981889249293, 0.6673650111094562, 0.31488784731245373, 0.036641195328853016, 0.5564465400634557, 0.16988188750789618, 0.39138388509912797, 0.08939017580333011, 0.323550186852402, 0.2716956191648898, 0.5783870075965377, 0.9116954885809974, 0.22872127145517163]
Training loss = 0.02573739449183146
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.74625
Training loss = 0.020158259471257527
step = 1, Training Accuracy: 0.7
Training loss = 0.021281842986742655
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.01824327290058136
step = 3, Training Accuracy: 0.78
Training loss = 0.019430435200532278
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.01913669168949127
step = 5, Training Accuracy: 0.7233333333333334
Training loss = 0.019769932627677917
step = 6, Training Accuracy: 0.75
Training loss = 0.018484873175621034
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.018961976865927377
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.0200542938709259
step = 9, Training Accuracy: 0.74
Training loss = 0.017351803779602052
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01569366047779719
step = 11, Training Accuracy: 0.81
Training loss = 0.01552530566851298
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.016353820860385896
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.016440131564935047
step = 14, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.76125
params:  [0.3018432440311667, 0.574724219103867, 0.11769641730464464, 0.01, 0.30857283120372364, 0.43086999841305124, 0.62996196636831, 0.99, 0.7092081268836962, 0.8625642245521834, 0.03295363408190455, 0.18238319998571353, 0.9829330448584097, 0.01, 0.474030306591572, 0.22357162606166994, 0.36527086524202274, 0.5315025788619089, 0.4842842947397439, 0.5139998744111063, 0.01, 0.780186264830504, 0.3983830959663699, 0.8651761650017538]
Training loss = 0.023020386695861816
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.755
Training loss = 0.021743610401948294
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.021073852181434632
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.0218327130873998
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.022118829488754273
step = 4, Training Accuracy: 0.72
Training loss = 0.02149362176656723
step = 5, Training Accuracy: 0.7066666666666667
Training loss = 0.02018067757288615
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.019240285654862722
step = 7, Training Accuracy: 0.72
Training loss = 0.019107969800631206
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.021079188982645672
step = 9, Training Accuracy: 0.71
Training loss = 0.01785037577152252
step = 10, Training Accuracy: 0.79
Training loss = 0.017865414321422576
step = 11, Training Accuracy: 0.75
Training loss = 0.018817942440509796
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.01740394761164983
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.020434510310490928
step = 14, Training Accuracy: 0.74
Validation Accuracy: 0.745
params:  [0.5307358039043946, 0.2983379807720078, 0.5921704488884758, 0.05775297208070898, 0.18271487403724776, 0.9127245229790271, 0.415630476652992, 0.4500938193572402, 0.9868603436119904, 0.4401179316831422, 0.7661985675052295, 0.99, 0.808927656467171, 0.4616797969315375, 0.1653487536813906, 0.16389430288690587, 0.644887199900259, 0.46632131678692634, 0.149406575970357, 0.4169992123149865, 0.01, 0.4609728618124652, 0.33610560475030715, 0.29494171641213984]
Training loss = 0.01880545973777771
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77
Training loss = 0.018737154205640157
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01705388496319453
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.018453693290551503
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016618691384792328
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.013544362684090932
step = 5, Training Accuracy: 0.8533333333333334
Training loss = 0.015258720715840658
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014162143270174662
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.015251727898915608
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.014838303327560426
step = 9, Training Accuracy: 0.83
Training loss = 0.014194158414999644
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.014700661698977152
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.012947110583384831
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.015988679975271224
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.013309555848439535
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.735
[[0.01, 0.49099864663122983, 0.5694691376448697, 0.36676650118623777, 0.01, 0.3800736546684577, 0.6117336135092296, 0.7113222808189507, 0.9007315160470983, 0.19100376718959108, 0.6839632109019338, 0.33848235781270286, 0.40288429259227854, 0.01, 0.11152872760906873, 0.5784635879199014, 0.3576891456982781, 0.7309713605234873, 0.16175833539433834, 0.5919475585200975, 0.07132584299873407, 0.48457504798258333, 0.47779167852425575, 0.34927666762882803], [0.36739375821775583, 0.15231281084656217, 0.4424027359708502, 0.19432695511904996, 0.01, 0.99, 0.7510591333710058, 0.4054445749655134, 0.9067512815073054, 0.2659634610088857, 0.46848362800176446, 0.6809455542743065, 0.8976310430536687, 0.01, 0.3859967096295573, 0.5562195566341931, 0.9347402533253305, 0.805334757432707, 0.26933137182314415, 0.7713905687940066, 0.42872797300345244, 0.2871346380400394, 0.4714230376485993, 0.8740721907170028], [0.01959421198565997, 0.16292653937371088, 0.2752290257466255, 0.3228144507099703, 0.01, 0.99, 0.47142556820467574, 0.47869159168770714, 0.6798427767624654, 0.4370902372925741, 0.3482348812015601, 0.20515416608576692, 0.33217621167245676, 0.16777042337601228, 0.625650154104748, 0.27197151148523857, 0.7665038506126434, 0.49272476541040977, 0.06921900743622333, 0.3607800030607656, 0.1622606418872402, 0.7386932335431841, 0.8333909186300308, 0.01], [0.14795445649131872, 0.4310860143845043, 0.4484180679174452, 0.5999343395483163, 0.41658873762976095, 0.5696202372056125, 0.9860108400789933, 0.7511159200518406, 0.99, 0.5164866597224961, 0.14597404773442552, 0.14257115154067443, 0.5027464533336192, 0.22673042068482102, 0.5333008101341463, 0.40970341548284894, 0.44200574978343343, 0.6817502595957361, 0.18687772876238448, 0.7242708753496063, 0.4343679532237374, 0.7506653128776486, 0.3377374594413821, 0.01], [0.5188194917208395, 0.30071282947710487, 0.6281604717256973, 0.37305294527920485, 0.37685477128183686, 0.3825807300039009, 0.99, 0.46240137688202704, 0.1914773034765102, 0.6832010832220897, 0.17011253681954328, 0.398017288998913, 0.665796593156221, 0.048017651134037734, 0.7705633347107677, 0.08798731405572757, 0.5109962224199547, 0.01, 0.516273148409399, 0.7748725356044338, 0.01, 0.5651228502321725, 0.42027622819027366, 0.6307213967391216], [0.08703771177324382, 0.5663972006349398, 0.42535825144280237, 0.2273446852235681, 0.01, 0.8295712896846248, 0.581708010569758, 0.5643265485472791, 0.7236265043935967, 0.49691223855717137, 0.6087178956780637, 0.6004981889249293, 0.6673650111094562, 0.31488784731245373, 0.036641195328853016, 0.5564465400634557, 0.16988188750789618, 0.39138388509912797, 0.08939017580333011, 0.323550186852402, 0.2716956191648898, 0.5783870075965377, 0.9116954885809974, 0.22872127145517163], [0.3018432440311667, 0.574724219103867, 0.11769641730464464, 0.01, 0.30857283120372364, 0.43086999841305124, 0.62996196636831, 0.99, 0.7092081268836962, 0.8625642245521834, 0.03295363408190455, 0.18238319998571353, 0.9829330448584097, 0.01, 0.474030306591572, 0.22357162606166994, 0.36527086524202274, 0.5315025788619089, 0.4842842947397439, 0.5139998744111063, 0.01, 0.780186264830504, 0.3983830959663699, 0.8651761650017538], [0.5307358039043946, 0.2983379807720078, 0.5921704488884758, 0.05775297208070898, 0.18271487403724776, 0.9127245229790271, 0.415630476652992, 0.4500938193572402, 0.9868603436119904, 0.4401179316831422, 0.7661985675052295, 0.99, 0.808927656467171, 0.4616797969315375, 0.1653487536813906, 0.16389430288690587, 0.644887199900259, 0.46632131678692634, 0.149406575970357, 0.4169992123149865, 0.01, 0.4609728618124652, 0.33610560475030715, 0.29494171641213984]]
3  	8     	0.751406	0.00956959	0.735  	0.7675 
params:  [0.2893999251512786, 0.24523104932906276, 0.7680627940672102, 0.3482166960621, 0.05820463635928419, 0.39326050309844857, 0.7722273412820304, 0.2927906630990976, 0.6449807498785003, 0.16524562872141046, 0.38610641492853154, 0.2740681658245411, 0.7749106239758858, 0.3732659888142295, 0.613724782370769, 0.2675997271007921, 0.01, 0.01, 0.1947584899013606, 0.99, 0.6634739103158074, 0.8056560707932225, 0.5192016282667515, 0.6294984647551699]
Training loss = 0.02331503470738729
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.7425
Training loss = 0.020082860589027404
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.019066693981488546
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.01836652199427287
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.0191666250427564
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.01648719886938731
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.018157151142756144
step = 6, Training Accuracy: 0.76
Training loss = 0.016711709201335908
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015132373770078023
step = 8, Training Accuracy: 0.8
Training loss = 0.017026530901590984
step = 9, Training Accuracy: 0.7866666666666666
Training loss = 0.015303548574447632
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.014274822324514389
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.015652200480302175
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014396149863799413
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.014579973071813584
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74625
params:  [0.41298877918323984, 0.4787005167757402, 0.5517094874431819, 0.511791855087905, 0.0560689781574574, 0.4843499654320054, 0.8508741669878424, 0.5618067390300576, 0.40053529072477734, 0.3423190041851708, 0.4254200857920609, 0.625855302552597, 0.6220042605391028, 0.06027921789384401, 0.6336733462730187, 0.23643662353956135, 0.519295881669829, 0.10569413964296567, 0.3628816716184989, 0.9028294381765509, 0.01, 0.6322381330392457, 0.6803899106363895, 0.488830336901363]
Training loss = 0.02173789213101069
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.73125
Training loss = 0.021833436489105226
step = 1, Training Accuracy: 0.77
Training loss = 0.019998721778392792
step = 2, Training Accuracy: 0.7166666666666667
Training loss = 0.02199111372232437
step = 3, Training Accuracy: 0.6966666666666667
Training loss = 0.020427560806274413
step = 4, Training Accuracy: 0.71
Training loss = 0.02150208463271459
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.01810163011153539
step = 6, Training Accuracy: 0.74
Training loss = 0.01824575940767924
step = 7, Training Accuracy: 0.77
Training loss = 0.020466620127360027
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.01812451700369517
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.02093494345744451
step = 10, Training Accuracy: 0.7
Training loss = 0.02043000260988871
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.01765620897213618
step = 12, Training Accuracy: 0.78
Training loss = 0.017761395176251728
step = 13, Training Accuracy: 0.7733333333333333
Training loss = 0.02100143978993098
step = 14, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.72625
params:  [0.6588733033097741, 0.3786989911234114, 0.6553185582424186, 0.38854340487934313, 0.18117572905582402, 0.6402200625817102, 0.8871823757001118, 0.37045439682137093, 0.6552679752090482, 0.8786698500119658, 0.4875680527612952, 0.6605172350572895, 0.7469656992913971, 0.01, 0.6349486296955986, 0.05314684762557331, 0.44672774830103307, 0.34841730092771084, 0.5259739711826394, 0.7510883215035544, 0.27959914115419526, 0.7479095024784188, 0.5374191532939366, 0.9645982053340072]
Training loss = 0.023719754715760547
step = 0, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.75625
Training loss = 0.01932749589284261
step = 1, Training Accuracy: 0.74
Training loss = 0.02004584530989329
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.018813560207684835
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017825061678886412
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.018095911145210267
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.01698431561390559
step = 6, Training Accuracy: 0.78
Training loss = 0.01882866770029068
step = 7, Training Accuracy: 0.7366666666666667
Training loss = 0.02006738195816676
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.016502803961435955
step = 9, Training Accuracy: 0.78
Training loss = 0.017429205377896627
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.016694659491380055
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.018139394521713256
step = 12, Training Accuracy: 0.79
Training loss = 0.01594186027844747
step = 13, Training Accuracy: 0.77
Training loss = 0.015689890384674072
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.765
params:  [0.5092881988256649, 0.3647831662368041, 0.8781373312722973, 0.3804335116329508, 0.01, 0.38040487655714683, 0.5456297649481952, 0.6045049354751627, 0.5702635478685986, 0.7208017942661306, 0.3026096347060126, 0.5539821579186881, 0.6138025263784564, 0.02523530144294421, 0.37298124084880424, 0.3063427640669843, 0.3076184019632486, 0.3385410000750847, 0.10053429748767734, 0.5304378681907148, 0.1590726751227349, 0.597222834437443, 0.9597086726994417, 0.5341867355498205]
Training loss = 0.021734145085016886
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.75625
Training loss = 0.02127809335788091
step = 1, Training Accuracy: 0.72
Training loss = 0.02024400552113851
step = 2, Training Accuracy: 0.7066666666666667
Training loss = 0.019323578874270123
step = 3, Training Accuracy: 0.7166666666666667
Training loss = 0.020126067797342936
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.02032672921816508
step = 5, Training Accuracy: 0.74
Training loss = 0.019351640343666078
step = 6, Training Accuracy: 0.73
Training loss = 0.016827059189478556
step = 7, Training Accuracy: 0.7533333333333333
Training loss = 0.019242702921231588
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.018302545249462128
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.019274495939413706
step = 10, Training Accuracy: 0.7566666666666667
Training loss = 0.016805126070976256
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.017019857863585154
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.017513086001078287
step = 13, Training Accuracy: 0.76
Training loss = 0.017945290903250376
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.74
params:  [0.4214324143914839, 0.22113366424252365, 0.3204623908444957, 0.6608895702201059, 0.05111072723715765, 0.4582513914730243, 0.8585501947182631, 0.3690882278716998, 0.36075795559372636, 0.810181502671843, 0.1850770291523616, 0.5237487751974897, 0.30589139361694984, 0.2943112084076364, 0.11938534699831188, 0.3958518825225447, 0.1179960493196116, 0.01, 0.01601446588314548, 0.5823850331997374, 0.1402810684831195, 0.8824719973067239, 0.42413348574225135, 0.5099487329229233]
Training loss = 0.02088995506366094
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.75625
Training loss = 0.018076187918583553
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.018293724556763966
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.01835180213054021
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016806703805923463
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.017303730348745983
step = 5, Training Accuracy: 0.79
Training loss = 0.01585869203011195
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.015139041791359583
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.014106030563513438
step = 8, Training Accuracy: 0.82
Training loss = 0.015116378962993622
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015470216969648997
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014757415850957235
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.015402702192465464
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.015373090008894603
step = 13, Training Accuracy: 0.82
Training loss = 0.012970880965391795
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7625
params:  [0.5005025202699872, 0.3994275358688993, 0.9776093546728624, 0.37845498800978594, 0.01, 0.5056763447436994, 0.8560228417912565, 0.22225236784611901, 0.30320967738011817, 0.8066310218452857, 0.5319177177085441, 0.48468412991276044, 0.9285925657563712, 0.04682559003202906, 0.5048585034963915, 0.01, 0.539644470007804, 0.3393669232013857, 0.41250576019611546, 0.39563221121768316, 0.1409978672231792, 0.6803095055936159, 0.2819356280755503, 0.4799722582928841]
Training loss = 0.016563244611024857
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.78125
Training loss = 0.016208526988824207
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.013540215889612834
step = 2, Training Accuracy: 0.83
Training loss = 0.013353275756041208
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.012098604043324789
step = 4, Training Accuracy: 0.87
Training loss = 0.010788887739181519
step = 5, Training Accuracy: 0.8633333333333333
Training loss = 0.010342084020376206
step = 6, Training Accuracy: 0.87
Training loss = 0.010838565280040105
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.012772582521041234
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.01134147067864736
step = 9, Training Accuracy: 0.86
Training loss = 0.008529377231995264
step = 10, Training Accuracy: 0.8866666666666667
Training loss = 0.011688259343306223
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.012546777526537578
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.010694646139939626
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.01102070152759552
step = 14, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.785
params:  [0.24367275066497163, 0.4515999248073289, 0.4168227750842022, 0.4628105090720904, 0.6601728937375315, 0.5098350355270836, 0.8023417838994809, 0.4557860937914204, 0.4586115798105578, 0.4570243869707952, 0.534028355589033, 0.2863564982953998, 0.9046011500504257, 0.07780597403617594, 0.15076934176554146, 0.28513423092525353, 0.3733455020509859, 0.01, 0.02718315309592584, 0.9389389754637945, 0.07573216551672413, 0.7951397091204477, 0.9820096179738216, 0.4774546494214166]
Training loss = 0.02528998593489329
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.7675
Training loss = 0.023040971159934996
step = 1, Training Accuracy: 0.71
Training loss = 0.020350620845953623
step = 2, Training Accuracy: 0.7266666666666667
Training loss = 0.023386152784029644
step = 3, Training Accuracy: 0.72
Training loss = 0.019412057797114055
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.019644516905148825
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.0201042906443278
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.019919991791248322
step = 7, Training Accuracy: 0.72
Training loss = 0.019787660439809163
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.017402938604354858
step = 9, Training Accuracy: 0.7633333333333333
Training loss = 0.018526928623517354
step = 10, Training Accuracy: 0.72
Training loss = 0.01782495379447937
step = 11, Training Accuracy: 0.77
Training loss = 0.019381978511810304
step = 12, Training Accuracy: 0.73
Training loss = 0.016961669425169627
step = 13, Training Accuracy: 0.78
Training loss = 0.01836118280887604
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.7625
params:  [0.26921817027502726, 0.42581483847399415, 0.99, 0.01, 0.3680694182568413, 0.432103623930323, 0.99, 0.39308830126107247, 0.37631388240517916, 0.7707381503699242, 0.3279938921704025, 0.6918957271104522, 0.7227035850764383, 0.25606468137398636, 0.6858425440771301, 0.4733445361550873, 0.5588599401678845, 0.27380946801628325, 0.3374486113847291, 0.5918408116671903, 0.01, 0.94578132010806, 0.3862842746829197, 0.7330059782575661]
Training loss = 0.019367639223734537
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.755
Training loss = 0.0185741729537646
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.017192196349302927
step = 2, Training Accuracy: 0.77
Training loss = 0.01661228507757187
step = 3, Training Accuracy: 0.8
Training loss = 0.01545349195599556
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015854169030984244
step = 5, Training Accuracy: 0.81
Training loss = 0.014510922431945801
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014103329181671143
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.014345326522986094
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013052472869555156
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013763702710469564
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.014390569627285004
step = 11, Training Accuracy: 0.84
Training loss = 0.013352025548617046
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.013167794793844223
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.010606478949387868
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.76375
[[0.2893999251512786, 0.24523104932906276, 0.7680627940672102, 0.3482166960621, 0.05820463635928419, 0.39326050309844857, 0.7722273412820304, 0.2927906630990976, 0.6449807498785003, 0.16524562872141046, 0.38610641492853154, 0.2740681658245411, 0.7749106239758858, 0.3732659888142295, 0.613724782370769, 0.2675997271007921, 0.01, 0.01, 0.1947584899013606, 0.99, 0.6634739103158074, 0.8056560707932225, 0.5192016282667515, 0.6294984647551699], [0.41298877918323984, 0.4787005167757402, 0.5517094874431819, 0.511791855087905, 0.0560689781574574, 0.4843499654320054, 0.8508741669878424, 0.5618067390300576, 0.40053529072477734, 0.3423190041851708, 0.4254200857920609, 0.625855302552597, 0.6220042605391028, 0.06027921789384401, 0.6336733462730187, 0.23643662353956135, 0.519295881669829, 0.10569413964296567, 0.3628816716184989, 0.9028294381765509, 0.01, 0.6322381330392457, 0.6803899106363895, 0.488830336901363], [0.6588733033097741, 0.3786989911234114, 0.6553185582424186, 0.38854340487934313, 0.18117572905582402, 0.6402200625817102, 0.8871823757001118, 0.37045439682137093, 0.6552679752090482, 0.8786698500119658, 0.4875680527612952, 0.6605172350572895, 0.7469656992913971, 0.01, 0.6349486296955986, 0.05314684762557331, 0.44672774830103307, 0.34841730092771084, 0.5259739711826394, 0.7510883215035544, 0.27959914115419526, 0.7479095024784188, 0.5374191532939366, 0.9645982053340072], [0.5092881988256649, 0.3647831662368041, 0.8781373312722973, 0.3804335116329508, 0.01, 0.38040487655714683, 0.5456297649481952, 0.6045049354751627, 0.5702635478685986, 0.7208017942661306, 0.3026096347060126, 0.5539821579186881, 0.6138025263784564, 0.02523530144294421, 0.37298124084880424, 0.3063427640669843, 0.3076184019632486, 0.3385410000750847, 0.10053429748767734, 0.5304378681907148, 0.1590726751227349, 0.597222834437443, 0.9597086726994417, 0.5341867355498205], [0.4214324143914839, 0.22113366424252365, 0.3204623908444957, 0.6608895702201059, 0.05111072723715765, 0.4582513914730243, 0.8585501947182631, 0.3690882278716998, 0.36075795559372636, 0.810181502671843, 0.1850770291523616, 0.5237487751974897, 0.30589139361694984, 0.2943112084076364, 0.11938534699831188, 0.3958518825225447, 0.1179960493196116, 0.01, 0.01601446588314548, 0.5823850331997374, 0.1402810684831195, 0.8824719973067239, 0.42413348574225135, 0.5099487329229233], [0.5005025202699872, 0.3994275358688993, 0.9776093546728624, 0.37845498800978594, 0.01, 0.5056763447436994, 0.8560228417912565, 0.22225236784611901, 0.30320967738011817, 0.8066310218452857, 0.5319177177085441, 0.48468412991276044, 0.9285925657563712, 0.04682559003202906, 0.5048585034963915, 0.01, 0.539644470007804, 0.3393669232013857, 0.41250576019611546, 0.39563221121768316, 0.1409978672231792, 0.6803095055936159, 0.2819356280755503, 0.4799722582928841], [0.24367275066497163, 0.4515999248073289, 0.4168227750842022, 0.4628105090720904, 0.6601728937375315, 0.5098350355270836, 0.8023417838994809, 0.4557860937914204, 0.4586115798105578, 0.4570243869707952, 0.534028355589033, 0.2863564982953998, 0.9046011500504257, 0.07780597403617594, 0.15076934176554146, 0.28513423092525353, 0.3733455020509859, 0.01, 0.02718315309592584, 0.9389389754637945, 0.07573216551672413, 0.7951397091204477, 0.9820096179738216, 0.4774546494214166], [0.26921817027502726, 0.42581483847399415, 0.99, 0.01, 0.3680694182568413, 0.432103623930323, 0.99, 0.39308830126107247, 0.37631388240517916, 0.7707381503699242, 0.3279938921704025, 0.6918957271104522, 0.7227035850764383, 0.25606468137398636, 0.6858425440771301, 0.4733445361550873, 0.5588599401678845, 0.27380946801628325, 0.3374486113847291, 0.5918408116671903, 0.01, 0.94578132010806, 0.3862842746829197, 0.7330059782575661]]
4  	8     	0.756406	0.0169609 	0.72625	0.785  
params:  [0.7916074855838398, 0.38602814625696125, 0.99, 0.144044712332414, 0.237169422475059, 0.20742926803049028, 0.8464217368491984, 0.40491713815233715, 0.3342153357037307, 0.7347340543546141, 0.8219816048595279, 0.6214250452242378, 0.7883776313510765, 0.07433650242087225, 0.48377431994223236, 0.4983261341199515, 0.230246322976504, 0.2204138410318235, 0.46757781554323263, 0.5972828883195694, 0.2361703040599268, 0.6733702825151062, 0.368357152033859, 0.6853745245447549]
Training loss = 0.020695986449718474
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.78
Training loss = 0.018944995005925495
step = 1, Training Accuracy: 0.72
Training loss = 0.018185536762078604
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.017263986070950827
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017368767460187277
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01612358073393504
step = 5, Training Accuracy: 0.7633333333333333
Training loss = 0.014697886804739635
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014326014816761016
step = 7, Training Accuracy: 0.78
Training loss = 0.014943756461143494
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.01418275997042656
step = 9, Training Accuracy: 0.8166666666666667
Training loss = 0.013485922912756602
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.015179569025834401
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01632197380065918
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.014614028334617614
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.015987497468789417
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.79625
params:  [0.3907336816022373, 0.38803544960638386, 0.7119539277183898, 0.14850266390947608, 0.1881819170801753, 0.6286145497971435, 0.7808756219802322, 0.14833912017212209, 0.28347732455220076, 0.99, 0.5772427178305628, 0.22421556233518936, 0.7105865558257005, 0.046498271093868504, 0.7497934588773141, 0.5721992172026982, 0.4750760118332465, 0.45837403029733104, 0.34303635445530134, 0.99, 0.2360892153233315, 0.8977005710980557, 0.99, 0.29302155187726736]
Training loss = 0.026707921624183655
step = 0, Training Accuracy: 0.65
Validation Accuracy: 0.77625
Training loss = 0.023902876178423564
step = 1, Training Accuracy: 0.6866666666666666
Training loss = 0.022374174197514852
step = 2, Training Accuracy: 0.6766666666666666
Training loss = 0.023272822201251982
step = 3, Training Accuracy: 0.69
Training loss = 0.0237339190642039
step = 4, Training Accuracy: 0.66
Training loss = 0.022010228633880614
step = 5, Training Accuracy: 0.71
Training loss = 0.020031328996022543
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.022689069012800853
step = 7, Training Accuracy: 0.6733333333333333
Training loss = 0.02275369067986806
step = 8, Training Accuracy: 0.7
Training loss = 0.0219063671429952
step = 9, Training Accuracy: 0.6666666666666666
Training loss = 0.020689846376578013
step = 10, Training Accuracy: 0.7133333333333334
Training loss = 0.022106141249338785
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.020484504699707032
step = 12, Training Accuracy: 0.73
Training loss = 0.021462578376134238
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.020769051412741345
step = 14, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.77875
params:  [0.6005741993736752, 0.44225684829112466, 0.8067518554989097, 0.6728257166514647, 0.3384568461024038, 0.9265354761128615, 0.7867602731351827, 0.4479411539028619, 0.4067786098989584, 0.39581119905208934, 0.40201614466732716, 0.1774732617181925, 0.47260857676858553, 0.23288292247536818, 0.6611701382712433, 0.01, 0.4416978031995817, 0.15971514509393028, 0.2910486859351994, 0.35391446926537484, 0.3682914756434106, 0.3182923303639871, 0.3626284564191165, 0.6021208537414091]
Training loss = 0.019970393081506093
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7725
Training loss = 0.019257638355096182
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.01794328232606252
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.018501145243644716
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.01697658807039261
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016844149629275003
step = 5, Training Accuracy: 0.77
Training loss = 0.016985618472099305
step = 6, Training Accuracy: 0.79
Training loss = 0.01638920545578003
step = 7, Training Accuracy: 0.81
Training loss = 0.01667266885439555
step = 8, Training Accuracy: 0.8
Training loss = 0.015512040654818217
step = 9, Training Accuracy: 0.8
Training loss = 0.014333444734414419
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.012670301993687948
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.013640762915213904
step = 12, Training Accuracy: 0.82
Training loss = 0.015127144157886505
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01357852816581726
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.775
params:  [0.52791612190415, 0.16970864714955095, 0.9253886242010774, 0.3483643444735911, 0.057143137968896786, 0.9854822484255201, 0.4757438212852666, 0.31348395365167603, 0.49665143591179517, 0.5538832743345564, 0.6732416695567295, 0.8601225534795447, 0.8897822802122367, 0.01, 0.6859396365781092, 0.010680967475932696, 0.46770119413344613, 0.01, 0.22778257821695902, 0.2917619344972072, 0.01630136361623616, 0.7880335789759203, 0.3721745131981523, 0.99]
Training loss = 0.015049210786819457
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7775
Training loss = 0.013006732513507207
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.012295004824797312
step = 2, Training Accuracy: 0.83
Training loss = 0.01341354454557101
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.011832882811625799
step = 4, Training Accuracy: 0.85
Training loss = 0.010681102176507314
step = 5, Training Accuracy: 0.8633333333333333
Training loss = 0.008506905138492584
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.00942277063926061
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.009165776669979095
step = 8, Training Accuracy: 0.88
Training loss = 0.009134414121508599
step = 9, Training Accuracy: 0.9
Training loss = 0.007246760527292887
step = 10, Training Accuracy: 0.8933333333333333
Training loss = 0.009909502739707629
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.008673221866289774
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.007787493367989858
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.0075257200996081035
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.77125
params:  [0.01, 0.10469421818614544, 0.8964834820285134, 0.14953302413815442, 0.01, 0.6419379661242508, 0.5143110373972493, 0.3871630691664798, 0.5881616436805839, 0.6141727931631147, 0.1332788399089928, 0.5472212857464209, 0.9434624533436825, 0.4188871813975247, 0.7958297434450881, 0.362770392706522, 0.9759120229782832, 0.26997932141553493, 0.7252181443116514, 0.49005439596718436, 0.2347450500356847, 0.5895242344680494, 0.7040659669332456, 0.37560432457474063]
Training loss = 0.02094295104344686
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.78625
Training loss = 0.01856251319249471
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01553795983393987
step = 2, Training Accuracy: 0.8
Training loss = 0.014402954677740732
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.015219519833723704
step = 4, Training Accuracy: 0.82
Training loss = 0.014911878705024719
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.012831881095965704
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.012625941187143325
step = 7, Training Accuracy: 0.85
Training loss = 0.013193140079577764
step = 8, Training Accuracy: 0.83
Training loss = 0.012034545838832855
step = 9, Training Accuracy: 0.86
Training loss = 0.013546643654505412
step = 10, Training Accuracy: 0.83
Training loss = 0.013103353480497995
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.011844149678945542
step = 12, Training Accuracy: 0.86
Training loss = 0.010155048370361328
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.009747605919837952
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.76625
params:  [0.5799388368247861, 0.7962791942876926, 0.99, 0.1765107543766434, 0.32264984383576234, 0.7836762313768686, 0.99, 0.47397173753760236, 0.5683186201878222, 0.6508335811162105, 0.23263763185789466, 0.41520185717189817, 0.99, 0.01, 0.327123511397432, 0.01, 0.7081489988857399, 0.34053470253360135, 0.5579413988164106, 0.6842704739088283, 0.27714678941997206, 0.7978720697180541, 0.04192695938423846, 0.924259944038603]
Training loss = 0.023997580111026765
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.76625
Training loss = 0.021380024154980977
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.019119847317536673
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.018541809618473053
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.017482179900010428
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.019087293843428294
step = 5, Training Accuracy: 0.7566666666666667
Training loss = 0.018652307788530987
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.01652377814054489
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01689579665660858
step = 8, Training Accuracy: 0.76
Training loss = 0.017999010682106017
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.014603909154733022
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016670292913913726
step = 11, Training Accuracy: 0.7566666666666667
Training loss = 0.014895467162132263
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016669446229934694
step = 13, Training Accuracy: 0.75
Training loss = 0.013496109396219253
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.75875
params:  [0.6230483436649112, 0.8928958094222186, 0.99, 0.4294218551010121, 0.03963229023083727, 0.4397420457694715, 0.99, 0.2976324895898897, 0.22815870977116676, 0.6490911034517854, 0.6440303657965571, 0.6029682317621954, 0.6449332075661565, 0.23858024078908657, 0.47335096314631153, 0.01, 0.14709612698005242, 0.4472864357742835, 0.4458862921367866, 0.5935842098064463, 0.2143315963255053, 0.99, 0.10539324343953027, 0.99]
Training loss = 0.019528433183828988
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.77375
Training loss = 0.019174981117248534
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018294352690378823
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.016534265677134195
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.01782865862051646
step = 4, Training Accuracy: 0.7466666666666667
Training loss = 0.01529795080423355
step = 5, Training Accuracy: 0.79
Training loss = 0.015825304786364236
step = 6, Training Accuracy: 0.7933333333333333
Training loss = 0.016312183539072673
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.0180320997039477
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.015675500979026157
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015326113303502401
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.015213830868403116
step = 11, Training Accuracy: 0.8
Training loss = 0.015536454419294993
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015844628314177194
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.013478191792964936
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.755
params:  [0.9534182832809339, 0.30031992210292985, 0.7286289552274436, 0.01, 0.20179682017606826, 0.7692828648546051, 0.6868033013411233, 0.17139616891764942, 0.6123837327278213, 0.841346017826174, 0.4387405441276433, 0.31277855712162583, 0.6716838843122266, 0.24584251590740858, 0.652616946090883, 0.23359825704452988, 0.3228375839547183, 0.2251535904450299, 0.7434874413215171, 0.5064661088177269, 0.5191504039493078, 0.99, 0.44978003337573225, 0.5918954555414513]
Training loss = 0.019017807692289352
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7575
Training loss = 0.016545238693555196
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.017996662358442942
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.016261758208274843
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.015373453199863434
step = 4, Training Accuracy: 0.81
Training loss = 0.014728094140688578
step = 5, Training Accuracy: 0.8366666666666667
Training loss = 0.015384015341599782
step = 6, Training Accuracy: 0.78
Training loss = 0.014985894362131755
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.013819343745708465
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.01480098009109497
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.013715112805366516
step = 10, Training Accuracy: 0.8433333333333334
Training loss = 0.012740053832530976
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.012663446515798569
step = 12, Training Accuracy: 0.85
Training loss = 0.0142523393034935
step = 13, Training Accuracy: 0.81
Training loss = 0.011773818135261536
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.75625
[[0.7916074855838398, 0.38602814625696125, 0.99, 0.144044712332414, 0.237169422475059, 0.20742926803049028, 0.8464217368491984, 0.40491713815233715, 0.3342153357037307, 0.7347340543546141, 0.8219816048595279, 0.6214250452242378, 0.7883776313510765, 0.07433650242087225, 0.48377431994223236, 0.4983261341199515, 0.230246322976504, 0.2204138410318235, 0.46757781554323263, 0.5972828883195694, 0.2361703040599268, 0.6733702825151062, 0.368357152033859, 0.6853745245447549], [0.3907336816022373, 0.38803544960638386, 0.7119539277183898, 0.14850266390947608, 0.1881819170801753, 0.6286145497971435, 0.7808756219802322, 0.14833912017212209, 0.28347732455220076, 0.99, 0.5772427178305628, 0.22421556233518936, 0.7105865558257005, 0.046498271093868504, 0.7497934588773141, 0.5721992172026982, 0.4750760118332465, 0.45837403029733104, 0.34303635445530134, 0.99, 0.2360892153233315, 0.8977005710980557, 0.99, 0.29302155187726736], [0.6005741993736752, 0.44225684829112466, 0.8067518554989097, 0.6728257166514647, 0.3384568461024038, 0.9265354761128615, 0.7867602731351827, 0.4479411539028619, 0.4067786098989584, 0.39581119905208934, 0.40201614466732716, 0.1774732617181925, 0.47260857676858553, 0.23288292247536818, 0.6611701382712433, 0.01, 0.4416978031995817, 0.15971514509393028, 0.2910486859351994, 0.35391446926537484, 0.3682914756434106, 0.3182923303639871, 0.3626284564191165, 0.6021208537414091], [0.52791612190415, 0.16970864714955095, 0.9253886242010774, 0.3483643444735911, 0.057143137968896786, 0.9854822484255201, 0.4757438212852666, 0.31348395365167603, 0.49665143591179517, 0.5538832743345564, 0.6732416695567295, 0.8601225534795447, 0.8897822802122367, 0.01, 0.6859396365781092, 0.010680967475932696, 0.46770119413344613, 0.01, 0.22778257821695902, 0.2917619344972072, 0.01630136361623616, 0.7880335789759203, 0.3721745131981523, 0.99], [0.01, 0.10469421818614544, 0.8964834820285134, 0.14953302413815442, 0.01, 0.6419379661242508, 0.5143110373972493, 0.3871630691664798, 0.5881616436805839, 0.6141727931631147, 0.1332788399089928, 0.5472212857464209, 0.9434624533436825, 0.4188871813975247, 0.7958297434450881, 0.362770392706522, 0.9759120229782832, 0.26997932141553493, 0.7252181443116514, 0.49005439596718436, 0.2347450500356847, 0.5895242344680494, 0.7040659669332456, 0.37560432457474063], [0.5799388368247861, 0.7962791942876926, 0.99, 0.1765107543766434, 0.32264984383576234, 0.7836762313768686, 0.99, 0.47397173753760236, 0.5683186201878222, 0.6508335811162105, 0.23263763185789466, 0.41520185717189817, 0.99, 0.01, 0.327123511397432, 0.01, 0.7081489988857399, 0.34053470253360135, 0.5579413988164106, 0.6842704739088283, 0.27714678941997206, 0.7978720697180541, 0.04192695938423846, 0.924259944038603], [0.6230483436649112, 0.8928958094222186, 0.99, 0.4294218551010121, 0.03963229023083727, 0.4397420457694715, 0.99, 0.2976324895898897, 0.22815870977116676, 0.6490911034517854, 0.6440303657965571, 0.6029682317621954, 0.6449332075661565, 0.23858024078908657, 0.47335096314631153, 0.01, 0.14709612698005242, 0.4472864357742835, 0.4458862921367866, 0.5935842098064463, 0.2143315963255053, 0.99, 0.10539324343953027, 0.99], [0.9534182832809339, 0.30031992210292985, 0.7286289552274436, 0.01, 0.20179682017606826, 0.7692828648546051, 0.6868033013411233, 0.17139616891764942, 0.6123837327278213, 0.841346017826174, 0.4387405441276433, 0.31277855712162583, 0.6716838843122266, 0.24584251590740858, 0.652616946090883, 0.23359825704452988, 0.3228375839547183, 0.2251535904450299, 0.7434874413215171, 0.5064661088177269, 0.5191504039493078, 0.99, 0.44978003337573225, 0.5918954555414513]]
5  	8     	0.769688	0.0129716 	0.755  	0.79625
params:  [0.9684768046023005, 0.35019589244466975, 0.99, 0.46970915130059127, 0.01, 0.7594772438249406, 0.7868265434955967, 0.18357937530439258, 0.036062466115807434, 0.8197825718811874, 0.8090436502127936, 0.6574753084818027, 0.8504622137720127, 0.279313154186265, 0.7057005442549577, 0.49643767293287283, 0.7627401916812664, 0.3050567660559063, 0.5130791924703471, 0.42479389715854, 0.5604011674755187, 0.7354032486842541, 0.6742375137833663, 0.3726238210112915]
Training loss = 0.016242037216822307
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.76375
Training loss = 0.014662186702092489
step = 1, Training Accuracy: 0.8
Training loss = 0.014467731366554897
step = 2, Training Accuracy: 0.78
Training loss = 0.014921369055906932
step = 3, Training Accuracy: 0.79
Training loss = 0.013828245500723521
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014316689670085907
step = 5, Training Accuracy: 0.82
Training loss = 0.013766118387381236
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.013172745953003566
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.011724811891714731
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.011963493625322978
step = 9, Training Accuracy: 0.87
Training loss = 0.009876107027133306
step = 10, Training Accuracy: 0.89
Training loss = 0.010410723139842351
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.01015550598502159
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.010982496291399002
step = 13, Training Accuracy: 0.85
Training loss = 0.011011256476243337
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.77125
params:  [0.46117777037599456, 0.061092958086063354, 0.9740262462919572, 0.5095503576995686, 0.31687266323958324, 0.02921188947257608, 0.99, 0.37410009865248595, 0.5141419249851656, 0.29861730406086423, 0.6235750574122942, 0.22723054927870728, 0.99, 0.01, 0.4291091552613039, 0.27045508705697097, 0.4497067553977525, 0.11984056539266999, 0.05456302071255886, 0.6946923339893121, 0.5646427348549382, 0.23135701267843867, 0.3079503380087113, 0.4420535474638764]
Training loss = 0.017620379229386647
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.78125
Training loss = 0.017028082311153412
step = 1, Training Accuracy: 0.78
Training loss = 0.014555018693208695
step = 2, Training Accuracy: 0.82
Training loss = 0.013850343227386475
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.013078892081975936
step = 4, Training Accuracy: 0.83
Training loss = 0.013210819959640503
step = 5, Training Accuracy: 0.8366666666666667
Training loss = 0.012494140962759654
step = 6, Training Accuracy: 0.86
Training loss = 0.010176687215765317
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.011578109413385392
step = 8, Training Accuracy: 0.86
Training loss = 0.010775761008262634
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011118482251962027
step = 10, Training Accuracy: 0.8666666666666667
Training loss = 0.009772123595078785
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.010481860488653183
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.00879206729431947
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.007852033078670502
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.76625
params:  [0.23933046501740618, 0.5929488159217874, 0.9341462542019245, 0.07660056110476024, 0.26029461815916427, 0.6514343975092772, 0.99, 0.28489672065649496, 0.2865950061557726, 0.17940308892478452, 0.6531736579150199, 0.45517356345128107, 0.9387139665264246, 0.04520149394149076, 0.1273749642725716, 0.6464680456497678, 0.15091587382454538, 0.01, 0.3471935783071706, 0.6336283869118123, 0.2339951188752471, 0.9392097200751283, 0.44046666489106034, 0.7194992890506915]
Training loss = 0.023155154983202617
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7575
Training loss = 0.020871627430121106
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.01580404669046402
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.017040360967318216
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.016169861306746802
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015400787095228831
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015343270798524222
step = 6, Training Accuracy: 0.79
Training loss = 0.015254410256942113
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.013547245760758718
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.01671779841184616
step = 9, Training Accuracy: 0.77
Training loss = 0.016077946027119955
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.015474194089571635
step = 11, Training Accuracy: 0.83
Training loss = 0.015384802222251892
step = 12, Training Accuracy: 0.78
Training loss = 0.014825468063354491
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01357130616903305
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7975
params:  [0.621007714246598, 0.2226694821930075, 0.5867810785302423, 0.029101482034321308, 0.07552228096315189, 0.7702009927985131, 0.99, 0.30297593326652844, 0.6861257571605697, 0.8164692234291816, 0.6550580862175687, 0.5142548379829418, 0.99, 0.01, 0.9305255486152111, 0.13938594204063726, 0.17500654021419446, 0.3326967337890617, 0.13599391367700486, 0.6639128859727107, 0.05606486499710553, 0.6396419202756315, 0.7027004482980439, 0.8256372377094466]
Training loss = 0.017791763246059418
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79
Training loss = 0.0194421116511027
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018807581464449566
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.014816827674706778
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.015112726191679637
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.015164359509944915
step = 5, Training Accuracy: 0.81
Training loss = 0.013341688712437948
step = 6, Training Accuracy: 0.8
Training loss = 0.014639330605665843
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013322835365931193
step = 8, Training Accuracy: 0.84
Training loss = 0.014861342112223307
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.011692136079072952
step = 10, Training Accuracy: 0.85
Training loss = 0.01491654674212138
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.013582933843135834
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.011652198433876038
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.01512428472439448
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.77625
params:  [0.3738509883282739, 0.12407590606439584, 0.99, 0.2755218066179716, 0.06901894610522247, 0.4547233117184694, 0.8764771423340856, 0.5096272752224679, 0.30097033605349616, 0.40682382771642317, 0.15096779775105384, 0.28413690820174997, 0.99, 0.21187713396597035, 0.9742249133561781, 0.4476777932044547, 0.4405057501618948, 0.10461467442405187, 0.4617436750090014, 0.7803916758424752, 0.01, 0.5228564801703749, 0.5875023690789174, 0.3094748821578214]
Training loss = 0.014487006068229676
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.765
Training loss = 0.017340394059816997
step = 1, Training Accuracy: 0.83
Training loss = 0.01446701392531395
step = 2, Training Accuracy: 0.82
Training loss = 0.016583480536937714
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.014952750702699025
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.012811096409956615
step = 5, Training Accuracy: 0.8533333333333334
Training loss = 0.013899368842442831
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.013704092601935069
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.011759557624657949
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.012490311712026596
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013085408806800843
step = 10, Training Accuracy: 0.85
Training loss = 0.012689312249422073
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.011317270845174789
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.013901233623425166
step = 13, Training Accuracy: 0.88
Training loss = 0.010442926486333211
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.765
params:  [0.35612451513289556, 0.6056918640981384, 0.99, 0.01, 0.09076264754354826, 0.7365278673992834, 0.7784129173350597, 0.36596971670144124, 0.014929051538094817, 0.4753919629565422, 0.7134862882473592, 0.11007515311375465, 0.7365530988755145, 0.1186045272679965, 0.4669990568626182, 0.03243151852045334, 0.38663101627664936, 0.24281432847849913, 0.5575403734204489, 0.7269815297009625, 0.3881048818776515, 0.9018103364605541, 0.253363340430922, 0.6470570030585506]
Training loss = 0.020820664366086324
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.78
Training loss = 0.02369762976964315
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.019883613288402557
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.017608851492404938
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.015556922753651937
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.014304320613543193
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.015350803236166636
step = 6, Training Accuracy: 0.82
Training loss = 0.014192279328902562
step = 7, Training Accuracy: 0.8433333333333334
Training loss = 0.014086291193962097
step = 8, Training Accuracy: 0.82
Training loss = 0.01721289793650309
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.015227317611376445
step = 10, Training Accuracy: 0.8
Training loss = 0.01297032505273819
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.014612017770608266
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.0138113734126091
step = 13, Training Accuracy: 0.83
Training loss = 0.013801331222057343
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.76625
params:  [0.5996134458080523, 0.26923662831719153, 0.99, 0.14594956814664312, 0.1317605249839255, 0.3336213671016043, 0.7908355418376127, 0.516845198231817, 0.16444553620985475, 0.8526329980900053, 0.823714080306599, 0.5800938835437807, 0.771656457738903, 0.16425842929803408, 0.8938264591104456, 0.4747883269289792, 0.7038980894859246, 0.32309182817632753, 0.5840064908894225, 0.566030763812335, 0.343080297902213, 0.6826234270714228, 0.7539887230132103, 0.6478573002488747]
Training loss = 0.02018560270468394
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7925
Training loss = 0.020526799360911053
step = 1, Training Accuracy: 0.71
Training loss = 0.017357378005981444
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.016133098204930623
step = 3, Training Accuracy: 0.79
Training loss = 0.01780184845129649
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014333003759384155
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.014425830245018005
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01557662417491277
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01419696494936943
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.014838709980249404
step = 9, Training Accuracy: 0.81
Training loss = 0.016575009226799012
step = 10, Training Accuracy: 0.78
Training loss = 0.01648967166741689
step = 11, Training Accuracy: 0.8
Training loss = 0.015183437069257101
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013179265360037485
step = 13, Training Accuracy: 0.84
Training loss = 0.013448446492354075
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.79
params:  [0.4327582652295935, 0.6944688725787914, 0.9549380326508092, 0.06113678902772879, 0.12075504894355095, 0.6056462359526753, 0.848170632940027, 0.7124389640909587, 0.43028347459095084, 0.4451204009385259, 0.99, 0.36324804406374017, 0.7330807979678737, 0.21871150175995413, 0.9543563390628156, 0.3172418264647181, 0.5196398122975052, 0.4365835772195294, 0.6798304080956923, 0.797295008169918, 0.22166627059797214, 0.18302548270480845, 0.11805374593229145, 0.44306901703134993]
Training loss = 0.01898370881875356
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.79
Training loss = 0.017857874929904937
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.013632883826891581
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.014939303000768026
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015198701719443004
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.014939271807670594
step = 5, Training Accuracy: 0.82
Training loss = 0.014891550441582998
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.013160016139348347
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.015236128767331441
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013607127616802852
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.013524057269096374
step = 10, Training Accuracy: 0.8533333333333334
Training loss = 0.012227926750977834
step = 11, Training Accuracy: 0.85
Training loss = 0.012806868255138397
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.014043056964874267
step = 13, Training Accuracy: 0.82
Training loss = 0.013051543136437733
step = 14, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.79125
[[0.9684768046023005, 0.35019589244466975, 0.99, 0.46970915130059127, 0.01, 0.7594772438249406, 0.7868265434955967, 0.18357937530439258, 0.036062466115807434, 0.8197825718811874, 0.8090436502127936, 0.6574753084818027, 0.8504622137720127, 0.279313154186265, 0.7057005442549577, 0.49643767293287283, 0.7627401916812664, 0.3050567660559063, 0.5130791924703471, 0.42479389715854, 0.5604011674755187, 0.7354032486842541, 0.6742375137833663, 0.3726238210112915], [0.46117777037599456, 0.061092958086063354, 0.9740262462919572, 0.5095503576995686, 0.31687266323958324, 0.02921188947257608, 0.99, 0.37410009865248595, 0.5141419249851656, 0.29861730406086423, 0.6235750574122942, 0.22723054927870728, 0.99, 0.01, 0.4291091552613039, 0.27045508705697097, 0.4497067553977525, 0.11984056539266999, 0.05456302071255886, 0.6946923339893121, 0.5646427348549382, 0.23135701267843867, 0.3079503380087113, 0.4420535474638764], [0.23933046501740618, 0.5929488159217874, 0.9341462542019245, 0.07660056110476024, 0.26029461815916427, 0.6514343975092772, 0.99, 0.28489672065649496, 0.2865950061557726, 0.17940308892478452, 0.6531736579150199, 0.45517356345128107, 0.9387139665264246, 0.04520149394149076, 0.1273749642725716, 0.6464680456497678, 0.15091587382454538, 0.01, 0.3471935783071706, 0.6336283869118123, 0.2339951188752471, 0.9392097200751283, 0.44046666489106034, 0.7194992890506915], [0.621007714246598, 0.2226694821930075, 0.5867810785302423, 0.029101482034321308, 0.07552228096315189, 0.7702009927985131, 0.99, 0.30297593326652844, 0.6861257571605697, 0.8164692234291816, 0.6550580862175687, 0.5142548379829418, 0.99, 0.01, 0.9305255486152111, 0.13938594204063726, 0.17500654021419446, 0.3326967337890617, 0.13599391367700486, 0.6639128859727107, 0.05606486499710553, 0.6396419202756315, 0.7027004482980439, 0.8256372377094466], [0.3738509883282739, 0.12407590606439584, 0.99, 0.2755218066179716, 0.06901894610522247, 0.4547233117184694, 0.8764771423340856, 0.5096272752224679, 0.30097033605349616, 0.40682382771642317, 0.15096779775105384, 0.28413690820174997, 0.99, 0.21187713396597035, 0.9742249133561781, 0.4476777932044547, 0.4405057501618948, 0.10461467442405187, 0.4617436750090014, 0.7803916758424752, 0.01, 0.5228564801703749, 0.5875023690789174, 0.3094748821578214], [0.35612451513289556, 0.6056918640981384, 0.99, 0.01, 0.09076264754354826, 0.7365278673992834, 0.7784129173350597, 0.36596971670144124, 0.014929051538094817, 0.4753919629565422, 0.7134862882473592, 0.11007515311375465, 0.7365530988755145, 0.1186045272679965, 0.4669990568626182, 0.03243151852045334, 0.38663101627664936, 0.24281432847849913, 0.5575403734204489, 0.7269815297009625, 0.3881048818776515, 0.9018103364605541, 0.253363340430922, 0.6470570030585506], [0.5996134458080523, 0.26923662831719153, 0.99, 0.14594956814664312, 0.1317605249839255, 0.3336213671016043, 0.7908355418376127, 0.516845198231817, 0.16444553620985475, 0.8526329980900053, 0.823714080306599, 0.5800938835437807, 0.771656457738903, 0.16425842929803408, 0.8938264591104456, 0.4747883269289792, 0.7038980894859246, 0.32309182817632753, 0.5840064908894225, 0.566030763812335, 0.343080297902213, 0.6826234270714228, 0.7539887230132103, 0.6478573002488747], [0.4327582652295935, 0.6944688725787914, 0.9549380326508092, 0.06113678902772879, 0.12075504894355095, 0.6056462359526753, 0.848170632940027, 0.7124389640909587, 0.43028347459095084, 0.4451204009385259, 0.99, 0.36324804406374017, 0.7330807979678737, 0.21871150175995413, 0.9543563390628156, 0.3172418264647181, 0.5196398122975052, 0.4365835772195294, 0.6798304080956923, 0.797295008169918, 0.22166627059797214, 0.18302548270480845, 0.11805374593229145, 0.44306901703134993]]
6  	8     	0.777969	0.0122145 	0.765  	0.7975 
params:  [0.4547655952874812, 0.28165898860798266, 0.9313497090353844, 0.09877739858393775, 0.24967746486496376, 0.6916135867405594, 0.99, 0.4257214368754503, 0.19390022902356963, 0.4087688945547061, 0.6642954384403978, 0.1675478210603933, 0.9104932877364046, 0.08072778117207509, 0.6188051243787557, 0.3807347582284515, 0.01, 0.13443597441535338, 0.04224432738021644, 0.5474947406862557, 0.1310079010276153, 0.9145625456850546, 0.1915213004747974, 0.900541543870349]
Training loss = 0.01379122351606687
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.77875
Training loss = 0.014162208139896392
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.012976870437463124
step = 2, Training Accuracy: 0.84
Training loss = 0.012810276051362356
step = 3, Training Accuracy: 0.8
Training loss = 0.013579551378885906
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.011799459209044775
step = 5, Training Accuracy: 0.83
Training loss = 0.008943885341286659
step = 6, Training Accuracy: 0.9
Training loss = 0.012194774945576985
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.010041142205397287
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.009446107844511668
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.010089970231056213
step = 10, Training Accuracy: 0.87
Training loss = 0.009514467294017474
step = 11, Training Accuracy: 0.87
Training loss = 0.010501348425944647
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.008822708129882813
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.010199526449044545
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.7675
params:  [0.4491086820559641, 0.5110112673570865, 0.8731044710710582, 0.18218755082068294, 0.3650997462797822, 0.21605293979567114, 0.99, 0.5389306222703668, 0.233419236545455, 0.17544182845835327, 0.5731093655427737, 0.5015190586068116, 0.99, 0.01, 0.49726851364050745, 0.694593240228059, 0.39460950335574463, 0.1463446569283618, 0.23974226312300628, 0.5161554718255529, 0.434586626846959, 0.99, 0.42466523379905513, 0.33664723473173774]
Training loss = 0.023154229323069254
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.7925
Training loss = 0.02188187559445699
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.01827932059764862
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.01733618090550105
step = 3, Training Accuracy: 0.77
Training loss = 0.01694865455230077
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.016872756481170655
step = 5, Training Accuracy: 0.79
Training loss = 0.016325881083806355
step = 6, Training Accuracy: 0.79
Training loss = 0.016583640178044638
step = 7, Training Accuracy: 0.8
Training loss = 0.018188485701878865
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01585794101158778
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.014650087257226309
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01708432654539744
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01583507239818573
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015647535820802052
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.01460818295677503
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.80375
params:  [0.46673565977876436, 0.4524581956952515, 0.99, 0.18407950443766083, 0.01, 0.7988272990473315, 0.7725031874201638, 0.44508254846009104, 0.39547024291776, 0.01, 0.5763782370379694, 0.4210971943806549, 0.6462304696377787, 0.2248099558221034, 0.09715266481129547, 0.5964360926924384, 0.39891572745218307, 0.22704627741918626, 0.649080395792835, 0.7603421000437551, 0.24518981723775254, 0.7398003436107724, 0.6720523499236201, 0.5438036977087074]
Training loss = 0.020732980767885843
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.7975
Training loss = 0.018994742035865785
step = 1, Training Accuracy: 0.76
Training loss = 0.017923609614372255
step = 2, Training Accuracy: 0.76
Training loss = 0.018372803926467896
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01698396702607473
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01710997074842453
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01614625573158264
step = 6, Training Accuracy: 0.81
Training loss = 0.01653067777554194
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01638575777411461
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.01618027885754903
step = 9, Training Accuracy: 0.79
Training loss = 0.014936095376809438
step = 10, Training Accuracy: 0.78
Training loss = 0.015941848754882814
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.014680629670619965
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.015189614395300548
step = 13, Training Accuracy: 0.81
Training loss = 0.013835170815388362
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78625
params:  [0.5276525019826608, 0.39720278997649616, 0.9043898479492474, 0.01, 0.01, 0.5761523622871182, 0.8963461011337522, 0.38611137170199117, 0.29201210299753866, 0.40827945667374743, 0.6401064156845556, 0.3677593771010182, 0.59998766268035, 0.01, 0.42760742933362667, 0.23458347443070654, 0.4253623917021472, 0.01, 0.5943349780281952, 0.4585632599330561, 0.17284405623672772, 0.7670717273372735, 0.01, 0.47476174770263724]
Training loss = 0.01889764944712321
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.79125
Training loss = 0.016488847434520722
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.01400173266728719
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014636397163073222
step = 3, Training Accuracy: 0.81
Training loss = 0.013307460397481919
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.013208609024683635
step = 5, Training Accuracy: 0.83
Training loss = 0.013079474866390228
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01139972562591235
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.012432815382877986
step = 8, Training Accuracy: 0.86
Training loss = 0.012438202202320099
step = 9, Training Accuracy: 0.86
Training loss = 0.012206736902395885
step = 10, Training Accuracy: 0.85
Training loss = 0.009976968864599863
step = 11, Training Accuracy: 0.88
Training loss = 0.01151965618133545
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.009887592196464538
step = 13, Training Accuracy: 0.88
Training loss = 0.009387685457865396
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.76375
params:  [0.5543142221000357, 0.21038090032019152, 0.99, 0.14840117843510553, 0.5588480728367612, 0.8188663732033883, 0.99, 0.40305956025570033, 0.20246762762894918, 0.6530789933361466, 0.8645799006645253, 0.3115976266840341, 0.5890457413106946, 0.36639414943621085, 0.3108058327285407, 0.41881553978163677, 0.7828279742162638, 0.0505288652801687, 0.4199492823240118, 0.6411730388248528, 0.17888864521405046, 0.3857840244300751, 0.7987487736612409, 0.6701104621524522]
Training loss = 0.02044792006413142
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.75875
Training loss = 0.0183017232020696
step = 1, Training Accuracy: 0.75
Training loss = 0.01836209903160731
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.015264309446016948
step = 3, Training Accuracy: 0.83
Training loss = 0.01493773361047109
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.015815945168336232
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.013885550796985627
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.014239748865365982
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.014176063189903896
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.014423090070486068
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.012715701758861542
step = 10, Training Accuracy: 0.8466666666666667
Training loss = 0.014264083007971445
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.014595569372177123
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.015043148795763652
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.011695516308148702
step = 14, Training Accuracy: 0.86
Validation Accuracy: 0.785
params:  [0.29200000421851197, 0.44082630119215194, 0.99, 0.377436481900627, 0.01, 0.4756540281344428, 0.99, 0.6207305527978217, 0.1717973598964842, 0.22164370679755768, 0.57127821173485, 0.4144368678282941, 0.99, 0.10621141675908706, 0.4085763460497661, 0.7241081134422214, 0.14078599821697332, 0.07571478114505453, 0.49514934275992417, 0.947219257659554, 0.3337262965489811, 0.5186326276567831, 0.1320978530989807, 0.7024560932796862]
Training loss = 0.018955986201763152
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.78375
Training loss = 0.017223569949467978
step = 1, Training Accuracy: 0.78
Training loss = 0.016133105953534444
step = 2, Training Accuracy: 0.77
Training loss = 0.016180777549743654
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.01642706722021103
step = 4, Training Accuracy: 0.8
Training loss = 0.014700215061505635
step = 5, Training Accuracy: 0.8
Training loss = 0.013965920011202494
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.015066576699415843
step = 7, Training Accuracy: 0.8
Training loss = 0.01433837076028188
step = 8, Training Accuracy: 0.83
Training loss = 0.01397234171628952
step = 9, Training Accuracy: 0.81
Training loss = 0.013928058048089345
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.014178647100925446
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.013575561741987863
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.01327253133058548
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.013161695996920268
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.78375
params:  [0.18353204062917858, 0.49961998021881854, 0.99, 0.01, 0.38561903462695835, 0.6743765487568376, 0.8885116241813884, 0.6447987957940126, 0.5673030903873751, 0.4141904488383375, 0.8063958087659858, 0.4547343219188101, 0.5352591290246742, 0.01, 0.6987095158753405, 0.5018914803029472, 0.01, 0.48362169881800393, 0.2413714941493689, 0.6297770192895416, 0.10664356615056014, 0.47434609755306956, 0.6047398035823759, 0.12927544401059554]
Training loss = 0.022843373318513233
step = 0, Training Accuracy: 0.69
Validation Accuracy: 0.795
Training loss = 0.020604473451773325
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.02166710873444875
step = 2, Training Accuracy: 0.7233333333333334
Training loss = 0.01822776238123576
step = 3, Training Accuracy: 0.7366666666666667
Training loss = 0.017117601831754047
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016942047874132792
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.018836026390393574
step = 6, Training Accuracy: 0.76
Training loss = 0.016717272301514943
step = 7, Training Accuracy: 0.77
Training loss = 0.016516083975632985
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.018686496714750925
step = 9, Training Accuracy: 0.78
Training loss = 0.01945199797550837
step = 10, Training Accuracy: 0.75
Training loss = 0.016796501974264782
step = 11, Training Accuracy: 0.79
Training loss = 0.016732786695162455
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.015975997547308603
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01668260246515274
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.76375
params:  [0.2713528804985672, 0.40535520617918286, 0.9436885083410991, 0.2892170944681718, 0.1664506612878857, 0.5017143969969905, 0.818235647049337, 0.42548533075098016, 0.5821488312462618, 0.20842324017454017, 0.7733403720400048, 0.665373665566005, 0.8558660702358184, 0.01, 0.44589168943146146, 0.99, 0.48881132406166505, 0.20456930709029206, 0.7014295395878809, 0.6952520584476618, 0.01, 0.5287705532796587, 0.4152925259570498, 0.5718244270594699]
Training loss = 0.01473094493150711
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77375
Training loss = 0.014129673937956493
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01607565442721049
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01430628091096878
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.014243340194225311
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.01375398168961207
step = 5, Training Accuracy: 0.8233333333333334
Training loss = 0.014052124420801799
step = 6, Training Accuracy: 0.8466666666666667
Training loss = 0.013418951431910197
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.012747701853513718
step = 8, Training Accuracy: 0.86
Training loss = 0.012253623803456624
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.012224597831567128
step = 10, Training Accuracy: 0.8633333333333333
Training loss = 0.01017113909125328
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.013591089447339376
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.013174985547860464
step = 13, Training Accuracy: 0.83
Training loss = 0.011120969280600548
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.77375
[[0.4547655952874812, 0.28165898860798266, 0.9313497090353844, 0.09877739858393775, 0.24967746486496376, 0.6916135867405594, 0.99, 0.4257214368754503, 0.19390022902356963, 0.4087688945547061, 0.6642954384403978, 0.1675478210603933, 0.9104932877364046, 0.08072778117207509, 0.6188051243787557, 0.3807347582284515, 0.01, 0.13443597441535338, 0.04224432738021644, 0.5474947406862557, 0.1310079010276153, 0.9145625456850546, 0.1915213004747974, 0.900541543870349], [0.4491086820559641, 0.5110112673570865, 0.8731044710710582, 0.18218755082068294, 0.3650997462797822, 0.21605293979567114, 0.99, 0.5389306222703668, 0.233419236545455, 0.17544182845835327, 0.5731093655427737, 0.5015190586068116, 0.99, 0.01, 0.49726851364050745, 0.694593240228059, 0.39460950335574463, 0.1463446569283618, 0.23974226312300628, 0.5161554718255529, 0.434586626846959, 0.99, 0.42466523379905513, 0.33664723473173774], [0.46673565977876436, 0.4524581956952515, 0.99, 0.18407950443766083, 0.01, 0.7988272990473315, 0.7725031874201638, 0.44508254846009104, 0.39547024291776, 0.01, 0.5763782370379694, 0.4210971943806549, 0.6462304696377787, 0.2248099558221034, 0.09715266481129547, 0.5964360926924384, 0.39891572745218307, 0.22704627741918626, 0.649080395792835, 0.7603421000437551, 0.24518981723775254, 0.7398003436107724, 0.6720523499236201, 0.5438036977087074], [0.5276525019826608, 0.39720278997649616, 0.9043898479492474, 0.01, 0.01, 0.5761523622871182, 0.8963461011337522, 0.38611137170199117, 0.29201210299753866, 0.40827945667374743, 0.6401064156845556, 0.3677593771010182, 0.59998766268035, 0.01, 0.42760742933362667, 0.23458347443070654, 0.4253623917021472, 0.01, 0.5943349780281952, 0.4585632599330561, 0.17284405623672772, 0.7670717273372735, 0.01, 0.47476174770263724], [0.5543142221000357, 0.21038090032019152, 0.99, 0.14840117843510553, 0.5588480728367612, 0.8188663732033883, 0.99, 0.40305956025570033, 0.20246762762894918, 0.6530789933361466, 0.8645799006645253, 0.3115976266840341, 0.5890457413106946, 0.36639414943621085, 0.3108058327285407, 0.41881553978163677, 0.7828279742162638, 0.0505288652801687, 0.4199492823240118, 0.6411730388248528, 0.17888864521405046, 0.3857840244300751, 0.7987487736612409, 0.6701104621524522], [0.29200000421851197, 0.44082630119215194, 0.99, 0.377436481900627, 0.01, 0.4756540281344428, 0.99, 0.6207305527978217, 0.1717973598964842, 0.22164370679755768, 0.57127821173485, 0.4144368678282941, 0.99, 0.10621141675908706, 0.4085763460497661, 0.7241081134422214, 0.14078599821697332, 0.07571478114505453, 0.49514934275992417, 0.947219257659554, 0.3337262965489811, 0.5186326276567831, 0.1320978530989807, 0.7024560932796862], [0.18353204062917858, 0.49961998021881854, 0.99, 0.01, 0.38561903462695835, 0.6743765487568376, 0.8885116241813884, 0.6447987957940126, 0.5673030903873751, 0.4141904488383375, 0.8063958087659858, 0.4547343219188101, 0.5352591290246742, 0.01, 0.6987095158753405, 0.5018914803029472, 0.01, 0.48362169881800393, 0.2413714941493689, 0.6297770192895416, 0.10664356615056014, 0.47434609755306956, 0.6047398035823759, 0.12927544401059554], [0.2713528804985672, 0.40535520617918286, 0.9436885083410991, 0.2892170944681718, 0.1664506612878857, 0.5017143969969905, 0.818235647049337, 0.42548533075098016, 0.5821488312462618, 0.20842324017454017, 0.7733403720400048, 0.665373665566005, 0.8558660702358184, 0.01, 0.44589168943146146, 0.99, 0.48881132406166505, 0.20456930709029206, 0.7014295395878809, 0.6952520584476618, 0.01, 0.5287705532796587, 0.4152925259570498, 0.5718244270594699]]
7  	8     	0.778438	0.0129716 	0.76375	0.80375
params:  [0.28457267294017535, 0.20114009099116528, 0.9604582130098598, 0.5123039430653643, 0.11488824275718662, 0.18733338136453292, 0.99, 0.3566845257457399, 0.11318712703099038, 0.3939778332523538, 0.5852021522176671, 0.18219226739815442, 0.7712341970321477, 0.2752046135025769, 0.5288699216370365, 0.44314210792385944, 0.36387006089759494, 0.01, 0.5982882587548759, 0.6777974659907813, 0.31348953703713506, 0.99, 0.369311329270704, 0.8556800715285493]
Training loss = 0.01765624980131785
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.795
Training loss = 0.015515685280164082
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014102106789747874
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.012537972728411357
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.01612712780634562
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.011923911223808925
step = 5, Training Accuracy: 0.83
Training loss = 0.0122504693766435
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.011773279458284379
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.0117172006269296
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.012702715297540029
step = 9, Training Accuracy: 0.85
Training loss = 0.011545506169398626
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.013397945314645767
step = 11, Training Accuracy: 0.8533333333333334
Training loss = 0.010553881724675496
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.010929281264543534
step = 13, Training Accuracy: 0.86
Training loss = 0.01126670092344284
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7775
params:  [0.29120268880429323, 0.33372203510831394, 0.99, 0.2196785976294362, 0.31086459216205337, 0.29641826534349336, 0.7546494622861715, 0.2890807202413891, 0.3362439454661382, 0.2598027835476085, 0.3810505838825539, 0.5661619459045124, 0.7484116844450137, 0.3038859850328516, 0.017484441518455796, 0.5182946571229916, 0.16637640920138141, 0.01, 0.3192515150858133, 0.7003751733194347, 0.17706909795705927, 0.83159810271397, 0.37080465814695046, 0.417507435059877]
Training loss = 0.02278105398019155
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78125
Training loss = 0.015300143162409465
step = 1, Training Accuracy: 0.79
Training loss = 0.01728373517592748
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.014868540664513905
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.01609909415245056
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01517866830031077
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014432955781618755
step = 6, Training Accuracy: 0.84
Training loss = 0.014746755063533783
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.012945671677589416
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.01368624081214269
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.012300459891557694
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.012504324813683828
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.012296334207057953
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.011384086310863495
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.011872353901465734
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.7775
params:  [0.5581570596735128, 0.309851394750672, 0.7248489594571608, 0.11335005173441637, 0.14771361602282057, 0.4848766314419811, 0.99, 0.606794886650927, 0.25645797484983673, 0.1681022979102711, 0.6385092180101445, 0.5711635485209315, 0.8849959238326373, 0.0467099870702915, 0.3140766395667605, 0.5561444793019443, 0.6135331441628281, 0.01, 0.40590866337031445, 0.5390017998769068, 0.3921969668869095, 0.7844965299117167, 0.20632345177749845, 0.7663391152634471]
Training loss = 0.01822891136010488
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.77875
Training loss = 0.015341934065024059
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01394003560145696
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.015529889663060506
step = 3, Training Accuracy: 0.81
Training loss = 0.016828916072845458
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.013846887449423473
step = 5, Training Accuracy: 0.84
Training loss = 0.012333112756411234
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.012277063578367234
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013074973275264105
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.012781311770280203
step = 9, Training Accuracy: 0.84
Training loss = 0.012413252741098404
step = 10, Training Accuracy: 0.83
Training loss = 0.013130078564087549
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.012179123759269715
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.01144905373454094
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.01125746543208758
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.77375
params:  [0.43901514534549363, 0.6385465448896344, 0.9187280237295198, 0.3324561703507152, 0.2951051593895955, 0.25907360355764997, 0.7687544470366016, 0.48181127868819384, 0.1278491481009097, 0.13269483599236181, 0.7410964565707742, 0.3597576878242615, 0.8546825793778422, 0.15337481174554765, 0.23068337280947177, 0.8602726732830367, 0.18308299589299548, 0.323229118326556, 0.01, 0.7679132758733599, 0.8066944471920883, 0.9215979073625513, 0.5264925518740831, 0.5403952240546577]
Training loss = 0.019591544369856516
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.78
Training loss = 0.01782830834388733
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.017054469982783
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01532950649658839
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.015011696914831797
step = 4, Training Accuracy: 0.82
Training loss = 0.016506858070691428
step = 5, Training Accuracy: 0.8
Training loss = 0.01495513454079628
step = 6, Training Accuracy: 0.79
Training loss = 0.016438225309054057
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01524292101462682
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.01343870500723521
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.015598443448543548
step = 10, Training Accuracy: 0.8266666666666667
Training loss = 0.015250480274359384
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.016759587426980336
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.013169976472854615
step = 13, Training Accuracy: 0.85
Training loss = 0.014553934335708618
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.78625
params:  [0.20776085144270867, 0.4325449905447493, 0.99, 0.36709552243335264, 0.6779522201243415, 0.1607887322471488, 0.837926880145398, 0.3619056808577181, 0.24056178783511523, 0.060561530263494895, 0.7892485558007686, 0.9114589488060545, 0.8592995876212443, 0.01, 0.17268143505610337, 0.5160757975757709, 0.2712020418299057, 0.13638075684458678, 0.3239700077863652, 0.7163708156084339, 0.17560801110259688, 0.8038863607063715, 0.2934442176516371, 0.1839756982029625]
Training loss = 0.017623898088932038
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.77875
Training loss = 0.015614743828773498
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.015808645486831665
step = 2, Training Accuracy: 0.81
Training loss = 0.014703076829512914
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.013040971060593922
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.012727532188097636
step = 5, Training Accuracy: 0.81
Training loss = 0.014173972904682159
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.013231875399748485
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.013811865399281184
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.010846577510237694
step = 9, Training Accuracy: 0.87
Training loss = 0.011429222176472346
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.014007037530342738
step = 11, Training Accuracy: 0.84
Training loss = 0.011735204756259918
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.012494467447201412
step = 13, Training Accuracy: 0.84
Training loss = 0.012486594617366792
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.79
params:  [0.4709266029799916, 0.41247075785096793, 0.99, 0.36915801669292914, 0.4822135066509379, 0.20657013346108666, 0.770443966028939, 0.5401975630017882, 0.033106757726599184, 0.269945023652172, 0.16643674968605754, 0.9073888507803236, 0.7571154815785572, 0.01, 0.2133032806991309, 0.6406435679490323, 0.9067690574083956, 0.01, 0.41776529777389443, 0.8475703138560315, 0.6211338773911, 0.9024860348293464, 0.7716279015455143, 0.3009026670126222]
Training loss = 0.02079411079486211
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7775
Training loss = 0.02175645778576533
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01718005875746409
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.018916023472944896
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.017135914067427316
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.016820933322111764
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.018749116361141203
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01595276653766632
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.0145699542760849
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.019556885461012523
step = 9, Training Accuracy: 0.76
Training loss = 0.016207365492979686
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.01612371375163396
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01685306578874588
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.014450006633996964
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.014872681299845378
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78
params:  [0.533103367085194, 0.3310709472783946, 0.9571978031774377, 0.01179388084943575, 0.33563662486255663, 0.24736420944029774, 0.8859023888450599, 0.7527470033482015, 0.01, 0.01, 0.2231247713501081, 0.1950889000534427, 0.8967897048635746, 0.01, 0.37040765600364994, 0.5804315951985008, 0.21621376566626038, 0.19404271827186625, 0.2019483584712045, 0.801637245474134, 0.36886479700789454, 0.754033327835232, 0.5106572675031197, 0.1582012825491234]
Training loss = 0.01891984244187673
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7875
Training loss = 0.017926410734653474
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01652328610420227
step = 2, Training Accuracy: 0.82
Training loss = 0.01550380140542984
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015419169465700785
step = 4, Training Accuracy: 0.8
Training loss = 0.015920024812221528
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.014176768263181051
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014111312727133432
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.015940007468064624
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015899772743384045
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.01477530986070633
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01506898432970047
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014895750284194946
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014440366377433142
step = 13, Training Accuracy: 0.81
Training loss = 0.014771329760551453
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.78875
params:  [0.6481717672107895, 0.532389246060544, 0.7945167939283208, 0.42082445374356603, 0.2789626258292812, 0.37149624773457496, 0.99, 0.7123322462679181, 0.2805751755877536, 0.47974899976930685, 0.38985249513648174, 0.6349158573894661, 0.817789930853306, 0.01, 0.5178004094762821, 0.5392316801761949, 0.3910823517374738, 0.2795296590972245, 0.2761169148934396, 0.39732674718462546, 0.18096200446392222, 0.7122478002189786, 0.3885216796221148, 0.8417973445659934]
Training loss = 0.02092738340298335
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78375
Training loss = 0.02025765776634216
step = 1, Training Accuracy: 0.73
Training loss = 0.017125840187072753
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.01770555118719737
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01859499583641688
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.016453084846337635
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.01661634922027588
step = 6, Training Accuracy: 0.81
Training loss = 0.015935746332009632
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015427667001883189
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015334972639878592
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.019535004695256552
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.01540460040171941
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.01551887720823288
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.015915908614794413
step = 13, Training Accuracy: 0.8
Training loss = 0.016129129727681477
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.76625
[[0.28457267294017535, 0.20114009099116528, 0.9604582130098598, 0.5123039430653643, 0.11488824275718662, 0.18733338136453292, 0.99, 0.3566845257457399, 0.11318712703099038, 0.3939778332523538, 0.5852021522176671, 0.18219226739815442, 0.7712341970321477, 0.2752046135025769, 0.5288699216370365, 0.44314210792385944, 0.36387006089759494, 0.01, 0.5982882587548759, 0.6777974659907813, 0.31348953703713506, 0.99, 0.369311329270704, 0.8556800715285493], [0.29120268880429323, 0.33372203510831394, 0.99, 0.2196785976294362, 0.31086459216205337, 0.29641826534349336, 0.7546494622861715, 0.2890807202413891, 0.3362439454661382, 0.2598027835476085, 0.3810505838825539, 0.5661619459045124, 0.7484116844450137, 0.3038859850328516, 0.017484441518455796, 0.5182946571229916, 0.16637640920138141, 0.01, 0.3192515150858133, 0.7003751733194347, 0.17706909795705927, 0.83159810271397, 0.37080465814695046, 0.417507435059877], [0.5581570596735128, 0.309851394750672, 0.7248489594571608, 0.11335005173441637, 0.14771361602282057, 0.4848766314419811, 0.99, 0.606794886650927, 0.25645797484983673, 0.1681022979102711, 0.6385092180101445, 0.5711635485209315, 0.8849959238326373, 0.0467099870702915, 0.3140766395667605, 0.5561444793019443, 0.6135331441628281, 0.01, 0.40590866337031445, 0.5390017998769068, 0.3921969668869095, 0.7844965299117167, 0.20632345177749845, 0.7663391152634471], [0.43901514534549363, 0.6385465448896344, 0.9187280237295198, 0.3324561703507152, 0.2951051593895955, 0.25907360355764997, 0.7687544470366016, 0.48181127868819384, 0.1278491481009097, 0.13269483599236181, 0.7410964565707742, 0.3597576878242615, 0.8546825793778422, 0.15337481174554765, 0.23068337280947177, 0.8602726732830367, 0.18308299589299548, 0.323229118326556, 0.01, 0.7679132758733599, 0.8066944471920883, 0.9215979073625513, 0.5264925518740831, 0.5403952240546577], [0.20776085144270867, 0.4325449905447493, 0.99, 0.36709552243335264, 0.6779522201243415, 0.1607887322471488, 0.837926880145398, 0.3619056808577181, 0.24056178783511523, 0.060561530263494895, 0.7892485558007686, 0.9114589488060545, 0.8592995876212443, 0.01, 0.17268143505610337, 0.5160757975757709, 0.2712020418299057, 0.13638075684458678, 0.3239700077863652, 0.7163708156084339, 0.17560801110259688, 0.8038863607063715, 0.2934442176516371, 0.1839756982029625], [0.4709266029799916, 0.41247075785096793, 0.99, 0.36915801669292914, 0.4822135066509379, 0.20657013346108666, 0.770443966028939, 0.5401975630017882, 0.033106757726599184, 0.269945023652172, 0.16643674968605754, 0.9073888507803236, 0.7571154815785572, 0.01, 0.2133032806991309, 0.6406435679490323, 0.9067690574083956, 0.01, 0.41776529777389443, 0.8475703138560315, 0.6211338773911, 0.9024860348293464, 0.7716279015455143, 0.3009026670126222], [0.533103367085194, 0.3310709472783946, 0.9571978031774377, 0.01179388084943575, 0.33563662486255663, 0.24736420944029774, 0.8859023888450599, 0.7527470033482015, 0.01, 0.01, 0.2231247713501081, 0.1950889000534427, 0.8967897048635746, 0.01, 0.37040765600364994, 0.5804315951985008, 0.21621376566626038, 0.19404271827186625, 0.2019483584712045, 0.801637245474134, 0.36886479700789454, 0.754033327835232, 0.5106572675031197, 0.1582012825491234], [0.6481717672107895, 0.532389246060544, 0.7945167939283208, 0.42082445374356603, 0.2789626258292812, 0.37149624773457496, 0.99, 0.7123322462679181, 0.2805751755877536, 0.47974899976930685, 0.38985249513648174, 0.6349158573894661, 0.817789930853306, 0.01, 0.5178004094762821, 0.5392316801761949, 0.3910823517374738, 0.2795296590972245, 0.2761169148934396, 0.39732674718462546, 0.18096200446392222, 0.7122478002189786, 0.3885216796221148, 0.8417973445659934]]
8  	8     	0.78    	0.0075519 	0.76625	0.79   
params:  [0.5147474995797051, 0.6158043467148657, 0.99, 0.11375276685602634, 0.16684019808839773, 0.4618420036496816, 0.7173034592660411, 0.4035624299461185, 0.01, 0.08324486599054223, 0.6512088801982635, 0.7690773170737468, 0.6626967419053442, 0.06627852028015692, 0.2694832982621482, 0.6757563096374757, 0.01, 0.1366074160292163, 0.3452888005503521, 0.580004648764932, 0.11048790528097532, 0.8973704595225995, 0.274076917395392, 0.2460020604729277]
Training loss = 0.01804211566845576
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7725
Training loss = 0.016935234467188517
step = 1, Training Accuracy: 0.76
Training loss = 0.017939480741818746
step = 2, Training Accuracy: 0.8
Training loss = 0.014138427525758744
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.015410240292549133
step = 4, Training Accuracy: 0.79
Training loss = 0.015294747054576873
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.014828461805979411
step = 6, Training Accuracy: 0.81
Training loss = 0.014524586846431096
step = 7, Training Accuracy: 0.81
Training loss = 0.014042051136493683
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.01412018209695816
step = 9, Training Accuracy: 0.81
Training loss = 0.013629465401172637
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01359259953101476
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.013398413360118867
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.012815359234809875
step = 13, Training Accuracy: 0.84
Training loss = 0.013288872440656026
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.77625
params:  [0.10099493938983575, 0.5419324615152877, 0.9595716284999395, 0.3340430018126675, 0.639228437828071, 0.01, 0.9264458921613983, 0.45813094302376167, 0.4663170636030024, 0.047602995143469114, 0.5504767168790898, 0.6198753738061495, 0.99, 0.15771490923884082, 0.18945088431929674, 0.7328277562586375, 0.04058620506945665, 0.06347360819784163, 0.5749539614183691, 0.768259076831453, 0.7367215067497128, 0.99, 0.5693975792085277, 0.8097267868001746]
Training loss = 0.017494870374600093
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
Training loss = 0.018212653199831643
step = 1, Training Accuracy: 0.78
Training loss = 0.01526290198167165
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.016145735681056976
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.018309142291545868
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015362712542215983
step = 5, Training Accuracy: 0.83
Training loss = 0.014140750964482625
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.013758376340071361
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.01635443598031998
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.014544617931048075
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016243972877661387
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014499611755212149
step = 11, Training Accuracy: 0.82
Training loss = 0.014717523753643037
step = 12, Training Accuracy: 0.8133333333333334
Training loss = 0.013488816618919373
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.01572309816877047
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.77625
params:  [0.4826684116377468, 0.3116011334832542, 0.9701772895501536, 0.07520809557219099, 0.570856541005992, 0.01064200875540966, 0.6198386940475974, 0.5239056865214101, 0.33375756272341683, 0.030267955796381314, 0.6960745317977399, 0.6350126582881194, 0.9032828720168393, 0.01, 0.4124355495419616, 0.23785452900918225, 0.39502020783225084, 0.09298969677007043, 0.7009320090186323, 0.823564728326589, 0.7062637414721586, 0.8890418770912143, 0.01, 0.01]
Training loss = 0.017490060925483705
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.7875
Training loss = 0.016148165961106617
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.015404729545116425
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.015316289365291596
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.013663143863280615
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.0137181356549263
step = 5, Training Accuracy: 0.83
Training loss = 0.013670149942239125
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.013801986475785573
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.011713204483191172
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013471583922704061
step = 9, Training Accuracy: 0.83
Training loss = 0.013269193768501282
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.013482721149921417
step = 11, Training Accuracy: 0.83
Training loss = 0.013552004595597585
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.013279227316379547
step = 13, Training Accuracy: 0.83
Training loss = 0.012667046288649241
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.7825
params:  [0.15854114114506018, 0.7359237582027571, 0.99, 0.25143790993529297, 0.7055038052036823, 0.4100905984310163, 0.7752307692087412, 0.6190305542995816, 0.2925341799891828, 0.01, 0.5722779262424991, 0.6432112114768915, 0.5301711460350504, 0.01, 0.19163055606703816, 0.6929093851965787, 0.46905361035159066, 0.16845721663435687, 0.2451567211881258, 0.7913044423556517, 0.6550157616295498, 0.9526611792907297, 0.494696982523824, 0.16845546292888158]
Training loss = 0.02016667346159617
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.7725
Training loss = 0.018147469063599906
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.017588228285312653
step = 2, Training Accuracy: 0.77
Training loss = 0.017675714989503225
step = 3, Training Accuracy: 0.7533333333333333
Training loss = 0.017594581842422484
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.015901189744472504
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.018296444416046144
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.01691307008266449
step = 7, Training Accuracy: 0.7733333333333333
Training loss = 0.016401505966981252
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015615446964899698
step = 9, Training Accuracy: 0.79
Training loss = 0.016846792101860047
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016922267576058705
step = 11, Training Accuracy: 0.78
Training loss = 0.02021426200866699
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.01641532003879547
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015959929625193277
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78125
params:  [0.36909878149336367, 0.3629681832945284, 0.99, 0.42233890376340777, 0.5660232216212837, 0.05539345525284767, 0.99, 0.3686431306330654, 0.01, 0.33560016721720953, 0.3917666491946368, 0.5158711133397966, 0.99, 0.030121064478993435, 0.5653543475721554, 0.4073339753812312, 0.2827018041410747, 0.10578961410470647, 0.20865920848642425, 0.7726048520691975, 0.23115998825533246, 0.8418397736556146, 0.40309416349525506, 0.11864560062122967]
Training loss = 0.015764970978101093
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.77875
Training loss = 0.015102974474430084
step = 1, Training Accuracy: 0.8
Training loss = 0.01478933130701383
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.012857513775428136
step = 3, Training Accuracy: 0.84
Training loss = 0.013069774210453033
step = 4, Training Accuracy: 0.85
Training loss = 0.0138731649518013
step = 5, Training Accuracy: 0.84
Training loss = 0.013355972270170848
step = 6, Training Accuracy: 0.81
Training loss = 0.01561188797156016
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.011953158130248387
step = 8, Training Accuracy: 0.85
Training loss = 0.011764404624700546
step = 9, Training Accuracy: 0.84
Training loss = 0.013946625739336013
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.01357716903090477
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01354069655140241
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.011775913933912913
step = 13, Training Accuracy: 0.86
Training loss = 0.01323105812072754
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.7975
params:  [0.32029725031746425, 0.7934373419781213, 0.7016903705470304, 0.2394613501201344, 0.34089932111480326, 0.12727791395597937, 0.8647137342082205, 0.7636484956000953, 0.4973062607756983, 0.08433884287090555, 0.5986536369431465, 0.6292799299645967, 0.99, 0.01, 0.22450894618403425, 0.8745369267042786, 0.3715346157419386, 0.19455867423888523, 0.011781971106637923, 0.9325087699257948, 0.4587006642297325, 0.9229659576965933, 0.41401579360462704, 0.01]
Training loss = 0.017802647898594537
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79375
Training loss = 0.017081676125526427
step = 1, Training Accuracy: 0.77
Training loss = 0.018090411126613616
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015501410365104676
step = 3, Training Accuracy: 0.77
Training loss = 0.015348926385243733
step = 4, Training Accuracy: 0.77
Training loss = 0.01389873613913854
step = 5, Training Accuracy: 0.8333333333333334
Training loss = 0.014519736965497335
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.01785882979631424
step = 7, Training Accuracy: 0.7766666666666666
Training loss = 0.016253925065199536
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.014684863289197285
step = 9, Training Accuracy: 0.81
Training loss = 0.015223039984703065
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01437737892071406
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.015291060209274293
step = 12, Training Accuracy: 0.78
Training loss = 0.013586740990479787
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014092530806859334
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78625
params:  [0.33640865608549403, 0.5136793664427626, 0.8604874091484783, 0.013858636664153795, 0.28982067946468415, 0.3075856423726707, 0.99, 0.4915206516783137, 0.0974040272627572, 0.18154081964856472, 0.578359067778994, 0.801507583364033, 0.8829889142786603, 0.01, 0.2844667292811583, 0.9209785366286604, 0.01, 0.11924787815473137, 0.3176697379165914, 0.6135837780625633, 0.11626518707340325, 0.777033358669486, 0.39568757432024404, 0.49889411775303916]
Training loss = 0.01886345197757085
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.79125
Training loss = 0.019543562332789102
step = 1, Training Accuracy: 0.76
Training loss = 0.01957521806160609
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01608515650033951
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016484948893388113
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.014670895983775457
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01609315241376559
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.015564559400081635
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.015627231498559317
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015156676371892294
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015390331049760182
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.016229305962721506
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.015322955250740051
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01494050552447637
step = 13, Training Accuracy: 0.82
Training loss = 0.015682050089041392
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7875
params:  [0.46512991761955175, 0.2829690202819668, 0.99, 0.06206952894452991, 0.6382847474239574, 0.5595407556116152, 0.99, 0.45952497519041635, 0.46666324228096767, 0.12754870101362273, 0.6548357122212316, 0.5065910952385374, 0.6006105419636194, 0.010785112600126451, 0.19184323760557287, 0.7766479137070293, 0.33204045047323794, 0.17023382174840324, 0.062464772850756606, 0.7237852738564854, 0.1611677269513114, 0.5836361291240241, 0.5335871467718964, 0.5024297661037984]
Training loss = 0.01808345710237821
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.79
Training loss = 0.01615981976191203
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.01450153261423111
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.013668930927912394
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.013801469802856445
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.013763480186462403
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.01275166466832161
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.012980164935191472
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.012823950002590815
step = 8, Training Accuracy: 0.85
Training loss = 0.013896139015754065
step = 9, Training Accuracy: 0.82
Training loss = 0.015177534570296606
step = 10, Training Accuracy: 0.83
Training loss = 0.010756840258836746
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.01509124497572581
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.012438368598620096
step = 13, Training Accuracy: 0.84
Training loss = 0.012017514258623123
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.795
[[0.5147474995797051, 0.6158043467148657, 0.99, 0.11375276685602634, 0.16684019808839773, 0.4618420036496816, 0.7173034592660411, 0.4035624299461185, 0.01, 0.08324486599054223, 0.6512088801982635, 0.7690773170737468, 0.6626967419053442, 0.06627852028015692, 0.2694832982621482, 0.6757563096374757, 0.01, 0.1366074160292163, 0.3452888005503521, 0.580004648764932, 0.11048790528097532, 0.8973704595225995, 0.274076917395392, 0.2460020604729277], [0.10099493938983575, 0.5419324615152877, 0.9595716284999395, 0.3340430018126675, 0.639228437828071, 0.01, 0.9264458921613983, 0.45813094302376167, 0.4663170636030024, 0.047602995143469114, 0.5504767168790898, 0.6198753738061495, 0.99, 0.15771490923884082, 0.18945088431929674, 0.7328277562586375, 0.04058620506945665, 0.06347360819784163, 0.5749539614183691, 0.768259076831453, 0.7367215067497128, 0.99, 0.5693975792085277, 0.8097267868001746], [0.4826684116377468, 0.3116011334832542, 0.9701772895501536, 0.07520809557219099, 0.570856541005992, 0.01064200875540966, 0.6198386940475974, 0.5239056865214101, 0.33375756272341683, 0.030267955796381314, 0.6960745317977399, 0.6350126582881194, 0.9032828720168393, 0.01, 0.4124355495419616, 0.23785452900918225, 0.39502020783225084, 0.09298969677007043, 0.7009320090186323, 0.823564728326589, 0.7062637414721586, 0.8890418770912143, 0.01, 0.01], [0.15854114114506018, 0.7359237582027571, 0.99, 0.25143790993529297, 0.7055038052036823, 0.4100905984310163, 0.7752307692087412, 0.6190305542995816, 0.2925341799891828, 0.01, 0.5722779262424991, 0.6432112114768915, 0.5301711460350504, 0.01, 0.19163055606703816, 0.6929093851965787, 0.46905361035159066, 0.16845721663435687, 0.2451567211881258, 0.7913044423556517, 0.6550157616295498, 0.9526611792907297, 0.494696982523824, 0.16845546292888158], [0.36909878149336367, 0.3629681832945284, 0.99, 0.42233890376340777, 0.5660232216212837, 0.05539345525284767, 0.99, 0.3686431306330654, 0.01, 0.33560016721720953, 0.3917666491946368, 0.5158711133397966, 0.99, 0.030121064478993435, 0.5653543475721554, 0.4073339753812312, 0.2827018041410747, 0.10578961410470647, 0.20865920848642425, 0.7726048520691975, 0.23115998825533246, 0.8418397736556146, 0.40309416349525506, 0.11864560062122967], [0.32029725031746425, 0.7934373419781213, 0.7016903705470304, 0.2394613501201344, 0.34089932111480326, 0.12727791395597937, 0.8647137342082205, 0.7636484956000953, 0.4973062607756983, 0.08433884287090555, 0.5986536369431465, 0.6292799299645967, 0.99, 0.01, 0.22450894618403425, 0.8745369267042786, 0.3715346157419386, 0.19455867423888523, 0.011781971106637923, 0.9325087699257948, 0.4587006642297325, 0.9229659576965933, 0.41401579360462704, 0.01], [0.33640865608549403, 0.5136793664427626, 0.8604874091484783, 0.013858636664153795, 0.28982067946468415, 0.3075856423726707, 0.99, 0.4915206516783137, 0.0974040272627572, 0.18154081964856472, 0.578359067778994, 0.801507583364033, 0.8829889142786603, 0.01, 0.2844667292811583, 0.9209785366286604, 0.01, 0.11924787815473137, 0.3176697379165914, 0.6135837780625633, 0.11626518707340325, 0.777033358669486, 0.39568757432024404, 0.49889411775303916], [0.46512991761955175, 0.2829690202819668, 0.99, 0.06206952894452991, 0.6382847474239574, 0.5595407556116152, 0.99, 0.45952497519041635, 0.46666324228096767, 0.12754870101362273, 0.6548357122212316, 0.5065910952385374, 0.6006105419636194, 0.010785112600126451, 0.19184323760557287, 0.7766479137070293, 0.33204045047323794, 0.17023382174840324, 0.062464772850756606, 0.7237852738564854, 0.1611677269513114, 0.5836361291240241, 0.5335871467718964, 0.5024297661037984]]
9  	8     	0.785312	0.00738849	0.77625	0.7975 
params:  [0.5556961562529823, 0.13612937056358343, 0.99, 0.4242054590716958, 0.5766409292791236, 0.5911850471874005, 0.8506170527582769, 0.3704846516994011, 0.274331652123314, 0.3053496714749509, 0.1671258426696937, 0.6768972024852292, 0.99, 0.01, 0.587975529496757, 0.3039837454831234, 0.34736718129625194, 0.2369190211869978, 0.2888201536842664, 0.3560541099408648, 0.6186597515709865, 0.8929886944284531, 0.5465334219191125, 0.0923266536628149]
Training loss = 0.01397747943798701
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.79125
Training loss = 0.013299725999434788
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.013648302753766378
step = 2, Training Accuracy: 0.83
Training loss = 0.012518262962500254
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.011920718650023143
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.012477559943993886
step = 5, Training Accuracy: 0.86
Training loss = 0.010443997929493586
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.01052042841911316
step = 7, Training Accuracy: 0.86
Training loss = 0.013697502464056014
step = 8, Training Accuracy: 0.86
Training loss = 0.010575980246067047
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.010699776510397593
step = 10, Training Accuracy: 0.88
Training loss = 0.011108171790838242
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.010088508228460948
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.009503722041845322
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.010557354589303335
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.785
params:  [0.3149309687976496, 0.1269302304261869, 0.99, 0.020894719578324367, 0.45955298286417345, 0.31490306250077527, 0.8511947080126862, 0.13887401241544195, 0.18442835403362937, 0.23468027826231952, 0.6120959704413186, 0.3996171685238995, 0.9883731059641361, 0.01, 0.9259060884858503, 0.505177053430861, 0.14428477845089183, 0.277135877741559, 0.06693978000227228, 0.6765510395346419, 0.08981110771557864, 0.7769854225266933, 0.5578533760093123, 0.37737628649582267]
Training loss = 0.01784913053115209
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.775
Training loss = 0.01569779376188914
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.015147651632626852
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014868973046541214
step = 3, Training Accuracy: 0.83
Training loss = 0.013846198817094167
step = 4, Training Accuracy: 0.84
Training loss = 0.015388350288073222
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.013194422721862793
step = 6, Training Accuracy: 0.82
Training loss = 0.011583647678295771
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.013044264664252599
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.014674365818500519
step = 9, Training Accuracy: 0.83
Training loss = 0.01230539470911026
step = 10, Training Accuracy: 0.8333333333333334
Training loss = 0.011658628384272257
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.011912532945473989
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.011558864216009776
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.013053350945313772
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7675
params:  [0.5113142239659141, 0.5352373212199945, 0.9742575303567785, 0.19387524145560403, 0.7664686834252246, 0.01, 0.99, 0.3132300204255538, 0.32499397503176664, 0.19704869828535299, 0.45489080930984616, 0.6186071600263195, 0.8909623880369771, 0.01, 0.4220210060871085, 0.5629336811604638, 0.26720750066758436, 0.07475893490323293, 0.01, 0.8042826317009076, 0.01, 0.6629282563400303, 0.4767305955264921, 0.01]
Training loss = 0.01951992650826772
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.75625
Training loss = 0.017885879973570506
step = 1, Training Accuracy: 0.76
Training loss = 0.015463347931702932
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.016678676505883536
step = 3, Training Accuracy: 0.78
Training loss = 0.013545540571212768
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.015427189767360688
step = 5, Training Accuracy: 0.7966666666666666
Training loss = 0.014304173092047373
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015313851684331893
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.014992615481217702
step = 8, Training Accuracy: 0.79
Training loss = 0.01474547117948532
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014406354029973348
step = 10, Training Accuracy: 0.8
Training loss = 0.016038909554481506
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.014251546760400136
step = 12, Training Accuracy: 0.82
Training loss = 0.013553117364645005
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.014036545157432556
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77
params:  [0.3534334331288562, 0.6407358614271794, 0.9683943757253186, 0.5646067332770324, 0.43668014878942846, 0.22590660256790182, 0.8580467249085303, 0.1330845837498409, 0.2932176668220775, 0.4910211295677673, 0.7058156273639155, 0.8776929424305211, 0.7655447605100659, 0.04382375069110338, 0.4682184275223946, 0.5867362565009872, 0.22582635588303482, 0.3708240779305827, 0.41304925698398853, 0.6162013371551744, 0.4578405010439175, 0.6980677872797245, 0.1748854172354405, 0.9214933548214592]
Training loss = 0.01813870479663213
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7675
Training loss = 0.01705916037162145
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.015268045564492544
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.017469556232293446
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.01456884890794754
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.015769758820533754
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.01594411810239156
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.015249587694803874
step = 7, Training Accuracy: 0.8
Training loss = 0.014266999463240306
step = 8, Training Accuracy: 0.79
Training loss = 0.014963070154190064
step = 9, Training Accuracy: 0.79
Training loss = 0.015673433740933735
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.014676498969395955
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.014212658007939657
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.014723826348781586
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014807832638422648
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7575
params:  [0.4575334121988722, 0.4838351354434704, 0.931296161682143, 0.3793108366694475, 0.48101572854407715, 0.3301626810609415, 0.99, 0.3866018843268235, 0.34728688384386497, 0.36757813645318604, 0.5401122056171526, 0.4791703641823718, 0.99, 0.01, 0.16472171439524025, 0.7865670606250234, 0.22451871822465494, 0.35001316207393074, 0.17708973834307365, 0.99, 0.3962516794025086, 0.9115830439635491, 0.45529900453321354, 0.390462926899729]
Training loss = 0.015978857378164926
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77125
Training loss = 0.017986376484235126
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.017515187958876292
step = 2, Training Accuracy: 0.76
Training loss = 0.015283792118231455
step = 3, Training Accuracy: 0.8
Training loss = 0.014821920494238536
step = 4, Training Accuracy: 0.81
Training loss = 0.014458736081918082
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01703857531150182
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.017620014746983846
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.01614450936516126
step = 8, Training Accuracy: 0.81
Training loss = 0.015753650565942127
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01546844760576884
step = 10, Training Accuracy: 0.79
Training loss = 0.014049787322680156
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.016311503847440085
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.013235669434070587
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.01669265220562617
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.765
params:  [0.557310306346015, 0.5470889656133459, 0.9714736277539096, 0.5701432106387113, 0.6244293772050212, 0.01, 0.8986846454281083, 0.7032869804961462, 0.21670961582945558, 0.3598117576732891, 0.6673823860985955, 0.09292104480517116, 0.8364846583398494, 0.035186705488854206, 0.5066837709919718, 0.7456353714667481, 0.3292511105706074, 0.2906140339797164, 0.45963140844013783, 0.99, 0.01240476118639916, 0.6460147096241099, 0.5636436436895351, 0.3758483309430969]
Training loss = 0.017971933583418528
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.76125
Training loss = 0.017568508287270863
step = 1, Training Accuracy: 0.77
Training loss = 0.01536054015159607
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01594655692577362
step = 3, Training Accuracy: 0.8
Training loss = 0.015492276847362518
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01781728009382884
step = 5, Training Accuracy: 0.7366666666666667
Training loss = 0.016906216144561767
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.016796221534411113
step = 7, Training Accuracy: 0.79
Training loss = 0.01620148539543152
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.017956091562906902
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.015676257212956745
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.016174809237321217
step = 11, Training Accuracy: 0.78
Training loss = 0.015683077971140543
step = 12, Training Accuracy: 0.79
Training loss = 0.014578721821308135
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.016604013442993164
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.7675
params:  [0.1844599200852512, 0.852651790247681, 0.859480622605146, 0.11415523316810414, 0.8631243411040143, 0.410022286053333, 0.99, 0.3377150296858934, 0.13204889166441197, 0.3587735554227326, 0.6026685811589444, 0.6335056173663361, 0.8815509689255998, 0.01, 0.44842665421643535, 0.6274118956101539, 0.5378481290931028, 0.01, 0.33462414194469803, 0.7300737004209169, 0.11547898112261255, 0.6828526991454041, 0.8845357475311388, 0.35461093350125417]
Training loss = 0.020309617618719737
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.78
Training loss = 0.018450602889060974
step = 1, Training Accuracy: 0.77
Training loss = 0.017368236581484477
step = 2, Training Accuracy: 0.8
Training loss = 0.018103115856647492
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.018522442281246186
step = 4, Training Accuracy: 0.73
Training loss = 0.0179827747742335
step = 5, Training Accuracy: 0.7433333333333333
Training loss = 0.020228295028209685
step = 6, Training Accuracy: 0.73
Training loss = 0.020623189210891724
step = 7, Training Accuracy: 0.7066666666666667
Training loss = 0.01851360112428665
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.02025772472222646
step = 9, Training Accuracy: 0.74
Training loss = 0.01788993924856186
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.018997935553391774
step = 11, Training Accuracy: 0.76
Training loss = 0.01706735481818517
step = 12, Training Accuracy: 0.7366666666666667
Training loss = 0.017580265899499257
step = 13, Training Accuracy: 0.8
Training loss = 0.018193609714508056
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.77125
params:  [0.4453455624485602, 0.47387943901222707, 0.99, 0.30206663707695586, 0.6989342703724805, 0.41312305370836133, 0.947684117763256, 0.47330264258221366, 0.38591942901212833, 0.22864507596917605, 0.3260009352419226, 0.99, 0.7198012410304342, 0.01, 0.24197362696007327, 0.7055851467799759, 0.01, 0.01, 0.014826954562661726, 0.6819570406415078, 0.05749676332077791, 0.6710021971856437, 0.35775544190327885, 0.1144754156916305]
Training loss = 0.016818852225939433
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79125
Training loss = 0.014042838513851165
step = 1, Training Accuracy: 0.83
Training loss = 0.016333474119504292
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.016275982062021892
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01581608682870865
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.015084410309791565
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.014742896457513173
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.01568181162079175
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.015233595271905263
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015236278474330902
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.014374857246875762
step = 10, Training Accuracy: 0.8166666666666667
Training loss = 0.013131494422753652
step = 11, Training Accuracy: 0.81
Training loss = 0.015131252805391947
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.013458751986424128
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.012696378777424494
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.7925
[[0.5556961562529823, 0.13612937056358343, 0.99, 0.4242054590716958, 0.5766409292791236, 0.5911850471874005, 0.8506170527582769, 0.3704846516994011, 0.274331652123314, 0.3053496714749509, 0.1671258426696937, 0.6768972024852292, 0.99, 0.01, 0.587975529496757, 0.3039837454831234, 0.34736718129625194, 0.2369190211869978, 0.2888201536842664, 0.3560541099408648, 0.6186597515709865, 0.8929886944284531, 0.5465334219191125, 0.0923266536628149], [0.3149309687976496, 0.1269302304261869, 0.99, 0.020894719578324367, 0.45955298286417345, 0.31490306250077527, 0.8511947080126862, 0.13887401241544195, 0.18442835403362937, 0.23468027826231952, 0.6120959704413186, 0.3996171685238995, 0.9883731059641361, 0.01, 0.9259060884858503, 0.505177053430861, 0.14428477845089183, 0.277135877741559, 0.06693978000227228, 0.6765510395346419, 0.08981110771557864, 0.7769854225266933, 0.5578533760093123, 0.37737628649582267], [0.5113142239659141, 0.5352373212199945, 0.9742575303567785, 0.19387524145560403, 0.7664686834252246, 0.01, 0.99, 0.3132300204255538, 0.32499397503176664, 0.19704869828535299, 0.45489080930984616, 0.6186071600263195, 0.8909623880369771, 0.01, 0.4220210060871085, 0.5629336811604638, 0.26720750066758436, 0.07475893490323293, 0.01, 0.8042826317009076, 0.01, 0.6629282563400303, 0.4767305955264921, 0.01], [0.3534334331288562, 0.6407358614271794, 0.9683943757253186, 0.5646067332770324, 0.43668014878942846, 0.22590660256790182, 0.8580467249085303, 0.1330845837498409, 0.2932176668220775, 0.4910211295677673, 0.7058156273639155, 0.8776929424305211, 0.7655447605100659, 0.04382375069110338, 0.4682184275223946, 0.5867362565009872, 0.22582635588303482, 0.3708240779305827, 0.41304925698398853, 0.6162013371551744, 0.4578405010439175, 0.6980677872797245, 0.1748854172354405, 0.9214933548214592], [0.4575334121988722, 0.4838351354434704, 0.931296161682143, 0.3793108366694475, 0.48101572854407715, 0.3301626810609415, 0.99, 0.3866018843268235, 0.34728688384386497, 0.36757813645318604, 0.5401122056171526, 0.4791703641823718, 0.99, 0.01, 0.16472171439524025, 0.7865670606250234, 0.22451871822465494, 0.35001316207393074, 0.17708973834307365, 0.99, 0.3962516794025086, 0.9115830439635491, 0.45529900453321354, 0.390462926899729], [0.557310306346015, 0.5470889656133459, 0.9714736277539096, 0.5701432106387113, 0.6244293772050212, 0.01, 0.8986846454281083, 0.7032869804961462, 0.21670961582945558, 0.3598117576732891, 0.6673823860985955, 0.09292104480517116, 0.8364846583398494, 0.035186705488854206, 0.5066837709919718, 0.7456353714667481, 0.3292511105706074, 0.2906140339797164, 0.45963140844013783, 0.99, 0.01240476118639916, 0.6460147096241099, 0.5636436436895351, 0.3758483309430969], [0.1844599200852512, 0.852651790247681, 0.859480622605146, 0.11415523316810414, 0.8631243411040143, 0.410022286053333, 0.99, 0.3377150296858934, 0.13204889166441197, 0.3587735554227326, 0.6026685811589444, 0.6335056173663361, 0.8815509689255998, 0.01, 0.44842665421643535, 0.6274118956101539, 0.5378481290931028, 0.01, 0.33462414194469803, 0.7300737004209169, 0.11547898112261255, 0.6828526991454041, 0.8845357475311388, 0.35461093350125417], [0.4453455624485602, 0.47387943901222707, 0.99, 0.30206663707695586, 0.6989342703724805, 0.41312305370836133, 0.947684117763256, 0.47330264258221366, 0.38591942901212833, 0.22864507596917605, 0.3260009352419226, 0.99, 0.7198012410304342, 0.01, 0.24197362696007327, 0.7055851467799759, 0.01, 0.01, 0.014826954562661726, 0.6819570406415078, 0.05749676332077791, 0.6710021971856437, 0.35775544190327885, 0.1144754156916305]]
10 	8     	0.772031	0.0105686 	0.7575 	0.7925 
params:  [0.3476398538573186, 0.7635625511081341, 0.99, 0.3421034262399584, 0.678304727964951, 0.46310335996913393, 0.99, 0.6494148584138453, 0.5372438584352777, 0.17207696707210723, 0.40728849875908524, 0.9422458335872642, 0.99, 0.01, 0.44231045484858555, 0.4820353638952741, 0.01, 0.01, 0.01, 0.8439181524996774, 0.46637246910977403, 0.5395828648429695, 0.38815400667991823, 0.13042624765788027]
Training loss = 0.01788129409154256
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79
Training loss = 0.016480285227298736
step = 1, Training Accuracy: 0.81
Training loss = 0.016573452651500703
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01736467033624649
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015971673130989076
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014981899956862131
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.016861192186673483
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.014003201921780905
step = 7, Training Accuracy: 0.81
Training loss = 0.015358373820781708
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.014586534897486368
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015040988822778066
step = 10, Training Accuracy: 0.8366666666666667
Training loss = 0.01334101657072703
step = 11, Training Accuracy: 0.82
Training loss = 0.015630011955897014
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013958036998907726
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.014224556883176169
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78875
params:  [0.5134473685967758, 0.40207952794737833, 0.8924594197507069, 0.380020475268155, 0.888792063535453, 0.3096360092708688, 0.8382439653595518, 0.3628424179467742, 0.2120324703884158, 0.06230086375117186, 0.2009179191281774, 0.6857837505237724, 0.970530841390573, 0.01, 0.12704256781769685, 0.6149334625764673, 0.0772407473838601, 0.2404587953570822, 0.04281636148112988, 0.8598498738797345, 0.44429157272598063, 0.9428181908600994, 0.7420411693289589, 0.21168002093853044]
Training loss = 0.019076844155788423
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.785
Training loss = 0.01902416944503784
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01713675061861674
step = 2, Training Accuracy: 0.76
Training loss = 0.01716607908407847
step = 3, Training Accuracy: 0.78
Training loss = 0.016572287877400716
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.019016606708367665
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.015032394031683605
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015537389914194743
step = 7, Training Accuracy: 0.8
Training loss = 0.016470434566338857
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.014456712106863657
step = 9, Training Accuracy: 0.79
Training loss = 0.01649500072002411
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.014422287543614706
step = 11, Training Accuracy: 0.82
Training loss = 0.01654225339492162
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01601736048857371
step = 13, Training Accuracy: 0.7766666666666666
Training loss = 0.016476536293824513
step = 14, Training Accuracy: 0.81
Validation Accuracy: 0.775
params:  [0.4624206206171246, 0.5093340795645284, 0.99, 0.34380919641934854, 0.9589732338328418, 0.6928994751315519, 0.7820695575215005, 0.4535389619288717, 0.4436263534838386, 0.08325501078655506, 0.39128598747147153, 0.99, 0.99, 0.01, 0.07439029451803808, 0.38699054064733374, 0.01, 0.09333606608314891, 0.05809538778335284, 0.9836990353630699, 0.11133379185625751, 0.9153814697980127, 0.32250291343946574, 0.09658566015783748]
Training loss = 0.01701164275407791
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.785
Training loss = 0.01792596677939097
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.018140613238016766
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.016620061993598938
step = 3, Training Accuracy: 0.77
Training loss = 0.016820198396841686
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.016121742725372316
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.016510505974292756
step = 6, Training Accuracy: 0.79
Training loss = 0.014325558841228485
step = 7, Training Accuracy: 0.81
Training loss = 0.016306601762771607
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01592872480551402
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015773902585109075
step = 10, Training Accuracy: 0.79
Training loss = 0.014572215080261231
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.01453710546096166
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.0162197016676267
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.014470737477143605
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.7925
params:  [0.27107899048734474, 0.48766302128359873, 0.99, 0.06741208813602712, 0.7268368833546917, 0.05809114699462209, 0.8647803632874453, 0.8029310412832051, 0.6793955905538087, 0.3097388123258162, 0.4401336334932643, 0.7233672684040267, 0.8949871533151745, 0.07476258851327774, 0.14742022869256213, 0.7423366303536112, 0.310211088230408, 0.35135294487863195, 0.06914642071887035, 0.6517988436668238, 0.20961077115267965, 0.7480297542304356, 0.4294420032916085, 0.05924767118503675]
Training loss = 0.015882522066434226
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7925
Training loss = 0.015848615169525147
step = 1, Training Accuracy: 0.8
Training loss = 0.016032585601011912
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.017605859140555066
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.014960224628448487
step = 4, Training Accuracy: 0.79
Training loss = 0.014263238112131755
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.015350500841935476
step = 6, Training Accuracy: 0.79
Training loss = 0.014370671212673188
step = 7, Training Accuracy: 0.79
Training loss = 0.01509452263514201
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01422057956457138
step = 9, Training Accuracy: 0.82
Training loss = 0.015179656197627385
step = 10, Training Accuracy: 0.8233333333333334
Training loss = 0.01345424105723699
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.01556222399075826
step = 12, Training Accuracy: 0.78
Training loss = 0.012702382405598959
step = 13, Training Accuracy: 0.82
Training loss = 0.013143819471200307
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.805
params:  [0.5729755436380659, 0.1351428605482124, 0.8253661046334246, 0.17271318838176838, 0.5454549308960328, 0.45171116582957066, 0.6826601287158507, 0.17249549716082982, 0.3957386326854029, 0.25708019006483257, 0.31549647040229606, 0.7461512346291378, 0.7839987970721688, 0.28065468790701564, 0.41457928816589856, 0.5073976889838305, 0.16563395891748725, 0.23474240106088917, 0.25755741072783134, 0.8197328504566167, 0.01, 0.9053758430479675, 0.4905947842859826, 0.03600474006544416]
Training loss = 0.01637628858288129
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79875
Training loss = 0.016305483182271322
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.017074943284193674
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.01590345650911331
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01525250236193339
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.014797148257493974
step = 5, Training Accuracy: 0.81
Training loss = 0.014148181875546773
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016530254284540812
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014974953730901082
step = 8, Training Accuracy: 0.8
Training loss = 0.013736601968606313
step = 9, Training Accuracy: 0.81
Training loss = 0.014400255481402079
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.013522335588932037
step = 11, Training Accuracy: 0.82
Training loss = 0.014481898794571558
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.012227649788061777
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.014496429562568665
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78125
params:  [0.4674502058580655, 0.37747057157713704, 0.99, 0.4159867198692467, 0.6158281669122367, 0.39564078130118896, 0.8455250826402062, 0.3631433320548772, 0.01685956988090792, 0.29355421862138736, 0.42297489141834266, 0.6461109483227754, 0.8288533226477091, 0.01, 0.4238427755697305, 0.6173619752755914, 0.3705636284434256, 0.019138479410550954, 0.34335164600035867, 0.7200124678670413, 0.3811695988383992, 0.7908363176313356, 0.5817733631468407, 0.01]
Training loss = 0.016706210176150003
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.78125
Training loss = 0.015938410758972166
step = 1, Training Accuracy: 0.81
Training loss = 0.016678094466527304
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.014796862900257111
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.015264267126719158
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.015339741905530294
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.014121685922145844
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.017543532749017078
step = 7, Training Accuracy: 0.7633333333333333
Training loss = 0.013773769438266754
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.01382514794667562
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.015695486118396124
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015102586845556895
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01558561940987905
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.015754796663920084
step = 13, Training Accuracy: 0.81
Training loss = 0.013753218253453573
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78125
params:  [0.2693210681962551, 0.5133710464229921, 0.9508517895219472, 0.2979403532037341, 0.8053316726121836, 0.2961881514789172, 0.7553991121918336, 0.4120759645914517, 0.34803190892355007, 0.2738808906118336, 0.3625430896006484, 0.755163588282327, 0.7821915054233272, 0.01, 0.38776569195364746, 0.5920169123610975, 0.591043460963014, 0.153486997885656, 0.01, 0.6193472587869896, 0.02351019012182315, 0.804461151005364, 0.32201465797787715, 0.2531034215901762]
Training loss = 0.014672021170457204
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.78
Training loss = 0.01608246773481369
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.015892569919427234
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.014264033635457357
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.014278256793816885
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.013445702145497004
step = 5, Training Accuracy: 0.8266666666666667
Training loss = 0.013149121056000392
step = 6, Training Accuracy: 0.83
Training loss = 0.01331023226181666
step = 7, Training Accuracy: 0.84
Training loss = 0.012550444602966308
step = 8, Training Accuracy: 0.84
Training loss = 0.013423137764135996
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.012734035203854244
step = 10, Training Accuracy: 0.84
Training loss = 0.012987252771854401
step = 11, Training Accuracy: 0.84
Training loss = 0.012676043013731639
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.012768786152203877
step = 13, Training Accuracy: 0.8266666666666667
Training loss = 0.01249041199684143
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.78375
params:  [0.7845769207355459, 0.26192026725516815, 0.99, 0.36488525461629745, 0.9420356119977067, 0.4571539542435887, 0.9327839052629845, 0.3545062249612882, 0.24848626466447277, 0.16688086279331615, 0.28138216787589343, 0.7415613159500941, 0.8302757952584285, 0.2993244617659117, 0.24690267019743894, 0.4895694777273546, 0.22291375273856015, 0.01, 0.023247099873066876, 0.8542619553478539, 0.01, 0.5782163952369527, 0.29236371169140707, 0.19028105619591318]
Training loss = 0.014736322164535522
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.78875
Training loss = 0.015053413311640422
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.01323359449704488
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.013076219509045283
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.015390710433324179
step = 4, Training Accuracy: 0.79
Training loss = 0.013558136721452077
step = 5, Training Accuracy: 0.8133333333333334
Training loss = 0.014119839519262314
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.013385067681471506
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.011845893114805222
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.012378437618414562
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.014150786499182383
step = 10, Training Accuracy: 0.8533333333333334
Training loss = 0.012134397327899933
step = 11, Training Accuracy: 0.85
Training loss = 0.01326358844836553
step = 12, Training Accuracy: 0.83
Training loss = 0.012274445195992788
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.011760350118080775
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.79125
[[0.3476398538573186, 0.7635625511081341, 0.99, 0.3421034262399584, 0.678304727964951, 0.46310335996913393, 0.99, 0.6494148584138453, 0.5372438584352777, 0.17207696707210723, 0.40728849875908524, 0.9422458335872642, 0.99, 0.01, 0.44231045484858555, 0.4820353638952741, 0.01, 0.01, 0.01, 0.8439181524996774, 0.46637246910977403, 0.5395828648429695, 0.38815400667991823, 0.13042624765788027], [0.5134473685967758, 0.40207952794737833, 0.8924594197507069, 0.380020475268155, 0.888792063535453, 0.3096360092708688, 0.8382439653595518, 0.3628424179467742, 0.2120324703884158, 0.06230086375117186, 0.2009179191281774, 0.6857837505237724, 0.970530841390573, 0.01, 0.12704256781769685, 0.6149334625764673, 0.0772407473838601, 0.2404587953570822, 0.04281636148112988, 0.8598498738797345, 0.44429157272598063, 0.9428181908600994, 0.7420411693289589, 0.21168002093853044], [0.4624206206171246, 0.5093340795645284, 0.99, 0.34380919641934854, 0.9589732338328418, 0.6928994751315519, 0.7820695575215005, 0.4535389619288717, 0.4436263534838386, 0.08325501078655506, 0.39128598747147153, 0.99, 0.99, 0.01, 0.07439029451803808, 0.38699054064733374, 0.01, 0.09333606608314891, 0.05809538778335284, 0.9836990353630699, 0.11133379185625751, 0.9153814697980127, 0.32250291343946574, 0.09658566015783748], [0.27107899048734474, 0.48766302128359873, 0.99, 0.06741208813602712, 0.7268368833546917, 0.05809114699462209, 0.8647803632874453, 0.8029310412832051, 0.6793955905538087, 0.3097388123258162, 0.4401336334932643, 0.7233672684040267, 0.8949871533151745, 0.07476258851327774, 0.14742022869256213, 0.7423366303536112, 0.310211088230408, 0.35135294487863195, 0.06914642071887035, 0.6517988436668238, 0.20961077115267965, 0.7480297542304356, 0.4294420032916085, 0.05924767118503675], [0.5729755436380659, 0.1351428605482124, 0.8253661046334246, 0.17271318838176838, 0.5454549308960328, 0.45171116582957066, 0.6826601287158507, 0.17249549716082982, 0.3957386326854029, 0.25708019006483257, 0.31549647040229606, 0.7461512346291378, 0.7839987970721688, 0.28065468790701564, 0.41457928816589856, 0.5073976889838305, 0.16563395891748725, 0.23474240106088917, 0.25755741072783134, 0.8197328504566167, 0.01, 0.9053758430479675, 0.4905947842859826, 0.03600474006544416], [0.4674502058580655, 0.37747057157713704, 0.99, 0.4159867198692467, 0.6158281669122367, 0.39564078130118896, 0.8455250826402062, 0.3631433320548772, 0.01685956988090792, 0.29355421862138736, 0.42297489141834266, 0.6461109483227754, 0.8288533226477091, 0.01, 0.4238427755697305, 0.6173619752755914, 0.3705636284434256, 0.019138479410550954, 0.34335164600035867, 0.7200124678670413, 0.3811695988383992, 0.7908363176313356, 0.5817733631468407, 0.01], [0.2693210681962551, 0.5133710464229921, 0.9508517895219472, 0.2979403532037341, 0.8053316726121836, 0.2961881514789172, 0.7553991121918336, 0.4120759645914517, 0.34803190892355007, 0.2738808906118336, 0.3625430896006484, 0.755163588282327, 0.7821915054233272, 0.01, 0.38776569195364746, 0.5920169123610975, 0.591043460963014, 0.153486997885656, 0.01, 0.6193472587869896, 0.02351019012182315, 0.804461151005364, 0.32201465797787715, 0.2531034215901762], [0.7845769207355459, 0.26192026725516815, 0.99, 0.36488525461629745, 0.9420356119977067, 0.4571539542435887, 0.9327839052629845, 0.3545062249612882, 0.24848626466447277, 0.16688086279331615, 0.28138216787589343, 0.7415613159500941, 0.8302757952584285, 0.2993244617659117, 0.24690267019743894, 0.4895694777273546, 0.22291375273856015, 0.01, 0.023247099873066876, 0.8542619553478539, 0.01, 0.5782163952369527, 0.29236371169140707, 0.19028105619591318]]
11 	8     	0.787344	0.00862494	0.775  	0.805  
params:  [0.36211667685290133, 0.5267279532704531, 0.99, 0.2933477385086222, 0.5164709746786307, 0.28754086493089054, 0.7034723170213434, 0.35413448490979904, 0.8041635895035569, 0.3570700266667056, 0.3074774970784885, 0.8921744748617711, 0.9258320105594054, 0.053133904208069974, 0.31506685804424744, 0.3186240065323552, 0.3957691769084905, 0.26050851764475863, 0.01, 0.7049507464115558, 0.4521842901986082, 0.99, 0.5006942683265946, 0.38369330045497674]
Training loss = 0.019014317095279693
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.79375
Training loss = 0.01857822611927986
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017920826574166614
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.0161124449968338
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.01575788656870524
step = 4, Training Accuracy: 0.8
Training loss = 0.01710019459327062
step = 5, Training Accuracy: 0.7933333333333333
Training loss = 0.01534549574057261
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015262784163157146
step = 7, Training Accuracy: 0.81
Training loss = 0.014940697054068247
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.016886748671531678
step = 9, Training Accuracy: 0.78
Training loss = 0.014969818492730458
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01394448017080625
step = 11, Training Accuracy: 0.7933333333333333
Training loss = 0.01437930812438329
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014927231123050054
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.016021348734696707
step = 14, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7725
params:  [0.27804523443909757, 0.621115554751849, 0.7655591518106853, 0.03355995827666669, 0.9149641874671048, 0.1160115300788169, 0.7905225350553481, 0.4631428587036052, 0.05228589696995445, 0.33755572523726435, 0.5228448038893158, 0.8044521697030288, 0.8649043778045797, 0.014527634498774492, 0.01, 0.6274949485815212, 0.32066145424532944, 0.24192665347570352, 0.01, 0.9629649042795069, 0.01, 0.7271857929553142, 0.2256072794504513, 0.01]
Training loss = 0.018774861991405486
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.765
Training loss = 0.014996246993541717
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016414776047070822
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.015635500649611157
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.01618650769193967
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.016820682684580486
step = 5, Training Accuracy: 0.8
Training loss = 0.015812698702017465
step = 6, Training Accuracy: 0.79
Training loss = 0.015738595724105835
step = 7, Training Accuracy: 0.83
Training loss = 0.015576914548873902
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015952722430229188
step = 9, Training Accuracy: 0.7766666666666666
Training loss = 0.014815416137377422
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.014420999189217885
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.014295291205247243
step = 12, Training Accuracy: 0.8033333333333333
Training loss = 0.014436250279347102
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.017107698818047842
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7875
params:  [0.4592700448335963, 0.38291317694371174, 0.99, 0.14287283854209934, 0.8849629410907587, 0.35705180197171366, 0.5921256172501239, 0.7616427270684568, 0.5331252867284806, 0.10762891763558632, 0.5714797343478699, 0.7436454554714507, 0.9386284201492788, 0.08350337920168424, 0.37845433095166275, 0.5593547758823421, 0.4054665787745018, 0.01, 0.01, 0.8157911412149806, 0.01, 0.9119741424998855, 0.15584411682475224, 0.01]
Training loss = 0.014381994505723318
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.79375
Training loss = 0.014033007224400838
step = 1, Training Accuracy: 0.84
Training loss = 0.014057152370611826
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.013659268617630005
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.014204845130443573
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.014575505157311758
step = 5, Training Accuracy: 0.81
Training loss = 0.013478279014428456
step = 6, Training Accuracy: 0.82
Training loss = 0.0129903581738472
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.01174227664868037
step = 8, Training Accuracy: 0.84
Training loss = 0.01338286946217219
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.012968136171499889
step = 10, Training Accuracy: 0.8366666666666667
Training loss = 0.012193411588668823
step = 11, Training Accuracy: 0.83
Training loss = 0.01245205208659172
step = 12, Training Accuracy: 0.84
Training loss = 0.011954313268264134
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.012446330214540164
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.775
params:  [0.36434238199579766, 0.5010456199004725, 0.7716658002382882, 0.1697709858550847, 0.952192275186275, 0.01, 0.8414761890727733, 0.8013083738058502, 0.5362328053287047, 0.15510442797705187, 0.4792728264895245, 0.7150436439325751, 0.9843642187004992, 0.012558078596804884, 0.17680688111191428, 0.4786132594960766, 0.17429193042942528, 0.5248142927501218, 0.0865351915378323, 0.6019030303969328, 0.0446957686788754, 0.737895527262398, 0.2139099992664397, 0.10873861721408143]
Training loss = 0.015356238583723704
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.77625
Training loss = 0.014669046998023986
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01547589768966039
step = 2, Training Accuracy: 0.83
Training loss = 0.016532564262549083
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.014685416519641876
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.013658608098824819
step = 5, Training Accuracy: 0.8
Training loss = 0.014251219481229782
step = 6, Training Accuracy: 0.79
Training loss = 0.013579404950141906
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.013324010769526164
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.013204578310251236
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.013733916232983272
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01582722713549932
step = 11, Training Accuracy: 0.82
Training loss = 0.013390963276227316
step = 12, Training Accuracy: 0.83
Training loss = 0.012669743796189625
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.01547737181186676
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.7825
params:  [0.3213161735382759, 0.4391590747756056, 0.9353101990070121, 0.46062059534999444, 0.564327109872881, 0.22304031393498958, 0.7373168078617688, 0.5976496569719497, 0.30756832188369837, 0.012619493262431142, 0.3547815354217823, 0.99, 0.7786614960692022, 0.16784737374167025, 0.2970227692828478, 0.6452346570737951, 0.12963555481196049, 0.6752965908841159, 0.01, 0.9308303662603072, 0.33555912926235354, 0.6574783053969627, 0.713029148874847, 0.04724263265255466]
Training loss = 0.020074956516424814
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.79
Training loss = 0.018678027490774792
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.016753831803798677
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.020839340289433798
step = 3, Training Accuracy: 0.76
Training loss = 0.017248200327157973
step = 4, Training Accuracy: 0.78
Training loss = 0.01956793963909149
step = 5, Training Accuracy: 0.7766666666666666
Training loss = 0.017509409189224244
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.016113639672597248
step = 7, Training Accuracy: 0.8166666666666667
Training loss = 0.020010964175065357
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.016569195489088695
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.015093105733394624
step = 10, Training Accuracy: 0.7966666666666666
Training loss = 0.016072214742501575
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.01595526874065399
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.016300216217835745
step = 13, Training Accuracy: 0.77
Training loss = 0.015602870980898539
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.78875
params:  [0.2477845738980693, 0.5481407265242396, 0.99, 0.19807256937435677, 0.6064839508688604, 0.2812920973432659, 0.9419475359532695, 0.486811487448902, 0.7513659989760133, 0.11219544053731288, 0.23572490604651944, 0.9690758889293183, 0.9687286728412043, 0.01, 0.01, 0.6883139103974127, 0.01, 0.01, 0.20930741642490358, 0.7729898733199526, 0.11041601172336632, 0.6826357573597938, 0.1982876833053789, 0.09475233421977718]
Training loss = 0.014725699822107951
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.78375
Training loss = 0.014760815352201462
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.016778336862723033
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.016134512225786844
step = 3, Training Accuracy: 0.78
Training loss = 0.01458266407251358
step = 4, Training Accuracy: 0.8
Training loss = 0.014983289937178294
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.016160270869731902
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.013491137474775315
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.013079833736022313
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.013852993746598562
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.01557057281335195
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.011994945804278056
step = 11, Training Accuracy: 0.85
Training loss = 0.014307523767153421
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013871884147326152
step = 13, Training Accuracy: 0.8133333333333334
Training loss = 0.013721959789594014
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.76625
params:  [0.1962122767068527, 0.5838391748045492, 0.99, 0.5836953183869164, 0.7185293369098711, 0.38865479227137156, 0.99, 0.7290450085920618, 0.47952801229687825, 0.050157918194749324, 0.668970149167903, 0.7355737443706066, 0.99, 0.0457956517438847, 0.1312010752200457, 0.8325674256461892, 0.2629021089458544, 0.01, 0.01, 0.7881856242230615, 0.026611510820802237, 0.8509415569870356, 0.5011113998836128, 0.055142165578435126]
Training loss = 0.018588429888089498
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.77
Training loss = 0.01876833657423655
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.018087615768114726
step = 2, Training Accuracy: 0.76
Training loss = 0.01629073530435562
step = 3, Training Accuracy: 0.79
Training loss = 0.015126211643218994
step = 4, Training Accuracy: 0.81
Training loss = 0.0158683043718338
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.016718335251013437
step = 6, Training Accuracy: 0.7766666666666666
Training loss = 0.01599064071973165
step = 7, Training Accuracy: 0.81
Training loss = 0.017001635829607644
step = 8, Training Accuracy: 0.78
Training loss = 0.015065805514653524
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.01715951810280482
step = 10, Training Accuracy: 0.7866666666666666
Training loss = 0.014898318449656169
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.01677776505549749
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01495076835155487
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.01436880737543106
step = 14, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.78875
params:  [0.5000212615432286, 0.4171153805109483, 0.9846236146746352, 0.33553835424741063, 0.8061773568790614, 0.04693881469046274, 0.99, 0.7140596650835933, 0.4030663133449014, 0.290722912523385, 0.33151042881688, 0.7044069531154027, 0.99, 0.01, 0.06097102799197611, 0.8529092461315558, 0.10613475626766464, 0.01, 0.0517943993902673, 0.9785538608327822, 0.2822939424765039, 0.40913449291780596, 0.390446811347611, 0.3008391328150583]
Training loss = 0.017829522589842477
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.78375
Training loss = 0.018149604598681132
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.016261895100275675
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01671511640151342
step = 3, Training Accuracy: 0.79
Training loss = 0.017744074761867522
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.017086993753910065
step = 5, Training Accuracy: 0.7466666666666667
Training loss = 0.01365559349457423
step = 6, Training Accuracy: 0.83
Training loss = 0.01629731963078181
step = 7, Training Accuracy: 0.8
Training loss = 0.01602833280960719
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.015503329833348593
step = 9, Training Accuracy: 0.81
Training loss = 0.016457227965195973
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015541358590126038
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.014474411507447561
step = 12, Training Accuracy: 0.8233333333333334
Training loss = 0.015372638901074728
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.01587775468826294
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.795
[[0.36211667685290133, 0.5267279532704531, 0.99, 0.2933477385086222, 0.5164709746786307, 0.28754086493089054, 0.7034723170213434, 0.35413448490979904, 0.8041635895035569, 0.3570700266667056, 0.3074774970784885, 0.8921744748617711, 0.9258320105594054, 0.053133904208069974, 0.31506685804424744, 0.3186240065323552, 0.3957691769084905, 0.26050851764475863, 0.01, 0.7049507464115558, 0.4521842901986082, 0.99, 0.5006942683265946, 0.38369330045497674], [0.27804523443909757, 0.621115554751849, 0.7655591518106853, 0.03355995827666669, 0.9149641874671048, 0.1160115300788169, 0.7905225350553481, 0.4631428587036052, 0.05228589696995445, 0.33755572523726435, 0.5228448038893158, 0.8044521697030288, 0.8649043778045797, 0.014527634498774492, 0.01, 0.6274949485815212, 0.32066145424532944, 0.24192665347570352, 0.01, 0.9629649042795069, 0.01, 0.7271857929553142, 0.2256072794504513, 0.01], [0.4592700448335963, 0.38291317694371174, 0.99, 0.14287283854209934, 0.8849629410907587, 0.35705180197171366, 0.5921256172501239, 0.7616427270684568, 0.5331252867284806, 0.10762891763558632, 0.5714797343478699, 0.7436454554714507, 0.9386284201492788, 0.08350337920168424, 0.37845433095166275, 0.5593547758823421, 0.4054665787745018, 0.01, 0.01, 0.8157911412149806, 0.01, 0.9119741424998855, 0.15584411682475224, 0.01], [0.36434238199579766, 0.5010456199004725, 0.7716658002382882, 0.1697709858550847, 0.952192275186275, 0.01, 0.8414761890727733, 0.8013083738058502, 0.5362328053287047, 0.15510442797705187, 0.4792728264895245, 0.7150436439325751, 0.9843642187004992, 0.012558078596804884, 0.17680688111191428, 0.4786132594960766, 0.17429193042942528, 0.5248142927501218, 0.0865351915378323, 0.6019030303969328, 0.0446957686788754, 0.737895527262398, 0.2139099992664397, 0.10873861721408143], [0.3213161735382759, 0.4391590747756056, 0.9353101990070121, 0.46062059534999444, 0.564327109872881, 0.22304031393498958, 0.7373168078617688, 0.5976496569719497, 0.30756832188369837, 0.012619493262431142, 0.3547815354217823, 0.99, 0.7786614960692022, 0.16784737374167025, 0.2970227692828478, 0.6452346570737951, 0.12963555481196049, 0.6752965908841159, 0.01, 0.9308303662603072, 0.33555912926235354, 0.6574783053969627, 0.713029148874847, 0.04724263265255466], [0.2477845738980693, 0.5481407265242396, 0.99, 0.19807256937435677, 0.6064839508688604, 0.2812920973432659, 0.9419475359532695, 0.486811487448902, 0.7513659989760133, 0.11219544053731288, 0.23572490604651944, 0.9690758889293183, 0.9687286728412043, 0.01, 0.01, 0.6883139103974127, 0.01, 0.01, 0.20930741642490358, 0.7729898733199526, 0.11041601172336632, 0.6826357573597938, 0.1982876833053789, 0.09475233421977718], [0.1962122767068527, 0.5838391748045492, 0.99, 0.5836953183869164, 0.7185293369098711, 0.38865479227137156, 0.99, 0.7290450085920618, 0.47952801229687825, 0.050157918194749324, 0.668970149167903, 0.7355737443706066, 0.99, 0.0457956517438847, 0.1312010752200457, 0.8325674256461892, 0.2629021089458544, 0.01, 0.01, 0.7881856242230615, 0.026611510820802237, 0.8509415569870356, 0.5011113998836128, 0.055142165578435126], [0.5000212615432286, 0.4171153805109483, 0.9846236146746352, 0.33553835424741063, 0.8061773568790614, 0.04693881469046274, 0.99, 0.7140596650835933, 0.4030663133449014, 0.290722912523385, 0.33151042881688, 0.7044069531154027, 0.99, 0.01, 0.06097102799197611, 0.8529092461315558, 0.10613475626766464, 0.01, 0.0517943993902673, 0.9785538608327822, 0.2822939424765039, 0.40913449291780596, 0.390446811347611, 0.3008391328150583]]
12 	8     	0.782031	0.0092055 	0.76625	0.795  
params:  [0.24134312435577182, 0.05148931173668131, 0.99, 0.28895971436922513, 0.6653825983822902, 0.08948895990880426, 0.7085494779194917, 0.7476008717705813, 0.2284662300271732, 0.6115827580614379, 0.3117556667990369, 0.7210662708621992, 0.8456603629132671, 0.28268073110981773, 0.074560887249915, 0.782597898149171, 0.054889544918880664, 0.18886972510818545, 0.15922269400046016, 0.9106976515025643, 0.1610476410124757, 0.462449363691127, 0.6329882226538656, 0.01]
Training loss = 0.01566175421079
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.8025
Training loss = 0.014454813897609711
step = 1, Training Accuracy: 0.82
Training loss = 0.014615541597207387
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.012662172913551331
step = 3, Training Accuracy: 0.84
Training loss = 0.012994161595900854
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.013798105269670487
step = 5, Training Accuracy: 0.8166666666666667
Training loss = 0.012952664891878764
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.012168196042378744
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.012927516847848892
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.012905510018269221
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011217072755098343
step = 10, Training Accuracy: 0.8566666666666667
Training loss = 0.013467081586519877
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.012769585847854615
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.012474116086959839
step = 13, Training Accuracy: 0.84
Training loss = 0.012799376447995503
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7975
params:  [0.31035393585949955, 0.41359072726860074, 0.9470208788522307, 0.6830855005631846, 0.8804427433046507, 0.05279622249789406, 0.99, 0.755444539298485, 0.3872097547143182, 0.36904017034140696, 0.47813738460594385, 0.7948120102977217, 0.8251139413479491, 0.22884128566919024, 0.14647296765961376, 0.99, 0.16054216575575647, 0.2249301289184048, 0.01, 0.99, 0.3113655880530619, 0.42103582659595706, 0.5624930458861377, 0.0658906134624643]
Training loss = 0.01788499583800634
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.8025
Training loss = 0.016886790593465168
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.018893171151479086
step = 2, Training Accuracy: 0.77
Training loss = 0.017433302501837413
step = 3, Training Accuracy: 0.77
Training loss = 0.01668089598417282
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.017615320483843486
step = 5, Training Accuracy: 0.76
Training loss = 0.019034458299477894
step = 6, Training Accuracy: 0.7666666666666667
Training loss = 0.016185816625754038
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015322375098864237
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.01614268347620964
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.01600095917781194
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.016162576973438262
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.017978338499863942
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.018873507777849834
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.01547004908323288
step = 14, Training Accuracy: 0.78
Validation Accuracy: 0.78625
params:  [0.20525159343442265, 0.4068264777526187, 0.99, 0.501810899761109, 0.8470578019326358, 0.4790245917481178, 0.7987491710785455, 0.909578660645112, 0.42333192858813884, 0.4049200475736514, 0.32535718592929264, 0.7256276299293745, 0.6711194131419663, 0.6274721273022262, 0.29800825856965124, 0.554369876660248, 0.017365754470541034, 0.12438266968756358, 0.06971558624208632, 0.8653030544790528, 0.16720694030193348, 0.7432706684458829, 0.2342627883735034, 0.26152105062697706]
Training loss = 0.01633344203233719
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.78625
Training loss = 0.016045044362545013
step = 1, Training Accuracy: 0.76
Training loss = 0.017544264098008473
step = 2, Training Accuracy: 0.75
Training loss = 0.015212824741999309
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.016971653600533803
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.01602844973405202
step = 5, Training Accuracy: 0.7733333333333333
Training loss = 0.01406927540898323
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.015547018547852834
step = 7, Training Accuracy: 0.79
Training loss = 0.01591395159562429
step = 8, Training Accuracy: 0.77
Training loss = 0.016190219720204672
step = 9, Training Accuracy: 0.79
Training loss = 0.015311058163642883
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.01671711544195811
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.015002585649490356
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.015711008807023367
step = 13, Training Accuracy: 0.81
Training loss = 0.015295436183611552
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.77375
params:  [0.35216590728741975, 0.7999868451576585, 0.6837190760417344, 0.24923365085455376, 0.6848245585094273, 0.21495681915209924, 0.99, 0.8320989498782085, 0.33571872078086823, 0.01, 0.5415405193114703, 0.9889470727524279, 0.7711076892121116, 0.06290265367119226, 0.1854376319330227, 0.8841809876066306, 0.19707196126442733, 0.14640115384655678, 0.2657558166399245, 0.8502766029849124, 0.5158752733631714, 0.6286782117738823, 0.8426397413577533, 0.026125619860895732]
Training loss = 0.023758543729782103
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.77625
Training loss = 0.018692492445309957
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.01873082717259725
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.018762473165988922
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.02061301718155543
step = 4, Training Accuracy: 0.7133333333333334
Training loss = 0.021499972641468048
step = 5, Training Accuracy: 0.74
Training loss = 0.020342126488685608
step = 6, Training Accuracy: 0.7533333333333333
Training loss = 0.02023933579524358
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.024087668458620707
step = 8, Training Accuracy: 0.68
Training loss = 0.019060942033926647
step = 9, Training Accuracy: 0.77
Training loss = 0.019675720731417337
step = 10, Training Accuracy: 0.7466666666666667
Training loss = 0.01916342377662659
step = 11, Training Accuracy: 0.7133333333333334
Training loss = 0.019124207894007365
step = 12, Training Accuracy: 0.7333333333333333
Training loss = 0.018456722994645437
step = 13, Training Accuracy: 0.76
Training loss = 0.020093663434187573
step = 14, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.80625
params:  [0.26586560522403635, 0.7304155422446029, 0.9116714312137599, 0.08150153075943606, 0.7158390549678938, 0.01, 0.99, 0.8468242944694345, 0.4962325772888782, 0.10077116771046342, 0.43338569513011993, 0.9442369795561262, 0.9036704941658881, 0.01, 0.01, 0.9012837506531857, 0.01, 0.15407703979294202, 0.3181119386599172, 0.6237828176640738, 0.2633858975365008, 0.6321487937088507, 0.477387834620873, 0.18640070468861947]
Training loss = 0.01722635527451833
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7925
Training loss = 0.01769118070602417
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01756456047296524
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.019237750669320423
step = 3, Training Accuracy: 0.76
Training loss = 0.015852608184019724
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.017333154131968818
step = 5, Training Accuracy: 0.79
Training loss = 0.015979640980561573
step = 6, Training Accuracy: 0.79
Training loss = 0.016083120306332906
step = 7, Training Accuracy: 0.76
Training loss = 0.01788929631312688
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.015181352893511454
step = 9, Training Accuracy: 0.8133333333333334
Training loss = 0.015880140364170073
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.013990288774172466
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.017046762506167094
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01639565497636795
step = 13, Training Accuracy: 0.7833333333333333
Training loss = 0.015596893181403478
step = 14, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.7825
params:  [0.6256614582664193, 0.31561218155295157, 0.99, 0.24027015314402683, 0.5322174324954468, 0.01, 0.6726042538330377, 0.8028159506673154, 0.1788912990377847, 0.01, 0.7095506268739991, 0.7644842680386371, 0.99, 0.01, 0.0775320083114602, 0.7433422002728352, 0.18898725814770487, 0.3111667692045813, 0.01, 0.9728599652916393, 0.16013709137830026, 0.15382841306477485, 0.618295831172265, 0.23692935361552442]
Training loss = 0.017940632700920105
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7875
Training loss = 0.016958198845386505
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.017150620619455974
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016652332345644633
step = 3, Training Accuracy: 0.81
Training loss = 0.01568103571732839
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.017427055438359578
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01698746919631958
step = 6, Training Accuracy: 0.8
Training loss = 0.016686336100101472
step = 7, Training Accuracy: 0.76
Training loss = 0.01699894239505132
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.01597649812698364
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.01578184207280477
step = 10, Training Accuracy: 0.8133333333333334
Training loss = 0.015127701709667842
step = 11, Training Accuracy: 0.8
Training loss = 0.018813246190547944
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.015706357657909394
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.014697718222935995
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.805
params:  [0.2480388074853509, 0.35082990203222336, 0.7467743864249737, 0.16745243707087717, 0.514575332051655, 0.01, 0.99, 0.5523361409577106, 0.3859888201390228, 0.01, 0.3157812382608078, 0.633365696085793, 0.99, 0.1989602913228665, 0.16578358655002254, 0.9497591420377942, 0.14870423678432176, 0.10515308743057335, 0.11036211121480685, 0.8908264350294514, 0.4896189187531885, 0.36744773769810846, 0.26742735693739705, 0.15159116101545425]
Training loss = 0.015576995213826498
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.81125
Training loss = 0.015253776411215465
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.014231573343276977
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.01592137783765793
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.015753221114476523
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.014007808367411296
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.015545829037825266
step = 6, Training Accuracy: 0.81
Training loss = 0.013579621861378352
step = 7, Training Accuracy: 0.8066666666666666
Training loss = 0.012909003595511118
step = 8, Training Accuracy: 0.82
Training loss = 0.014767175118128459
step = 9, Training Accuracy: 0.82
Training loss = 0.013870719770590465
step = 10, Training Accuracy: 0.82
Training loss = 0.013480090498924256
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.013659627040227254
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.0142112664381663
step = 13, Training Accuracy: 0.82
Training loss = 0.013843595087528228
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.79125
params:  [0.3168775717046756, 0.39333579851269723, 0.9088661247351473, 0.42853432544338127, 0.8537649529753241, 0.1556553714999904, 0.99, 0.47611668831799536, 0.741105284678487, 0.01, 0.4484374680205552, 0.8929242007698688, 0.7213570410641933, 0.08547558268021886, 0.04584362837550375, 0.7286927866019243, 0.08668632185340246, 0.04820608145254454, 0.07320498641641508, 0.885474550096277, 0.28722859001974665, 0.6259739747869087, 0.2842531624536692, 0.01]
Training loss = 0.017401492049296696
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7975
Training loss = 0.01897918512423833
step = 1, Training Accuracy: 0.78
Training loss = 0.016844932536284128
step = 2, Training Accuracy: 0.73
Training loss = 0.01839223931233088
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.016792326668898263
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01718943327665329
step = 5, Training Accuracy: 0.72
Training loss = 0.015637336174647014
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.014733849267164867
step = 7, Training Accuracy: 0.79
Training loss = 0.014498344759146372
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.014145057648420334
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015545854369799297
step = 10, Training Accuracy: 0.7666666666666667
Training loss = 0.015145880877971649
step = 11, Training Accuracy: 0.81
Training loss = 0.015762416422367097
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01572972963253657
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.015165269474188487
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
[[0.24134312435577182, 0.05148931173668131, 0.99, 0.28895971436922513, 0.6653825983822902, 0.08948895990880426, 0.7085494779194917, 0.7476008717705813, 0.2284662300271732, 0.6115827580614379, 0.3117556667990369, 0.7210662708621992, 0.8456603629132671, 0.28268073110981773, 0.074560887249915, 0.782597898149171, 0.054889544918880664, 0.18886972510818545, 0.15922269400046016, 0.9106976515025643, 0.1610476410124757, 0.462449363691127, 0.6329882226538656, 0.01], [0.31035393585949955, 0.41359072726860074, 0.9470208788522307, 0.6830855005631846, 0.8804427433046507, 0.05279622249789406, 0.99, 0.755444539298485, 0.3872097547143182, 0.36904017034140696, 0.47813738460594385, 0.7948120102977217, 0.8251139413479491, 0.22884128566919024, 0.14647296765961376, 0.99, 0.16054216575575647, 0.2249301289184048, 0.01, 0.99, 0.3113655880530619, 0.42103582659595706, 0.5624930458861377, 0.0658906134624643], [0.20525159343442265, 0.4068264777526187, 0.99, 0.501810899761109, 0.8470578019326358, 0.4790245917481178, 0.7987491710785455, 0.909578660645112, 0.42333192858813884, 0.4049200475736514, 0.32535718592929264, 0.7256276299293745, 0.6711194131419663, 0.6274721273022262, 0.29800825856965124, 0.554369876660248, 0.017365754470541034, 0.12438266968756358, 0.06971558624208632, 0.8653030544790528, 0.16720694030193348, 0.7432706684458829, 0.2342627883735034, 0.26152105062697706], [0.35216590728741975, 0.7999868451576585, 0.6837190760417344, 0.24923365085455376, 0.6848245585094273, 0.21495681915209924, 0.99, 0.8320989498782085, 0.33571872078086823, 0.01, 0.5415405193114703, 0.9889470727524279, 0.7711076892121116, 0.06290265367119226, 0.1854376319330227, 0.8841809876066306, 0.19707196126442733, 0.14640115384655678, 0.2657558166399245, 0.8502766029849124, 0.5158752733631714, 0.6286782117738823, 0.8426397413577533, 0.026125619860895732], [0.26586560522403635, 0.7304155422446029, 0.9116714312137599, 0.08150153075943606, 0.7158390549678938, 0.01, 0.99, 0.8468242944694345, 0.4962325772888782, 0.10077116771046342, 0.43338569513011993, 0.9442369795561262, 0.9036704941658881, 0.01, 0.01, 0.9012837506531857, 0.01, 0.15407703979294202, 0.3181119386599172, 0.6237828176640738, 0.2633858975365008, 0.6321487937088507, 0.477387834620873, 0.18640070468861947], [0.6256614582664193, 0.31561218155295157, 0.99, 0.24027015314402683, 0.5322174324954468, 0.01, 0.6726042538330377, 0.8028159506673154, 0.1788912990377847, 0.01, 0.7095506268739991, 0.7644842680386371, 0.99, 0.01, 0.0775320083114602, 0.7433422002728352, 0.18898725814770487, 0.3111667692045813, 0.01, 0.9728599652916393, 0.16013709137830026, 0.15382841306477485, 0.618295831172265, 0.23692935361552442], [0.2480388074853509, 0.35082990203222336, 0.7467743864249737, 0.16745243707087717, 0.514575332051655, 0.01, 0.99, 0.5523361409577106, 0.3859888201390228, 0.01, 0.3157812382608078, 0.633365696085793, 0.99, 0.1989602913228665, 0.16578358655002254, 0.9497591420377942, 0.14870423678432176, 0.10515308743057335, 0.11036211121480685, 0.8908264350294514, 0.4896189187531885, 0.36744773769810846, 0.26742735693739705, 0.15159116101545425], [0.3168775717046756, 0.39333579851269723, 0.9088661247351473, 0.42853432544338127, 0.8537649529753241, 0.1556553714999904, 0.99, 0.47611668831799536, 0.741105284678487, 0.01, 0.4484374680205552, 0.8929242007698688, 0.7213570410641933, 0.08547558268021886, 0.04584362837550375, 0.7286927866019243, 0.08668632185340246, 0.04820608145254454, 0.07320498641641508, 0.885474550096277, 0.28722859001974665, 0.6259739747869087, 0.2842531624536692, 0.01]]
13 	8     	0.791406	0.0104103 	0.77375	0.80625
params:  [0.3132928631005317, 0.2743503160229126, 0.5540518527445745, 0.10396115787704524, 0.8940585491940787, 0.3044583020824243, 0.8153155202386669, 0.99, 0.42764998635564266, 0.1554084923532206, 0.5809351780871369, 0.6992152760490203, 0.6638704837587781, 0.27514439671498125, 0.3323750335457484, 0.9390781312133911, 0.5332148783369915, 0.1408304797827715, 0.01, 0.5617962269326405, 0.38880723980809845, 0.2692112580252226, 0.5751824758712799, 0.48877982678894133]
Training loss = 0.015761996706326803
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.795
Training loss = 0.0164826570947965
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.015798125863075257
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.015603942722082138
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.01551021953423818
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.015526719987392426
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.015239365051190059
step = 6, Training Accuracy: 0.8
Training loss = 0.014137594203154247
step = 7, Training Accuracy: 0.81
Training loss = 0.015869033932685853
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.013384025742610295
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.01469786504904429
step = 10, Training Accuracy: 0.83
Training loss = 0.014315697600444158
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.014206462502479554
step = 12, Training Accuracy: 0.8066666666666666
Training loss = 0.014345862368742624
step = 13, Training Accuracy: 0.7933333333333333
Training loss = 0.013515826463699341
step = 14, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.805
params:  [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01]
Training loss = 0.017867169082164763
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.79625
Training loss = 0.017990358422199884
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01628911594549815
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.01793512503306071
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01795981466770172
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.015053940912087758
step = 5, Training Accuracy: 0.8
Training loss = 0.015122793465852737
step = 6, Training Accuracy: 0.8133333333333334
Training loss = 0.01594723512729009
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.019816536605358124
step = 8, Training Accuracy: 0.7833333333333333
Training loss = 0.015349411765734355
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.015404236912727355
step = 10, Training Accuracy: 0.7933333333333333
Training loss = 0.016165798505147298
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.015772102375825246
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.013997955024242401
step = 13, Training Accuracy: 0.8
Training loss = 0.014435583750406902
step = 14, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.81125
params:  [0.1976806989715719, 0.4662392255497988, 0.99, 0.39807561797634383, 0.6945163314202992, 0.01, 0.6956962263820807, 0.5394766654416709, 0.18749514880396712, 0.1081631266238128, 0.46019720083757765, 0.8395414088546094, 0.660791921385456, 0.42961055831543915, 0.12563484089013438, 0.7412320184415511, 0.06803418643267534, 0.13141214992852207, 0.4101320098578352, 0.6879514824521615, 0.5888833674735017, 0.3322397841540839, 0.8964959738826647, 0.11169591973165352]
Training loss = 0.01618364453315735
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.8125
Training loss = 0.018269532918930055
step = 1, Training Accuracy: 0.74
Training loss = 0.01665387620528539
step = 2, Training Accuracy: 0.78
Training loss = 0.017920080622037253
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.01706535279750824
step = 4, Training Accuracy: 0.75
Training loss = 0.016315860450267793
step = 5, Training Accuracy: 0.8
Training loss = 0.01604593833287557
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.016524546444416047
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.01665715823570887
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.015585870146751404
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.017110660473505655
step = 10, Training Accuracy: 0.7766666666666666
Training loss = 0.0173320139447848
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.016525647441546124
step = 12, Training Accuracy: 0.8
Training loss = 0.014730077932278315
step = 13, Training Accuracy: 0.8166666666666667
Training loss = 0.015015139281749725
step = 14, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.79875
params:  [0.35507250567515997, 0.5619851477272976, 0.8561921312672975, 0.24553854231765268, 0.5314733850507563, 0.25294280571551614, 0.942838741889036, 0.6412649424997192, 0.04169257857349892, 0.01, 0.657555421490532, 0.795687558311308, 0.7762018424372004, 0.15065354769164122, 0.4394140150615159, 0.9260620384603369, 0.5543046374252536, 0.34468183276981423, 0.4218299315767311, 0.99, 0.3650276254917218, 0.4541677217443269, 0.8398067613255016, 0.01]
Training loss = 0.019514174660046894
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.8025
Training loss = 0.017355788946151734
step = 1, Training Accuracy: 0.7666666666666667
Training loss = 0.016700288057327272
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.01805836985508601
step = 3, Training Accuracy: 0.7333333333333333
Training loss = 0.01956435590982437
step = 4, Training Accuracy: 0.73
Training loss = 0.018728430966536203
step = 5, Training Accuracy: 0.7533333333333333
Training loss = 0.018505111237366995
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.016144217848777773
step = 7, Training Accuracy: 0.78
Training loss = 0.016940202713012695
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01733900745709737
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.01646474649508794
step = 10, Training Accuracy: 0.7833333333333333
Training loss = 0.017483234206835428
step = 11, Training Accuracy: 0.7366666666666667
Training loss = 0.017645624577999116
step = 12, Training Accuracy: 0.77
Training loss = 0.015284659365812938
step = 13, Training Accuracy: 0.7966666666666666
Training loss = 0.01640373428662618
step = 14, Training Accuracy: 0.77
Validation Accuracy: 0.80875
params:  [0.3491221542490772, 0.6055883338595063, 0.9309545997226264, 0.01, 0.6979855961633828, 0.1945543974233638, 0.99, 0.5168187748485575, 0.23944793228824568, 0.24485754235462806, 0.3506051700524975, 0.99, 0.6403822227725702, 0.01, 0.20918836067582622, 0.7365601159605009, 0.22442245879811115, 0.5284614819154853, 0.38613376352847445, 0.8160335904685428, 0.6708074523960614, 0.411398897956273, 0.7725369117482057, 0.20231140746894566]
Training loss = 0.02110934376716614
step = 0, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.805
Training loss = 0.019500392476717632
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.020323410431543985
step = 2, Training Accuracy: 0.71
Training loss = 0.018648892641067505
step = 3, Training Accuracy: 0.75
Training loss = 0.019235973755518596
step = 4, Training Accuracy: 0.75
Training loss = 0.019992312788963316
step = 5, Training Accuracy: 0.72
Training loss = 0.018932285110155742
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018897838592529297
step = 7, Training Accuracy: 0.75
Training loss = 0.01798134982585907
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.017657697399457297
step = 9, Training Accuracy: 0.7533333333333333
Training loss = 0.01837162474791209
step = 10, Training Accuracy: 0.7533333333333333
Training loss = 0.018217960894107817
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.019647820393244426
step = 12, Training Accuracy: 0.7833333333333333
Training loss = 0.01963750203450521
step = 13, Training Accuracy: 0.7
Training loss = 0.018629181881745657
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.79625
params:  [0.4677104910115446, 0.6004604688396039, 0.756713195130059, 0.34240924803112477, 0.42243852969883033, 0.10299657939441012, 0.9586059505720652, 0.7356736242973622, 0.01, 0.27078502210948474, 0.43870337976563767, 0.8218684661709728, 0.99, 0.01, 0.019424698296506143, 0.6533579562929891, 0.12163735596175759, 0.3493307264442117, 0.2887789029415397, 0.99, 0.27763462378322307, 0.38065533979945726, 0.9137093575063248, 0.01]
Training loss = 0.02103258450826009
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.8025
Training loss = 0.01996997445821762
step = 1, Training Accuracy: 0.73
Training loss = 0.01955336978038152
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.018852769633134207
step = 3, Training Accuracy: 0.74
Training loss = 0.019867522219816844
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.019052850306034087
step = 5, Training Accuracy: 0.74
Training loss = 0.01828850696484248
step = 6, Training Accuracy: 0.7633333333333333
Training loss = 0.019207026958465576
step = 7, Training Accuracy: 0.77
Training loss = 0.017753306726614633
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.018333680331707
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.017607609530289968
step = 10, Training Accuracy: 0.7633333333333333
Training loss = 0.021228624880313872
step = 11, Training Accuracy: 0.76
Training loss = 0.01927724073330561
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.018992911875247955
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.018262362480163573
step = 14, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.7975
params:  [0.2783522270683858, 0.4246737596155347, 0.828875435049343, 0.18568191830495612, 0.43435921909131747, 0.33997486836910085, 0.9186099772564016, 0.9598320620456261, 0.2452656902766348, 0.5258182699077231, 0.4402529261602701, 0.903622062184337, 0.4823398289587698, 0.2209325615744984, 0.22566590201260017, 0.8245892296289503, 0.15983017964606253, 0.07108846586556775, 0.3024417826200434, 0.72647776239974, 0.8229483116069063, 0.3781881220914361, 0.8930166339863121, 0.01]
Training loss = 0.019044423699378966
step = 0, Training Accuracy: 0.76
Validation Accuracy: 0.7975
Training loss = 0.017327473958333332
step = 1, Training Accuracy: 0.81
Training loss = 0.01714949031670888
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.016045370797316234
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01750259886185328
step = 4, Training Accuracy: 0.76
Training loss = 0.01767084966103236
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01629518578449885
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.018177315990130106
step = 7, Training Accuracy: 0.7266666666666667
Training loss = 0.017305638194084167
step = 8, Training Accuracy: 0.7666666666666667
Training loss = 0.016787342131137847
step = 9, Training Accuracy: 0.7966666666666666
Training loss = 0.016839722990989684
step = 10, Training Accuracy: 0.7733333333333333
Training loss = 0.015858050882816315
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01566604564587275
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.016918935775756837
step = 13, Training Accuracy: 0.7866666666666666
Training loss = 0.017058749596277872
step = 14, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.79375
params:  [0.3060668005152578, 0.3430648661300615, 0.7630051964118214, 0.7110105844802587, 0.6328739719971357, 0.07328094797328487, 0.99, 0.6180888629126065, 0.15238597541384222, 0.21915051002925906, 0.2025050574613959, 0.670134912808823, 0.8796918932467961, 0.27477343968682977, 0.2020919060468696, 0.781802852738887, 0.11639272345296894, 0.4243662450542475, 0.16537364175365235, 0.8064232979478053, 0.27927923679068956, 0.3506138963330703, 0.745587087600945, 0.2133728188652097]
Training loss = 0.017083651423454284
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.79625
Training loss = 0.017018948793411256
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01932839204867681
step = 2, Training Accuracy: 0.78
Training loss = 0.01600754827260971
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01647634824117025
step = 4, Training Accuracy: 0.7533333333333333
Training loss = 0.016346232096354166
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.01666429966688156
step = 6, Training Accuracy: 0.8
Training loss = 0.016731135249137878
step = 7, Training Accuracy: 0.7666666666666667
Training loss = 0.01620760897795359
step = 8, Training Accuracy: 0.78
Training loss = 0.0177166282137235
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.016482727974653243
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.015722772777080535
step = 11, Training Accuracy: 0.8
Training loss = 0.016793337364991504
step = 12, Training Accuracy: 0.77
Training loss = 0.015112150808175405
step = 13, Training Accuracy: 0.8
Training loss = 0.017290295958518984
step = 14, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.78875
[[0.3132928631005317, 0.2743503160229126, 0.5540518527445745, 0.10396115787704524, 0.8940585491940787, 0.3044583020824243, 0.8153155202386669, 0.99, 0.42764998635564266, 0.1554084923532206, 0.5809351780871369, 0.6992152760490203, 0.6638704837587781, 0.27514439671498125, 0.3323750335457484, 0.9390781312133911, 0.5332148783369915, 0.1408304797827715, 0.01, 0.5617962269326405, 0.38880723980809845, 0.2692112580252226, 0.5751824758712799, 0.48877982678894133], [0.4546697027176039, 0.686768414501576, 0.9204193933550783, 0.2179701676469351, 0.745324316140274, 0.1048188233508859, 0.99, 0.7815456771647684, 0.5141275004897095, 0.01, 0.7030585412151354, 0.6010050009290273, 0.9232729239222149, 0.22283673116023392, 0.01, 0.7843266009278341, 0.13323349436411253, 0.01, 0.18908445647472388, 0.8208260937274724, 0.4242244345592337, 0.3905102105198264, 0.5607986532858978, 0.01], [0.1976806989715719, 0.4662392255497988, 0.99, 0.39807561797634383, 0.6945163314202992, 0.01, 0.6956962263820807, 0.5394766654416709, 0.18749514880396712, 0.1081631266238128, 0.46019720083757765, 0.8395414088546094, 0.660791921385456, 0.42961055831543915, 0.12563484089013438, 0.7412320184415511, 0.06803418643267534, 0.13141214992852207, 0.4101320098578352, 0.6879514824521615, 0.5888833674735017, 0.3322397841540839, 0.8964959738826647, 0.11169591973165352], [0.35507250567515997, 0.5619851477272976, 0.8561921312672975, 0.24553854231765268, 0.5314733850507563, 0.25294280571551614, 0.942838741889036, 0.6412649424997192, 0.04169257857349892, 0.01, 0.657555421490532, 0.795687558311308, 0.7762018424372004, 0.15065354769164122, 0.4394140150615159, 0.9260620384603369, 0.5543046374252536, 0.34468183276981423, 0.4218299315767311, 0.99, 0.3650276254917218, 0.4541677217443269, 0.8398067613255016, 0.01], [0.3491221542490772, 0.6055883338595063, 0.9309545997226264, 0.01, 0.6979855961633828, 0.1945543974233638, 0.99, 0.5168187748485575, 0.23944793228824568, 0.24485754235462806, 0.3506051700524975, 0.99, 0.6403822227725702, 0.01, 0.20918836067582622, 0.7365601159605009, 0.22442245879811115, 0.5284614819154853, 0.38613376352847445, 0.8160335904685428, 0.6708074523960614, 0.411398897956273, 0.7725369117482057, 0.20231140746894566], [0.4677104910115446, 0.6004604688396039, 0.756713195130059, 0.34240924803112477, 0.42243852969883033, 0.10299657939441012, 0.9586059505720652, 0.7356736242973622, 0.01, 0.27078502210948474, 0.43870337976563767, 0.8218684661709728, 0.99, 0.01, 0.019424698296506143, 0.6533579562929891, 0.12163735596175759, 0.3493307264442117, 0.2887789029415397, 0.99, 0.27763462378322307, 0.38065533979945726, 0.9137093575063248, 0.01], [0.2783522270683858, 0.4246737596155347, 0.828875435049343, 0.18568191830495612, 0.43435921909131747, 0.33997486836910085, 0.9186099772564016, 0.9598320620456261, 0.2452656902766348, 0.5258182699077231, 0.4402529261602701, 0.903622062184337, 0.4823398289587698, 0.2209325615744984, 0.22566590201260017, 0.8245892296289503, 0.15983017964606253, 0.07108846586556775, 0.3024417826200434, 0.72647776239974, 0.8229483116069063, 0.3781881220914361, 0.8930166339863121, 0.01], [0.3060668005152578, 0.3430648661300615, 0.7630051964118214, 0.7110105844802587, 0.6328739719971357, 0.07328094797328487, 0.99, 0.6180888629126065, 0.15238597541384222, 0.21915051002925906, 0.2025050574613959, 0.670134912808823, 0.8796918932467961, 0.27477343968682977, 0.2020919060468696, 0.781802852738887, 0.11639272345296894, 0.4243662450542475, 0.16537364175365235, 0.8064232979478053, 0.27927923679068956, 0.3506138963330703, 0.745587087600945, 0.2133728188652097]]
14 	8     	0.8     	0.00720785	0.78875	0.81125
params:  [0.264195384198241, 0.8327627071486238, 0.99, 0.2919552227102, 0.6617294156948426, 0.292127425445328, 0.99, 0.6568925110676404, 0.2202556844500421, 0.07072939463370512, 0.9745469720960563, 0.8878406395304722, 0.7857200518098447, 0.18541610647302603, 0.5185201940447343, 0.9852845783524649, 0.17204206585093348, 0.01, 0.26846983182060896, 0.7941383220244639, 0.6921127309961128, 0.6408806579666522, 0.36267539400886734, 0.3409442412482649]
Training loss = 0.018576987385749817
step = 0, Training Accuracy: 0.7366666666666667
Validation Accuracy: 0.78375
Training loss = 0.018742579519748687
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.01819673031568527
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.018843311965465546
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.015682372351487478
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.016715002655982972
step = 5, Training Accuracy: 0.75
Training loss = 0.01769608646631241
step = 6, Training Accuracy: 0.7333333333333333
Training loss = 0.01811826040347417
step = 7, Training Accuracy: 0.75
Training loss = 0.015907400051752726
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015527278482913971
step = 9, Training Accuracy: 0.7666666666666667
Training loss = 0.016138986547787983
step = 10, Training Accuracy: 0.77
Training loss = 0.01799649159113566
step = 11, Training Accuracy: 0.74
Training loss = 0.01748806893825531
step = 12, Training Accuracy: 0.7566666666666667
Training loss = 0.01842192351818085
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.016136071681976318
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.775
params:  [0.339875574669313, 0.49918526367170485, 0.9103131729312753, 0.2921138294708268, 0.592729889478813, 0.04733421639045679, 0.8708130052521681, 0.6986986182123717, 0.11526296887332793, 0.01, 0.49863821809760206, 0.3041749255729081, 0.9370777873379817, 0.3682141412666046, 0.3497733626235008, 0.6513921387237186, 0.2171177645454672, 0.3367381038467938, 0.14510380180874308, 0.9719311626705511, 0.35012179259604226, 0.4397445006708199, 0.6343259584512478, 0.01]
Training loss = 0.017445944845676423
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7875
Training loss = 0.016197131474812825
step = 1, Training Accuracy: 0.79
Training loss = 0.018133200903733573
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.01677565594514211
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.0171061310172081
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.015037091275056202
step = 5, Training Accuracy: 0.8066666666666666
Training loss = 0.01668030391136805
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.018189760843912762
step = 7, Training Accuracy: 0.77
Training loss = 0.01575687378644943
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.016701995333035787
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.016246014336744944
step = 10, Training Accuracy: 0.8066666666666666
Training loss = 0.01619057923555374
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.01573076511422793
step = 12, Training Accuracy: 0.7933333333333333
Training loss = 0.01661807030439377
step = 13, Training Accuracy: 0.8
Training loss = 0.014762283861637115
step = 14, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.78625
params:  [0.36175465810626667, 0.771650759062442, 0.7363225930573026, 0.22487186137139234, 0.6812426706706778, 0.18405382797239736, 0.9184744593008435, 0.941653922557535, 0.4553939280332371, 0.14331983387947428, 0.38599113930726503, 0.6298885137194282, 0.7769220319503658, 0.01, 0.3005492131866617, 0.9061876460156759, 0.26650072347246456, 0.5004738521650314, 0.2260024318370635, 0.8164852990538298, 0.28638885168840256, 0.40063581554151895, 0.6281572124265404, 0.01]
Training loss = 0.019926685591538748
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7925
Training loss = 0.017727797230084736
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.017428320348262787
step = 2, Training Accuracy: 0.79
Training loss = 0.017518082857131956
step = 3, Training Accuracy: 0.74
Training loss = 0.01927064190308253
step = 4, Training Accuracy: 0.7366666666666667
Training loss = 0.019027674595514934
step = 5, Training Accuracy: 0.7666666666666667
Training loss = 0.018847871522108713
step = 6, Training Accuracy: 0.74
Training loss = 0.01974724342425664
step = 7, Training Accuracy: 0.73
Training loss = 0.017322413822015128
step = 8, Training Accuracy: 0.7633333333333333
Training loss = 0.01676650583744049
step = 9, Training Accuracy: 0.77
Training loss = 0.019133825898170472
step = 10, Training Accuracy: 0.74
Training loss = 0.01799640109141668
step = 11, Training Accuracy: 0.75
Training loss = 0.01852283736069997
step = 12, Training Accuracy: 0.7533333333333333
Training loss = 0.020166855851809183
step = 13, Training Accuracy: 0.7566666666666667
Training loss = 0.017234105865160623
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.78875
params:  [0.17117094757979062, 0.562749669741782, 0.9107227668498393, 0.42144999224048196, 0.7934752343186503, 0.01, 0.99, 0.8788513593591532, 0.33109563034450157, 0.1485040263917836, 0.6509762766690409, 0.8675417963069617, 0.9396955510232586, 0.2055151334150429, 0.5142524634652317, 0.9457438657247512, 0.1934868126710969, 0.01, 0.01, 0.99, 0.7866551231813665, 0.3867595065441036, 0.40635275022483214, 0.22982045200965512]
Training loss = 0.01683007260163625
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.78375
Training loss = 0.015809138615926106
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015279199182987212
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.016987585524717966
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.016597538789113363
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.015344666043917338
step = 5, Training Accuracy: 0.8033333333333333
Training loss = 0.015903458893299104
step = 6, Training Accuracy: 0.82
Training loss = 0.01488207459449768
step = 7, Training Accuracy: 0.79
Training loss = 0.01744352827469508
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.016049488484859466
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.015577459832032521
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.015261089106400809
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.015134805937608083
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.01640701174736023
step = 13, Training Accuracy: 0.7666666666666667
Training loss = 0.015440543194611868
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.765
params:  [0.4008643310405859, 0.6970832428896392, 0.6921465124529463, 0.31159731064700824, 0.5183754730430326, 0.20330278629902013, 0.8766073886147915, 0.5119165459923263, 0.31697879640361776, 0.18037506744844686, 0.921059628177124, 0.7831873472983643, 0.6934200336567652, 0.36826278810134483, 0.3504534534051691, 0.7343773879304665, 0.30850300087222926, 0.012558828208181605, 0.29175046251727366, 0.8503370080205277, 0.5675169197573675, 0.15692576949330364, 0.4233546125926121, 0.01]
Training loss = 0.0180417537689209
step = 0, Training Accuracy: 0.78
Validation Accuracy: 0.7575
Training loss = 0.016427661776542663
step = 1, Training Accuracy: 0.77
Training loss = 0.016034269481897356
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.0151752707362175
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.013296748300393423
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.014783408095439276
step = 5, Training Accuracy: 0.8
Training loss = 0.016704900761445363
step = 6, Training Accuracy: 0.7866666666666666
Training loss = 0.016763779024283092
step = 7, Training Accuracy: 0.7833333333333333
Training loss = 0.015583472053209941
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.017197786768277486
step = 9, Training Accuracy: 0.7733333333333333
Training loss = 0.015200260678927104
step = 10, Training Accuracy: 0.79
Training loss = 0.01597342679897944
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01536469320456187
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.013834127485752105
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.014397180924812953
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.79375
params:  [0.31806373231064583, 0.7004146877955908, 0.9241992853399167, 0.15734469722429356, 0.7690065965975826, 0.16744390263309666, 0.99, 0.6294838415134705, 0.15333447614677823, 0.01, 0.7540234242417452, 0.6557357394330954, 0.608272190380877, 0.24722778697680314, 0.15884481714697887, 0.9480568903741388, 0.2797062382995208, 0.01, 0.30161297707495616, 0.99, 0.2259718740141242, 0.6367290542397763, 0.9419613122464084, 0.01]
Training loss = 0.020206294556458792
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.79125
Training loss = 0.020664776861667632
step = 1, Training Accuracy: 0.7366666666666667
Training loss = 0.02142822861671448
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.01867156396309535
step = 3, Training Accuracy: 0.7
Training loss = 0.020345319410165152
step = 4, Training Accuracy: 0.72
Training loss = 0.018290942311286928
step = 5, Training Accuracy: 0.7133333333333334
Training loss = 0.01900339702765147
step = 6, Training Accuracy: 0.7433333333333333
Training loss = 0.018766363362471263
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01867619752883911
step = 8, Training Accuracy: 0.7533333333333333
Training loss = 0.021070469419161478
step = 9, Training Accuracy: 0.71
Training loss = 0.01917880912621816
step = 10, Training Accuracy: 0.71
Training loss = 0.01781214604775111
step = 11, Training Accuracy: 0.7433333333333333
Training loss = 0.01880502571662267
step = 12, Training Accuracy: 0.7466666666666667
Training loss = 0.018320874671141307
step = 13, Training Accuracy: 0.7166666666666667
Training loss = 0.018465299606323243
step = 14, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.79625
params:  [0.5937523206015756, 0.7531293208684413, 0.8681318324338217, 0.37150880876685777, 0.5652679776268087, 0.35252084741669515, 0.9586415399113765, 0.6042245416683718, 0.42199023968538935, 0.054873540749076445, 0.8897170305649649, 0.8986510533425169, 0.5898628229877102, 0.26151670936810983, 0.22200610301845736, 0.9771873744906776, 0.4761126925691952, 0.052647781272982, 0.1804610088006462, 0.910647533136, 0.5547155977239764, 0.38096439803621907, 0.267197748673559, 0.2831251344285397]
Training loss = 0.014412931303183238
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.79
Training loss = 0.015549450516700744
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.01603226105372111
step = 2, Training Accuracy: 0.78
Training loss = 0.01404803141951561
step = 3, Training Accuracy: 0.81
Training loss = 0.01518844445546468
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015562501549720765
step = 5, Training Accuracy: 0.7866666666666666
Training loss = 0.01464643637339274
step = 6, Training Accuracy: 0.83
Training loss = 0.014928218126296997
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014388850728670756
step = 8, Training Accuracy: 0.82
Training loss = 0.016828654011090596
step = 9, Training Accuracy: 0.8
Training loss = 0.015324331323305766
step = 10, Training Accuracy: 0.8033333333333333
Training loss = 0.01631461242834727
step = 11, Training Accuracy: 0.7866666666666666
Training loss = 0.014591604967912038
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.0150905575354894
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.014923920730749766
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.77875
params:  [0.3424718510161171, 0.5908409749173555, 0.99, 0.384163221470892, 0.4985053702552006, 0.12591723023284987, 0.753488950279287, 0.721898391959197, 0.6906904386951185, 0.01, 0.6921688520418043, 0.5572368756827177, 0.9530442228407054, 0.05342257500841674, 0.5299656768878469, 0.7226205290934133, 0.14425545335852244, 0.01, 0.3089766455052878, 0.99, 0.4757151307622566, 0.03412030956825213, 0.660024458981339, 0.01]
Training loss = 0.0171780193845431
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.78625
Training loss = 0.01780468742052714
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.01729487955570221
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.018194449692964555
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.016905243595441183
step = 4, Training Accuracy: 0.74
Training loss = 0.016877658466498056
step = 5, Training Accuracy: 0.7833333333333333
Training loss = 0.014868525962034862
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.015789087414741516
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.01843883087237676
step = 8, Training Accuracy: 0.75
Training loss = 0.015041351268688838
step = 9, Training Accuracy: 0.79
Training loss = 0.016788917581240338
step = 10, Training Accuracy: 0.77
Training loss = 0.015901770939429603
step = 11, Training Accuracy: 0.77
Training loss = 0.01857567419608434
step = 12, Training Accuracy: 0.73
Training loss = 0.018037529786427815
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.015957110226154328
step = 14, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.7925
[[0.264195384198241, 0.8327627071486238, 0.99, 0.2919552227102, 0.6617294156948426, 0.292127425445328, 0.99, 0.6568925110676404, 0.2202556844500421, 0.07072939463370512, 0.9745469720960563, 0.8878406395304722, 0.7857200518098447, 0.18541610647302603, 0.5185201940447343, 0.9852845783524649, 0.17204206585093348, 0.01, 0.26846983182060896, 0.7941383220244639, 0.6921127309961128, 0.6408806579666522, 0.36267539400886734, 0.3409442412482649], [0.339875574669313, 0.49918526367170485, 0.9103131729312753, 0.2921138294708268, 0.592729889478813, 0.04733421639045679, 0.8708130052521681, 0.6986986182123717, 0.11526296887332793, 0.01, 0.49863821809760206, 0.3041749255729081, 0.9370777873379817, 0.3682141412666046, 0.3497733626235008, 0.6513921387237186, 0.2171177645454672, 0.3367381038467938, 0.14510380180874308, 0.9719311626705511, 0.35012179259604226, 0.4397445006708199, 0.6343259584512478, 0.01], [0.36175465810626667, 0.771650759062442, 0.7363225930573026, 0.22487186137139234, 0.6812426706706778, 0.18405382797239736, 0.9184744593008435, 0.941653922557535, 0.4553939280332371, 0.14331983387947428, 0.38599113930726503, 0.6298885137194282, 0.7769220319503658, 0.01, 0.3005492131866617, 0.9061876460156759, 0.26650072347246456, 0.5004738521650314, 0.2260024318370635, 0.8164852990538298, 0.28638885168840256, 0.40063581554151895, 0.6281572124265404, 0.01], [0.17117094757979062, 0.562749669741782, 0.9107227668498393, 0.42144999224048196, 0.7934752343186503, 0.01, 0.99, 0.8788513593591532, 0.33109563034450157, 0.1485040263917836, 0.6509762766690409, 0.8675417963069617, 0.9396955510232586, 0.2055151334150429, 0.5142524634652317, 0.9457438657247512, 0.1934868126710969, 0.01, 0.01, 0.99, 0.7866551231813665, 0.3867595065441036, 0.40635275022483214, 0.22982045200965512], [0.4008643310405859, 0.6970832428896392, 0.6921465124529463, 0.31159731064700824, 0.5183754730430326, 0.20330278629902013, 0.8766073886147915, 0.5119165459923263, 0.31697879640361776, 0.18037506744844686, 0.921059628177124, 0.7831873472983643, 0.6934200336567652, 0.36826278810134483, 0.3504534534051691, 0.7343773879304665, 0.30850300087222926, 0.012558828208181605, 0.29175046251727366, 0.8503370080205277, 0.5675169197573675, 0.15692576949330364, 0.4233546125926121, 0.01], [0.31806373231064583, 0.7004146877955908, 0.9241992853399167, 0.15734469722429356, 0.7690065965975826, 0.16744390263309666, 0.99, 0.6294838415134705, 0.15333447614677823, 0.01, 0.7540234242417452, 0.6557357394330954, 0.608272190380877, 0.24722778697680314, 0.15884481714697887, 0.9480568903741388, 0.2797062382995208, 0.01, 0.30161297707495616, 0.99, 0.2259718740141242, 0.6367290542397763, 0.9419613122464084, 0.01], [0.5937523206015756, 0.7531293208684413, 0.8681318324338217, 0.37150880876685777, 0.5652679776268087, 0.35252084741669515, 0.9586415399113765, 0.6042245416683718, 0.42199023968538935, 0.054873540749076445, 0.8897170305649649, 0.8986510533425169, 0.5898628229877102, 0.26151670936810983, 0.22200610301845736, 0.9771873744906776, 0.4761126925691952, 0.052647781272982, 0.1804610088006462, 0.910647533136, 0.5547155977239764, 0.38096439803621907, 0.267197748673559, 0.2831251344285397], [0.3424718510161171, 0.5908409749173555, 0.99, 0.384163221470892, 0.4985053702552006, 0.12591723023284987, 0.753488950279287, 0.721898391959197, 0.6906904386951185, 0.01, 0.6921688520418043, 0.5572368756827177, 0.9530442228407054, 0.05342257500841674, 0.5299656768878469, 0.7226205290934133, 0.14425545335852244, 0.01, 0.3089766455052878, 0.99, 0.4757151307622566, 0.03412030956825213, 0.660024458981339, 0.01]]
15 	8     	0.784531	0.0100766 	0.765  	0.79625
params:  [0.5610545058351945, 0.8539966407064861, 0.99, 0.3030147928205421, 0.48377827054354466, 0.3276794247040352, 0.99, 0.6494567528409746, 0.3680194409725897, 0.01, 0.7719599082563088, 0.9220788717371112, 0.6503962586452968, 0.7562174154085752, 0.34898727882013447, 0.5822076891038586, 0.3238762052948222, 0.04860334709272189, 0.3353280503163413, 0.99, 0.40390450749189666, 0.3966972097839411, 0.497019566852702, 0.16282926991646154]
Training loss = 0.01937459111213684
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.795
Training loss = 0.02116418828566869
step = 1, Training Accuracy: 0.74
Training loss = 0.018214304745197297
step = 2, Training Accuracy: 0.7566666666666667
Training loss = 0.0175395600994428
step = 3, Training Accuracy: 0.7766666666666666
Training loss = 0.020336245497067768
step = 4, Training Accuracy: 0.7333333333333333
Training loss = 0.019708975652853646
step = 5, Training Accuracy: 0.7333333333333333
