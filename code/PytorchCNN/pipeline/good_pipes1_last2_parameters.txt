params:  [0.6490142459033698, 0.01, 0.01, 0.40592201480314355, 0.16028107392892293, 0.3609746921562613, 0.629434679742456, 0.29798040789437485, 0.32582435088618106, 0.9737638446522174, 0.3964255795819125, 0.3964206542496659, 0.9235756235890742, 0.6943065614302077, 0.01, 0.7302304187458726, 0.1961506638996729]
[0.6490142459033698, 0.01, 0.01, 0.40592201480314355, 0.16028107392892293, 0.3609746921562613, 0.629434679742456, 0.29798040789437485, 0.32582435088618106, 0.9737638446522174, 0.3964255795819125, 0.3964206542496659, 0.9235756235890742, 0.6943065614302077, 0.01, 0.7302304187458726, 0.1961506638996729]
Training loss = 0.043410863081614175
step = 0, Training Accuracy: 0.34
Validation Accuracy: 0.30375
Training loss = 0.03821241060892741
step = 1, Training Accuracy: 0.4066666666666667
Training loss = 0.036018604040145875
step = 2, Training Accuracy: 0.47333333333333333
Training loss = 0.04071064233779907
step = 3, Training Accuracy: 0.3933333333333333
Training loss = 0.035873475273450216
step = 4, Training Accuracy: 0.45
Training loss = 0.03416021168231964
step = 5, Training Accuracy: 0.49333333333333335
Validation Accuracy: 0.50625
Training loss = 0.03373443563779195
step = 6, Training Accuracy: 0.5233333333333333
Training loss = 0.032512511610984805
step = 7, Training Accuracy: 0.49666666666666665
Training loss = 0.03196832517782847
step = 8, Training Accuracy: 0.49333333333333335
Training loss = 0.030555445353190103
step = 9, Training Accuracy: 0.5333333333333333
Training loss = 0.03123065153757731
step = 10, Training Accuracy: 0.5433333333333333
Validation Accuracy: 0.54375
Training loss = 0.030801935791969298
step = 11, Training Accuracy: 0.5766666666666667
Training loss = 0.02921990712483724
step = 12, Training Accuracy: 0.5633333333333334
Training loss = 0.029800535837809245
step = 13, Training Accuracy: 0.5833333333333334
Training loss = 0.029414305885632833
step = 14, Training Accuracy: 0.58
Validation Accuracy: 0.52125
params:  [0.5942741997785822, 0.8890167886860145, 0.15282134966451424, 0.24582520839535027, 0.1198083930243585, 0.6127094055037016, 0.12136859343997586, 0.4626174992452865, 0.4999434435796265, 0.07257554413596295, 0.48692512807304383, 0.398933776520706, 0.9063612973431329, 0.07630888959941257, 0.01, 0.3366851826424452, 0.18268672131322988]
[0.5942741997785822, 0.8890167886860145, 0.15282134966451424, 0.24582520839535027, 0.1198083930243585, 0.6127094055037016, 0.12136859343997586, 0.4626174992452865, 0.4999434435796265, 0.07257554413596295, 0.48692512807304383, 0.398933776520706, 0.9063612973431329, 0.07630888959941257, 0.01, 0.3366851826424452, 0.18268672131322988]
Training loss = 0.03295392076174418
step = 0, Training Accuracy: 0.5466666666666666
Validation Accuracy: 0.535
Training loss = 0.03241751909255981
step = 1, Training Accuracy: 0.5233333333333333
Training loss = 0.03060072978337606
step = 2, Training Accuracy: 0.55
Training loss = 0.03212890028953552
step = 3, Training Accuracy: 0.5366666666666666
Training loss = 0.029183549284935
step = 4, Training Accuracy: 0.59
Training loss = 0.028855533997217814
step = 5, Training Accuracy: 0.6233333333333333
Validation Accuracy: 0.55
Training loss = 0.030790640314420064
step = 6, Training Accuracy: 0.5466666666666666
Training loss = 0.02969220797220866
step = 7, Training Accuracy: 0.5866666666666667
Training loss = 0.028950194915135702
step = 8, Training Accuracy: 0.59
Training loss = 0.028672752181688944
step = 9, Training Accuracy: 0.62
Training loss = 0.027836599548657734
step = 10, Training Accuracy: 0.6233333333333333
Validation Accuracy: 0.55375
Training loss = 0.02932937999566396
step = 11, Training Accuracy: 0.6066666666666667
Training loss = 0.026179489692052204
step = 12, Training Accuracy: 0.6466666666666666
Training loss = 0.026420736908912657
step = 13, Training Accuracy: 0.6366666666666667
Training loss = 0.0265693465868632
step = 14, Training Accuracy: 0.64
Validation Accuracy: 0.54625
params:  [0.7467634736309567, 0.43641882020387174, 0.650470001199008, 0.19514170204539708, 0.08404673748158739, 0.05644340288977179, 0.37633555798988005, 0.01, 0.4319721819501945, 0.7215399739986231, 0.5257250374274037, 0.06821085199713756, 0.01, 0.5626590785014266, 0.01, 0.5514104843569911, 0.5972251908184385]
[0.7467634736309567, 0.43641882020387174, 0.650470001199008, 0.19514170204539708, 0.08404673748158739, 0.05644340288977179, 0.37633555798988005, 0.01, 0.4319721819501945, 0.7215399739986231, 0.5257250374274037, 0.06821085199713756, 0.01, 0.5626590785014266, 0.01, 0.5514104843569911, 0.5972251908184385]
Training loss = 0.03434626042842865
step = 0, Training Accuracy: 0.52
Validation Accuracy: 0.545
Training loss = 0.03173737565676371
step = 1, Training Accuracy: 0.54
Training loss = 0.03290153602759043
step = 2, Training Accuracy: 0.5066666666666667
Training loss = 0.030853262941042583
step = 3, Training Accuracy: 0.53
Training loss = 0.02896488626797994
step = 4, Training Accuracy: 0.56
Training loss = 0.03040150225162506
step = 5, Training Accuracy: 0.56
Validation Accuracy: 0.4125
Training loss = 0.033572370608647664
step = 6, Training Accuracy: 0.5166666666666667
Training loss = 0.03153240362803141
step = 7, Training Accuracy: 0.5633333333333334
Training loss = 0.029908533891042074
step = 8, Training Accuracy: 0.57
Training loss = 0.02809456686178843
step = 9, Training Accuracy: 0.6
Training loss = 0.028644117911656698
step = 10, Training Accuracy: 0.6
Validation Accuracy: 0.5475
Training loss = 0.02708807627360026
step = 11, Training Accuracy: 0.66
Training loss = 0.02746508995691935
step = 12, Training Accuracy: 0.65
Training loss = 0.027745266755421955
step = 13, Training Accuracy: 0.6433333333333333
Training loss = 0.027612276077270508
step = 14, Training Accuracy: 0.62
Validation Accuracy: 0.52125
params:  [0.38447531587510503, 0.7402053419045802, 0.5770910800515927, 0.01, 0.01, 0.4443023070008549, 0.3229143953130797, 0.4450636301925665, 0.7593302048033744, 0.4072362872446356, 0.21490140969987515, 0.7460507024015262, 0.775966523415452, 0.6835028866522603, 0.01, 0.5993790294210692, 0.8010598693676072]
[0.38447531587510503, 0.7402053419045802, 0.5770910800515927, 0.01, 0.01, 0.4443023070008549, 0.3229143953130797, 0.4450636301925665, 0.7593302048033744, 0.4072362872446356, 0.21490140969987515, 0.7460507024015262, 0.775966523415452, 0.6835028866522603, 0.01, 0.5993790294210692, 0.8010598693676072]
Training loss = 0.03382291793823242
step = 0, Training Accuracy: 0.5066666666666667
Validation Accuracy: 0.5325
Training loss = 0.0298530650138855
step = 1, Training Accuracy: 0.56
Training loss = 0.029678421417872112
step = 2, Training Accuracy: 0.5866666666666667
Training loss = 0.02676690330108007
step = 3, Training Accuracy: 0.6333333333333333
Training loss = 0.027272921204566956
step = 4, Training Accuracy: 0.65
Training loss = 0.028599853316942852
step = 5, Training Accuracy: 0.6166666666666667
Validation Accuracy: 0.44375
Training loss = 0.029521164695421855
step = 6, Training Accuracy: 0.5866666666666667
Training loss = 0.028502719799677532
step = 7, Training Accuracy: 0.6533333333333333
Training loss = 0.027816986838976543
step = 8, Training Accuracy: 0.6233333333333333
Training loss = 0.02676574965318044
step = 9, Training Accuracy: 0.6366666666666667
Training loss = 0.029837515552838645
step = 10, Training Accuracy: 0.5933333333333334
Validation Accuracy: 0.5175
Training loss = 0.02615283668041229
step = 11, Training Accuracy: 0.6566666666666666
Training loss = 0.02695296049118042
step = 12, Training Accuracy: 0.6666666666666666
Training loss = 0.028412059744199115
step = 13, Training Accuracy: 0.6566666666666666
Training loss = 0.027098130385080972
step = 14, Training Accuracy: 0.63
Validation Accuracy: 0.6
params:  [0.6084908075142903, 0.7767015467557881, 0.44046710478685724, 0.2674317669820797, 0.01, 0.5275282329606507, 0.37696446152690644, 0.3111856011845725, 0.492780787138118, 0.01, 0.9360597634108685, 0.45591885493368123, 0.9280776366064574, 0.6084186816525242, 0.01, 0.7465707513125671, 0.25745191913204374]
[0.6084908075142903, 0.7767015467557881, 0.44046710478685724, 0.2674317669820797, 0.01, 0.5275282329606507, 0.37696446152690644, 0.3111856011845725, 0.492780787138118, 0.01, 0.9360597634108685, 0.45591885493368123, 0.9280776366064574, 0.6084186816525242, 0.01, 0.7465707513125671, 0.25745191913204374]
Training loss = 0.02975764830907186
step = 0, Training Accuracy: 0.6066666666666667
Validation Accuracy: 0.6
Training loss = 0.027844098011652628
step = 1, Training Accuracy: 0.5833333333333334
Training loss = 0.027367007931073505
step = 2, Training Accuracy: 0.6266666666666667
Training loss = 0.024015661776065827
step = 3, Training Accuracy: 0.6933333333333334
Training loss = 0.025021601915359497
step = 4, Training Accuracy: 0.6733333333333333
Training loss = 0.025942182143529256
step = 5, Training Accuracy: 0.6566666666666666
Validation Accuracy: 0.56875
Training loss = 0.024620469411214194
step = 6, Training Accuracy: 0.62
Training loss = 0.02516131063302358
step = 7, Training Accuracy: 0.6466666666666666
Training loss = 0.02305065095424652
step = 8, Training Accuracy: 0.67
Training loss = 0.024462239543596904
step = 9, Training Accuracy: 0.68
Training loss = 0.02374978264172872
step = 10, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.60625
Training loss = 0.022538871864477793
step = 11, Training Accuracy: 0.7233333333333334
Training loss = 0.020195754170417787
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.020186280409495036
step = 13, Training Accuracy: 0.77
Training loss = 0.02183378517627716
step = 14, Training Accuracy: 0.7033333333333334
Validation Accuracy: 0.6125
params:  [0.34947288692463907, 0.26295719332078926, 0.33486737032607156, 0.4116499149873001, 0.3888360831193728, 0.06094551556036443, 0.3490342207270194, 0.042055444051542434, 0.3683680226873362, 0.7905934971598667, 0.4957899314710788, 0.6206468966006735, 0.3077386055365551, 0.5986253328979053, 0.306366667056654, 0.28938407183679427, 0.37380640317039227]
[0.34947288692463907, 0.26295719332078926, 0.33486737032607156, 0.4116499149873001, 0.3888360831193728, 0.06094551556036443, 0.3490342207270194, 0.042055444051542434, 0.3683680226873362, 0.7905934971598667, 0.4957899314710788, 0.6206468966006735, 0.3077386055365551, 0.5986253328979053, 0.306366667056654, 0.28938407183679427, 0.37380640317039227]
Training loss = 0.03869646072387695
step = 0, Training Accuracy: 0.48
Validation Accuracy: 0.38
Training loss = 0.0325071507692337
step = 1, Training Accuracy: 0.4766666666666667
Training loss = 0.028815398613611858
step = 2, Training Accuracy: 0.54
Training loss = 0.027353350321451822
step = 3, Training Accuracy: 0.65
Training loss = 0.027399145166079202
step = 4, Training Accuracy: 0.59
Training loss = 0.02406386226415634
step = 5, Training Accuracy: 0.6533333333333333
Validation Accuracy: 0.64125
Training loss = 0.02526516278584798
step = 6, Training Accuracy: 0.64
Training loss = 0.02209895114103953
step = 7, Training Accuracy: 0.68
Training loss = 0.022590086062749228
step = 8, Training Accuracy: 0.7466666666666667
Training loss = 0.022306981285413106
step = 9, Training Accuracy: 0.6733333333333333
Training loss = 0.022944048444430033
step = 10, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.6475
Training loss = 0.02278951366742452
step = 11, Training Accuracy: 0.6933333333333334
Training loss = 0.021542449196179706
step = 12, Training Accuracy: 0.6933333333333334
Training loss = 0.02124114692211151
step = 13, Training Accuracy: 0.7133333333333334
Training loss = 0.02103779951731364
step = 14, Training Accuracy: 0.73
Validation Accuracy: 0.67625
params:  [0.3971856450419692, 0.3229198024217603, 0.4237975360334171, 0.27562504389899656, 0.99, 0.5180690629823079, 0.4587125040319016, 0.11606325538080708, 0.01, 0.5772651172168293, 0.5190400105162184, 0.99, 0.5878819237110282, 0.45161428650019725, 0.01, 0.47766622527014985, 0.8428468443545061]
[0.3971856450419692, 0.3229198024217603, 0.4237975360334171, 0.27562504389899656, 0.99, 0.5180690629823079, 0.4587125040319016, 0.11606325538080708, 0.01, 0.5772651172168293, 0.5190400105162184, 0.99, 0.5878819237110282, 0.45161428650019725, 0.01, 0.47766622527014985, 0.8428468443545061]
Training loss = 0.025173116326332092
step = 0, Training Accuracy: 0.6833333333333333
Validation Accuracy: 0.66
Training loss = 0.022240601181983947
step = 1, Training Accuracy: 0.7033333333333334
Training loss = 0.021419378916422527
step = 2, Training Accuracy: 0.6966666666666667
Training loss = 0.02073496361573537
step = 3, Training Accuracy: 0.74
Training loss = 0.020120621422926584
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.019673366645971933
step = 5, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.675
Training loss = 0.019085806210835776
step = 6, Training Accuracy: 0.77
Training loss = 0.019035969972610475
step = 7, Training Accuracy: 0.78
Training loss = 0.02055136650800705
step = 8, Training Accuracy: 0.7766666666666666
Training loss = 0.018977575500806174
step = 9, Training Accuracy: 0.78
Training loss = 0.019844200710455576
step = 10, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.7
Training loss = 0.018582156896591186
step = 11, Training Accuracy: 0.7766666666666666
Training loss = 0.018228149910767873
step = 12, Training Accuracy: 0.76
Training loss = 0.02041691263516744
step = 13, Training Accuracy: 0.73
Training loss = 0.017874768177668254
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.705
params:  [0.7255799098060323, 0.47541106252388776, 0.01464221921550185, 0.3539022257751415, 0.01, 0.3489573037651402, 0.49656207619295906, 0.19083939639652575, 0.2967773477858351, 0.99, 0.6427237948067478, 0.04611134782898241, 0.8875049599474965, 0.22718376356157832, 0.2690556158589458, 0.2028391024607935, 0.9649803215052618]
[0.7255799098060323, 0.47541106252388776, 0.01464221921550185, 0.3539022257751415, 0.01, 0.3489573037651402, 0.49656207619295906, 0.19083939639652575, 0.2967773477858351, 0.99, 0.6427237948067478, 0.04611134782898241, 0.8875049599474965, 0.22718376356157832, 0.2690556158589458, 0.2028391024607935, 0.9649803215052618]
Training loss = 0.02812260588010152
step = 0, Training Accuracy: 0.6466666666666666
Validation Accuracy: 0.66375
Training loss = 0.02139854907989502
step = 1, Training Accuracy: 0.7066666666666667
Training loss = 0.02079261263211568
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.02066028942664464
step = 3, Training Accuracy: 0.7566666666666667
Training loss = 0.020198802947998046
step = 4, Training Accuracy: 0.72
Training loss = 0.01863024115562439
step = 5, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.67125
Training loss = 0.01926288942495982
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017912597556908924
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.018037466108798982
step = 8, Training Accuracy: 0.78
Training loss = 0.017031508187452953
step = 9, Training Accuracy: 0.8
Training loss = 0.017764013608296714
step = 10, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.69375
Training loss = 0.017111062904198966
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.019456672072410582
step = 12, Training Accuracy: 0.77
Training loss = 0.01748845318953196
step = 13, Training Accuracy: 0.8033333333333333
Training loss = 0.014625271757443745
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.6725
gen	nevals	avg     	std      	min    	max  
0  	8     	0.606875	0.0681594	0.52125	0.705
params:  [0.01, 0.4780483814193737, 0.415548192841912, 0.47338159159539517, 0.48438183337731167, 0.012311743114508056, 0.798929057674767, 0.35117906761779416, 0.035668336669429035, 0.7127472094542847, 0.5802745719268556, 0.6172622921477553, 0.8307872118769886, 0.6118548219927428, 0.01, 0.3483607379002776, 0.3432991515942402]
[0.01, 0.4780483814193737, 0.415548192841912, 0.47338159159539517, 0.48438183337731167, 0.012311743114508056, 0.798929057674767, 0.35117906761779416, 0.035668336669429035, 0.7127472094542847, 0.5802745719268556, 0.6172622921477553, 0.8307872118769886, 0.6118548219927428, 0.01, 0.3483607379002776, 0.3432991515942402]
Training loss = 0.026694819927215577
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.71625
Training loss = 0.02218502720197042
step = 1, Training Accuracy: 0.7133333333333334
Training loss = 0.02016468584537506
step = 2, Training Accuracy: 0.73
Training loss = 0.021644985675811766
step = 3, Training Accuracy: 0.7433333333333333
Training loss = 0.02106184760729472
step = 4, Training Accuracy: 0.7233333333333334
Training loss = 0.016909536520640055
step = 5, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.70875
Training loss = 0.016465811133384703
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.015001179377237956
step = 7, Training Accuracy: 0.7966666666666666
Training loss = 0.01649323950211207
step = 8, Training Accuracy: 0.7866666666666666
Training loss = 0.015928950061400732
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.015559533933798471
step = 10, Training Accuracy: 0.78
Validation Accuracy: 0.675
Training loss = 0.015001886586348216
step = 11, Training Accuracy: 0.8166666666666667
Training loss = 0.013244958519935608
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.01282996028661728
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.014054060180981954
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.6775
params:  [0.6244730057747098, 0.413380156557954, 0.3235566672469676, 0.8264286586293692, 0.7012464868328137, 0.22535865408312622, 0.921796627280596, 0.3247879645341015, 0.01, 0.6790736280724383, 0.6727415991706351, 0.99, 0.2887636222219142, 0.2927574544444471, 0.2848089122710793, 0.01, 0.29957085445416975]
[0.6244730057747098, 0.413380156557954, 0.3235566672469676, 0.8264286586293692, 0.7012464868328137, 0.22535865408312622, 0.921796627280596, 0.3247879645341015, 0.01, 0.6790736280724383, 0.6727415991706351, 0.99, 0.2887636222219142, 0.2927574544444471, 0.2848089122710793, 0.01, 0.29957085445416975]
Training loss = 0.03646072526772817
step = 0, Training Accuracy: 0.5666666666666667
Validation Accuracy: 0.53125
Training loss = 0.02537311315536499
step = 1, Training Accuracy: 0.6533333333333333
Training loss = 0.026565070947011313
step = 2, Training Accuracy: 0.6333333333333333
Training loss = 0.022411339481671653
step = 3, Training Accuracy: 0.6833333333333333
Training loss = 0.024778911769390108
step = 4, Training Accuracy: 0.6866666666666666
Training loss = 0.02216236690680186
step = 5, Training Accuracy: 0.6666666666666666
Validation Accuracy: 0.6975
Training loss = 0.021245954334735872
step = 6, Training Accuracy: 0.6866666666666666
Training loss = 0.021082959671815237
step = 7, Training Accuracy: 0.6966666666666667
Training loss = 0.022769426107406617
step = 8, Training Accuracy: 0.72
Training loss = 0.019532236655553183
step = 9, Training Accuracy: 0.7333333333333333
Training loss = 0.02076025664806366
step = 10, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.71
Training loss = 0.018467882573604585
step = 11, Training Accuracy: 0.7466666666666667
Training loss = 0.02119296173254649
step = 12, Training Accuracy: 0.72
Training loss = 0.019122986892859142
step = 13, Training Accuracy: 0.7533333333333333
Training loss = 0.018998002012570698
step = 14, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.69375
params:  [0.5187529264652215, 0.4868637770822246, 0.45366454112438226, 0.24018807641424078, 0.9549590760870335, 0.01, 0.02774713443297855, 0.16462709008309195, 0.04371597400051708, 0.99, 0.5029189718362286, 0.99, 0.8834050673337119, 0.5850903432149137, 0.025840500933174693, 0.6174951182767485, 0.5477992998624834]
[0.5187529264652215, 0.4868637770822246, 0.45366454112438226, 0.24018807641424078, 0.9549590760870335, 0.01, 0.02774713443297855, 0.16462709008309195, 0.04371597400051708, 0.99, 0.5029189718362286, 0.99, 0.8834050673337119, 0.5850903432149137, 0.025840500933174693, 0.6174951182767485, 0.5477992998624834]
Training loss = 0.02395627458890279
step = 0, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.69625
Training loss = 0.022051689624786378
step = 1, Training Accuracy: 0.72
Training loss = 0.02469633440176646
step = 2, Training Accuracy: 0.6833333333333333
Training loss = 0.02350954274336497
step = 3, Training Accuracy: 0.6633333333333333
Training loss = 0.022022114197413126
step = 4, Training Accuracy: 0.72
Training loss = 0.021042035520076753
step = 5, Training Accuracy: 0.69
Validation Accuracy: 0.71125
Training loss = 0.021110622286796568
step = 6, Training Accuracy: 0.6966666666666667
Training loss = 0.020380990505218508
step = 7, Training Accuracy: 0.7433333333333333
Training loss = 0.020258069733778635
step = 8, Training Accuracy: 0.7266666666666667
Training loss = 0.021239831149578094
step = 9, Training Accuracy: 0.6833333333333333
Training loss = 0.018583416839440665
step = 10, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.66875
Training loss = 0.02123363455136617
step = 11, Training Accuracy: 0.7666666666666667
Training loss = 0.019646348456541698
step = 12, Training Accuracy: 0.73
Training loss = 0.020604209701220195
step = 13, Training Accuracy: 0.7233333333333334
Training loss = 0.018495451907316843
step = 14, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.715
params:  [0.7793362803209612, 0.317314532966037, 0.4888135736759802, 0.4263645189292798, 0.30808620516799246, 0.4920660812302098, 0.4031965852230406, 0.07102824546415586, 0.01, 0.25812178508735467, 0.7151799851489142, 0.49217646823153904, 0.7659692049690137, 0.7312880280801056, 0.2657521617547922, 0.39706200260122115, 0.6329986060086182]
[0.7793362803209612, 0.317314532966037, 0.4888135736759802, 0.4263645189292798, 0.30808620516799246, 0.4920660812302098, 0.4031965852230406, 0.07102824546415586, 0.01, 0.25812178508735467, 0.7151799851489142, 0.49217646823153904, 0.7659692049690137, 0.7312880280801056, 0.2657521617547922, 0.39706200260122115, 0.6329986060086182]
Training loss = 0.02609600156545639
step = 0, Training Accuracy: 0.6433333333333333
Validation Accuracy: 0.7025
Training loss = 0.026088353395462036
step = 1, Training Accuracy: 0.6433333333333333
Training loss = 0.022077584366003673
step = 2, Training Accuracy: 0.7033333333333334
Training loss = 0.02325382242600123
step = 3, Training Accuracy: 0.7
Training loss = 0.02177714228630066
step = 4, Training Accuracy: 0.6933333333333334
Training loss = 0.024580117662747702
step = 5, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.7
Training loss = 0.021558148662249248
step = 6, Training Accuracy: 0.6933333333333334
Training loss = 0.02169406195481618
step = 7, Training Accuracy: 0.6733333333333333
Training loss = 0.022957003712654113
step = 8, Training Accuracy: 0.69
Training loss = 0.01831321527560552
step = 9, Training Accuracy: 0.7566666666666667
Training loss = 0.020770887931187948
step = 10, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.70625
Training loss = 0.019721393783887226
step = 11, Training Accuracy: 0.7533333333333333
Training loss = 0.02303567816813787
step = 12, Training Accuracy: 0.7133333333333334
Training loss = 0.02406214435895284
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.022205413083235422
step = 14, Training Accuracy: 0.66
Validation Accuracy: 0.71
params:  [0.01, 0.1796468561837201, 0.09171953957230763, 0.030722290068084912, 0.692201070566806, 0.48230094587378863, 0.9508136746870616, 0.31428655316483695, 0.01, 0.9091580639132735, 0.8283697367673291, 0.99, 0.6879348111668664, 0.04888414553109638, 0.6693710367281422, 0.99, 0.99]
[0.01, 0.1796468561837201, 0.09171953957230763, 0.030722290068084912, 0.692201070566806, 0.48230094587378863, 0.9508136746870616, 0.31428655316483695, 0.01, 0.9091580639132735, 0.8283697367673291, 0.99, 0.6879348111668664, 0.04888414553109638, 0.6693710367281422, 0.99, 0.99]
Training loss = 0.023832549055417377
step = 0, Training Accuracy: 0.6866666666666666
Validation Accuracy: 0.66625
Training loss = 0.020932072202364604
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.023510845104853312
step = 2, Training Accuracy: 0.72
Training loss = 0.020412272314230602
step = 3, Training Accuracy: 0.7166666666666667
Training loss = 0.02196980873743693
step = 4, Training Accuracy: 0.74
Training loss = 0.016633383830388388
step = 5, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.715
Training loss = 0.01640407274166743
step = 6, Training Accuracy: 0.7966666666666666
Training loss = 0.018119917511940004
step = 7, Training Accuracy: 0.7933333333333333
Training loss = 0.014835358063379923
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.017014678021272024
step = 9, Training Accuracy: 0.7833333333333333
Training loss = 0.016693448424339296
step = 10, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7425
Training loss = 0.01623581240574519
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.01503792275985082
step = 12, Training Accuracy: 0.7866666666666666
Training loss = 0.015180379052956898
step = 13, Training Accuracy: 0.78
Training loss = 0.01532879908879598
step = 14, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.73
params:  [0.024142177539771104, 0.15290122832102182, 0.0744781253813307, 0.3262369657108454, 0.21479859471795731, 0.24114849457309967, 0.44634262612780656, 0.08842795180518614, 0.27086743028914395, 0.5866203311857118, 0.789704535295171, 0.17531279007791456, 0.7613593818634131, 0.4416472763955545, 0.01, 0.39651034154217285, 0.99]
[0.024142177539771104, 0.15290122832102182, 0.0744781253813307, 0.3262369657108454, 0.21479859471795731, 0.24114849457309967, 0.44634262612780656, 0.08842795180518614, 0.27086743028914395, 0.5866203311857118, 0.789704535295171, 0.17531279007791456, 0.7613593818634131, 0.4416472763955545, 0.01, 0.39651034154217285, 0.99]
Training loss = 0.022657892008622487
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.67
Training loss = 0.01975697785615921
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.018158524930477142
step = 2, Training Accuracy: 0.7733333333333333
Training loss = 0.01654261221488317
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.01557286262512207
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.014542371233304341
step = 5, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.72125
Training loss = 0.014604939272006352
step = 6, Training Accuracy: 0.8
Training loss = 0.014301784535249074
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.0142083677649498
step = 8, Training Accuracy: 0.8033333333333333
Training loss = 0.013823353747526804
step = 9, Training Accuracy: 0.84
Training loss = 0.01232746551434199
step = 10, Training Accuracy: 0.85
Validation Accuracy: 0.72875
Training loss = 0.011000045090913773
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.011293944269418717
step = 12, Training Accuracy: 0.83
Training loss = 0.011481607009967169
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.010819519559542337
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7175
params:  [0.9327502803984857, 0.11213281519569368, 0.5085296258352715, 0.7394850491940268, 0.4706074642232619, 0.5520320064830867, 0.6902630324558923, 0.01, 0.01, 0.33096294930459497, 0.2830309883180891, 0.9740496716358522, 0.24861734905691013, 0.14169498352604987, 0.33736325021816144, 0.5810717796328889, 0.99]
[0.9327502803984857, 0.11213281519569368, 0.5085296258352715, 0.7394850491940268, 0.4706074642232619, 0.5520320064830867, 0.6902630324558923, 0.01, 0.01, 0.33096294930459497, 0.2830309883180891, 0.9740496716358522, 0.24861734905691013, 0.14169498352604987, 0.33736325021816144, 0.5810717796328889, 0.99]
Training loss = 0.02944212039311727
step = 0, Training Accuracy: 0.65
Validation Accuracy: 0.62
Training loss = 0.025892486174901325
step = 1, Training Accuracy: 0.6066666666666667
Training loss = 0.0233042307694753
step = 2, Training Accuracy: 0.68
Training loss = 0.023347652157147725
step = 3, Training Accuracy: 0.6466666666666666
Training loss = 0.023386043111483255
step = 4, Training Accuracy: 0.66
Training loss = 0.02166152497132619
step = 5, Training Accuracy: 0.6766666666666666
Validation Accuracy: 0.74125
Training loss = 0.02272607624530792
step = 6, Training Accuracy: 0.68
Training loss = 0.021407230099042256
step = 7, Training Accuracy: 0.7
Training loss = 0.019896441002686817
step = 8, Training Accuracy: 0.7433333333333333
Training loss = 0.020616030097007753
step = 9, Training Accuracy: 0.7133333333333334
Training loss = 0.02175065577030182
step = 10, Training Accuracy: 0.7
Validation Accuracy: 0.73
Training loss = 0.020559349159399668
step = 11, Training Accuracy: 0.74
Training loss = 0.020706131259600323
step = 12, Training Accuracy: 0.72
Training loss = 0.02091894527276357
step = 13, Training Accuracy: 0.7033333333333334
Training loss = 0.022047204673290254
step = 14, Training Accuracy: 0.71
Validation Accuracy: 0.7025
params:  [0.49681204824536385, 0.01, 0.18018456480704606, 0.05639407279099018, 0.01, 0.19922810674796226, 0.26125608339303447, 0.044690658231351904, 0.01, 0.99, 0.3865163657671106, 0.5730219069764837, 0.44726472536483464, 0.6783141268666755, 0.1579254433886898, 0.5459519832830775, 0.9135712616205305]
[0.49681204824536385, 0.01, 0.18018456480704606, 0.05639407279099018, 0.01, 0.19922810674796226, 0.26125608339303447, 0.044690658231351904, 0.01, 0.99, 0.3865163657671106, 0.5730219069764837, 0.44726472536483464, 0.6783141268666755, 0.1579254433886898, 0.5459519832830775, 0.9135712616205305]
Training loss = 0.02110391527414322
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.74
Training loss = 0.01861887554327647
step = 1, Training Accuracy: 0.7566666666666667
Training loss = 0.017315889497598012
step = 2, Training Accuracy: 0.77
Training loss = 0.017184928953647614
step = 3, Training Accuracy: 0.7866666666666666
Training loss = 0.01590506821870804
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.017689266602198283
step = 5, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.72375
Training loss = 0.015966175496578215
step = 6, Training Accuracy: 0.8033333333333333
Training loss = 0.014512187043825786
step = 7, Training Accuracy: 0.79
Training loss = 0.014982546269893647
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.014382448395093282
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.013150516798098881
step = 10, Training Accuracy: 0.85
Validation Accuracy: 0.75625
Training loss = 0.013967615266640981
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.012769581278165182
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.012824643353621165
step = 13, Training Accuracy: 0.83
Training loss = 0.012894269774357477
step = 14, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.73875
1  	8     	0.710625	0.0183073	0.6775 	0.73875
params:  [0.28501068853080025, 0.1573611931059347, 0.17124292471983052, 0.01, 0.01, 0.01, 0.7260680068117814, 0.012155126641506447, 0.4457434261190108, 0.99, 0.4557966711371906, 0.551737316441054, 0.5624219270324122, 0.13464663346657035, 0.28481112807909953, 0.8908345977030647, 0.4956984921403912]
[0.28501068853080025, 0.1573611931059347, 0.17124292471983052, 0.01, 0.01, 0.01, 0.7260680068117814, 0.012155126641506447, 0.4457434261190108, 0.99, 0.4557966711371906, 0.551737316441054, 0.5624219270324122, 0.13464663346657035, 0.28481112807909953, 0.8908345977030647, 0.4956984921403912]
Training loss = 0.021646446188290914
step = 0, Training Accuracy: 0.73
Validation Accuracy: 0.7
Training loss = 0.02008980353673299
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.017312350074450176
step = 2, Training Accuracy: 0.79
Training loss = 0.01546869546175003
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.018311887880166373
step = 4, Training Accuracy: 0.7766666666666666
Training loss = 0.018317170043786368
step = 5, Training Accuracy: 0.73
Validation Accuracy: 0.71125
Training loss = 0.014913566807905832
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.015262588063875834
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.016006666123867034
step = 8, Training Accuracy: 0.8
Training loss = 0.012950594623883566
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.01322978973388672
step = 10, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.72
Training loss = 0.014042167365550995
step = 11, Training Accuracy: 0.83
Training loss = 0.010285787085692088
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.011917832990487416
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.01058064063390096
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.73125
params:  [0.01, 0.01, 0.41067451323149706, 0.37489680674745585, 0.020635616511232857, 0.1613587801368307, 0.32278213440419384, 0.5880057988866487, 0.06819007822782627, 0.99, 0.37202365709418905, 0.6105508046579664, 0.3066865142082265, 0.3700472922638206, 0.7027345664258066, 0.1335467911706486, 0.3617699026093062]
[0.01, 0.01, 0.41067451323149706, 0.37489680674745585, 0.020635616511232857, 0.1613587801368307, 0.32278213440419384, 0.5880057988866487, 0.06819007822782627, 0.99, 0.37202365709418905, 0.6105508046579664, 0.3066865142082265, 0.3700472922638206, 0.7027345664258066, 0.1335467911706486, 0.3617699026093062]
Training loss = 0.021832185486952465
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.71125
Training loss = 0.01660207410653432
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.014947679936885833
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.01557456076145172
step = 3, Training Accuracy: 0.81
Training loss = 0.012920318245887756
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.013862884491682053
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.72875
Training loss = 0.016171548316876092
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.013975416421890258
step = 7, Training Accuracy: 0.82
Training loss = 0.013294454663991928
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.012993772675593694
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011472247143586477
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.7175
Training loss = 0.008988580902417501
step = 11, Training Accuracy: 0.89
Training loss = 0.01027579536040624
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.0107496311267217
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.00933263160288334
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.69125
params:  [0.17701705707737742, 0.01, 0.04157435838807287, 0.01, 0.4327610606539718, 0.6870082603731711, 0.47381592988914756, 0.01, 0.01, 0.5135156830814545, 0.5239807788227688, 0.01, 0.5702013037880164, 0.7296241406341049, 0.24824371166689985, 0.7068670747082983, 0.99]
[0.17701705707737742, 0.01, 0.04157435838807287, 0.01, 0.4327610606539718, 0.6870082603731711, 0.47381592988914756, 0.01, 0.01, 0.5135156830814545, 0.5239807788227688, 0.01, 0.5702013037880164, 0.7296241406341049, 0.24824371166689985, 0.7068670747082983, 0.99]
Training loss = 0.02560307115316391
step = 0, Training Accuracy: 0.7
Validation Accuracy: 0.7325
Training loss = 0.022289968927701315
step = 1, Training Accuracy: 0.67
Training loss = 0.02086784243583679
step = 2, Training Accuracy: 0.72
Training loss = 0.019010553459326427
step = 3, Training Accuracy: 0.74
Training loss = 0.020662038226922353
step = 4, Training Accuracy: 0.75
Training loss = 0.017658464908599854
step = 5, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.7425
Training loss = 0.01879514773686727
step = 6, Training Accuracy: 0.7233333333333334
Training loss = 0.018261438608169554
step = 7, Training Accuracy: 0.7566666666666667
Training loss = 0.019215695758660636
step = 8, Training Accuracy: 0.73
Training loss = 0.01739639699459076
step = 9, Training Accuracy: 0.7433333333333333
Training loss = 0.017520819505055744
step = 10, Training Accuracy: 0.78
Validation Accuracy: 0.705
Training loss = 0.017564546565214792
step = 11, Training Accuracy: 0.7733333333333333
Training loss = 0.01689498523871104
step = 12, Training Accuracy: 0.7666666666666667
Training loss = 0.015907770494620006
step = 13, Training Accuracy: 0.77
Training loss = 0.01732822944720586
step = 14, Training Accuracy: 0.76
Validation Accuracy: 0.7075
params:  [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.6935931744576941, 0.01, 0.03912131168598895, 0.99, 0.3014958579048658, 0.5907389616848585, 0.6693814939241983, 0.99, 0.5273514695734263, 0.6066224919603553, 0.397551748639387]
[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.6935931744576941, 0.01, 0.03912131168598895, 0.99, 0.3014958579048658, 0.5907389616848585, 0.6693814939241983, 0.99, 0.5273514695734263, 0.6066224919603553, 0.397551748639387]
Training loss = 0.02431468536456426
step = 0, Training Accuracy: 0.7266666666666667
Validation Accuracy: 0.72625
Training loss = 0.0204752908150355
step = 1, Training Accuracy: 0.76
Training loss = 0.017957321802775063
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.01720963418483734
step = 3, Training Accuracy: 0.77
Training loss = 0.017044829030831654
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.016085801819960277
step = 5, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7375
Training loss = 0.014146257936954499
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.014471114575862885
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.013659496903419494
step = 8, Training Accuracy: 0.8233333333333334
Training loss = 0.01414829323689143
step = 9, Training Accuracy: 0.8266666666666667
Training loss = 0.01458103487888972
step = 10, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7425
Training loss = 0.013826109667619069
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.010968560526768367
step = 12, Training Accuracy: 0.85
Training loss = 0.012171245614687602
step = 13, Training Accuracy: 0.85
Training loss = 0.011361034115155539
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.73
params:  [0.01, 0.01, 0.01, 0.12273561481251644, 0.24010710857822415, 0.7225428738158554, 0.5759350791072628, 0.01, 0.01, 0.99, 0.5003927554622375, 0.2860286817596347, 0.347770841394104, 0.479087966815996, 0.540085200195116, 0.5166845793728387, 0.99]
[0.01, 0.01, 0.01, 0.12273561481251644, 0.24010710857822415, 0.7225428738158554, 0.5759350791072628, 0.01, 0.01, 0.99, 0.5003927554622375, 0.2860286817596347, 0.347770841394104, 0.479087966815996, 0.540085200195116, 0.5166845793728387, 0.99]
Training loss = 0.030772124727567036
step = 0, Training Accuracy: 0.6633333333333333
Validation Accuracy: 0.67625
Training loss = 0.023527358869711558
step = 1, Training Accuracy: 0.6833333333333333
Training loss = 0.020615048309167227
step = 2, Training Accuracy: 0.71
Training loss = 0.020998173256715137
step = 3, Training Accuracy: 0.7466666666666667
Training loss = 0.01697329560915629
step = 4, Training Accuracy: 0.7666666666666667
Training loss = 0.018513655265172322
step = 5, Training Accuracy: 0.78
Validation Accuracy: 0.69625
Training loss = 0.018491873840490978
step = 6, Training Accuracy: 0.7466666666666667
Training loss = 0.016414292752742768
step = 7, Training Accuracy: 0.75
Training loss = 0.017500744064648945
step = 8, Training Accuracy: 0.78
Training loss = 0.01754330555597941
step = 9, Training Accuracy: 0.77
Training loss = 0.01667569428682327
step = 10, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.72625
Training loss = 0.013304660618305207
step = 11, Training Accuracy: 0.83
Training loss = 0.014848740299542746
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.014031882087389628
step = 13, Training Accuracy: 0.81
Training loss = 0.014303966661294302
step = 14, Training Accuracy: 0.8
Validation Accuracy: 0.71625
params:  [0.01, 0.01, 0.03525660368933686, 0.52091897813961, 0.01, 0.17692867654654743, 0.05289979189239025, 0.7783043139138504, 0.5056982239093721, 0.99, 0.455631180315231, 0.43417832114598887, 0.6089969294296688, 0.99, 0.3362679064129178, 0.14544611412555075, 0.8212265428525947]
[0.01, 0.01, 0.03525660368933686, 0.52091897813961, 0.01, 0.17692867654654743, 0.05289979189239025, 0.7783043139138504, 0.5056982239093721, 0.99, 0.455631180315231, 0.43417832114598887, 0.6089969294296688, 0.99, 0.3362679064129178, 0.14544611412555075, 0.8212265428525947]
Training loss = 0.021411916613578795
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.70375
Training loss = 0.017420200804869335
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015296509265899658
step = 2, Training Accuracy: 0.82
Training loss = 0.014792029857635497
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.015136637638012568
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.013296035230159759
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.73
Training loss = 0.014818241695562999
step = 6, Training Accuracy: 0.83
Training loss = 0.011967683906356493
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.010932733714580535
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.0114335697889328
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.012523260613282522
step = 10, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.7425
Training loss = 0.010210059980551401
step = 11, Training Accuracy: 0.9
Training loss = 0.01044336348772049
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.009403853515783945
step = 13, Training Accuracy: 0.88
Training loss = 0.009228348806500435
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7225
params:  [0.7659057774359898, 0.10943439504352465, 0.20167220994490023, 0.01683426543207156, 0.01, 0.425886539584365, 0.5443354773272255, 0.23069949843945992, 0.3053472430355989, 0.99, 0.5161427954687994, 0.48076980374785877, 0.6772636443097992, 0.05643783582967998, 0.01, 0.9436844189811017, 0.4480292950429689]
[0.7659057774359898, 0.10943439504352465, 0.20167220994490023, 0.01683426543207156, 0.01, 0.425886539584365, 0.5443354773272255, 0.23069949843945992, 0.3053472430355989, 0.99, 0.5161427954687994, 0.48076980374785877, 0.6772636443097992, 0.05643783582967998, 0.01, 0.9436844189811017, 0.4480292950429689]
Training loss = 0.026551264425118765
step = 0, Training Accuracy: 0.71
Validation Accuracy: 0.7175
Training loss = 0.022754890322685243
step = 1, Training Accuracy: 0.7433333333333333
Training loss = 0.01926823268334071
step = 2, Training Accuracy: 0.7466666666666667
Training loss = 0.017469180425008137
step = 3, Training Accuracy: 0.75
Training loss = 0.01780651479959488
step = 4, Training Accuracy: 0.7833333333333333
Training loss = 0.016934942305088043
step = 5, Training Accuracy: 0.78
Validation Accuracy: 0.74375
Training loss = 0.01530767858028412
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.0186498232682546
step = 7, Training Accuracy: 0.7466666666666667
Training loss = 0.01650299866994222
step = 8, Training Accuracy: 0.7933333333333333
Training loss = 0.01539545198281606
step = 9, Training Accuracy: 0.8033333333333333
Training loss = 0.015728583534558613
step = 10, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7225
Training loss = 0.013367201586564383
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.014998111526171367
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.013820196290810903
step = 13, Training Accuracy: 0.8233333333333334
Training loss = 0.013396586577097575
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.73
params:  [0.01, 0.3449283768463304, 0.33703503190131234, 0.3207035461354991, 0.11364565682504504, 0.12464260923051085, 0.17845552821511834, 0.5539094587259021, 0.466739723909716, 0.99, 0.8028725326849071, 0.42494255301618233, 0.7549652006274861, 0.4917785600729172, 0.12740765469695162, 0.2842739643805966, 0.9786588342717525]
[0.01, 0.3449283768463304, 0.33703503190131234, 0.3207035461354991, 0.11364565682504504, 0.12464260923051085, 0.17845552821511834, 0.5539094587259021, 0.466739723909716, 0.99, 0.8028725326849071, 0.42494255301618233, 0.7549652006274861, 0.4917785600729172, 0.12740765469695162, 0.2842739643805966, 0.9786588342717525]
Training loss = 0.022994119922320047
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.72875
Training loss = 0.019539806346098584
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01720302104949951
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.01716943194468816
step = 3, Training Accuracy: 0.79
Training loss = 0.01626210888226827
step = 4, Training Accuracy: 0.7566666666666667
Training loss = 0.0142708553870519
step = 5, Training Accuracy: 0.81
Validation Accuracy: 0.73875
Training loss = 0.014355512460072835
step = 6, Training Accuracy: 0.83
Training loss = 0.012592860162258149
step = 7, Training Accuracy: 0.84
Training loss = 0.01466317594051361
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.01338077113032341
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.0135133791466554
step = 10, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.71375
Training loss = 0.013378442525863647
step = 11, Training Accuracy: 0.8066666666666666
Training loss = 0.011664550751447678
step = 12, Training Accuracy: 0.8633333333333333
Training loss = 0.012124329408009848
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.01233336200316747
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.72
2  	8     	0.718594	0.0128154	0.69125	0.73125
params:  [0.4489530012600833, 0.08282225254977763, 0.01, 0.01, 0.33395481388099557, 0.01, 0.8580759075531277, 0.01, 0.7055549686384455, 0.3895848488586632, 0.5367413702441853, 0.3261070597073281, 0.6646092382702374, 0.01, 0.7185699233738038, 0.8520835525726637, 0.5189902041860163]
[0.4489530012600833, 0.08282225254977763, 0.01, 0.01, 0.33395481388099557, 0.01, 0.8580759075531277, 0.01, 0.7055549686384455, 0.3895848488586632, 0.5367413702441853, 0.3261070597073281, 0.6646092382702374, 0.01, 0.7185699233738038, 0.8520835525726637, 0.5189902041860163]
Training loss = 0.017775195340315502
step = 0, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.67125
Training loss = 0.015949486295382183
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.013505983750025432
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.014446222484111785
step = 3, Training Accuracy: 0.82
Training loss = 0.014063685834407806
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.011702191879351934
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.72125
Training loss = 0.011385136147340139
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.010847844431797664
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.011369689504305521
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.010878184934457143
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.011335575580596923
step = 10, Training Accuracy: 0.85
Validation Accuracy: 0.72375
Training loss = 0.01021333542962869
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.012213855385780334
step = 12, Training Accuracy: 0.86
Training loss = 0.008052719359596571
step = 13, Training Accuracy: 0.9066666666666666
Training loss = 0.007890902956326802
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.72875
params:  [0.01, 0.01, 0.5976959822713457, 0.6360078982711466, 0.14332690441338897, 0.03760792745897874, 0.7838331534995678, 0.3958286770406223, 0.6492266029352183, 0.5931517503428367, 0.4800325744959551, 0.44788070135908303, 0.41345075465959147, 0.23179294363947145, 0.6180434400921367, 0.5342174511486121, 0.38125245109154454]
[0.01, 0.01, 0.5976959822713457, 0.6360078982711466, 0.14332690441338897, 0.03760792745897874, 0.7838331534995678, 0.3958286770406223, 0.6492266029352183, 0.5931517503428367, 0.4800325744959551, 0.44788070135908303, 0.41345075465959147, 0.23179294363947145, 0.6180434400921367, 0.5342174511486121, 0.38125245109154454]
Training loss = 0.023137407700220745
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.725
Training loss = 0.01535036305586497
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.014965919057528178
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.011717191437880197
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.014730360507965088
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.014654902269442877
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.70625
Training loss = 0.012401226113239924
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.011767196704943975
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.011890818824370703
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.012024918397267659
step = 9, Training Accuracy: 0.88
Training loss = 0.010889630268017451
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.69125
Training loss = 0.011470934400955837
step = 11, Training Accuracy: 0.8333333333333334
Training loss = 0.010938649028539658
step = 12, Training Accuracy: 0.8633333333333333
Training loss = 0.008338546256224315
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.00827658861875534
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.72
params:  [0.2358578749302448, 0.01, 0.01, 0.01, 0.01, 0.0443253899679338, 0.6805307290435819, 0.16783193128845847, 0.10116551884524841, 0.99, 0.548670723495118, 0.714736356766362, 0.9898547270637876, 0.30838893514575605, 0.01, 0.523610916330153, 0.01]
[0.2358578749302448, 0.01, 0.01, 0.01, 0.01, 0.0443253899679338, 0.6805307290435819, 0.16783193128845847, 0.10116551884524841, 0.99, 0.548670723495118, 0.714736356766362, 0.9898547270637876, 0.30838893514575605, 0.01, 0.523610916330153, 0.01]
Training loss = 0.02903483510017395
step = 0, Training Accuracy: 0.6933333333333334
Validation Accuracy: 0.69125
Training loss = 0.0210793932278951
step = 1, Training Accuracy: 0.7333333333333333
Training loss = 0.017830896427234014
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.014715423981348674
step = 3, Training Accuracy: 0.8
Training loss = 0.015279004375139873
step = 4, Training Accuracy: 0.7866666666666666
Training loss = 0.014258269766966501
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.75125
Training loss = 0.012229634275039036
step = 6, Training Accuracy: 0.85
Training loss = 0.013168776830037435
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.010998639464378356
step = 8, Training Accuracy: 0.86
Training loss = 0.01349195271730423
step = 9, Training Accuracy: 0.82
Training loss = 0.010828505406777064
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.71875
Training loss = 0.01192306896050771
step = 11, Training Accuracy: 0.87
Training loss = 0.010389607151349385
step = 12, Training Accuracy: 0.88
Training loss = 0.00929702510436376
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.010081768830617269
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.7275
params:  [0.42860159287498434, 0.017354828707500963, 0.012248271714699527, 0.01, 0.1597343470914002, 0.5130770604265025, 0.7347032997657826, 0.01, 0.01, 0.99, 0.23188246475989535, 0.12113902816494576, 0.7483996598789249, 0.42543888308855554, 0.22829580366415458, 0.6760520565399819, 0.5001723323859213]
[0.42860159287498434, 0.017354828707500963, 0.012248271714699527, 0.01, 0.1597343470914002, 0.5130770604265025, 0.7347032997657826, 0.01, 0.01, 0.99, 0.23188246475989535, 0.12113902816494576, 0.7483996598789249, 0.42543888308855554, 0.22829580366415458, 0.6760520565399819, 0.5001723323859213]
Training loss = 0.028164630730946858
step = 0, Training Accuracy: 0.72
Validation Accuracy: 0.73375
Training loss = 0.01880567933122317
step = 1, Training Accuracy: 0.7466666666666667
Training loss = 0.018528042038281758
step = 2, Training Accuracy: 0.7833333333333333
Training loss = 0.015929463009039562
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01717342754205068
step = 4, Training Accuracy: 0.7633333333333333
Training loss = 0.01649788647890091
step = 5, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.72625
Training loss = 0.015243575274944306
step = 6, Training Accuracy: 0.83
Training loss = 0.014295848508675893
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.014287883043289184
step = 8, Training Accuracy: 0.79
Training loss = 0.013265172044436137
step = 9, Training Accuracy: 0.8233333333333334
Training loss = 0.013053432926535607
step = 10, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.7175
Training loss = 0.014806363433599472
step = 11, Training Accuracy: 0.8133333333333334
Training loss = 0.014303174714247385
step = 12, Training Accuracy: 0.86
Training loss = 0.012579963207244874
step = 13, Training Accuracy: 0.83
Training loss = 0.012346932093302409
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.71
params:  [0.3322500575281209, 0.01, 0.1710254689872515, 0.01, 0.18739989875585, 0.6453865790265444, 0.17273704389350925, 0.3265112722094544, 0.38186896897875716, 0.6474301173562678, 0.18579564337253915, 0.45003742102056477, 0.16900358096231144, 0.44969833694462924, 0.5376411349044573, 0.4829476730703824, 0.4510698916258409]
[0.3322500575281209, 0.01, 0.1710254689872515, 0.01, 0.18739989875585, 0.6453865790265444, 0.17273704389350925, 0.3265112722094544, 0.38186896897875716, 0.6474301173562678, 0.18579564337253915, 0.45003742102056477, 0.16900358096231144, 0.44969833694462924, 0.5376411349044573, 0.4829476730703824, 0.4510698916258409]
Training loss = 0.021704024175802868
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.72875
Training loss = 0.019918580551942188
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.016288554072380067
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.015778326590855915
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.018037423193454742
step = 4, Training Accuracy: 0.77
Training loss = 0.014001578589280446
step = 5, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7125
Training loss = 0.014233278532822927
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.01385164737701416
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.013201564947764078
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.012150163948535918
step = 9, Training Accuracy: 0.81
Training loss = 0.013601631621519724
step = 10, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7425
Training loss = 0.01346719205379486
step = 11, Training Accuracy: 0.85
Training loss = 0.011791580021381379
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.011495383928219477
step = 13, Training Accuracy: 0.86
Training loss = 0.011649514883756637
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.72
params:  [0.21880329086395522, 0.15514862069462906, 0.05301440177576372, 0.01, 0.01, 0.24108854815399816, 0.4775611101556524, 0.08112909560503409, 0.45340321661195454, 0.99, 0.32622456088965784, 0.43036564994621246, 0.3910798553622282, 0.29684073696589397, 0.4174865899962483, 0.9363969986693033, 0.5487513716878575]
[0.21880329086395522, 0.15514862069462906, 0.05301440177576372, 0.01, 0.01, 0.24108854815399816, 0.4775611101556524, 0.08112909560503409, 0.45340321661195454, 0.99, 0.32622456088965784, 0.43036564994621246, 0.3910798553622282, 0.29684073696589397, 0.4174865899962483, 0.9363969986693033, 0.5487513716878575]
Training loss = 0.018456734990080197
step = 0, Training Accuracy: 0.7633333333333333
Validation Accuracy: 0.715
Training loss = 0.014387818078200023
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.012611310084660848
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.012145025829474132
step = 3, Training Accuracy: 0.84
Training loss = 0.012847668925921122
step = 4, Training Accuracy: 0.84
Training loss = 0.01145950292547544
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.75
Training loss = 0.011043871541817983
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.010316851983467738
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.009888585209846496
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.008388316929340363
step = 9, Training Accuracy: 0.91
Training loss = 0.009974312136570613
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.7425
Training loss = 0.008014517873525619
step = 11, Training Accuracy: 0.92
Training loss = 0.006975091000398
step = 12, Training Accuracy: 0.9233333333333333
Training loss = 0.009069668302933376
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.01009459063410759
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.69375
params:  [0.5608982547578252, 0.29845328261338255, 0.2753750514803863, 0.01, 0.01, 0.3565345185115708, 0.5216443998383097, 0.3343113143315809, 0.6049961598558428, 0.99, 0.43460642385930937, 0.8864482865615555, 0.8169825433152567, 0.4345591105668367, 0.21011396997904358, 0.7729459187223068, 0.04859162650945792]
[0.5608982547578252, 0.29845328261338255, 0.2753750514803863, 0.01, 0.01, 0.3565345185115708, 0.5216443998383097, 0.3343113143315809, 0.6049961598558428, 0.99, 0.43460642385930937, 0.8864482865615555, 0.8169825433152567, 0.4345591105668367, 0.21011396997904358, 0.7729459187223068, 0.04859162650945792]
Training loss = 0.0219763986269633
step = 0, Training Accuracy: 0.7133333333333334
Validation Accuracy: 0.7375
Training loss = 0.017215862522522607
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.01605919380982717
step = 2, Training Accuracy: 0.7666666666666667
Training loss = 0.013718183437983195
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.01360805074373881
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.013413180311520894
step = 5, Training Accuracy: 0.84
Validation Accuracy: 0.735
Training loss = 0.011409056186676026
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.013419890403747558
step = 7, Training Accuracy: 0.8233333333333334
Training loss = 0.012592517137527466
step = 8, Training Accuracy: 0.84
Training loss = 0.01156592716773351
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.011548425753911335
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74
Training loss = 0.01286698857943217
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.011529309252897899
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.009878528316815694
step = 13, Training Accuracy: 0.8733333333333333
Training loss = 0.01069570228457451
step = 14, Training Accuracy: 0.85
Validation Accuracy: 0.73125
params:  [0.30617810018575653, 0.09970627380582532, 0.01, 0.01, 0.42117858819114357, 0.01, 0.99, 0.01, 0.20785010349670305, 0.99, 0.2139174081990075, 0.39012418597496257, 0.35524921023315875, 0.8383090011150192, 0.5445737304898444, 0.99, 0.32344340495371643]
[0.30617810018575653, 0.09970627380582532, 0.01, 0.01, 0.42117858819114357, 0.01, 0.99, 0.01, 0.20785010349670305, 0.99, 0.2139174081990075, 0.39012418597496257, 0.35524921023315875, 0.8383090011150192, 0.5445737304898444, 0.99, 0.32344340495371643]
Training loss = 0.03163730462392171
step = 0, Training Accuracy: 0.6066666666666667
Validation Accuracy: 0.7075
Training loss = 0.02311890761057536
step = 1, Training Accuracy: 0.6566666666666666
Training loss = 0.019261221488316854
step = 2, Training Accuracy: 0.7433333333333333
Training loss = 0.020639622906843822
step = 3, Training Accuracy: 0.7133333333333334
Training loss = 0.021667697429656983
step = 4, Training Accuracy: 0.6766666666666666
Training loss = 0.018836159507433572
step = 5, Training Accuracy: 0.74
Validation Accuracy: 0.73375
Training loss = 0.019210145175457
step = 6, Training Accuracy: 0.7366666666666667
Training loss = 0.019928450485070547
step = 7, Training Accuracy: 0.7166666666666667
Training loss = 0.017050969898700714
step = 8, Training Accuracy: 0.7566666666666667
Training loss = 0.017341514726479847
step = 9, Training Accuracy: 0.7466666666666667
Training loss = 0.01734240392843882
step = 10, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.73625
Training loss = 0.017678883870442707
step = 11, Training Accuracy: 0.7333333333333333
Training loss = 0.01852016419172287
step = 12, Training Accuracy: 0.7433333333333333
Training loss = 0.01953619142373403
step = 13, Training Accuracy: 0.73
Training loss = 0.01680348962545395
step = 14, Training Accuracy: 0.79
Validation Accuracy: 0.7225
3  	8     	0.719219	0.0114554	0.69375	0.73125
params:  [0.468594792045264, 0.5046832942333992, 0.30374204708740177, 0.01, 0.5780840677481669, 0.505662423749653, 0.4628161748730003, 0.01, 0.41985627257406355, 0.9781935715952892, 0.99, 0.7355456232522358, 0.4385532856764973, 0.12207281019923039, 0.754684335211905, 0.864139258359142, 0.26446702608568895]
[0.468594792045264, 0.5046832942333992, 0.30374204708740177, 0.01, 0.5780840677481669, 0.505662423749653, 0.4628161748730003, 0.01, 0.41985627257406355, 0.9781935715952892, 0.99, 0.7355456232522358, 0.4385532856764973, 0.12207281019923039, 0.754684335211905, 0.864139258359142, 0.26446702608568895]
Training loss = 0.023762122293313346
step = 0, Training Accuracy: 0.6966666666666667
Validation Accuracy: 0.70375
Training loss = 0.02015116314093272
step = 1, Training Accuracy: 0.75
Training loss = 0.020429350038369497
step = 2, Training Accuracy: 0.7366666666666667
Training loss = 0.018562302688757578
step = 3, Training Accuracy: 0.7733333333333333
Training loss = 0.019739454487959544
step = 4, Training Accuracy: 0.7266666666666667
Training loss = 0.018442900975545247
step = 5, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.71375
Training loss = 0.01618549237648646
step = 6, Training Accuracy: 0.7733333333333333
Training loss = 0.017664572497208913
step = 7, Training Accuracy: 0.77
Training loss = 0.017714209457238516
step = 8, Training Accuracy: 0.77
Training loss = 0.016485114296277362
step = 9, Training Accuracy: 0.81
Training loss = 0.013829106986522675
step = 10, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.70375
Training loss = 0.015000237077474594
step = 11, Training Accuracy: 0.7833333333333333
Training loss = 0.013914844493071237
step = 12, Training Accuracy: 0.82
Training loss = 0.01549707055091858
step = 13, Training Accuracy: 0.8
Training loss = 0.014882571796576182
step = 14, Training Accuracy: 0.83
Validation Accuracy: 0.7175
params:  [0.6698687845031343, 0.01, 0.01, 0.1297427642541906, 0.34535322607503977, 0.5596571098123795, 0.13913779375235813, 0.01, 0.47351170511799934, 0.843454440028627, 0.5997322413465673, 0.4853968248908793, 0.99, 0.4283403069058491, 0.7041510500364379, 0.5449594311147166, 0.01]
[0.6698687845031343, 0.01, 0.01, 0.1297427642541906, 0.34535322607503977, 0.5596571098123795, 0.13913779375235813, 0.01, 0.47351170511799934, 0.843454440028627, 0.5997322413465673, 0.4853968248908793, 0.99, 0.4283403069058491, 0.7041510500364379, 0.5449594311147166, 0.01]
Training loss = 0.028276652097702026
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.70875
Training loss = 0.021751893361409504
step = 1, Training Accuracy: 0.7166666666666667
Training loss = 0.0199705171585083
step = 2, Training Accuracy: 0.71
Training loss = 0.019313063323497772
step = 3, Training Accuracy: 0.7633333333333333
Training loss = 0.01861924320459366
step = 4, Training Accuracy: 0.74
Training loss = 0.018563097914059957
step = 5, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.74375
Training loss = 0.016643020510673522
step = 6, Training Accuracy: 0.79
Training loss = 0.016871091922124225
step = 7, Training Accuracy: 0.79
Training loss = 0.016174191832542418
step = 8, Training Accuracy: 0.8
Training loss = 0.016968264679114022
step = 9, Training Accuracy: 0.7366666666666667
Training loss = 0.01503081371386846
step = 10, Training Accuracy: 0.77
Validation Accuracy: 0.7175
Training loss = 0.014881941477457682
step = 11, Training Accuracy: 0.78
Training loss = 0.014843635360399883
step = 12, Training Accuracy: 0.79
Training loss = 0.017777049938837688
step = 13, Training Accuracy: 0.77
Training loss = 0.014257173935572307
step = 14, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.71625
params:  [0.14625196494132087, 0.5032548517698864, 0.4080654700379964, 0.01, 0.01, 0.01, 0.6022392145448714, 0.19633932267652174, 0.2672408789876036, 0.6596313871183199, 0.8567935791631331, 0.42202619049379864, 0.7531505779352229, 0.2650692261624736, 0.37448911937873963, 0.3301967139440643, 0.4943758160709836]
[0.14625196494132087, 0.5032548517698864, 0.4080654700379964, 0.01, 0.01, 0.01, 0.6022392145448714, 0.19633932267652174, 0.2672408789876036, 0.6596313871183199, 0.8567935791631331, 0.42202619049379864, 0.7531505779352229, 0.2650692261624736, 0.37448911937873963, 0.3301967139440643, 0.4943758160709836]
Training loss = 0.019512315392494203
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.715
Training loss = 0.016433381338914234
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.015720864335695903
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.013027084221442541
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.013223498264948527
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.011954667369524637
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.74625
Training loss = 0.011035259465376536
step = 6, Training Accuracy: 0.86
Training loss = 0.011040190507968266
step = 7, Training Accuracy: 0.87
Training loss = 0.009632763266563416
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.011583319952090582
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.009194910004734992
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.73625
Training loss = 0.010599277814229329
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.008559363981088003
step = 12, Training Accuracy: 0.9033333333333333
Training loss = 0.011395398726065953
step = 13, Training Accuracy: 0.87
Training loss = 0.00900874545176824
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.73
params:  [0.8945937401626015, 0.05817087283117156, 0.01, 0.18550452268250203, 0.2435501603667101, 0.267370084084056, 0.9819509532245971, 0.01, 0.3584045297886491, 0.6374204829610324, 0.44857287317587163, 0.7596994361972857, 0.6111325767243831, 0.01, 0.206888277304796, 0.4960615687194449, 0.01]
[0.8945937401626015, 0.05817087283117156, 0.01, 0.18550452268250203, 0.2435501603667101, 0.267370084084056, 0.9819509532245971, 0.01, 0.3584045297886491, 0.6374204829610324, 0.44857287317587163, 0.7596994361972857, 0.6111325767243831, 0.01, 0.206888277304796, 0.4960615687194449, 0.01]
Training loss = 0.023008164862791697
step = 0, Training Accuracy: 0.7333333333333333
Validation Accuracy: 0.70875
Training loss = 0.016427747706572213
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.013976288040479025
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.015202444295088451
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.012766759296258291
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.012632933954397838
step = 5, Training Accuracy: 0.83
Validation Accuracy: 0.73875
Training loss = 0.011128731270631154
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.010624822576840718
step = 7, Training Accuracy: 0.88
Training loss = 0.010104540884494782
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.009530238956212998
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.008246620893478393
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.72625
Training loss = 0.008313066984216372
step = 11, Training Accuracy: 0.8966666666666666
Training loss = 0.008626908585429192
step = 12, Training Accuracy: 0.9
Training loss = 0.007429364696145057
step = 13, Training Accuracy: 0.91
Training loss = 0.009480352848768234
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.70625
params:  [0.368890108728299, 0.11214747358845659, 0.555349662613483, 0.11065797707198476, 0.01, 0.044068168944133485, 0.5485309261733076, 0.2253229864099115, 0.17909889573767618, 0.8594160303041477, 0.24140151264213722, 0.954193976419009, 0.6026185325289066, 0.1777073289290025, 0.4395094986158252, 0.695711746598328, 0.01]
[0.368890108728299, 0.11214747358845659, 0.555349662613483, 0.11065797707198476, 0.01, 0.044068168944133485, 0.5485309261733076, 0.2253229864099115, 0.17909889573767618, 0.8594160303041477, 0.24140151264213722, 0.954193976419009, 0.6026185325289066, 0.1777073289290025, 0.4395094986158252, 0.695711746598328, 0.01]
Training loss = 0.020701412508885065
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.72375
Training loss = 0.015819133669137956
step = 1, Training Accuracy: 0.8033333333333333
Training loss = 0.01472037822008133
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.012976318299770355
step = 3, Training Accuracy: 0.86
Training loss = 0.012908772925535838
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.012709535111983618
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.74
Training loss = 0.01111216959853967
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.010866968681414922
step = 7, Training Accuracy: 0.86
Training loss = 0.011497493013739585
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.009627884874741236
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.010080437113841375
step = 10, Training Accuracy: 0.88
Validation Accuracy: 0.73375
Training loss = 0.011129611680905024
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.00903961514433225
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.009938914080460865
step = 13, Training Accuracy: 0.9
Training loss = 0.00837498222788175
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.73625
params:  [0.29027875593066665, 0.01, 0.01205466452509743, 0.244771416507572, 0.04310002708655708, 0.1657652303940872, 0.5066541968646545, 0.427263317216114, 0.2440402642248935, 0.7115304802601065, 0.5391273310149327, 0.6598418221125871, 0.2482012878154919, 0.6244567261585869, 0.4076151174694851, 0.674653504695401, 0.1676355812921351]
[0.29027875593066665, 0.01, 0.01205466452509743, 0.244771416507572, 0.04310002708655708, 0.1657652303940872, 0.5066541968646545, 0.427263317216114, 0.2440402642248935, 0.7115304802601065, 0.5391273310149327, 0.6598418221125871, 0.2482012878154919, 0.6244567261585869, 0.4076151174694851, 0.674653504695401, 0.1676355812921351]
Training loss = 0.024851499199867247
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.73625
Training loss = 0.019155592024326325
step = 1, Training Accuracy: 0.7733333333333333
Training loss = 0.01909819314877192
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.014491219719250997
step = 3, Training Accuracy: 0.85
Training loss = 0.014369127800067267
step = 4, Training Accuracy: 0.81
Training loss = 0.01410651445388794
step = 5, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.71625
Training loss = 0.014476799070835114
step = 6, Training Accuracy: 0.81
Training loss = 0.012820779631535212
step = 7, Training Accuracy: 0.8266666666666667
Training loss = 0.012634704808394114
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.012343740711609522
step = 9, Training Accuracy: 0.83
Training loss = 0.01305292546749115
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7125
Training loss = 0.012648940831422806
step = 11, Training Accuracy: 0.83
Training loss = 0.010574037929375966
step = 12, Training Accuracy: 0.88
Training loss = 0.010402135998010636
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.010290358612934748
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.72625
params:  [0.1461758239217677, 0.47589382081209974, 0.026675529698425002, 0.01, 0.01, 0.01, 0.2690149232698963, 0.01, 0.7576680196547064, 0.8281237764613696, 0.4873908413523007, 0.5327864479258708, 0.99, 0.24285949009143565, 0.45724347967383555, 0.9549369496755133, 0.06828630198595695]
[0.1461758239217677, 0.47589382081209974, 0.026675529698425002, 0.01, 0.01, 0.01, 0.2690149232698963, 0.01, 0.7576680196547064, 0.8281237764613696, 0.4873908413523007, 0.5327864479258708, 0.99, 0.24285949009143565, 0.45724347967383555, 0.9549369496755133, 0.06828630198595695]
Training loss = 0.022460687458515167
step = 0, Training Accuracy: 0.75
Validation Accuracy: 0.72625
Training loss = 0.018510425786177318
step = 1, Training Accuracy: 0.77
Training loss = 0.01757498989502589
step = 2, Training Accuracy: 0.8
Training loss = 0.015252666274706523
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015425933798154195
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.014541313598553339
step = 5, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.72875
Training loss = 0.014875388046105702
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.012422767728567123
step = 7, Training Accuracy: 0.87
Training loss = 0.012611332933108012
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.011785052319367727
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.011987679998079936
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.71125
Training loss = 0.009931113173564276
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.009502556125322978
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.010863304833571116
step = 13, Training Accuracy: 0.88
Training loss = 0.008419136057297388
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.7125
params:  [0.48248850648994235, 0.01001470610716998, 0.328971094872598, 0.01, 0.18472773124580133, 0.16116237258931576, 0.6622637052636016, 0.01, 0.4582475090903878, 0.8002979212621725, 0.6380582287811636, 0.3728511932564543, 0.4700618668650557, 0.01, 0.541541631268706, 0.245912657644062, 0.06977066120760939]
[0.48248850648994235, 0.01001470610716998, 0.328971094872598, 0.01, 0.18472773124580133, 0.16116237258931576, 0.6622637052636016, 0.01, 0.4582475090903878, 0.8002979212621725, 0.6380582287811636, 0.3728511932564543, 0.4700618668650557, 0.01, 0.541541631268706, 0.245912657644062, 0.06977066120760939]
Training loss = 0.020665737589200338
step = 0, Training Accuracy: 0.7533333333333333
Validation Accuracy: 0.7475
Training loss = 0.01859859178463618
step = 1, Training Accuracy: 0.79
Training loss = 0.014315629800160726
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.012592628399531047
step = 3, Training Accuracy: 0.85
Training loss = 0.012143688847621282
step = 4, Training Accuracy: 0.86
Training loss = 0.011602206379175186
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7675
Training loss = 0.011836729049682616
step = 6, Training Accuracy: 0.87
Training loss = 0.011612680852413177
step = 7, Training Accuracy: 0.85
Training loss = 0.009801727334658304
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.00914059579372406
step = 9, Training Accuracy: 0.88
Training loss = 0.008717798590660096
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.75625
Training loss = 0.00831045520802339
step = 11, Training Accuracy: 0.8966666666666666
Training loss = 0.009900416284799575
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.008147321442763011
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.007499923855066299
step = 14, Training Accuracy: 0.93
Validation Accuracy: 0.73
4  	8     	0.721875	0.00962175	0.70625	0.73625
params:  [0.2510958543080336, 0.01, 0.8393963415461276, 0.012718307065884872, 0.09038416961787238, 0.10195940720616267, 0.3903375174089413, 0.516850575164741, 0.5615312767084697, 0.8475988288096942, 0.01, 0.924444032195127, 0.6139104361165642, 0.14505519005188255, 0.4683692438270778, 0.5023583485996334, 0.29670112337379106]
[0.2510958543080336, 0.01, 0.8393963415461276, 0.012718307065884872, 0.09038416961787238, 0.10195940720616267, 0.3903375174089413, 0.516850575164741, 0.5615312767084697, 0.8475988288096942, 0.01, 0.924444032195127, 0.6139104361165642, 0.14505519005188255, 0.4683692438270778, 0.5023583485996334, 0.29670112337379106]
Training loss = 0.02338988771041234
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.7125
Training loss = 0.019490514397621155
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.018481434683005015
step = 2, Training Accuracy: 0.7633333333333333
Training loss = 0.016358072261015575
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.015897192160288492
step = 4, Training Accuracy: 0.7933333333333333
Training loss = 0.014567234019438425
step = 5, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.7075
Training loss = 0.01415510912736257
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.012579334676265716
step = 7, Training Accuracy: 0.83
Training loss = 0.01382437601685524
step = 8, Training Accuracy: 0.8133333333333334
Training loss = 0.01216063824792703
step = 9, Training Accuracy: 0.8066666666666666
Training loss = 0.012138312458992004
step = 10, Training Accuracy: 0.84
Validation Accuracy: 0.735
Training loss = 0.01155341327190399
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.012307914594809214
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.01293798714876175
step = 13, Training Accuracy: 0.83
Training loss = 0.01269641011953354
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.7275
params:  [0.28681355986860074, 0.48917467320901226, 0.2611834077729486, 0.01, 0.027973713296890267, 0.3261060173417, 0.06741140442783855, 0.01, 0.21635544190344114, 0.99, 0.8772320536742231, 0.99, 0.9477737956015612, 0.01, 0.9889152709304032, 0.660285655273604, 0.01]
[0.28681355986860074, 0.48917467320901226, 0.2611834077729486, 0.01, 0.027973713296890267, 0.3261060173417, 0.06741140442783855, 0.01, 0.21635544190344114, 0.99, 0.8772320536742231, 0.99, 0.9477737956015612, 0.01, 0.9889152709304032, 0.660285655273604, 0.01]
Training loss = 0.017861047486464183
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.71625
Training loss = 0.015251430173714955
step = 1, Training Accuracy: 0.8
Training loss = 0.014624468485514323
step = 2, Training Accuracy: 0.8033333333333333
Training loss = 0.01469624991218249
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.013127886752287547
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.012332890033721924
step = 5, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.72875
Training loss = 0.012304655015468598
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.011025514006614685
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.011827792127927145
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.010492701729138692
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.013043994903564454
step = 10, Training Accuracy: 0.83
Validation Accuracy: 0.73875
Training loss = 0.011186001400152842
step = 11, Training Accuracy: 0.8533333333333334
Training loss = 0.008798606644074123
step = 12, Training Accuracy: 0.9033333333333333
Training loss = 0.009797078147530556
step = 13, Training Accuracy: 0.8733333333333333
Training loss = 0.010308709492286046
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.7125
params:  [0.39255719996781824, 0.3364714753576665, 0.3566417005721114, 0.01, 0.01, 0.01, 0.3604757168361502, 0.312225960396338, 0.33291689727344037, 0.99, 0.585693292561695, 0.9321926839331418, 0.635862754369492, 0.01, 0.46239104185754243, 0.41400221374310675, 0.05996067454636837]
[0.39255719996781824, 0.3364714753576665, 0.3566417005721114, 0.01, 0.01, 0.01, 0.3604757168361502, 0.312225960396338, 0.33291689727344037, 0.99, 0.585693292561695, 0.9321926839331418, 0.635862754369492, 0.01, 0.46239104185754243, 0.41400221374310675, 0.05996067454636837]
Training loss = 0.019799754321575165
step = 0, Training Accuracy: 0.7433333333333333
Validation Accuracy: 0.73625
Training loss = 0.016911304593086242
step = 1, Training Accuracy: 0.7633333333333333
Training loss = 0.013725930402676265
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.014049703975518545
step = 3, Training Accuracy: 0.82
Training loss = 0.012461642076571782
step = 4, Training Accuracy: 0.8666666666666667
Training loss = 0.012696689466635385
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.73375
Training loss = 0.00934273545940717
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.010383854856093725
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.00940283050139745
step = 8, Training Accuracy: 0.89
Training loss = 0.0093936259051164
step = 9, Training Accuracy: 0.88
Training loss = 0.00848036545018355
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.71625
Training loss = 0.007848338211576144
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.0074758546551068625
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.008539353509744009
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.008554329425096511
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7275
params:  [0.4679928772968091, 0.38053543809211654, 0.24132846430070254, 0.01, 0.2751383363301556, 0.036746989288618914, 0.5397414875493596, 0.5091978263742156, 0.21444698745126223, 0.99, 0.31488049687190284, 0.99, 0.5126079715144867, 0.23333018283293575, 0.18511933047792825, 0.9220543557595204, 0.21110033920605975]
[0.4679928772968091, 0.38053543809211654, 0.24132846430070254, 0.01, 0.2751383363301556, 0.036746989288618914, 0.5397414875493596, 0.5091978263742156, 0.21444698745126223, 0.99, 0.31488049687190284, 0.99, 0.5126079715144867, 0.23333018283293575, 0.18511933047792825, 0.9220543557595204, 0.21110033920605975]
Training loss = 0.026332288682460785
step = 0, Training Accuracy: 0.7166666666666667
Validation Accuracy: 0.715
Training loss = 0.02133470892906189
step = 1, Training Accuracy: 0.72
Training loss = 0.017956602573394775
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.015159048438072205
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.014381329317887624
step = 4, Training Accuracy: 0.7966666666666666
Training loss = 0.015521026253700256
step = 5, Training Accuracy: 0.8
Validation Accuracy: 0.74625
Training loss = 0.014801478584607442
step = 6, Training Accuracy: 0.8066666666666666
Training loss = 0.013982660075028738
step = 7, Training Accuracy: 0.82
Training loss = 0.01367552876472473
step = 8, Training Accuracy: 0.82
Training loss = 0.013733978470166525
step = 9, Training Accuracy: 0.82
Training loss = 0.012264225482940674
step = 10, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.72375
Training loss = 0.013243508835633596
step = 11, Training Accuracy: 0.8366666666666667
Training loss = 0.014367785255114238
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.014094426234563192
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.011873933474222819
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.72125
params:  [0.41265266298002307, 0.3866692829693573, 0.8256859720250891, 0.26989876592413, 0.10190691526081344, 0.01, 0.6136150971530131, 0.5317908130145348, 0.14218207761452517, 0.7171552766894088, 0.5269217576741163, 0.631795605765501, 0.9202666043081622, 0.5755467186178509, 0.19025762958014303, 0.7856058209739001, 0.3794162201195901]
[0.41265266298002307, 0.3866692829693573, 0.8256859720250891, 0.26989876592413, 0.10190691526081344, 0.01, 0.6136150971530131, 0.5317908130145348, 0.14218207761452517, 0.7171552766894088, 0.5269217576741163, 0.631795605765501, 0.9202666043081622, 0.5755467186178509, 0.19025762958014303, 0.7856058209739001, 0.3794162201195901]
Training loss = 0.018572290241718293
step = 0, Training Accuracy: 0.7566666666666667
Validation Accuracy: 0.7275
Training loss = 0.018593095541000366
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01677106072505315
step = 2, Training Accuracy: 0.77
Training loss = 0.016364817718664805
step = 3, Training Accuracy: 0.7666666666666667
Training loss = 0.014716160694758097
step = 4, Training Accuracy: 0.8066666666666666
Training loss = 0.015733955601851146
step = 5, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.74125
Training loss = 0.015111672977606455
step = 6, Training Accuracy: 0.8
Training loss = 0.013563914597034455
step = 7, Training Accuracy: 0.8033333333333333
Training loss = 0.014874725937843322
step = 8, Training Accuracy: 0.7733333333333333
Training loss = 0.01424228717883428
step = 9, Training Accuracy: 0.7933333333333333
Training loss = 0.014309870600700379
step = 10, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.725
Training loss = 0.015245465934276581
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.014378807693719863
step = 12, Training Accuracy: 0.82
Training loss = 0.01280786911646525
step = 13, Training Accuracy: 0.85
Training loss = 0.014597757856051127
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7175
params:  [0.2680704197905852, 0.01, 0.6219548839417335, 0.02527666332811248, 0.01, 0.01, 0.12105406617653808, 0.4476682337504603, 0.41168476723760905, 0.99, 0.6335195896281568, 0.8728730782904207, 0.6255456207183095, 0.0642031618007109, 0.5508050201120794, 0.8069309484247666, 0.01]
[0.2680704197905852, 0.01, 0.6219548839417335, 0.02527666332811248, 0.01, 0.01, 0.12105406617653808, 0.4476682337504603, 0.41168476723760905, 0.99, 0.6335195896281568, 0.8728730782904207, 0.6255456207183095, 0.0642031618007109, 0.5508050201120794, 0.8069309484247666, 0.01]
Training loss = 0.01949232796827952
step = 0, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.69625
Training loss = 0.01646989901860555
step = 1, Training Accuracy: 0.8133333333333334
Training loss = 0.015622551639874777
step = 2, Training Accuracy: 0.7866666666666666
Training loss = 0.013545981248219808
step = 3, Training Accuracy: 0.81
Training loss = 0.014351481199264526
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.012999632060527802
step = 5, Training Accuracy: 0.84
Validation Accuracy: 0.7325
Training loss = 0.012081895669301351
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.012812498609224955
step = 7, Training Accuracy: 0.85
Training loss = 0.01223129431406657
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.01162723700205485
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.010008648286263149
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.725
Training loss = 0.011997059732675553
step = 11, Training Accuracy: 0.87
Training loss = 0.010382140974203745
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.009310302585363387
step = 13, Training Accuracy: 0.88
Training loss = 0.010152512490749359
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7275
params:  [0.27714914834197446, 0.28323456078137665, 0.4216107220166193, 0.10075727220452656, 0.3989776181265961, 0.01, 0.5372197287848777, 0.2940179629952355, 0.2696326320049573, 0.99, 0.4711465318878968, 0.5554903678960339, 0.7350854574179866, 0.01, 0.15129230878979122, 0.20305824994204225, 0.01]
[0.27714914834197446, 0.28323456078137665, 0.4216107220166193, 0.10075727220452656, 0.3989776181265961, 0.01, 0.5372197287848777, 0.2940179629952355, 0.2696326320049573, 0.99, 0.4711465318878968, 0.5554903678960339, 0.7350854574179866, 0.01, 0.15129230878979122, 0.20305824994204225, 0.01]
Training loss = 0.021216525733470916
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7075
Training loss = 0.017399306396643322
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.01391142358382543
step = 2, Training Accuracy: 0.84
Training loss = 0.011109359314044316
step = 3, Training Accuracy: 0.89
Training loss = 0.011185947706302006
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.010526648412148157
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.73125
Training loss = 0.010244119465351104
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.009185335983832678
step = 7, Training Accuracy: 0.89
Training loss = 0.007769855459531148
step = 8, Training Accuracy: 0.9233333333333333
Training loss = 0.008377311726411183
step = 9, Training Accuracy: 0.9366666666666666
Training loss = 0.008516533474127452
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.7225
Training loss = 0.006964731762806575
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.007581803500652313
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.006949101388454437
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.006846338721613089
step = 14, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.725
params:  [0.10396604529148426, 0.01, 0.33753557693645325, 0.17274876780734771, 0.30240358542603746, 0.21762306998720476, 0.6149708655714095, 0.01, 0.6794555122549708, 0.99, 0.26744431975714555, 0.37759076982464745, 0.7759491161426666, 0.4454096020128364, 0.27337048856983137, 0.369481723279778, 0.3512618524950288]
[0.10396604529148426, 0.01, 0.33753557693645325, 0.17274876780734771, 0.30240358542603746, 0.21762306998720476, 0.6149708655714095, 0.01, 0.6794555122549708, 0.99, 0.26744431975714555, 0.37759076982464745, 0.7759491161426666, 0.4454096020128364, 0.27337048856983137, 0.369481723279778, 0.3512618524950288]
Training loss = 0.023900698522726693
step = 0, Training Accuracy: 0.7466666666666667
Validation Accuracy: 0.72
Training loss = 0.020091982185840608
step = 1, Training Accuracy: 0.7266666666666667
Training loss = 0.017260387539863586
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.013944157461325327
step = 3, Training Accuracy: 0.8
Training loss = 0.012510905812184015
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.012996491690476735
step = 5, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.7525
Training loss = 0.011227436487873395
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.012572457691033682
step = 7, Training Accuracy: 0.8333333333333334
Training loss = 0.009412039120992024
step = 8, Training Accuracy: 0.89
Training loss = 0.011344810475905737
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.009308602809906006
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7325
Training loss = 0.01241956114768982
step = 11, Training Accuracy: 0.8533333333333334
Training loss = 0.009858678529659907
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.008801638732353846
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.00958453081548214
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73875
5  	8     	0.724688	0.00736201	0.7125 	0.73875
params:  [0.7332445712354576, 0.14614649300346041, 0.487386238268768, 0.01, 0.14700507888665934, 0.42509791015241105, 0.6552646805125099, 0.19795207791599975, 0.4311052505214509, 0.8631281748938526, 0.2558232427943201, 0.583988058224222, 0.7239331928141867, 0.01, 0.49372465264296905, 0.3857988978134971, 0.025100266034527197]
[0.7332445712354576, 0.14614649300346041, 0.487386238268768, 0.01, 0.14700507888665934, 0.42509791015241105, 0.6552646805125099, 0.19795207791599975, 0.4311052505214509, 0.8631281748938526, 0.2558232427943201, 0.583988058224222, 0.7239331928141867, 0.01, 0.49372465264296905, 0.3857988978134971, 0.025100266034527197]
Training loss = 0.017309817175070446
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.725
Training loss = 0.01621546526749929
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.01145607148607572
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.011496660361687342
step = 3, Training Accuracy: 0.87
Training loss = 0.010166649371385575
step = 4, Training Accuracy: 0.8866666666666667
Training loss = 0.010651173343261083
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.74
Training loss = 0.010946502735217412
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.008655148794253668
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.008277318477630614
step = 8, Training Accuracy: 0.9133333333333333
Training loss = 0.009719100271662076
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.007318604327738285
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7275
Training loss = 0.006866286620497704
step = 11, Training Accuracy: 0.9433333333333334
Training loss = 0.009229295551776887
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.006118753204743067
step = 13, Training Accuracy: 0.9466666666666667
Training loss = 0.006120912532011668
step = 14, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.71875
params:  [0.20497778356599297, 0.19438853278078386, 0.3717905602328263, 0.01, 0.01, 0.28788639732826615, 0.567559004775281, 0.1857595340969212, 0.6232843400193189, 0.99, 0.30332652184876296, 0.5324048305450125, 0.9685310691244681, 0.28294268139947515, 0.4145813576266998, 0.5721485582350643, 0.01]
[0.20497778356599297, 0.19438853278078386, 0.3717905602328263, 0.01, 0.01, 0.28788639732826615, 0.567559004775281, 0.1857595340969212, 0.6232843400193189, 0.99, 0.30332652184876296, 0.5324048305450125, 0.9685310691244681, 0.28294268139947515, 0.4145813576266998, 0.5721485582350643, 0.01]
Training loss = 0.02117885837952296
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.74375
Training loss = 0.014486154516537984
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.01345399558544159
step = 2, Training Accuracy: 0.82
Training loss = 0.012087655464808146
step = 3, Training Accuracy: 0.8366666666666667
Training loss = 0.012013412018616994
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.010991527835528056
step = 5, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.75375
Training loss = 0.00996332456668218
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.00891710862517357
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.008683673838774364
step = 8, Training Accuracy: 0.88
Training loss = 0.00844830925265948
step = 9, Training Accuracy: 0.9
Training loss = 0.008902417868375779
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.755
Training loss = 0.007679975579182307
step = 11, Training Accuracy: 0.9166666666666666
Training loss = 0.007519658356904983
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.009002666001518568
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.006817839741706848
step = 14, Training Accuracy: 0.92
Validation Accuracy: 0.745
params:  [0.2965839890007494, 0.01, 0.029707127313055748, 0.20566971761857533, 0.38447908507196565, 0.01, 0.7698772927495321, 0.01, 0.6093414785667302, 0.639095269133386, 0.27371923266783804, 0.6874091394355852, 0.5172419553830261, 0.8637235145045233, 0.35972600281334993, 0.7974230597870153, 0.46932393099205605]
[0.2965839890007494, 0.01, 0.029707127313055748, 0.20566971761857533, 0.38447908507196565, 0.01, 0.7698772927495321, 0.01, 0.6093414785667302, 0.639095269133386, 0.27371923266783804, 0.6874091394355852, 0.5172419553830261, 0.8637235145045233, 0.35972600281334993, 0.7974230597870153, 0.46932393099205605]
Training loss = 0.02323548952738444
step = 0, Training Accuracy: 0.7066666666666667
Validation Accuracy: 0.74875
Training loss = 0.02057001421848933
step = 1, Training Accuracy: 0.7233333333333334
Training loss = 0.019512221614519754
step = 2, Training Accuracy: 0.7133333333333334
Training loss = 0.017756962776184083
step = 3, Training Accuracy: 0.74
Training loss = 0.015609228213628134
step = 4, Training Accuracy: 0.78
Training loss = 0.016635515888532
step = 5, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.74
Training loss = 0.013704325209061305
step = 6, Training Accuracy: 0.83
Training loss = 0.015628424485524494
step = 7, Training Accuracy: 0.7866666666666666
Training loss = 0.014912862579027812
step = 8, Training Accuracy: 0.8066666666666666
Training loss = 0.015497176746527353
step = 9, Training Accuracy: 0.83
Training loss = 0.01618407517671585
step = 10, Training Accuracy: 0.78
Validation Accuracy: 0.705
Training loss = 0.015698380569616952
step = 11, Training Accuracy: 0.7966666666666666
Training loss = 0.015528856068849564
step = 12, Training Accuracy: 0.81
Training loss = 0.013661374400059382
step = 13, Training Accuracy: 0.8066666666666666
Training loss = 0.012821084856986999
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.74
params:  [0.1278958060510522, 0.01, 0.8510960795644213, 0.214260563064962, 0.21741198893231564, 0.01662813929011514, 0.2996704985898614, 0.4223533536497518, 0.4674818435026441, 0.7736228114945247, 0.1614421568448514, 0.9279135681060184, 0.6379782422749355, 0.01, 0.2620884864833308, 0.1356537805341782, 0.5456549951185101]
[0.1278958060510522, 0.01, 0.8510960795644213, 0.214260563064962, 0.21741198893231564, 0.01662813929011514, 0.2996704985898614, 0.4223533536497518, 0.4674818435026441, 0.7736228114945247, 0.1614421568448514, 0.9279135681060184, 0.6379782422749355, 0.01, 0.2620884864833308, 0.1356537805341782, 0.5456549951185101]
Training loss = 0.019793891708056132
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.7475
Training loss = 0.017697223623593647
step = 1, Training Accuracy: 0.7766666666666666
Training loss = 0.01837411860624949
step = 2, Training Accuracy: 0.79
Training loss = 0.013372697234153748
step = 3, Training Accuracy: 0.7933333333333333
Training loss = 0.01389637976884842
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.013293600132067998
step = 5, Training Accuracy: 0.83
Validation Accuracy: 0.725
Training loss = 0.012730869948863983
step = 6, Training Accuracy: 0.85
Training loss = 0.011977535486221314
step = 7, Training Accuracy: 0.82
Training loss = 0.01265643984079361
step = 8, Training Accuracy: 0.86
Training loss = 0.011891743640104929
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.011938346922397614
step = 10, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.7175
Training loss = 0.011878271003564199
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.009661901295185089
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.009255848626295726
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.009507633050282797
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7225
params:  [0.3213012873167971, 0.26313899957478126, 0.47019470942999086, 0.01, 0.33016027168661755, 0.4545940807229885, 0.47029636940069164, 0.5953092787108076, 0.6459621899139846, 0.99, 0.01, 0.6489870034037694, 0.7843834409228474, 0.3621500029389651, 0.22622460053835844, 0.4266980987955011, 0.25582919707172724]
[0.3213012873167971, 0.26313899957478126, 0.47019470942999086, 0.01, 0.33016027168661755, 0.4545940807229885, 0.47029636940069164, 0.5953092787108076, 0.6459621899139846, 0.99, 0.01, 0.6489870034037694, 0.7843834409228474, 0.3621500029389651, 0.22622460053835844, 0.4266980987955011, 0.25582919707172724]
Training loss = 0.01947907477617264
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.73625
Training loss = 0.01980448395013809
step = 1, Training Accuracy: 0.7533333333333333
Training loss = 0.01589193214972814
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.015421071747938792
step = 3, Training Accuracy: 0.7833333333333333
Training loss = 0.01456500361363093
step = 4, Training Accuracy: 0.8133333333333334
Training loss = 0.01370006670554479
step = 5, Training Accuracy: 0.8
Validation Accuracy: 0.7575
Training loss = 0.01324250598748525
step = 6, Training Accuracy: 0.8166666666666667
Training loss = 0.01453079362710317
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.013669500549634298
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.013653118858734766
step = 9, Training Accuracy: 0.84
Training loss = 0.013123517235120138
step = 10, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.75
Training loss = 0.010895218128959338
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.01355885570247968
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.010627844681342443
step = 13, Training Accuracy: 0.88
Training loss = 0.011655806750059127
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7425
params:  [0.6161154044078643, 0.01, 0.7751113096415121, 0.01, 0.1362327486700734, 0.22996604092000605, 0.31878946307438794, 0.32299473790119404, 0.8966893033310379, 0.8864271844536291, 0.6014320067299063, 0.6835795512994779, 0.9724130323192289, 0.8989723302317427, 0.2778781456346143, 0.5299654895381564, 0.11788512080184077]
[0.6161154044078643, 0.01, 0.7751113096415121, 0.01, 0.1362327486700734, 0.22996604092000605, 0.31878946307438794, 0.32299473790119404, 0.8966893033310379, 0.8864271844536291, 0.6014320067299063, 0.6835795512994779, 0.9724130323192289, 0.8989723302317427, 0.2778781456346143, 0.5299654895381564, 0.11788512080184077]
Training loss = 0.019364315470059713
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.745
Training loss = 0.01852373997370402
step = 1, Training Accuracy: 0.7866666666666666
Training loss = 0.015857980449994404
step = 2, Training Accuracy: 0.7966666666666666
Training loss = 0.01340648869673411
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.013490875562032064
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.012875256240367889
step = 5, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.7525
Training loss = 0.0126130643983682
step = 6, Training Accuracy: 0.84
Training loss = 0.01385738824804624
step = 7, Training Accuracy: 0.8
Training loss = 0.013278922587633133
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.013875864992539087
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.013421776741743087
step = 10, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.74625
Training loss = 0.010020428399244944
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.011739665120840072
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.011318963865439097
step = 13, Training Accuracy: 0.8666666666666667
Training loss = 0.011165466209252675
step = 14, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.74125
params:  [0.35929376363454646, 0.08502383283474263, 0.01, 0.01, 0.01, 0.24579289402066953, 0.6302998777923301, 0.31626721364809274, 0.6454203316515034, 0.99, 0.08389607216886322, 0.6984319973740816, 0.748353793440682, 0.01, 0.35977337895703776, 0.4200381319088681, 0.30350638433610694]
[0.35929376363454646, 0.08502383283474263, 0.01, 0.01, 0.01, 0.24579289402066953, 0.6302998777923301, 0.31626721364809274, 0.6454203316515034, 0.99, 0.08389607216886322, 0.6984319973740816, 0.748353793440682, 0.01, 0.35977337895703776, 0.4200381319088681, 0.30350638433610694]
Training loss = 0.016628760471940042
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.74875
Training loss = 0.01554026703039805
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.012356785535812377
step = 2, Training Accuracy: 0.84
Training loss = 0.010576405972242355
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.010412656714518865
step = 4, Training Accuracy: 0.8633333333333333
Training loss = 0.009622907737890879
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.775
Training loss = 0.01024205339451631
step = 6, Training Accuracy: 0.87
Training loss = 0.00887364851931731
step = 7, Training Accuracy: 0.9
Training loss = 0.009622428466876347
step = 8, Training Accuracy: 0.88
Training loss = 0.006909630447626114
step = 9, Training Accuracy: 0.9233333333333333
Training loss = 0.007345668201645215
step = 10, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.76
Training loss = 0.007683860659599304
step = 11, Training Accuracy: 0.8966666666666666
Training loss = 0.0068678392966588335
step = 12, Training Accuracy: 0.9266666666666666
Training loss = 0.006056130453944206
step = 13, Training Accuracy: 0.94
Training loss = 0.005847981224457423
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.755
params:  [0.2548866558602254, 0.16655980412878102, 0.15850488126956624, 0.38334667530397887, 0.2549957650917874, 0.6785761049356436, 0.7237223064632357, 0.15837134993105306, 0.5756947502391884, 0.99, 0.01, 0.554858135550632, 0.3965095885589766, 0.31783278500676204, 0.28877608818537015, 0.6250988135961286, 0.2638769156390839]
[0.2548866558602254, 0.16655980412878102, 0.15850488126956624, 0.38334667530397887, 0.2549957650917874, 0.6785761049356436, 0.7237223064632357, 0.15837134993105306, 0.5756947502391884, 0.99, 0.01, 0.554858135550632, 0.3965095885589766, 0.31783278500676204, 0.28877608818537015, 0.6250988135961286, 0.2638769156390839]
Training loss = 0.017848687221606572
step = 0, Training Accuracy: 0.8
Validation Accuracy: 0.75875
Training loss = 0.015028731773296992
step = 1, Training Accuracy: 0.81
Training loss = 0.01399837205807368
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.012886167764663696
step = 3, Training Accuracy: 0.8233333333333334
Training loss = 0.009240584224462509
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.010641146848599115
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.74375
Training loss = 0.009306557451685269
step = 6, Training Accuracy: 0.91
Training loss = 0.009049325486024221
step = 7, Training Accuracy: 0.88
Training loss = 0.009293455605705579
step = 8, Training Accuracy: 0.89
Training loss = 0.00763955128689607
step = 9, Training Accuracy: 0.91
Training loss = 0.009547282656033834
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.73375
Training loss = 0.00713137611746788
step = 11, Training Accuracy: 0.92
Training loss = 0.009686472887794177
step = 12, Training Accuracy: 0.88
Training loss = 0.00900302194058895
step = 13, Training Accuracy: 0.9166666666666666
Training loss = 0.008990286439657212
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.7325
6  	8     	0.737188	0.0112283 	0.71875	0.755  
params:  [0.3103371042236413, 0.268772822706451, 0.3478940286858205, 0.01, 0.04591920057543786, 0.21911447103063753, 0.7530756156272438, 0.15960740581397986, 0.6924169184672299, 0.99, 0.23323213184841868, 0.49805071858981786, 0.99, 0.3059326294393288, 0.3432612238890683, 0.3460714911066846, 0.463143701021024]
[0.3103371042236413, 0.268772822706451, 0.3478940286858205, 0.01, 0.04591920057543786, 0.21911447103063753, 0.7530756156272438, 0.15960740581397986, 0.6924169184672299, 0.99, 0.23323213184841868, 0.49805071858981786, 0.99, 0.3059326294393288, 0.3432612238890683, 0.3460714911066846, 0.463143701021024]
Training loss = 0.017683888177076976
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7375
Training loss = 0.014374289363622665
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.014243835955858231
step = 2, Training Accuracy: 0.83
Training loss = 0.012076338132222494
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.010617453157901763
step = 4, Training Accuracy: 0.85
Training loss = 0.013171800921360652
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.74125
Training loss = 0.01011629710594813
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.01048808753490448
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.009114703486363092
step = 8, Training Accuracy: 0.9
Training loss = 0.009905994286139806
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.009039863049983977
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73125
Training loss = 0.009812877625226974
step = 11, Training Accuracy: 0.88
Training loss = 0.007962271223465602
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.008458842734495799
step = 13, Training Accuracy: 0.93
Training loss = 0.008750465040405591
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.7375
params:  [0.01, 0.23326168347917048, 0.36208104252027584, 0.1037540072366572, 0.30322609613459006, 0.8773236428656868, 0.6323586660818036, 0.3827609561095534, 0.3261039044165525, 0.99, 0.3491624247208701, 0.6598815039643883, 0.8418605252100447, 0.028638105178132617, 0.4684152561650337, 0.5380897300992262, 0.14254349993647825]
[0.01, 0.23326168347917048, 0.36208104252027584, 0.1037540072366572, 0.30322609613459006, 0.8773236428656868, 0.6323586660818036, 0.3827609561095534, 0.3261039044165525, 0.99, 0.3491624247208701, 0.6598815039643883, 0.8418605252100447, 0.028638105178132617, 0.4684152561650337, 0.5380897300992262, 0.14254349993647825]
Training loss = 0.019579244256019594
step = 0, Training Accuracy: 0.77
Validation Accuracy: 0.71625
Training loss = 0.012062546412150064
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.011531482934951782
step = 2, Training Accuracy: 0.85
Training loss = 0.010285380880037944
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.009708354026079178
step = 4, Training Accuracy: 0.87
Training loss = 0.008852271984020868
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.725
Training loss = 0.007628731628259023
step = 6, Training Accuracy: 0.9
Training loss = 0.008066720217466355
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.007443997785449028
step = 8, Training Accuracy: 0.92
Training loss = 0.0069902052978674575
step = 9, Training Accuracy: 0.9133333333333333
Training loss = 0.007614621495207151
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.7275
Training loss = 0.0060687639315923055
step = 11, Training Accuracy: 0.9233333333333333
Training loss = 0.0076907617598772045
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.005088843827446302
step = 13, Training Accuracy: 0.9366666666666666
Training loss = 0.00716113992035389
step = 14, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.7125
params:  [0.3826004768718011, 0.17768433920141521, 0.4712062548505163, 0.01, 0.01, 0.26511159197094397, 0.9072787598181296, 0.2235641538066498, 0.39935729700108585, 0.45527120384340114, 0.02727398506428852, 0.8228101959887961, 0.9062919985741271, 0.39427866637620546, 0.34316384448523085, 0.48137712818093226, 0.01]
[0.3826004768718011, 0.17768433920141521, 0.4712062548505163, 0.01, 0.01, 0.26511159197094397, 0.9072787598181296, 0.2235641538066498, 0.39935729700108585, 0.45527120384340114, 0.02727398506428852, 0.8228101959887961, 0.9062919985741271, 0.39427866637620546, 0.34316384448523085, 0.48137712818093226, 0.01]
Training loss = 0.020498564938704174
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.71375
Training loss = 0.017124904990196227
step = 1, Training Accuracy: 0.82
Training loss = 0.016155088643232982
step = 2, Training Accuracy: 0.83
Training loss = 0.013753925065199533
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.010633855561415355
step = 4, Training Accuracy: 0.8633333333333333
Training loss = 0.013395910660425822
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.73625
Training loss = 0.01069694995880127
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.011733642170826593
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.009196708897749583
step = 8, Training Accuracy: 0.9
Training loss = 0.008335810800393422
step = 9, Training Accuracy: 0.9133333333333333
Training loss = 0.008652184009552002
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.73625
Training loss = 0.00910737136999766
step = 11, Training Accuracy: 0.9
Training loss = 0.008836470941702524
step = 12, Training Accuracy: 0.9
Training loss = 0.007923928946256638
step = 13, Training Accuracy: 0.9066666666666666
Training loss = 0.008569003666440646
step = 14, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.7275
params:  [0.4310611743820092, 0.5037275416287776, 0.41570289265480453, 0.01, 0.01, 0.01, 0.4539136756262776, 0.36378852854626686, 0.7264183761499616, 0.99, 0.19030762580481048, 0.8471627663294217, 0.47557395839226474, 0.2795128265833027, 0.680081662364362, 0.5894005704423363, 0.01]
[0.4310611743820092, 0.5037275416287776, 0.41570289265480453, 0.01, 0.01, 0.01, 0.4539136756262776, 0.36378852854626686, 0.7264183761499616, 0.99, 0.19030762580481048, 0.8471627663294217, 0.47557395839226474, 0.2795128265833027, 0.680081662364362, 0.5894005704423363, 0.01]
Training loss = 0.018748467763264973
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.74125
Training loss = 0.01621585821111997
step = 1, Training Accuracy: 0.79
Training loss = 0.014491886049509049
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.012509550253550212
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.011857617298762004
step = 4, Training Accuracy: 0.85
Training loss = 0.011353028615315756
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7425
Training loss = 0.011308110107978185
step = 6, Training Accuracy: 0.8466666666666667
Training loss = 0.010852971424659094
step = 7, Training Accuracy: 0.87
Training loss = 0.011268576656778653
step = 8, Training Accuracy: 0.85
Training loss = 0.013182210624217988
step = 9, Training Accuracy: 0.85
Training loss = 0.010181101610263189
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.7125
Training loss = 0.010090868969758351
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.009021639327208201
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.008081522757808367
step = 13, Training Accuracy: 0.9
Training loss = 0.009595329016447067
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.72125
params:  [0.2230814596717737, 0.06614760525829425, 0.09228246100659031, 0.01748658315632246, 0.061677960063283875, 0.07870778470818549, 0.7002590792121625, 0.23521152578502094, 0.8728372068224165, 0.99, 0.46221532012207145, 0.5693920199421645, 0.6696833283402575, 0.29292586725239755, 0.3818908332846976, 0.3782971698880312, 0.4670254129280941]
[0.2230814596717737, 0.06614760525829425, 0.09228246100659031, 0.01748658315632246, 0.061677960063283875, 0.07870778470818549, 0.7002590792121625, 0.23521152578502094, 0.8728372068224165, 0.99, 0.46221532012207145, 0.5693920199421645, 0.6696833283402575, 0.29292586725239755, 0.3818908332846976, 0.3782971698880312, 0.4670254129280941]
Training loss = 0.015753280818462372
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.74
Training loss = 0.01379479502638181
step = 1, Training Accuracy: 0.84
Training loss = 0.013053883761167527
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.012301600525776545
step = 3, Training Accuracy: 0.84
Training loss = 0.009347577591737112
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.010217079768578212
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.73625
Training loss = 0.010515085061391195
step = 6, Training Accuracy: 0.89
Training loss = 0.009376493245363236
step = 7, Training Accuracy: 0.89
Training loss = 0.007958027770121893
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.007103112364808719
step = 9, Training Accuracy: 0.9066666666666666
Training loss = 0.007290243109067281
step = 10, Training Accuracy: 0.94
Validation Accuracy: 0.74625
Training loss = 0.007053803453842799
step = 11, Training Accuracy: 0.92
Training loss = 0.00719414787987868
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.0062186023344596225
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.007238828341166179
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.74125
params:  [0.16975386083036204, 0.1172101492165476, 0.1473760668403628, 0.01, 0.01, 0.37832371150449984, 0.5656108121775147, 0.24658574177217857, 0.5768025833681912, 0.6997232664025422, 0.01, 0.99, 0.6540978641121566, 0.12659531080291578, 0.49376988430705937, 0.06715145606826345, 0.4277937959129551]
[0.16975386083036204, 0.1172101492165476, 0.1473760668403628, 0.01, 0.01, 0.37832371150449984, 0.5656108121775147, 0.24658574177217857, 0.5768025833681912, 0.6997232664025422, 0.01, 0.99, 0.6540978641121566, 0.12659531080291578, 0.49376988430705937, 0.06715145606826345, 0.4277937959129551]
Training loss = 0.019751957058906554
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.73625
Training loss = 0.013010996033747991
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.012254671057065328
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.011248779793580374
step = 3, Training Accuracy: 0.86
Training loss = 0.009677436749140421
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.009149118612209956
step = 5, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.72
Training loss = 0.010386151174704233
step = 6, Training Accuracy: 0.88
Training loss = 0.009086261987686157
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.009534584681193033
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.010061022639274598
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.008055888861417771
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.725
Training loss = 0.008636227001746495
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.007692604909340541
step = 12, Training Accuracy: 0.92
Training loss = 0.005313767269253731
step = 13, Training Accuracy: 0.9466666666666667
Training loss = 0.005725796048839887
step = 14, Training Accuracy: 0.95
Validation Accuracy: 0.73625
params:  [0.5095727547536741, 0.08430019958820566, 0.33390916321632813, 0.1981507763902431, 0.28656063950928434, 0.4757577716340002, 0.712666527496061, 0.4667124339946691, 0.32660147347814306, 0.99, 0.3426305715287885, 0.7818717889488976, 0.8072569486019697, 0.5033626168110992, 0.12895804961756432, 0.3262431954780398, 0.3597550994330804]
[0.5095727547536741, 0.08430019958820566, 0.33390916321632813, 0.1981507763902431, 0.28656063950928434, 0.4757577716340002, 0.712666527496061, 0.4667124339946691, 0.32660147347814306, 0.99, 0.3426305715287885, 0.7818717889488976, 0.8072569486019697, 0.5033626168110992, 0.12895804961756432, 0.3262431954780398, 0.3597550994330804]
Training loss = 0.022645160357157388
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.735
Training loss = 0.017966204782327015
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.015522362987200418
step = 2, Training Accuracy: 0.8133333333333334
Training loss = 0.015268666446208954
step = 3, Training Accuracy: 0.82
Training loss = 0.01275622268517812
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.012280603597561519
step = 5, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74
Training loss = 0.012769071261088054
step = 6, Training Accuracy: 0.8333333333333334
Training loss = 0.01150926505525907
step = 7, Training Accuracy: 0.87
Training loss = 0.013277622858683268
step = 8, Training Accuracy: 0.87
Training loss = 0.011816534201304118
step = 9, Training Accuracy: 0.85
Training loss = 0.011501152912775675
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.7325
Training loss = 0.011353681683540345
step = 11, Training Accuracy: 0.85
Training loss = 0.010650120476881663
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.008441066940625509
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.01161913146575292
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.72125
params:  [0.18627123680440066, 0.28491889393276176, 0.01, 0.1065960479880157, 0.05921986276494622, 0.2876550067195613, 0.3269118062535603, 0.25280420644798246, 0.7963078231213727, 0.99, 0.15808799661595263, 0.7705322569676757, 0.9150713081838885, 0.01, 0.11046973809072244, 0.5945142695421304, 0.01]
[0.18627123680440066, 0.28491889393276176, 0.01, 0.1065960479880157, 0.05921986276494622, 0.2876550067195613, 0.3269118062535603, 0.25280420644798246, 0.7963078231213727, 0.99, 0.15808799661595263, 0.7705322569676757, 0.9150713081838885, 0.01, 0.11046973809072244, 0.5945142695421304, 0.01]
Training loss = 0.013218393822511037
step = 0, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.7325
Training loss = 0.014413739542166392
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.00999749630689621
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.010567528754472732
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.009554271896680197
step = 4, Training Accuracy: 0.88
Training loss = 0.007979543978969257
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.7625
Training loss = 0.007519386224448681
step = 6, Training Accuracy: 0.91
Training loss = 0.006949469347794851
step = 7, Training Accuracy: 0.92
Training loss = 0.007006211380163828
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.006185681596398354
step = 9, Training Accuracy: 0.94
Training loss = 0.006577948381503423
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.755
Training loss = 0.0065391883750756585
step = 11, Training Accuracy: 0.9266666666666666
Training loss = 0.006001073022683462
step = 12, Training Accuracy: 0.9366666666666666
Training loss = 0.005935577029983203
step = 13, Training Accuracy: 0.9433333333333334
Training loss = 0.005965994646151861
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.74
7  	8     	0.729688	0.00991664	0.7125 	0.74125
params:  [0.01, 0.31040422072371704, 0.22128301153056215, 0.06678570768001409, 0.01, 0.01, 0.8719786363004309, 0.2682922007894873, 0.99, 0.9448246987437069, 0.2671695811955417, 0.49245680314051177, 0.9119110454377416, 0.01, 0.21455262686050758, 0.6127703315547737, 0.08121947954238701]
[0.01, 0.31040422072371704, 0.22128301153056215, 0.06678570768001409, 0.01, 0.01, 0.8719786363004309, 0.2682922007894873, 0.99, 0.9448246987437069, 0.2671695811955417, 0.49245680314051177, 0.9119110454377416, 0.01, 0.21455262686050758, 0.6127703315547737, 0.08121947954238701]
Training loss = 0.0159766353170077
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.745
Training loss = 0.01795282855629921
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.014006476600964863
step = 2, Training Accuracy: 0.84
Training loss = 0.011133086929718653
step = 3, Training Accuracy: 0.87
Training loss = 0.010622753252585729
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.009794687827428181
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.76
Training loss = 0.01056703155239423
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.009102691809336344
step = 7, Training Accuracy: 0.89
Training loss = 0.008459781457980474
step = 8, Training Accuracy: 0.91
Training loss = 0.009871672441562018
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.008291497478882472
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.75
Training loss = 0.0074528049925963085
step = 11, Training Accuracy: 0.9233333333333333
Training loss = 0.0058159705996513366
step = 12, Training Accuracy: 0.95
Training loss = 0.008507744471232096
step = 13, Training Accuracy: 0.9
Training loss = 0.00632951021194458
step = 14, Training Accuracy: 0.9366666666666666
Validation Accuracy: 0.73875
params:  [0.20282920381260378, 0.01, 0.2081905443800316, 0.2575913196332932, 0.16171134597698691, 0.01, 0.6889137293913712, 0.16755167504194052, 0.6070939066445187, 0.99, 0.48029651026943554, 0.3505115808182851, 0.6924263158761789, 0.01, 0.01, 0.2781081070983316, 0.2872110040741163]
[0.20282920381260378, 0.01, 0.2081905443800316, 0.2575913196332932, 0.16171134597698691, 0.01, 0.6889137293913712, 0.16755167504194052, 0.6070939066445187, 0.99, 0.48029651026943554, 0.3505115808182851, 0.6924263158761789, 0.01, 0.01, 0.2781081070983316, 0.2872110040741163]
Training loss = 0.013952950735886892
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.745
Training loss = 0.012074884672959646
step = 1, Training Accuracy: 0.83
Training loss = 0.011417762339115144
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.009916602075099945
step = 3, Training Accuracy: 0.8733333333333333
Training loss = 0.008611903116106987
step = 4, Training Accuracy: 0.9033333333333333
Training loss = 0.007420375992854437
step = 5, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.73375
Training loss = 0.007184771448373794
step = 6, Training Accuracy: 0.9066666666666666
Training loss = 0.006842229863007863
step = 7, Training Accuracy: 0.92
Training loss = 0.007446659853061041
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.005612379238009453
step = 9, Training Accuracy: 0.9333333333333333
Training loss = 0.005894983895123005
step = 10, Training Accuracy: 0.9333333333333333
Validation Accuracy: 0.73625
Training loss = 0.004997493575016657
step = 11, Training Accuracy: 0.9466666666666667
Training loss = 0.005759328703085581
step = 12, Training Accuracy: 0.94
Training loss = 0.004656319643060366
step = 13, Training Accuracy: 0.9533333333333334
Training loss = 0.004180775508284568
step = 14, Training Accuracy: 0.9566666666666667
Validation Accuracy: 0.72875
params:  [0.021625247147878085, 0.13547673541301875, 0.12624557001094516, 0.18698643391178954, 0.01, 0.01, 0.7027757363367503, 0.14464446396841943, 0.6331003186417559, 0.7128558179145321, 0.4501954775452973, 0.6120637713149409, 0.6639296013290447, 0.1387807303723049, 0.8252885549668711, 0.724465543510664, 0.45316090271544196]
[0.021625247147878085, 0.13547673541301875, 0.12624557001094516, 0.18698643391178954, 0.01, 0.01, 0.7027757363367503, 0.14464446396841943, 0.6331003186417559, 0.7128558179145321, 0.4501954775452973, 0.6120637713149409, 0.6639296013290447, 0.1387807303723049, 0.8252885549668711, 0.724465543510664, 0.45316090271544196]
Training loss = 0.020884032746156057
step = 0, Training Accuracy: 0.7733333333333333
Validation Accuracy: 0.7425
Training loss = 0.014500877323249976
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.014589461584885915
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.013573184410730998
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.01249475121498108
step = 4, Training Accuracy: 0.8666666666666667
Training loss = 0.010467991828918458
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.72375
Training loss = 0.011225217978159587
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.010001379698514938
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.00964378426472346
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.010968484332164128
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.008215311417977015
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.73125
Training loss = 0.008346997052431106
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.008681224634250004
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.007065282464027405
step = 13, Training Accuracy: 0.92
Training loss = 0.008802816917498906
step = 14, Training Accuracy: 0.89
Validation Accuracy: 0.72875
params:  [0.4034352602965542, 0.3962114340432807, 0.01, 0.1175514538480643, 0.0734715116571407, 0.3302837433373075, 0.6092746986877914, 0.5375166043939955, 0.99, 0.99, 0.5159323402487612, 0.6189407912978324, 0.7926429851183654, 0.08731524660353504, 0.3312711285227202, 0.34147383665826275, 0.48082291584667086]
[0.4034352602965542, 0.3962114340432807, 0.01, 0.1175514538480643, 0.0734715116571407, 0.3302837433373075, 0.6092746986877914, 0.5375166043939955, 0.99, 0.99, 0.5159323402487612, 0.6189407912978324, 0.7926429851183654, 0.08731524660353504, 0.3312711285227202, 0.34147383665826275, 0.48082291584667086]
Training loss = 0.01584512531757355
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.735
Training loss = 0.012876682976881663
step = 1, Training Accuracy: 0.83
Training loss = 0.011979170789321264
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.011573858360449473
step = 3, Training Accuracy: 0.87
Training loss = 0.010605618258317311
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.010306142767270406
step = 5, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.73875
Training loss = 0.009045222178101539
step = 6, Training Accuracy: 0.87
Training loss = 0.008849668353796004
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.008267259101072948
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.009289402614037197
step = 9, Training Accuracy: 0.9
Training loss = 0.009470251649618148
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.72625
Training loss = 0.007499219824870428
step = 11, Training Accuracy: 0.9
Training loss = 0.007647360463937124
step = 12, Training Accuracy: 0.91
Training loss = 0.0067079271376132964
step = 13, Training Accuracy: 0.9066666666666666
Training loss = 0.006350259880224864
step = 14, Training Accuracy: 0.92
Validation Accuracy: 0.72625
params:  [0.01, 0.09740001319593874, 0.39361454274430835, 0.08890718334646489, 0.2059896181295316, 0.01, 0.2991711696504997, 0.17311636538318198, 0.6043742918488401, 0.99, 0.22720323406866003, 0.47734796446854416, 0.99, 0.1230442168151616, 0.20746400723385694, 0.18556217123804788, 0.4801896283177348]
[0.01, 0.09740001319593874, 0.39361454274430835, 0.08890718334646489, 0.2059896181295316, 0.01, 0.2991711696504997, 0.17311636538318198, 0.6043742918488401, 0.99, 0.22720323406866003, 0.47734796446854416, 0.99, 0.1230442168151616, 0.20746400723385694, 0.18556217123804788, 0.4801896283177348]
Training loss = 0.016455005407333374
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7425
Training loss = 0.01383587380250295
step = 1, Training Accuracy: 0.86
Training loss = 0.011621237595876058
step = 2, Training Accuracy: 0.86
Training loss = 0.009815435459216437
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.009641779959201813
step = 4, Training Accuracy: 0.8866666666666667
Training loss = 0.008706259032090505
step = 5, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.7475
Training loss = 0.009761428087949752
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.009259740511576334
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.00725057120124499
step = 8, Training Accuracy: 0.9233333333333333
Training loss = 0.007561190674702327
step = 9, Training Accuracy: 0.89
Training loss = 0.0059181295086940125
step = 10, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.74875
Training loss = 0.007636711200078328
step = 11, Training Accuracy: 0.92
Training loss = 0.005521753455201785
step = 12, Training Accuracy: 0.95
Training loss = 0.006765983228882153
step = 13, Training Accuracy: 0.93
Training loss = 0.005128654936949412
step = 14, Training Accuracy: 0.9566666666666667
Validation Accuracy: 0.75375
params:  [0.13506716714156755, 0.14968718478389487, 0.01, 0.24827992816028896, 0.01, 0.01, 0.5238472800984509, 0.46732558079306397, 0.99, 0.8092505640465105, 0.4015447774132269, 0.5653334882970462, 0.5895273862539129, 0.3331505398805208, 0.6118560406588254, 0.31448690064666057, 0.09312160127212762]
[0.13506716714156755, 0.14968718478389487, 0.01, 0.24827992816028896, 0.01, 0.01, 0.5238472800984509, 0.46732558079306397, 0.99, 0.8092505640465105, 0.4015447774132269, 0.5653334882970462, 0.5895273862539129, 0.3331505398805208, 0.6118560406588254, 0.31448690064666057, 0.09312160127212762]
Training loss = 0.015143635421991349
step = 0, Training Accuracy: 0.83
Validation Accuracy: 0.755
Training loss = 0.013895078947146734
step = 1, Training Accuracy: 0.84
Training loss = 0.012025932272275289
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.011395464738210043
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.008495314319928487
step = 4, Training Accuracy: 0.9033333333333333
Training loss = 0.011280922094980876
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.76125
Training loss = 0.008804147293170294
step = 6, Training Accuracy: 0.89
Training loss = 0.007887724836667378
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.007652066896359126
step = 8, Training Accuracy: 0.93
Training loss = 0.00723134742428859
step = 9, Training Accuracy: 0.92
Training loss = 0.006726405670245489
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.75875
Training loss = 0.006330829684933027
step = 11, Training Accuracy: 0.96
Training loss = 0.006070549214879672
step = 12, Training Accuracy: 0.94
Training loss = 0.006123909230033557
step = 13, Training Accuracy: 0.96
Training loss = 0.006510040362675985
step = 14, Training Accuracy: 0.94
Validation Accuracy: 0.76625
params:  [0.05761632591059232, 0.1580344598093259, 0.0554784136424913, 0.19367084752235983, 0.01, 0.18851622331945758, 0.5366299410958322, 0.5189415615774745, 0.99, 0.9573565036569814, 0.09031547041886218, 0.6183372970502915, 0.8576368410245305, 0.2629808656350297, 0.19846406356754406, 0.2740324641697357, 0.4993941076488187]
[0.05761632591059232, 0.1580344598093259, 0.0554784136424913, 0.19367084752235983, 0.01, 0.18851622331945758, 0.5366299410958322, 0.5189415615774745, 0.99, 0.9573565036569814, 0.09031547041886218, 0.6183372970502915, 0.8576368410245305, 0.2629808656350297, 0.19846406356754406, 0.2740324641697357, 0.4993941076488187]
Training loss = 0.018812453895807265
step = 0, Training Accuracy: 0.79
Validation Accuracy: 0.75875
Training loss = 0.015766867250204087
step = 1, Training Accuracy: 0.7833333333333333
Training loss = 0.015554976363976796
step = 2, Training Accuracy: 0.7933333333333333
Training loss = 0.016138282517592112
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.011963914434115092
step = 4, Training Accuracy: 0.85
Training loss = 0.012436794191598892
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.765
Training loss = 0.011279549449682236
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.011498930503924688
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.01042932818333308
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.00919840082526207
step = 9, Training Accuracy: 0.89
Training loss = 0.009134700993696848
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.7375
Training loss = 0.00991616778075695
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.008678785810867945
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.007360266397396723
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.008186309859156608
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.7625
params:  [0.20231314804918046, 0.01, 0.08592071201983933, 0.05345307135166643, 0.01, 0.01, 0.7924115706346664, 0.48563913019589877, 0.9039691167070699, 0.99, 0.38205911453277963, 0.744056780710097, 0.8193645226598181, 0.11630196399295868, 0.2157608033968092, 0.5564155494054993, 0.6230258829185473]
[0.20231314804918046, 0.01, 0.08592071201983933, 0.05345307135166643, 0.01, 0.01, 0.7924115706346664, 0.48563913019589877, 0.9039691167070699, 0.99, 0.38205911453277963, 0.744056780710097, 0.8193645226598181, 0.11630196399295868, 0.2157608033968092, 0.5564155494054993, 0.6230258829185473]
Training loss = 0.01512690136830012
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.7575
Training loss = 0.011642141739527384
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.011854501167933146
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.013635065754254659
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.011514747192462286
step = 4, Training Accuracy: 0.87
Training loss = 0.009906418571869533
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.755
Training loss = 0.01157396932442983
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.010353321383396785
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.008202931731939317
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.010647712995608648
step = 9, Training Accuracy: 0.88
Training loss = 0.008203708877166113
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.76125
Training loss = 0.0067281587173541385
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.007605196634928386
step = 12, Training Accuracy: 0.9
Training loss = 0.006761105755964915
step = 13, Training Accuracy: 0.92
Training loss = 0.007256477524836858
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.75375
8  	8     	0.744844	0.0151224 	0.72625	0.76625
params:  [0.15258764540241276, 0.32730769718107017, 0.14702583581031012, 0.29652332801507597, 0.371986059309165, 0.01, 0.3213696875289783, 0.38506903913453594, 0.99, 0.99, 0.6709939470634665, 0.6936741320093365, 0.6038645645652836, 0.13456463061726576, 0.3775693216878776, 0.2515538288802617, 0.48538159785608886]
[0.15258764540241276, 0.32730769718107017, 0.14702583581031012, 0.29652332801507597, 0.371986059309165, 0.01, 0.3213696875289783, 0.38506903913453594, 0.99, 0.99, 0.6709939470634665, 0.6936741320093365, 0.6038645645652836, 0.13456463061726576, 0.3775693216878776, 0.2515538288802617, 0.48538159785608886]
Training loss = 0.01859240690867106
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7425
Training loss = 0.016277449230353038
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.014488523652156193
step = 2, Training Accuracy: 0.8066666666666666
Training loss = 0.012687135934829712
step = 3, Training Accuracy: 0.88
Training loss = 0.013049468298753102
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.012419181317090989
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.75
Training loss = 0.01137258177002271
step = 6, Training Accuracy: 0.8466666666666667
Training loss = 0.010657923668622971
step = 7, Training Accuracy: 0.8866666666666667
Training loss = 0.010321898013353347
step = 8, Training Accuracy: 0.87
Training loss = 0.009424548596143723
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.008623716483513514
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.765
Training loss = 0.009794990569353103
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.009012341077129046
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.007701051533222199
step = 13, Training Accuracy: 0.91
Training loss = 0.009689801385005315
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.75
params:  [0.2225427875973339, 0.01, 0.12547773863970557, 0.30305763631724214, 0.08329204970595928, 0.2491714701691719, 0.2463587731331111, 0.7030095337021696, 0.5406311780487503, 0.99, 0.5758132830086846, 0.7301521342018849, 0.8763328054002721, 0.3224170536497404, 0.3689070769115895, 0.36341029405097347, 0.22121389214336196]
[0.2225427875973339, 0.01, 0.12547773863970557, 0.30305763631724214, 0.08329204970595928, 0.2491714701691719, 0.2463587731331111, 0.7030095337021696, 0.5406311780487503, 0.99, 0.5758132830086846, 0.7301521342018849, 0.8763328054002721, 0.3224170536497404, 0.3689070769115895, 0.36341029405097347, 0.22121389214336196]
Training loss = 0.01790366048614184
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.74875
Training loss = 0.014527245412270228
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.013289913137753805
step = 2, Training Accuracy: 0.85
Training loss = 0.01200969805320104
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.01014502689242363
step = 4, Training Accuracy: 0.88
Training loss = 0.01084272508819898
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.73875
Training loss = 0.009028092374404271
step = 6, Training Accuracy: 0.88
Training loss = 0.009651065915822982
step = 7, Training Accuracy: 0.88
Training loss = 0.0095016248524189
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.008314166913429896
step = 9, Training Accuracy: 0.9
Training loss = 0.007375354319810868
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.7425
Training loss = 0.006873082021872203
step = 11, Training Accuracy: 0.9233333333333333
Training loss = 0.00836946075161298
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.007241305857896805
step = 13, Training Accuracy: 0.9366666666666666
Training loss = 0.008671148841579756
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.7375
params:  [0.19257362849196547, 0.4658639401171916, 0.16267233805425793, 0.13414469521819905, 0.41645661337501466, 0.1487454503194508, 0.9645603914898921, 0.44709118596022906, 0.9717823541800598, 0.8059249485098124, 0.5223085648613682, 0.5109636725917441, 0.7533159630628744, 0.01, 0.4653392939614557, 0.46095729358299586, 0.4006554156203294]
[0.19257362849196547, 0.4658639401171916, 0.16267233805425793, 0.13414469521819905, 0.41645661337501466, 0.1487454503194508, 0.9645603914898921, 0.44709118596022906, 0.9717823541800598, 0.8059249485098124, 0.5223085648613682, 0.5109636725917441, 0.7533159630628744, 0.01, 0.4653392939614557, 0.46095729358299586, 0.4006554156203294]
Training loss = 0.01822996517022451
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7425
Training loss = 0.013795015811920166
step = 1, Training Accuracy: 0.82
Training loss = 0.012215679536263149
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.011171219696601231
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.014110255390405654
step = 4, Training Accuracy: 0.84
Training loss = 0.012509040832519532
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.71125
Training loss = 0.009981726507345835
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.010698308944702148
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.010198285281658172
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.010882289161284765
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.009977156321207683
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.72625
Training loss = 0.009213568940758705
step = 11, Training Accuracy: 0.8633333333333333
Training loss = 0.009386249830325445
step = 12, Training Accuracy: 0.87
Training loss = 0.007766054645180702
step = 13, Training Accuracy: 0.93
Training loss = 0.009806507180134455
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.7375
params:  [0.01, 0.3877389858925656, 0.01, 0.01, 0.16069812024153915, 0.01, 0.40447424044200314, 0.31345977046478707, 0.99, 0.99, 0.401815210200432, 0.6909567905282482, 0.5647220571643708, 0.14051059610253172, 0.4162104148625325, 0.4544673829223702, 0.46422591458704215]
[0.01, 0.3877389858925656, 0.01, 0.01, 0.16069812024153915, 0.01, 0.40447424044200314, 0.31345977046478707, 0.99, 0.99, 0.401815210200432, 0.6909567905282482, 0.5647220571643708, 0.14051059610253172, 0.4162104148625325, 0.4544673829223702, 0.46422591458704215]
Training loss = 0.01458278571565946
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.74
Training loss = 0.01437993253270785
step = 1, Training Accuracy: 0.83
Training loss = 0.011885845214128493
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.011649621427059173
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.011145774722099305
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.01115965927640597
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7475
Training loss = 0.00896646648645401
step = 6, Training Accuracy: 0.89
Training loss = 0.009550066739320755
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.009579194436470668
step = 8, Training Accuracy: 0.86
Training loss = 0.008030843685070673
step = 9, Training Accuracy: 0.89
Training loss = 0.008573186397552491
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.73125
Training loss = 0.008377275864283244
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.007714954415957133
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.008147054215272268
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.0065184095998605095
step = 14, Training Accuracy: 0.94
Validation Accuracy: 0.74125
params:  [0.0883261011031898, 0.2475498576949793, 0.1950899837057502, 0.20366750608194245, 0.01, 0.01, 0.4891921774917475, 0.6632332240894714, 0.6969074508805763, 0.879360924230432, 0.432945090414434, 0.5416040881107909, 0.99, 0.24274457651999054, 0.7066252461507334, 0.27748571939338806, 0.5081862893334341]
[0.0883261011031898, 0.2475498576949793, 0.1950899837057502, 0.20366750608194245, 0.01, 0.01, 0.4891921774917475, 0.6632332240894714, 0.6969074508805763, 0.879360924230432, 0.432945090414434, 0.5416040881107909, 0.99, 0.24274457651999054, 0.7066252461507334, 0.27748571939338806, 0.5081862893334341]
Training loss = 0.012578591754039128
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.745
Training loss = 0.010060333361228307
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.011728763828674953
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.009387747744719187
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.008452997207641602
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.009001394808292389
step = 5, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.7425
Training loss = 0.006906690448522568
step = 6, Training Accuracy: 0.92
Training loss = 0.0070238970716794335
step = 7, Training Accuracy: 0.9166666666666666
Training loss = 0.00605646384259065
step = 8, Training Accuracy: 0.9433333333333334
Training loss = 0.006323991790413857
step = 9, Training Accuracy: 0.93
Training loss = 0.007654230097929636
step = 10, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.7575
Training loss = 0.00489350289106369
step = 11, Training Accuracy: 0.9533333333333334
Training loss = 0.0056309322143594425
step = 12, Training Accuracy: 0.9466666666666667
Training loss = 0.005688671320676804
step = 13, Training Accuracy: 0.9266666666666666
Training loss = 0.0053417441745599115
step = 14, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.7475
params:  [0.01580777748619719, 0.05327599532361352, 0.14874056052604628, 0.26515417438697, 0.21939969527806716, 0.2085804333120297, 0.8489058769985901, 0.1570792386270689, 0.926641505350017, 0.99, 0.012081269640664749, 0.5662170419759012, 0.5024784247395447, 0.2860524255652, 0.46432859855475167, 0.371588841222774, 0.527504831230626]
[0.01580777748619719, 0.05327599532361352, 0.14874056052604628, 0.26515417438697, 0.21939969527806716, 0.2085804333120297, 0.8489058769985901, 0.1570792386270689, 0.926641505350017, 0.99, 0.012081269640664749, 0.5662170419759012, 0.5024784247395447, 0.2860524255652, 0.46432859855475167, 0.371588841222774, 0.527504831230626]
Training loss = 0.01462092955907186
step = 0, Training Accuracy: 0.8066666666666666
Validation Accuracy: 0.7475
Training loss = 0.011662379602591196
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.010191468298435211
step = 2, Training Accuracy: 0.87
Training loss = 0.010430173029502233
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.010495174080133438
step = 4, Training Accuracy: 0.86
Training loss = 0.01054388627409935
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.75125
Training loss = 0.009749352981646855
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.00997505580385526
step = 7, Training Accuracy: 0.87
Training loss = 0.00933402806520462
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.008266488164663315
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.008872198859850565
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.75
Training loss = 0.008591602891683578
step = 11, Training Accuracy: 0.91
Training loss = 0.006092059724032879
step = 12, Training Accuracy: 0.9433333333333334
Training loss = 0.006785758361220359
step = 13, Training Accuracy: 0.9166666666666666
Training loss = 0.007142995943625768
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.7525
params:  [0.11735924652414575, 0.33932764106522745, 0.06789845132241033, 0.3472570020008726, 0.038329277914869085, 0.01, 0.6827883348708594, 0.5072973915870529, 0.99, 0.8195869908597911, 0.3216454014576005, 0.9159679872499724, 0.6778116906859963, 0.4033266922551455, 0.49043920685322834, 0.314074660020883, 0.5521489237224932]
[0.11735924652414575, 0.33932764106522745, 0.06789845132241033, 0.3472570020008726, 0.038329277914869085, 0.01, 0.6827883348708594, 0.5072973915870529, 0.99, 0.8195869908597911, 0.3216454014576005, 0.9159679872499724, 0.6778116906859963, 0.4033266922551455, 0.49043920685322834, 0.314074660020883, 0.5521489237224932]
Training loss = 0.01604409938057264
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.74875
Training loss = 0.011683085461457571
step = 1, Training Accuracy: 0.86
Training loss = 0.013637444178263347
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.011202281266450882
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.00952043483654658
step = 4, Training Accuracy: 0.9
Training loss = 0.010593878229459126
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.7625
Training loss = 0.00937477839489778
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.011505028729637464
step = 7, Training Accuracy: 0.88
Training loss = 0.009337452252705892
step = 8, Training Accuracy: 0.92
Training loss = 0.00835036906103293
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.008041844367980958
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.7625
Training loss = 0.00888362002869447
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.008342640896638234
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.007393811717629433
step = 13, Training Accuracy: 0.92
Training loss = 0.007299315606554349
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.76
params:  [0.26815416554882526, 0.01, 0.2278814291838469, 0.39024623573398287, 0.01, 0.21261697102909022, 0.4482927785231163, 0.5453189057839855, 0.9875139593335384, 0.6542998002422001, 0.01, 0.793163706504649, 0.5888476477107669, 0.1226549271959638, 0.43169975236271785, 0.0675005108634674, 0.5243494328168282]
[0.26815416554882526, 0.01, 0.2278814291838469, 0.39024623573398287, 0.01, 0.21261697102909022, 0.4482927785231163, 0.5453189057839855, 0.9875139593335384, 0.6542998002422001, 0.01, 0.793163706504649, 0.5888476477107669, 0.1226549271959638, 0.43169975236271785, 0.0675005108634674, 0.5243494328168282]
Training loss = 0.012497469931840897
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.76
Training loss = 0.00976144959529241
step = 1, Training Accuracy: 0.89
Training loss = 0.012184423754612605
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.011498442689577738
step = 3, Training Accuracy: 0.8633333333333333
Training loss = 0.008943764964739481
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.007064137384295463
step = 5, Training Accuracy: 0.92
Validation Accuracy: 0.74625
Training loss = 0.009651939769585928
step = 6, Training Accuracy: 0.9
Training loss = 0.00824021135767301
step = 7, Training Accuracy: 0.89
Training loss = 0.007726309473315875
step = 8, Training Accuracy: 0.91
Training loss = 0.007809863463044167
step = 9, Training Accuracy: 0.9266666666666666
Training loss = 0.007721561590830485
step = 10, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.76125
Training loss = 0.006513411576549212
step = 11, Training Accuracy: 0.92
Training loss = 0.005615478803714116
step = 12, Training Accuracy: 0.9433333333333334
Training loss = 0.005822400227189064
step = 13, Training Accuracy: 0.9366666666666666
Training loss = 0.005370850538214048
step = 14, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.75125
9  	8     	0.747188	0.00741488	0.7375 	0.76   
params:  [0.15204880157651932, 0.4140460855330178, 0.01, 0.597823910688136, 0.4023665801855327, 0.22181897671827472, 0.4267511519672297, 0.2988584934706156, 0.9397994434213787, 0.8919852174854205, 0.1869820366515617, 0.8740975060136397, 0.6102710923249838, 0.2999311169520077, 0.24259735147786662, 0.3433306704867026, 0.4916987800137804]
[0.15204880157651932, 0.4140460855330178, 0.01, 0.597823910688136, 0.4023665801855327, 0.22181897671827472, 0.4267511519672297, 0.2988584934706156, 0.9397994434213787, 0.8919852174854205, 0.1869820366515617, 0.8740975060136397, 0.6102710923249838, 0.2999311169520077, 0.24259735147786662, 0.3433306704867026, 0.4916987800137804]
Training loss = 0.01645416458447774
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.75375
Training loss = 0.01551491285363833
step = 1, Training Accuracy: 0.82
Training loss = 0.015315661678711573
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.01648979584376017
step = 3, Training Accuracy: 0.82
Training loss = 0.015680555204550424
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.012755863467852275
step = 5, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.7375
Training loss = 0.012783029327789942
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.012217987428108852
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.013806743621826172
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.01335686892271042
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.010048173914353052
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7275
Training loss = 0.011319254636764526
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.010391765683889389
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.0092168756822745
step = 13, Training Accuracy: 0.89
Training loss = 0.009349622378746668
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.73875
params:  [0.19084896500077436, 0.2943020200686096, 0.01, 0.5961962690088306, 0.01, 0.11205441543759066, 0.7262330725479471, 0.4636102380001922, 0.8846096736068552, 0.99, 0.10423303165961136, 0.46974226559157584, 0.785286732487525, 0.1775515867822538, 0.325966058882968, 0.39226184113150864, 0.9667403500876708]
[0.19084896500077436, 0.2943020200686096, 0.01, 0.5961962690088306, 0.01, 0.11205441543759066, 0.7262330725479471, 0.4636102380001922, 0.8846096736068552, 0.99, 0.10423303165961136, 0.46974226559157584, 0.785286732487525, 0.1775515867822538, 0.325966058882968, 0.39226184113150864, 0.9667403500876708]
Training loss = 0.011780829628308615
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74875
Training loss = 0.01301953395207723
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.01134944791595141
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.012030596633752187
step = 3, Training Accuracy: 0.8633333333333333
Training loss = 0.011602796912193299
step = 4, Training Accuracy: 0.87
Training loss = 0.010352731396754583
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.73375
Training loss = 0.008523935253421466
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.009630963106950123
step = 7, Training Accuracy: 0.8866666666666667
Training loss = 0.009274290601412456
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.008295205583175024
step = 9, Training Accuracy: 0.91
Training loss = 0.006956031719843546
step = 10, Training Accuracy: 0.9366666666666666
Validation Accuracy: 0.7325
Training loss = 0.008331317901611329
step = 11, Training Accuracy: 0.91
Training loss = 0.007906032875180245
step = 12, Training Accuracy: 0.9
Training loss = 0.00722299965719382
step = 13, Training Accuracy: 0.91
Training loss = 0.007124883234500885
step = 14, Training Accuracy: 0.92
Validation Accuracy: 0.74875
params:  [0.09140080990054383, 0.5054739381903993, 0.01, 0.38041502835593805, 0.10189872377166893, 0.21168653725938757, 0.43473467435785773, 0.3171410845382663, 0.99, 0.8606412351370019, 0.2586296245155445, 0.7673768894230228, 0.70788868519228, 0.4222359666358641, 0.4891925312891353, 0.26382972291558726, 0.38979798764072326]
[0.09140080990054383, 0.5054739381903993, 0.01, 0.38041502835593805, 0.10189872377166893, 0.21168653725938757, 0.43473467435785773, 0.3171410845382663, 0.99, 0.8606412351370019, 0.2586296245155445, 0.7673768894230228, 0.70788868519228, 0.4222359666358641, 0.4891925312891353, 0.26382972291558726, 0.38979798764072326]
Training loss = 0.014304273823897044
step = 0, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.74375
Training loss = 0.012416786054770151
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.010972629835208258
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.010205050061146418
step = 3, Training Accuracy: 0.86
Training loss = 0.00982715462644895
step = 4, Training Accuracy: 0.9
Training loss = 0.011583274056514103
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.73875
Training loss = 0.008060067941745122
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.007530328383048375
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.006732918048898379
step = 8, Training Accuracy: 0.92
Training loss = 0.008791383455197017
step = 9, Training Accuracy: 0.89
Training loss = 0.008513685415188472
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.74375
Training loss = 0.00733323169251283
step = 11, Training Accuracy: 0.9266666666666666
Training loss = 0.006274897356828054
step = 12, Training Accuracy: 0.9333333333333333
Training loss = 0.007132169579466184
step = 13, Training Accuracy: 0.94
Training loss = 0.007344310730695724
step = 14, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.7525
params:  [0.20122403979101364, 0.26183916389006673, 0.2815111227573759, 0.3875409125022066, 0.38683312423026467, 0.1732583897602814, 0.742211594907078, 0.1491607080127973, 0.5979134445811857, 0.99, 0.20668866786963966, 0.99, 0.99, 0.2691153991709665, 0.3626642172193527, 0.4665931683885957, 0.6095880494836219]
[0.20122403979101364, 0.26183916389006673, 0.2815111227573759, 0.3875409125022066, 0.38683312423026467, 0.1732583897602814, 0.742211594907078, 0.1491607080127973, 0.5979134445811857, 0.99, 0.20668866786963966, 0.99, 0.99, 0.2691153991709665, 0.3626642172193527, 0.4665931683885957, 0.6095880494836219]
Training loss = 0.02056210607290268
step = 0, Training Accuracy: 0.7766666666666666
Validation Accuracy: 0.73375
Training loss = 0.017964517176151277
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.014783185968796412
step = 2, Training Accuracy: 0.83
Training loss = 0.014149584472179413
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.012593204230070114
step = 4, Training Accuracy: 0.8633333333333333
Training loss = 0.01264498288432757
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.72625
Training loss = 0.013407689929008483
step = 6, Training Accuracy: 0.84
Training loss = 0.013669190406799316
step = 7, Training Accuracy: 0.82
Training loss = 0.014451429496208828
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.011909367044766745
step = 9, Training Accuracy: 0.8366666666666667
Training loss = 0.012498985330263774
step = 10, Training Accuracy: 0.86
Validation Accuracy: 0.73
Training loss = 0.010579612106084824
step = 11, Training Accuracy: 0.87
Training loss = 0.01161589468518893
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.011558184176683426
step = 13, Training Accuracy: 0.8666666666666667
Training loss = 0.00991770659883817
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.73375
params:  [0.01, 0.24804203983133533, 0.1512087021915555, 0.0949815974868462, 0.16513105136179485, 0.01, 0.7690152190875043, 0.36618955581339346, 0.868813643070259, 0.99, 0.01, 0.99, 0.8195579394272052, 0.6861285666976777, 0.7466069657258504, 0.3475525472930272, 0.5798771294174256]
[0.01, 0.24804203983133533, 0.1512087021915555, 0.0949815974868462, 0.16513105136179485, 0.01, 0.7690152190875043, 0.36618955581339346, 0.868813643070259, 0.99, 0.01, 0.99, 0.8195579394272052, 0.6861285666976777, 0.7466069657258504, 0.3475525472930272, 0.5798771294174256]
Training loss = 0.014452404379844665
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.73125
Training loss = 0.014669074018796285
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.014771910657485326
step = 2, Training Accuracy: 0.83
Training loss = 0.012663531551758448
step = 3, Training Accuracy: 0.84
Training loss = 0.012989857792854309
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.009850636273622513
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.75
Training loss = 0.00988621031244596
step = 6, Training Accuracy: 0.89
Training loss = 0.010669903258482615
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.009379835724830627
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.009112343589464823
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.008523644804954529
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.755
Training loss = 0.00794397455950578
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.00947123224536578
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.00811286217222611
step = 13, Training Accuracy: 0.87
Training loss = 0.007132972925901413
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.73625
params:  [0.01, 0.407154011373127, 0.05128468866893485, 0.5098073230210697, 0.01, 0.16720059844522586, 0.7617661471710584, 0.6153544217837156, 0.99, 0.9029512179300023, 0.16560029641067278, 0.614287967403141, 0.44162216770351376, 0.40878029358103596, 0.6178594916947004, 0.3072323063931509, 0.6379504624037636]
[0.01, 0.407154011373127, 0.05128468866893485, 0.5098073230210697, 0.01, 0.16720059844522586, 0.7617661471710584, 0.6153544217837156, 0.99, 0.9029512179300023, 0.16560029641067278, 0.614287967403141, 0.44162216770351376, 0.40878029358103596, 0.6178594916947004, 0.3072323063931509, 0.6379504624037636]
Training loss = 0.015078943396608036
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74125
Training loss = 0.012287729034821192
step = 1, Training Accuracy: 0.8733333333333333
Training loss = 0.010758795191844304
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.010665421187877656
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.00933348536491394
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.008447690854469935
step = 5, Training Accuracy: 0.92
Validation Accuracy: 0.75125
Training loss = 0.010949246287345886
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.008240209966897965
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.007641916945576668
step = 8, Training Accuracy: 0.92
Training loss = 0.007609408001104991
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.009994285504023233
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.7375
Training loss = 0.006215839261809985
step = 11, Training Accuracy: 0.9333333333333333
Training loss = 0.006459679466982683
step = 12, Training Accuracy: 0.9233333333333333
Training loss = 0.006481735905011495
step = 13, Training Accuracy: 0.94
Training loss = 0.006216378808021545
step = 14, Training Accuracy: 0.9466666666666667
Validation Accuracy: 0.735
params:  [0.01, 0.2378609073131386, 0.01, 0.06873310077123951, 0.1343388381061594, 0.12197491396833508, 0.5249009138386589, 0.5770040516017261, 0.99, 0.774195495190533, 0.01, 0.606237583168465, 0.8641863762634424, 0.5598164187693064, 0.6485309993041767, 0.26278786590971076, 0.5261668065468995]
[0.01, 0.2378609073131386, 0.01, 0.06873310077123951, 0.1343388381061594, 0.12197491396833508, 0.5249009138386589, 0.5770040516017261, 0.99, 0.774195495190533, 0.01, 0.606237583168465, 0.8641863762634424, 0.5598164187693064, 0.6485309993041767, 0.26278786590971076, 0.5261668065468995]
Training loss = 0.015594269931316376
step = 0, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.7425
Training loss = 0.014067013909419378
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.014283783535162608
step = 2, Training Accuracy: 0.81
Training loss = 0.011875115384658177
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.012078955272833506
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.01205707957347234
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.745
Training loss = 0.00932866354783376
step = 6, Training Accuracy: 0.9133333333333333
Training loss = 0.00912652038037777
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.009581646124521892
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.009257892121871312
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.009355418582757314
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75
Training loss = 0.008526453375816345
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.009161296933889389
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.007762502133846283
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.008973227391640345
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.73875
params:  [0.3509016875425869, 0.025914519374950595, 0.19933401741255236, 0.31702586854079673, 0.17426154054505216, 0.3847283144114491, 0.5863265981914044, 0.26591665832836703, 0.8325936146105899, 0.8482579730255432, 0.21626293879624078, 0.6753203787247846, 0.6062606864047533, 0.46609696203686984, 0.4422086371570311, 0.5223056207714812, 0.3468055393664462]
[0.3509016875425869, 0.025914519374950595, 0.19933401741255236, 0.31702586854079673, 0.17426154054505216, 0.3847283144114491, 0.5863265981914044, 0.26591665832836703, 0.8325936146105899, 0.8482579730255432, 0.21626293879624078, 0.6753203787247846, 0.6062606864047533, 0.46609696203686984, 0.4422086371570311, 0.5223056207714812, 0.3468055393664462]
Training loss = 0.014521661351124445
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.74
Training loss = 0.014440288345019023
step = 1, Training Accuracy: 0.8266666666666667
Training loss = 0.013490763852993647
step = 2, Training Accuracy: 0.84
Training loss = 0.011791100849707921
step = 3, Training Accuracy: 0.8466666666666667
Training loss = 0.012843684603770575
step = 4, Training Accuracy: 0.84
Training loss = 0.013091415961583456
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.74875
Training loss = 0.01231446569164594
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.01090936596194903
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.011721145709355672
step = 8, Training Accuracy: 0.87
Training loss = 0.009306468317906063
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.009138510326544443
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7475
Training loss = 0.010880095859368642
step = 11, Training Accuracy: 0.87
Training loss = 0.010788768927256265
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.010968909511963526
step = 13, Training Accuracy: 0.8733333333333333
Training loss = 0.009491577992836635
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7375
10 	8     	0.740156	0.00632571	0.73375	0.7525 
params:  [0.03769266771636198, 0.5109404615509207, 0.2697610969547999, 0.4891664296215294, 0.01, 0.10698165862003263, 0.3529589116641667, 0.2069821646669055, 0.8999637093134657, 0.9082695376292006, 0.14396013572859115, 0.8094832424427838, 0.5685629913163979, 0.5989305702518362, 0.4131242898631347, 0.2666537908952152, 0.5725940064035383]
[0.03769266771636198, 0.5109404615509207, 0.2697610969547999, 0.4891664296215294, 0.01, 0.10698165862003263, 0.3529589116641667, 0.2069821646669055, 0.8999637093134657, 0.9082695376292006, 0.14396013572859115, 0.8094832424427838, 0.5685629913163979, 0.5989305702518362, 0.4131242898631347, 0.2666537908952152, 0.5725940064035383]
Training loss = 0.011117513279120127
step = 0, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.73625
Training loss = 0.010544410943984985
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.009917164494593938
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.009941157549619675
step = 3, Training Accuracy: 0.88
Training loss = 0.008141146798928578
step = 4, Training Accuracy: 0.9166666666666666
Training loss = 0.007988713632027309
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.74
Training loss = 0.007698211868604025
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.0071792812397082646
step = 7, Training Accuracy: 0.92
Training loss = 0.005582947904864947
step = 8, Training Accuracy: 0.9466666666666667
Training loss = 0.008356497238079706
step = 9, Training Accuracy: 0.9
Training loss = 0.006752013141910235
step = 10, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.74125
Training loss = 0.006560415054361025
step = 11, Training Accuracy: 0.9166666666666666
Training loss = 0.006761593694488207
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.0058191431562105815
step = 13, Training Accuracy: 0.9266666666666666
Training loss = 0.006570945084095001
step = 14, Training Accuracy: 0.9366666666666666
Validation Accuracy: 0.75
params:  [0.01, 0.5112262891506292, 0.01, 0.37565369442276203, 0.12136684490807798, 0.01, 0.3778051839668468, 0.19801151640154746, 0.8032964026040021, 0.658482218366188, 0.3294716404958705, 0.7315067101081528, 0.6665662135750693, 0.4063101710410465, 0.4846842738507612, 0.4570577745182661, 0.37572136558482017]
[0.01, 0.5112262891506292, 0.01, 0.37565369442276203, 0.12136684490807798, 0.01, 0.3778051839668468, 0.19801151640154746, 0.8032964026040021, 0.658482218366188, 0.3294716404958705, 0.7315067101081528, 0.6665662135750693, 0.4063101710410465, 0.4846842738507612, 0.4570577745182661, 0.37572136558482017]
Training loss = 0.01772908796866735
step = 0, Training Accuracy: 0.7866666666666666
Validation Accuracy: 0.75125
Training loss = 0.015360181232293447
step = 1, Training Accuracy: 0.8
Training loss = 0.014937925140062968
step = 2, Training Accuracy: 0.81
Training loss = 0.01418306733171145
step = 3, Training Accuracy: 0.83
Training loss = 0.01361387938261032
step = 4, Training Accuracy: 0.8266666666666667
Training loss = 0.00985556721687317
step = 5, Training Accuracy: 0.91
Validation Accuracy: 0.745
Training loss = 0.010387705117464065
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.01106560414036115
step = 7, Training Accuracy: 0.88
Training loss = 0.012330433030923208
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.01096950113773346
step = 9, Training Accuracy: 0.88
Training loss = 0.009180463800827663
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.75
Training loss = 0.007638607372840245
step = 11, Training Accuracy: 0.9266666666666666
Training loss = 0.009419119308392206
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.007705711672703425
step = 13, Training Accuracy: 0.9166666666666666
Training loss = 0.009454717586437861
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.75
params:  [0.2120918533429479, 0.45834568067474485, 0.013977036162994165, 0.4481409256618275, 0.20986670538045443, 0.04292316913269795, 0.402936722051202, 0.0649194480903949, 0.9498853818942214, 0.8959154710617425, 0.16174924124135048, 0.8403164494502644, 0.5032102767590161, 0.393623672155775, 0.4085667562388758, 0.26370095805491867, 0.5954694562550252]
[0.2120918533429479, 0.45834568067474485, 0.013977036162994165, 0.4481409256618275, 0.20986670538045443, 0.04292316913269795, 0.402936722051202, 0.0649194480903949, 0.9498853818942214, 0.8959154710617425, 0.16174924124135048, 0.8403164494502644, 0.5032102767590161, 0.393623672155775, 0.4085667562388758, 0.26370095805491867, 0.5954694562550252]
Training loss = 0.014439325183629989
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.76125
Training loss = 0.014065978626410167
step = 1, Training Accuracy: 0.84
Training loss = 0.014580036302407583
step = 2, Training Accuracy: 0.83
Training loss = 0.01357450008392334
step = 3, Training Accuracy: 0.8166666666666667
Training loss = 0.012870333393414816
step = 4, Training Accuracy: 0.83
Training loss = 0.011558101574579874
step = 5, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.7425
Training loss = 0.013348622073729832
step = 6, Training Accuracy: 0.85
Training loss = 0.012785003781318664
step = 7, Training Accuracy: 0.8433333333333334
Training loss = 0.012926736325025558
step = 8, Training Accuracy: 0.8266666666666667
Training loss = 0.012340693126122156
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.012496496786673864
step = 10, Training Accuracy: 0.83
Validation Accuracy: 0.74375
Training loss = 0.010873034720619519
step = 11, Training Accuracy: 0.86
Training loss = 0.01210384468237559
step = 12, Training Accuracy: 0.83
Training loss = 0.010211105744043986
step = 13, Training Accuracy: 0.8666666666666667
Training loss = 0.009236183563868204
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.7475
params:  [0.21846678265686786, 0.3211834351080404, 0.01, 0.5802180809721684, 0.01, 0.40744658433850345, 0.6856174960947599, 0.4721636798847722, 0.99, 0.6110752273579854, 0.01, 0.7239251899493327, 0.9372263171665423, 0.21566025712573483, 0.2174575653787456, 0.24586214922674454, 0.719183011410326]
[0.21846678265686786, 0.3211834351080404, 0.01, 0.5802180809721684, 0.01, 0.40744658433850345, 0.6856174960947599, 0.4721636798847722, 0.99, 0.6110752273579854, 0.01, 0.7239251899493327, 0.9372263171665423, 0.21566025712573483, 0.2174575653787456, 0.24586214922674454, 0.719183011410326]
Training loss = 0.014668073952198029
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.74
Training loss = 0.013952148258686065
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.01722413276632627
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.010785986681779226
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.01275613635778427
step = 4, Training Accuracy: 0.8366666666666667
Training loss = 0.01261676013469696
step = 5, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.7175
Training loss = 0.011025310754776
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.011364494562149047
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.01013885830839475
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.010691192299127579
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.010648780961831411
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.725
Training loss = 0.010179805705944697
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.009821826815605164
step = 12, Training Accuracy: 0.9033333333333333
Training loss = 0.008866530656814576
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.009494278406103451
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.72375
params:  [0.2662679918373133, 0.6234655431396208, 0.17931448057739718, 0.5075684019542602, 0.14094874054887047, 0.5111629827573676, 0.5408312961684596, 0.327241454300023, 0.99, 0.5809487615540052, 0.40928932420918596, 0.6071726505516144, 0.7622948675536867, 0.4784505474466908, 0.25582376566420206, 0.01, 0.8020869171449294]
[0.2662679918373133, 0.6234655431396208, 0.17931448057739718, 0.5075684019542602, 0.14094874054887047, 0.5111629827573676, 0.5408312961684596, 0.327241454300023, 0.99, 0.5809487615540052, 0.40928932420918596, 0.6071726505516144, 0.7622948675536867, 0.4784505474466908, 0.25582376566420206, 0.01, 0.8020869171449294]
Training loss = 0.01539196734627088
step = 0, Training Accuracy: 0.81
Validation Accuracy: 0.72375
Training loss = 0.018426431516806285
step = 1, Training Accuracy: 0.7966666666666666
Training loss = 0.014418313801288605
step = 2, Training Accuracy: 0.83
Training loss = 0.014280715237061183
step = 3, Training Accuracy: 0.81
Training loss = 0.013851175755262376
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.012901971886555353
step = 5, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.73625
Training loss = 0.014227423965930939
step = 6, Training Accuracy: 0.83
Training loss = 0.012689030667146047
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.013553337355454763
step = 8, Training Accuracy: 0.8366666666666667
Training loss = 0.012955011526743571
step = 9, Training Accuracy: 0.84
Training loss = 0.012111069063345591
step = 10, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.72875
Training loss = 0.011736322343349457
step = 11, Training Accuracy: 0.84
Training loss = 0.01313683122396469
step = 12, Training Accuracy: 0.8433333333333334
Training loss = 0.01221210241317749
step = 13, Training Accuracy: 0.8533333333333334
Training loss = 0.01091401512424151
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.73375
params:  [0.2064984082219538, 0.1903772051560384, 0.01, 0.5889161991400818, 0.14905628783528635, 0.3004009716451093, 0.35142185956275085, 0.5027966920423361, 0.8460568209243541, 0.8768839429713202, 0.01, 0.5488444334305262, 0.8220851637531122, 0.13494954345485027, 0.3914035953351953, 0.5951385891502363, 0.384356356492175]
[0.2064984082219538, 0.1903772051560384, 0.01, 0.5889161991400818, 0.14905628783528635, 0.3004009716451093, 0.35142185956275085, 0.5027966920423361, 0.8460568209243541, 0.8768839429713202, 0.01, 0.5488444334305262, 0.8220851637531122, 0.13494954345485027, 0.3914035953351953, 0.5951385891502363, 0.384356356492175]
Training loss = 0.016196484168370565
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.73375
Training loss = 0.013721692164738973
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.012243433594703675
step = 2, Training Accuracy: 0.86
Training loss = 0.013122465759515763
step = 3, Training Accuracy: 0.8466666666666667
Training loss = 0.011527088582515716
step = 4, Training Accuracy: 0.85
Training loss = 0.01204275334874789
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.75125
Training loss = 0.011856079598267873
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.011932728787263234
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.00947574220597744
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.009770984624822935
step = 9, Training Accuracy: 0.87
Training loss = 0.010300735831260682
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.76875
Training loss = 0.009620946943759917
step = 11, Training Accuracy: 0.86
Training loss = 0.009346086407701175
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.009007692684729894
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.008835853884617488
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.77
params:  [0.01, 0.2732889311905044, 0.2870050122489867, 0.24037410118445202, 0.01, 0.01, 0.7869353883697034, 0.48940135934830076, 0.99, 0.9896654290707356, 0.3132135880820587, 0.45265156243553384, 0.99, 0.23701287187272335, 0.237522693733141, 0.17512439967460505, 0.7541084155464952]
[0.01, 0.2732889311905044, 0.2870050122489867, 0.24037410118445202, 0.01, 0.01, 0.7869353883697034, 0.48940135934830076, 0.99, 0.9896654290707356, 0.3132135880820587, 0.45265156243553384, 0.99, 0.23701287187272335, 0.237522693733141, 0.17512439967460505, 0.7541084155464952]
Training loss = 0.015358957002560297
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.7675
Training loss = 0.012274291614691417
step = 1, Training Accuracy: 0.8333333333333334
Training loss = 0.011533632626136143
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.011119444072246552
step = 3, Training Accuracy: 0.86
Training loss = 0.008972076674302418
step = 4, Training Accuracy: 0.9
Training loss = 0.010129410723845165
step = 5, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.75125
Training loss = 0.008829966833194097
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.008199483156204224
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.009104197472333908
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.007118177016576131
step = 9, Training Accuracy: 0.9066666666666666
Training loss = 0.007869389355182648
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.73625
Training loss = 0.00772155483563741
step = 11, Training Accuracy: 0.9266666666666666
Training loss = 0.00721724917491277
step = 12, Training Accuracy: 0.91
Training loss = 0.007493104736010233
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.006924971776703994
step = 14, Training Accuracy: 0.93
Validation Accuracy: 0.74875
params:  [0.2406239580499095, 0.43506627817412963, 0.01, 0.5313064469845976, 0.24709981271582132, 0.01, 0.21347820519124555, 0.6145019002656529, 0.8575650982865803, 0.953812331046621, 0.01, 0.31330683009385685, 0.99, 0.5822477683360896, 0.6653103988923244, 0.22414189512699267, 0.2674913855524702]
[0.2406239580499095, 0.43506627817412963, 0.01, 0.5313064469845976, 0.24709981271582132, 0.01, 0.21347820519124555, 0.6145019002656529, 0.8575650982865803, 0.953812331046621, 0.01, 0.31330683009385685, 0.99, 0.5822477683360896, 0.6653103988923244, 0.22414189512699267, 0.2674913855524702]
Training loss = 0.01812030702829361
step = 0, Training Accuracy: 0.7666666666666667
Validation Accuracy: 0.75125
Training loss = 0.015491382579008738
step = 1, Training Accuracy: 0.7933333333333333
Training loss = 0.01347809041539828
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.012236027518908182
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.01589273810386658
step = 4, Training Accuracy: 0.7733333333333333
Training loss = 0.012179764260848363
step = 5, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.75625
Training loss = 0.012587431768576305
step = 6, Training Accuracy: 0.83
Training loss = 0.011032502899567287
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.012117358644803365
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.0107362728814284
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.010372880746920903
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.76125
Training loss = 0.010565056403477987
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.009197898308436077
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.011154577632745107
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.011321480721235275
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.75
11 	8     	0.746719	0.0126234 	0.72375	0.77   
params:  [0.16007978116048335, 0.28085117101820756, 0.11955526418354442, 0.3400977464899708, 0.01, 0.3790606514302519, 0.46969819635758814, 0.5784606442466182, 0.7079164273707854, 0.99, 0.01, 0.5724102984172689, 0.7386747354113213, 0.2624554697005575, 0.6002146515014416, 0.4139491197842346, 0.4784936387337619]
[0.16007978116048335, 0.28085117101820756, 0.11955526418354442, 0.3400977464899708, 0.01, 0.3790606514302519, 0.46969819635758814, 0.5784606442466182, 0.7079164273707854, 0.99, 0.01, 0.5724102984172689, 0.7386747354113213, 0.2624554697005575, 0.6002146515014416, 0.4139491197842346, 0.4784936387337619]
Training loss = 0.015672518859306973
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.74875
Training loss = 0.011497909451524417
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.009954415758450826
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.01113608588774999
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.010133017003536225
step = 4, Training Accuracy: 0.88
Training loss = 0.010239825944105784
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.7575
Training loss = 0.009315232833226522
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.009693423708279927
step = 7, Training Accuracy: 0.88
Training loss = 0.00781703529258569
step = 8, Training Accuracy: 0.9233333333333333
Training loss = 0.008797696928183238
step = 9, Training Accuracy: 0.89
Training loss = 0.00784931610027949
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.74125
Training loss = 0.006451789662241936
step = 11, Training Accuracy: 0.9366666666666666
Training loss = 0.008855856458346049
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.0074382977684338885
step = 13, Training Accuracy: 0.92
Training loss = 0.0076643753051757815
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.75125
params:  [0.3244094215604721, 0.47274544911792193, 0.01, 0.5671928478308268, 0.12580165246107267, 0.017626517560184823, 0.4712212623523676, 0.17118533370433897, 0.5940383074189823, 0.7761598208817582, 0.15628651956749712, 0.689367302123163, 0.8590787451985874, 0.319443579013116, 0.4444724336997007, 0.4481390397734845, 0.22868330415547997]
[0.3244094215604721, 0.47274544911792193, 0.01, 0.5671928478308268, 0.12580165246107267, 0.017626517560184823, 0.4712212623523676, 0.17118533370433897, 0.5940383074189823, 0.7761598208817582, 0.15628651956749712, 0.689367302123163, 0.8590787451985874, 0.319443579013116, 0.4444724336997007, 0.4481390397734845, 0.22868330415547997]
Training loss = 0.014802822669347127
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.75125
Training loss = 0.014702334652344386
step = 1, Training Accuracy: 0.81
Training loss = 0.014893306195735931
step = 2, Training Accuracy: 0.8
Training loss = 0.014664839406808218
step = 3, Training Accuracy: 0.8033333333333333
Training loss = 0.012858548065026601
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.012275475809971492
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.7475
Training loss = 0.012823388576507569
step = 6, Training Accuracy: 0.8433333333333334
Training loss = 0.012028393099705378
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.01061589151620865
step = 8, Training Accuracy: 0.8433333333333334
Training loss = 0.012821617871522903
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.010197358926137289
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74625
Training loss = 0.010718909551699957
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.009846172680457434
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.011187870502471924
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.010716400891542435
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.74625
params:  [0.21592176299268434, 0.47178088505381305, 0.01, 0.7230909061906046, 0.20215617181152523, 0.17199758657794828, 0.3272024948376948, 0.4509245458462492, 0.9880083300053147, 0.99, 0.027640902687119717, 0.7555699260283444, 0.8224169390516949, 0.2127989507873899, 0.44831116812888333, 0.3448391983025383, 0.48047737537920654]
[0.21592176299268434, 0.47178088505381305, 0.01, 0.7230909061906046, 0.20215617181152523, 0.17199758657794828, 0.3272024948376948, 0.4509245458462492, 0.9880083300053147, 0.99, 0.027640902687119717, 0.7555699260283444, 0.8224169390516949, 0.2127989507873899, 0.44831116812888333, 0.3448391983025383, 0.48047737537920654]
Training loss = 0.014032984574635823
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.75
Training loss = 0.014238675634066264
step = 1, Training Accuracy: 0.84
Training loss = 0.014097977379957835
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.013776087363560995
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.013652328699827194
step = 4, Training Accuracy: 0.8333333333333334
Training loss = 0.010301772058010101
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.76
Training loss = 0.011959068576494853
step = 6, Training Accuracy: 0.86
Training loss = 0.010966777702172597
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.010095177566011746
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.009762115776538849
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.009403985937436422
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7525
Training loss = 0.009202804416418076
step = 11, Training Accuracy: 0.89
Training loss = 0.009412417511145273
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.009481363892555238
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.010354309429725012
step = 14, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.75
params:  [0.01, 0.5596386439264086, 0.01, 0.3353975560355447, 0.057660311353264805, 0.4415001839214738, 0.6021602970141382, 0.08452708896202926, 0.99, 0.8106585897601546, 0.01, 0.5651793125472311, 0.5506661051821382, 0.11316821848887026, 0.27878641238388446, 0.49247684618285625, 0.4360715930154589]
[0.01, 0.5596386439264086, 0.01, 0.3353975560355447, 0.057660311353264805, 0.4415001839214738, 0.6021602970141382, 0.08452708896202926, 0.99, 0.8106585897601546, 0.01, 0.5651793125472311, 0.5506661051821382, 0.11316821848887026, 0.27878641238388446, 0.49247684618285625, 0.4360715930154589]
Training loss = 0.014515691300233205
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.75375
Training loss = 0.012949538628260295
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.012980791827042898
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.01246408631404241
step = 3, Training Accuracy: 0.85
Training loss = 0.011384959916273753
step = 4, Training Accuracy: 0.8866666666666667
Training loss = 0.010183262328306833
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.75
Training loss = 0.010117173492908478
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.012422494192918142
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.009456096142530441
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.010238417039314905
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.008991171270608903
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.75
Training loss = 0.009038577675819398
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.00887988785902659
step = 12, Training Accuracy: 0.89
Training loss = 0.00941695918639501
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.007305610626935959
step = 14, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.75
params:  [0.01, 0.3529247396988884, 0.01, 0.4515200126081347, 0.01, 0.16528084839735274, 0.41448076214095253, 0.053675221316313415, 0.765979959546624, 0.9817116829644952, 0.01, 0.41987588644177787, 0.7085002598221254, 0.38265710763096056, 0.075132464165024, 0.3375826374504567, 0.5168136751910227]
[0.01, 0.3529247396988884, 0.01, 0.4515200126081347, 0.01, 0.16528084839735274, 0.41448076214095253, 0.053675221316313415, 0.765979959546624, 0.9817116829644952, 0.01, 0.41987588644177787, 0.7085002598221254, 0.38265710763096056, 0.075132464165024, 0.3375826374504567, 0.5168136751910227]
Training loss = 0.012931447327136993
step = 0, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.745
Training loss = 0.011268846094608306
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.011133003880580266
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.01004937137166659
step = 3, Training Accuracy: 0.9
Training loss = 0.008660317758719126
step = 4, Training Accuracy: 0.9166666666666666
Training loss = 0.007765791341662407
step = 5, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.75625
Training loss = 0.008514087796211243
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.009162540286779404
step = 7, Training Accuracy: 0.9066666666666666
Training loss = 0.007929585898915927
step = 8, Training Accuracy: 0.9266666666666666
Training loss = 0.007746549844741821
step = 9, Training Accuracy: 0.9233333333333333
Training loss = 0.007512880563735962
step = 10, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.75625
Training loss = 0.006616815254092217
step = 11, Training Accuracy: 0.92
Training loss = 0.007381615117192268
step = 12, Training Accuracy: 0.93
Training loss = 0.006587408035993576
step = 13, Training Accuracy: 0.9333333333333333
Training loss = 0.0074167175094286605
step = 14, Training Accuracy: 0.93
Validation Accuracy: 0.755
params:  [0.19157604503314485, 0.5416413167069105, 0.01, 0.6358846762440266, 0.11683472478961622, 0.3448624241590648, 0.46379534104066106, 0.3789350809518849, 0.9154719121248234, 0.9248496170459874, 0.01, 0.7636955509993884, 0.5779566309412982, 0.17023070427008385, 0.49423996042321183, 0.7462319735706806, 0.2329586101993387]
[0.19157604503314485, 0.5416413167069105, 0.01, 0.6358846762440266, 0.11683472478961622, 0.3448624241590648, 0.46379534104066106, 0.3789350809518849, 0.9154719121248234, 0.9248496170459874, 0.01, 0.7636955509993884, 0.5779566309412982, 0.17023070427008385, 0.49423996042321183, 0.7462319735706806, 0.2329586101993387]
Training loss = 0.01428475817044576
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.75125
Training loss = 0.012284326155980428
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.013337146242459614
step = 2, Training Accuracy: 0.8566666666666667
Training loss = 0.012041126986344656
step = 3, Training Accuracy: 0.8333333333333334
Training loss = 0.010397602121035259
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.009102897544701893
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74375
Training loss = 0.009916548430919648
step = 6, Training Accuracy: 0.87
Training loss = 0.009116632491350174
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.01109990065296491
step = 8, Training Accuracy: 0.84
Training loss = 0.011726031949122747
step = 9, Training Accuracy: 0.8566666666666667
Training loss = 0.00873339315255483
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.7475
Training loss = 0.008747704128424327
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.009050328135490417
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.00915310025215149
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.0075150677561759945
step = 14, Training Accuracy: 0.9366666666666666
Validation Accuracy: 0.7425
params:  [0.2754378971236572, 0.466928455293248, 0.01, 0.35799630808819527, 0.01, 0.3896093655963938, 0.5700653862730742, 0.5890922009629149, 0.9893669413131307, 0.7421155244212589, 0.02483916104665118, 0.804584773848155, 0.7313105994908177, 0.4160335948936244, 0.2862902512189605, 0.5531595317578184, 0.4433054688538918]
[0.2754378971236572, 0.466928455293248, 0.01, 0.35799630808819527, 0.01, 0.3896093655963938, 0.5700653862730742, 0.5890922009629149, 0.9893669413131307, 0.7421155244212589, 0.02483916104665118, 0.804584773848155, 0.7313105994908177, 0.4160335948936244, 0.2862902512189605, 0.5531595317578184, 0.4433054688538918]
Training loss = 0.010343546730776627
step = 0, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7425
Training loss = 0.01247252568602562
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.012227877502640089
step = 2, Training Accuracy: 0.8466666666666667
Training loss = 0.009731391618649166
step = 3, Training Accuracy: 0.9133333333333333
Training loss = 0.008952846129735311
step = 4, Training Accuracy: 0.89
Training loss = 0.008850714514652887
step = 5, Training Accuracy: 0.9
Validation Accuracy: 0.7375
Training loss = 0.009754943599303564
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.008363885482152304
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.007452981372674306
step = 8, Training Accuracy: 0.8966666666666666
Training loss = 0.007259149154027303
step = 9, Training Accuracy: 0.93
Training loss = 0.00785790890455246
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.74
Training loss = 0.007446197470029195
step = 11, Training Accuracy: 0.91
Training loss = 0.008391983757416408
step = 12, Training Accuracy: 0.92
Training loss = 0.006828964004913966
step = 13, Training Accuracy: 0.9166666666666666
Training loss = 0.007465876018007596
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.74
params:  [0.11796069478407493, 0.3109928347728144, 0.3187585104581278, 0.37225569962771693, 0.1935884724609812, 0.5796559062730666, 0.456154073042657, 0.40727850823461237, 0.9045471451372968, 0.7772165579937393, 0.01, 0.647294069013413, 0.7521245130002003, 0.35548396956376416, 0.13477620482697977, 0.18172283447987153, 0.3239841182790544]
[0.11796069478407493, 0.3109928347728144, 0.3187585104581278, 0.37225569962771693, 0.1935884724609812, 0.5796559062730666, 0.456154073042657, 0.40727850823461237, 0.9045471451372968, 0.7772165579937393, 0.01, 0.647294069013413, 0.7521245130002003, 0.35548396956376416, 0.13477620482697977, 0.18172283447987153, 0.3239841182790544]
Training loss = 0.013429075082143149
step = 0, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.745
Training loss = 0.012712895919879277
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.012151419421037039
step = 2, Training Accuracy: 0.87
Training loss = 0.0095823402206103
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.007816979611913364
step = 4, Training Accuracy: 0.9166666666666666
Training loss = 0.009329888274272283
step = 5, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.7475
Training loss = 0.008241087421774864
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.008131805807352066
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.007138495594263077
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.0069071859618028
step = 9, Training Accuracy: 0.93
Training loss = 0.00813093143204848
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.75375
Training loss = 0.0058979094649354614
step = 11, Training Accuracy: 0.94
Training loss = 0.008166870127121608
step = 12, Training Accuracy: 0.92
Training loss = 0.006663069787124793
step = 13, Training Accuracy: 0.93
Training loss = 0.007189333438873291
step = 14, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.74125
12 	8     	0.747031	0.00503649	0.74   	0.755  
params:  [0.29397934160821115, 0.1944551850505378, 0.0498669534689166, 0.4043681972296142, 0.01, 0.4154305348441434, 0.4024496367944697, 0.517310166894409, 0.7077388333293738, 0.8592222251405068, 0.08027007584398782, 0.5150440284591665, 0.7115604296296392, 0.34448972385256377, 0.1941306361925102, 0.3889925321497427, 0.5004458987851679]
[0.29397934160821115, 0.1944551850505378, 0.0498669534689166, 0.4043681972296142, 0.01, 0.4154305348441434, 0.4024496367944697, 0.517310166894409, 0.7077388333293738, 0.8592222251405068, 0.08027007584398782, 0.5150440284591665, 0.7115604296296392, 0.34448972385256377, 0.1941306361925102, 0.3889925321497427, 0.5004458987851679]
Training loss = 0.010277143369118372
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.75125
Training loss = 0.012941359629233679
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.010799238532781601
step = 2, Training Accuracy: 0.9
Training loss = 0.0112118066350619
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.009696260889371236
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.007833562592665355
step = 5, Training Accuracy: 0.93
Validation Accuracy: 0.76375
Training loss = 0.008792719542980195
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.00896672176818053
step = 7, Training Accuracy: 0.9066666666666666
Training loss = 0.009565727561712265
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.008514419570565224
step = 9, Training Accuracy: 0.9
Training loss = 0.007965029055873553
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.75875
Training loss = 0.008381451964378357
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.007221042215824127
step = 12, Training Accuracy: 0.93
Training loss = 0.008583338111639022
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.006661105106274287
step = 14, Training Accuracy: 0.94
Validation Accuracy: 0.76
params:  [0.01, 0.2091275084012938, 0.01, 0.5449171835915805, 0.1295253251636922, 0.05603299895763034, 0.44946166557202083, 0.28263268236956207, 0.99, 0.99, 0.045195693702070355, 0.5653409399272357, 0.46437978837341387, 0.05727551626909316, 0.4204186312961824, 0.09098891285578048, 0.6694403655636754]
[0.01, 0.2091275084012938, 0.01, 0.5449171835915805, 0.1295253251636922, 0.05603299895763034, 0.44946166557202083, 0.28263268236956207, 0.99, 0.99, 0.045195693702070355, 0.5653409399272357, 0.46437978837341387, 0.05727551626909316, 0.4204186312961824, 0.09098891285578048, 0.6694403655636754]
Training loss = 0.011372940838336945
step = 0, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.76125
Training loss = 0.010671681314706803
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.01002249076962471
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.008922365208466848
step = 3, Training Accuracy: 0.9166666666666666
Training loss = 0.00733005627989769
step = 4, Training Accuracy: 0.91
Training loss = 0.008506205876668295
step = 5, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.74875
Training loss = 0.007769391586383184
step = 6, Training Accuracy: 0.9
Training loss = 0.007760418852170308
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.006398421972990036
step = 8, Training Accuracy: 0.9333333333333333
Training loss = 0.006761505305767059
step = 9, Training Accuracy: 0.9066666666666666
Training loss = 0.006025216343502204
step = 10, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.7525
Training loss = 0.008531716565291086
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.005884418735901515
step = 12, Training Accuracy: 0.9366666666666666
Training loss = 0.00566661157955726
step = 13, Training Accuracy: 0.95
Training loss = 0.0050740797072649
step = 14, Training Accuracy: 0.9433333333333334
Validation Accuracy: 0.75125
params:  [0.06048063609490348, 0.45637908766547536, 0.01, 0.44639515990415607, 0.13667664706912175, 0.28380757031968284, 0.2695439329778716, 0.3390487750813602, 0.7315576016666636, 0.9189731179285525, 0.14470764994786628, 0.43030908201901835, 0.6173395713684691, 0.4087635505446772, 0.29701595211187826, 0.6868337075310464, 0.5483239014370599]
[0.06048063609490348, 0.45637908766547536, 0.01, 0.44639515990415607, 0.13667664706912175, 0.28380757031968284, 0.2695439329778716, 0.3390487750813602, 0.7315576016666636, 0.9189731179285525, 0.14470764994786628, 0.43030908201901835, 0.6173395713684691, 0.4087635505446772, 0.29701595211187826, 0.6868337075310464, 0.5483239014370599]
Training loss = 0.013409513930479686
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.74875
Training loss = 0.015325827846924463
step = 1, Training Accuracy: 0.84
Training loss = 0.011432513445615768
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.009327740470568339
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.010194123685359956
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.01037981428205967
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7525
Training loss = 0.008053607642650604
step = 6, Training Accuracy: 0.91
Training loss = 0.009338617771863938
step = 7, Training Accuracy: 0.91
Training loss = 0.007007598305741946
step = 8, Training Accuracy: 0.9266666666666666
Training loss = 0.007330398658911387
step = 9, Training Accuracy: 0.9266666666666666
Training loss = 0.007612658937772115
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.74
Training loss = 0.007556592524051667
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.007699759428699811
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.008555780226985613
step = 13, Training Accuracy: 0.91
Training loss = 0.008506591940919558
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.735
params:  [0.11921956146710905, 0.4759162313107591, 0.01, 0.7054430948605444, 0.01, 0.21618037662587733, 0.42449018832058916, 0.21869248209552272, 0.8273078356131759, 0.9183035686723338, 0.01, 0.713053007186464, 0.554684512726611, 0.3112170005322441, 0.2129851745030435, 0.3654241133549653, 0.5383507637811223]
[0.11921956146710905, 0.4759162313107591, 0.01, 0.7054430948605444, 0.01, 0.21618037662587733, 0.42449018832058916, 0.21869248209552272, 0.8273078356131759, 0.9183035686723338, 0.01, 0.713053007186464, 0.554684512726611, 0.3112170005322441, 0.2129851745030435, 0.3654241133549653, 0.5383507637811223]
Training loss = 0.009669061576326687
step = 0, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.73125
Training loss = 0.013080583959817886
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.01049052357673645
step = 2, Training Accuracy: 0.87
Training loss = 0.01006505052248637
step = 3, Training Accuracy: 0.88
Training loss = 0.008613092585777244
step = 4, Training Accuracy: 0.9033333333333333
Training loss = 0.00919474557042122
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.73125
Training loss = 0.009639035736521084
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.007995405991872151
step = 7, Training Accuracy: 0.9266666666666666
Training loss = 0.009017107685407004
step = 8, Training Accuracy: 0.8966666666666666
Training loss = 0.008209587931632996
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.007260314598679543
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.7325
Training loss = 0.007834742789467176
step = 11, Training Accuracy: 0.89
Training loss = 0.006631581634283065
step = 12, Training Accuracy: 0.92
Training loss = 0.007321460669239362
step = 13, Training Accuracy: 0.9266666666666666
Training loss = 0.006265531455477078
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.7275
params:  [0.01, 0.4050180488376238, 0.01, 0.2999563852342745, 0.01, 0.2337718486585618, 0.4424935541225678, 0.7155915867594642, 0.8125689205921653, 0.99, 0.01, 0.5491555794669577, 0.9572084566715742, 0.30107938604616985, 0.2986402496798562, 0.5298443123839918, 0.602678954844709]
[0.01, 0.4050180488376238, 0.01, 0.2999563852342745, 0.01, 0.2337718486585618, 0.4424935541225678, 0.7155915867594642, 0.8125689205921653, 0.99, 0.01, 0.5491555794669577, 0.9572084566715742, 0.30107938604616985, 0.2986402496798562, 0.5298443123839918, 0.602678954844709]
Training loss = 0.014469855278730393
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.7325
Training loss = 0.012558524832129478
step = 1, Training Accuracy: 0.86
Training loss = 0.010566459049781164
step = 2, Training Accuracy: 0.88
Training loss = 0.00998772864540418
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.008941033780574799
step = 4, Training Accuracy: 0.8933333333333333
Training loss = 0.008380552753806114
step = 5, Training Accuracy: 0.9
Validation Accuracy: 0.73625
Training loss = 0.008415875633557638
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.00812262753645579
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.009302347054084142
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.008526915560166042
step = 9, Training Accuracy: 0.9233333333333333
Training loss = 0.008191566914319992
step = 10, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.73625
Training loss = 0.008223240027825037
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.008087074259916942
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.007309749846657118
step = 13, Training Accuracy: 0.92
Training loss = 0.00840193897485733
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.73375
params:  [0.27296532195173684, 0.31442173250938354, 0.01, 0.7065489932452084, 0.17903335278879978, 0.21421209691134457, 0.1122531642495298, 0.23722725006121148, 0.5992829987299222, 0.99, 0.05977539825654063, 0.315504484177323, 0.6583404831134522, 0.08209520389874483, 0.5292824142920507, 0.21708568744118145, 0.23857047839308576]
[0.27296532195173684, 0.31442173250938354, 0.01, 0.7065489932452084, 0.17903335278879978, 0.21421209691134457, 0.1122531642495298, 0.23722725006121148, 0.5992829987299222, 0.99, 0.05977539825654063, 0.315504484177323, 0.6583404831134522, 0.08209520389874483, 0.5292824142920507, 0.21708568744118145, 0.23857047839308576]
Training loss = 0.012748831460873286
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74375
Training loss = 0.013174339532852172
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.011440754383802414
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.008149608373641967
step = 3, Training Accuracy: 0.92
Training loss = 0.008183610662817954
step = 4, Training Accuracy: 0.9133333333333333
Training loss = 0.008847493256131808
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.74
Training loss = 0.007816008254885674
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.007308246369163196
step = 7, Training Accuracy: 0.92
Training loss = 0.007109757785995801
step = 8, Training Accuracy: 0.91
Training loss = 0.007744071508447329
step = 9, Training Accuracy: 0.91
Training loss = 0.007861895461877186
step = 10, Training Accuracy: 0.93
Validation Accuracy: 0.7475
Training loss = 0.005907666931549708
step = 11, Training Accuracy: 0.93
Training loss = 0.005776250213384629
step = 12, Training Accuracy: 0.93
Training loss = 0.008434227605660757
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.006676245828469594
step = 14, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.73625
params:  [0.01, 0.28210991231577603, 0.14471296797403937, 0.4384514305329037, 0.15555846566224735, 0.2702308393587761, 0.20897155102003712, 0.15702837472498365, 0.7042399152823432, 0.6921840340585439, 0.19043824392593728, 0.09971109109313142, 0.6560493731900175, 0.507561284774723, 0.1874816612984856, 0.29402189255212197, 0.6652762239764758]
[0.01, 0.28210991231577603, 0.14471296797403937, 0.4384514305329037, 0.15555846566224735, 0.2702308393587761, 0.20897155102003712, 0.15702837472498365, 0.7042399152823432, 0.6921840340585439, 0.19043824392593728, 0.09971109109313142, 0.6560493731900175, 0.507561284774723, 0.1874816612984856, 0.29402189255212197, 0.6652762239764758]
Training loss = 0.011117556393146514
step = 0, Training Accuracy: 0.83
Validation Accuracy: 0.7425
Training loss = 0.009788298557202021
step = 1, Training Accuracy: 0.8933333333333333
Training loss = 0.008722611467043558
step = 2, Training Accuracy: 0.88
Training loss = 0.010109867503245672
step = 3, Training Accuracy: 0.9
Training loss = 0.00799867091079553
step = 4, Training Accuracy: 0.8933333333333333
Training loss = 0.008821299821138382
step = 5, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.75
Training loss = 0.00924306000272433
step = 6, Training Accuracy: 0.8966666666666666
Training loss = 0.008831233407060305
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.0098101706802845
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.0074325695137182875
step = 9, Training Accuracy: 0.9266666666666666
Training loss = 0.00877518614133199
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.74625
Training loss = 0.008166735892494519
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.00795847515265147
step = 12, Training Accuracy: 0.89
Training loss = 0.008001320076485475
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.008287295227249464
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.75125
params:  [0.02480701078357024, 0.19469635189085405, 0.03797398633283879, 0.4014575401548126, 0.37781553384940053, 0.2980146117652469, 0.624062961477299, 0.257342348729113, 0.99, 0.99, 0.10820244754801871, 0.3720958827669929, 0.6505751685939103, 0.29755738372112883, 0.01, 0.14050361748178802, 0.41813108743321425]
[0.02480701078357024, 0.19469635189085405, 0.03797398633283879, 0.4014575401548126, 0.37781553384940053, 0.2980146117652469, 0.624062961477299, 0.257342348729113, 0.99, 0.99, 0.10820244754801871, 0.3720958827669929, 0.6505751685939103, 0.29755738372112883, 0.01, 0.14050361748178802, 0.41813108743321425]
Training loss = 0.011472783237695693
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.76375
Training loss = 0.0119353619714578
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.010831508835156758
step = 2, Training Accuracy: 0.8766666666666667
Training loss = 0.009217492838700613
step = 3, Training Accuracy: 0.88
Training loss = 0.009376889218886693
step = 4, Training Accuracy: 0.89
Training loss = 0.00801151712735494
step = 5, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.7375
Training loss = 0.007639021972815196
step = 6, Training Accuracy: 0.9066666666666666
Training loss = 0.007713372707366943
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.007402071381608645
step = 8, Training Accuracy: 0.91
Training loss = 0.006974740847945213
step = 9, Training Accuracy: 0.9133333333333333
Training loss = 0.006564368506272634
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.74625
Training loss = 0.007821799665689468
step = 11, Training Accuracy: 0.9
Training loss = 0.008735377093156178
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.00789512075483799
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.006899630626042684
step = 14, Training Accuracy: 0.9366666666666666
Validation Accuracy: 0.74625
13 	8     	0.742656	0.010429  	0.7275 	0.76   
params:  [0.12058880706386582, 0.14289683508429932, 0.18848475952409316, 0.3977794423631794, 0.01, 0.29416278222289066, 0.30328997492980825, 0.30476147505935935, 0.8576077755208221, 0.7548810455268573, 0.1106576050479877, 0.3607487446555781, 0.6052844240301389, 0.1301783135131891, 0.0995130267725553, 0.13668217998291424, 0.5381981130366021]
[0.12058880706386582, 0.14289683508429932, 0.18848475952409316, 0.3977794423631794, 0.01, 0.29416278222289066, 0.30328997492980825, 0.30476147505935935, 0.8576077755208221, 0.7548810455268573, 0.1106576050479877, 0.3607487446555781, 0.6052844240301389, 0.1301783135131891, 0.0995130267725553, 0.13668217998291424, 0.5381981130366021]
Training loss = 0.009806855320930482
step = 0, Training Accuracy: 0.9
Validation Accuracy: 0.75
Training loss = 0.01014631450176239
step = 1, Training Accuracy: 0.9066666666666666
Training loss = 0.008048296471436818
step = 2, Training Accuracy: 0.9133333333333333
Training loss = 0.007567350591222446
step = 3, Training Accuracy: 0.9
Training loss = 0.007427561456958453
step = 4, Training Accuracy: 0.9233333333333333
Training loss = 0.006601927168667317
step = 5, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.7475
Training loss = 0.006819134975473086
step = 6, Training Accuracy: 0.9466666666666667
Training loss = 0.007519091665744782
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.006692423944671949
step = 8, Training Accuracy: 0.93
Training loss = 0.007950552105903626
step = 9, Training Accuracy: 0.89
Training loss = 0.006329643800854683
step = 10, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.7575
Training loss = 0.0069768732041120525
step = 11, Training Accuracy: 0.91
Training loss = 0.005770928959051768
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.007106983785827955
step = 13, Training Accuracy: 0.91
Training loss = 0.006324354410171509
step = 14, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.745
params:  [0.14959780785302113, 0.18299755596405795, 0.1355884645730081, 0.6327090057957623, 0.28332568430109834, 0.3028139807741608, 0.5329085890166096, 0.43893934151793373, 0.9102432274943039, 0.6582615405328949, 0.10657834880100807, 0.6322433729054807, 0.45397526818303213, 0.21983300782796394, 0.31613809133138243, 0.3371989898061841, 0.6396067521328086]
[0.14959780785302113, 0.18299755596405795, 0.1355884645730081, 0.6327090057957623, 0.28332568430109834, 0.3028139807741608, 0.5329085890166096, 0.43893934151793373, 0.9102432274943039, 0.6582615405328949, 0.10657834880100807, 0.6322433729054807, 0.45397526818303213, 0.21983300782796394, 0.31613809133138243, 0.3371989898061841, 0.6396067521328086]
Training loss = 0.012882418582836787
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7475
Training loss = 0.01271779532233874
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.015854563415050506
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.013522756695747375
step = 3, Training Accuracy: 0.85
Training loss = 0.011017563243707021
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.01011906678477923
step = 5, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.7375
Training loss = 0.011575060735146204
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.011354971478382747
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.011486276884873707
step = 8, Training Accuracy: 0.85
Training loss = 0.009976852536201477
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.010102271363139152
step = 10, Training Accuracy: 0.86
Validation Accuracy: 0.7475
Training loss = 0.009614085505406062
step = 11, Training Accuracy: 0.88
Training loss = 0.00940946638584137
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.01095604660610358
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.014271205464998882
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.74
params:  [0.11539837309866441, 0.22333373098957154, 0.28916514568335555, 0.3725389423821117, 0.01, 0.09876822310589753, 0.5223983118001065, 0.5163591957389722, 0.7688513297778027, 0.8337934641361366, 0.01, 0.30597741079111074, 0.6053795702553021, 0.08391389656587703, 0.18396749223891523, 0.07332064195378404, 0.44769819600589683]
[0.11539837309866441, 0.22333373098957154, 0.28916514568335555, 0.3725389423821117, 0.01, 0.09876822310589753, 0.5223983118001065, 0.5163591957389722, 0.7688513297778027, 0.8337934641361366, 0.01, 0.30597741079111074, 0.6053795702553021, 0.08391389656587703, 0.18396749223891523, 0.07332064195378404, 0.44769819600589683]
Training loss = 0.012362735842665036
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74125
Training loss = 0.010166909272472064
step = 1, Training Accuracy: 0.8766666666666667
Training loss = 0.010568406532208125
step = 2, Training Accuracy: 0.88
Training loss = 0.00935603529214859
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.009270898848772049
step = 4, Training Accuracy: 0.8933333333333333
Training loss = 0.009096353401740392
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.735
Training loss = 0.009246280044317245
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.00843451033035914
step = 7, Training Accuracy: 0.9066666666666666
Training loss = 0.006527053080499172
step = 8, Training Accuracy: 0.93
Training loss = 0.006740367238720258
step = 9, Training Accuracy: 0.9266666666666666
Training loss = 0.007271659622589747
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.74625
Training loss = 0.006545573696494103
step = 11, Training Accuracy: 0.9166666666666666
Training loss = 0.006403109033902486
step = 12, Training Accuracy: 0.92
Training loss = 0.005951156814893087
step = 13, Training Accuracy: 0.9366666666666666
Training loss = 0.005661299414932728
step = 14, Training Accuracy: 0.9333333333333333
Validation Accuracy: 0.75125
params:  [0.14648595333009118, 0.10945151628587566, 0.12917341154336337, 0.6728289998710267, 0.12163314005628156, 0.43280948065573016, 0.46900266087854037, 0.6075392472848176, 0.99, 0.8394380780770857, 0.03744688792910634, 0.45974626840870086, 0.4654134461482888, 0.01, 0.2969660581531468, 0.2658594475258949, 0.5638019739197385]
[0.14648595333009118, 0.10945151628587566, 0.12917341154336337, 0.6728289998710267, 0.12163314005628156, 0.43280948065573016, 0.46900266087854037, 0.6075392472848176, 0.99, 0.8394380780770857, 0.03744688792910634, 0.45974626840870086, 0.4654134461482888, 0.01, 0.2969660581531468, 0.2658594475258949, 0.5638019739197385]
Training loss = 0.012198994606733323
step = 0, Training Accuracy: 0.8166666666666667
Validation Accuracy: 0.74
Training loss = 0.009386419405539831
step = 1, Training Accuracy: 0.8766666666666667
Training loss = 0.009396737118562063
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.00893359770377477
step = 3, Training Accuracy: 0.87
Training loss = 0.009877815544605255
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.007976688245932262
step = 5, Training Accuracy: 0.9
Validation Accuracy: 0.73625
Training loss = 0.00820891241232554
step = 6, Training Accuracy: 0.9133333333333333
Training loss = 0.007473610863089561
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.00809475839138031
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.007810267607371012
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.007368171215057373
step = 10, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.73625
Training loss = 0.007975457360347112
step = 11, Training Accuracy: 0.9
Training loss = 0.007630448738733927
step = 12, Training Accuracy: 0.92
Training loss = 0.006672926532725493
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.006046048104763031
step = 14, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.73
params:  [0.3469258898423696, 0.36513111919726593, 0.08719249091549777, 0.45342445627217653, 0.17539917579728928, 0.2770824568183261, 0.6569482393386941, 0.23514173845873004, 0.8709086299981269, 0.9216498251380861, 0.1552207125308061, 0.45937184387955515, 0.5098437866282644, 0.21529734327776534, 0.39064196599839807, 0.24512145364385624, 0.7957908010535824]
[0.3469258898423696, 0.36513111919726593, 0.08719249091549777, 0.45342445627217653, 0.17539917579728928, 0.2770824568183261, 0.6569482393386941, 0.23514173845873004, 0.8709086299981269, 0.9216498251380861, 0.1552207125308061, 0.45937184387955515, 0.5098437866282644, 0.21529734327776534, 0.39064196599839807, 0.24512145364385624, 0.7957908010535824]
Training loss = 0.014738869816064835
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.74
Training loss = 0.015427055756251017
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.014763306279977163
step = 2, Training Accuracy: 0.8266666666666667
Training loss = 0.01311609203616778
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.010658219009637832
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.010931860556205113
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.7475
Training loss = 0.012779138286908468
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.010284494211276373
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.009529326111078262
step = 8, Training Accuracy: 0.9
Training loss = 0.011186827048659325
step = 9, Training Accuracy: 0.8566666666666667
Training loss = 0.01143286183476448
step = 10, Training Accuracy: 0.88
Validation Accuracy: 0.76375
Training loss = 0.00900070128341516
step = 11, Training Accuracy: 0.9166666666666666
Training loss = 0.009975374986728033
step = 12, Training Accuracy: 0.88
Training loss = 0.009128229916095734
step = 13, Training Accuracy: 0.91
Training loss = 0.008625165273745855
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.765
params:  [0.01, 0.19765445391982978, 0.18200731816216995, 0.47707444327538623, 0.12926880482942732, 0.29418347533448613, 0.3949983903148064, 0.5259174723273964, 0.7294814787844099, 0.9661510023163803, 0.09759512969523171, 0.32312216823103923, 0.48349050861067416, 0.2770826524947989, 0.44173758620373743, 0.14896447613792324, 0.6375692819069773]
[0.01, 0.19765445391982978, 0.18200731816216995, 0.47707444327538623, 0.12926880482942732, 0.29418347533448613, 0.3949983903148064, 0.5259174723273964, 0.7294814787844099, 0.9661510023163803, 0.09759512969523171, 0.32312216823103923, 0.48349050861067416, 0.2770826524947989, 0.44173758620373743, 0.14896447613792324, 0.6375692819069773]
Training loss = 0.014534538785616558
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.77
Training loss = 0.012994287014007568
step = 1, Training Accuracy: 0.8366666666666667
Training loss = 0.01391357123851776
step = 2, Training Accuracy: 0.83
Training loss = 0.011381141593058903
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.011387977302074432
step = 4, Training Accuracy: 0.86
Training loss = 0.011774997562170028
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.755
Training loss = 0.012381995419661203
step = 6, Training Accuracy: 0.86
Training loss = 0.011341207176446915
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.01009480540951093
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.009630694588025412
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.009977657347917557
step = 10, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.7575
Training loss = 0.00960001620153586
step = 11, Training Accuracy: 0.88
Training loss = 0.009785601943731308
step = 12, Training Accuracy: 0.9
Training loss = 0.009802818397680919
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.009359643658002218
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.74875
params:  [0.21761957594195847, 0.12071576867782605, 0.01, 0.4232252739798881, 0.015827369696653576, 0.31766251181836597, 0.3374287767212431, 0.2563263788242288, 0.7021119250487391, 0.9145908301592112, 0.04745290929735653, 0.5794184223375145, 0.8370439097075224, 0.295325486710112, 0.29545123541140267, 0.5892281912893091, 0.3201250929452138]
[0.21761957594195847, 0.12071576867782605, 0.01, 0.4232252739798881, 0.015827369696653576, 0.31766251181836597, 0.3374287767212431, 0.2563263788242288, 0.7021119250487391, 0.9145908301592112, 0.04745290929735653, 0.5794184223375145, 0.8370439097075224, 0.295325486710112, 0.29545123541140267, 0.5892281912893091, 0.3201250929452138]
Training loss = 0.010623764817913373
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74375
Training loss = 0.011365664899349212
step = 1, Training Accuracy: 0.85
Training loss = 0.011526050741473833
step = 2, Training Accuracy: 0.85
Training loss = 0.010834348797798156
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.010573939581712087
step = 4, Training Accuracy: 0.87
Training loss = 0.009497300535440446
step = 5, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.7475
Training loss = 0.009566418826580048
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.008921148677666982
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.00862893501917521
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.009020342379808427
step = 9, Training Accuracy: 0.9
Training loss = 0.008576244115829468
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.74625
Training loss = 0.008652030279239019
step = 11, Training Accuracy: 0.9
Training loss = 0.007869148130218188
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.008495707660913468
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.010450952450434366
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.745
params:  [0.2957834988840968, 0.20194615954075626, 0.06867166833253999, 0.5084606560448374, 0.022771670669823954, 0.31194952793438413, 0.2411262186767712, 0.3003884924964564, 0.7084280350865745, 0.9696114233510578, 0.01, 0.6165490992790803, 0.5767140177503305, 0.6364464132390806, 0.1018466999044354, 0.30023378698840064, 0.7458333087611266]
[0.2957834988840968, 0.20194615954075626, 0.06867166833253999, 0.5084606560448374, 0.022771670669823954, 0.31194952793438413, 0.2411262186767712, 0.3003884924964564, 0.7084280350865745, 0.9696114233510578, 0.01, 0.6165490992790803, 0.5767140177503305, 0.6364464132390806, 0.1018466999044354, 0.30023378698840064, 0.7458333087611266]
Training loss = 0.012605927834908168
step = 0, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.745
Training loss = 0.01314197450876236
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.012398306205868721
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.011058744291464489
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.010870909839868546
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.010392363866170247
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.75625
Training loss = 0.009132236018776893
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.01129959871371587
step = 7, Training Accuracy: 0.88
Training loss = 0.010253656357526779
step = 8, Training Accuracy: 0.87
Training loss = 0.010053665389617284
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.009304151386022568
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7575
Training loss = 0.009235672851403554
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.008028774509827296
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.009420189137260119
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.010114936431248982
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.755
14 	8     	0.7475  	0.00970261	0.73   	0.765  
params:  [0.1489384269236612, 0.4782055083634409, 0.1560659692054564, 0.24308412532827112, 0.1739084515311689, 0.38463434500028837, 0.5942010321552824, 0.2874660842297861, 0.818504200106811, 0.8947628055538951, 0.01, 0.44475079088517006, 0.4932739943232957, 0.4916976410212816, 0.2176664315740765, 0.3346624151862487, 0.6367170996220912]
[0.1489384269236612, 0.4782055083634409, 0.1560659692054564, 0.24308412532827112, 0.1739084515311689, 0.38463434500028837, 0.5942010321552824, 0.2874660842297861, 0.818504200106811, 0.8947628055538951, 0.01, 0.44475079088517006, 0.4932739943232957, 0.4916976410212816, 0.2176664315740765, 0.3346624151862487, 0.6367170996220912]
Training loss = 0.010089598894119263
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.7625
Training loss = 0.010947668353716532
step = 1, Training Accuracy: 0.8766666666666667
Training loss = 0.013210465212663015
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.010513208036621412
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.010157467375199
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.008708199535806973
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75625
Training loss = 0.007680222019553184
step = 6, Training Accuracy: 0.9066666666666666
Training loss = 0.011242279907067616
step = 7, Training Accuracy: 0.86
Training loss = 0.010193992455800374
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.007196267147858938
step = 9, Training Accuracy: 0.92
Training loss = 0.008506644318501155
step = 10, Training Accuracy: 0.9266666666666666
Validation Accuracy: 0.76875
Training loss = 0.008645164594054223
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.008614526316523551
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.007154313611487548
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.0062347706158955895
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7625
params:  [0.317782977979274, 0.2448241577995792, 0.12845585404045531, 0.6805319085965494, 0.13392280718710575, 0.4800469024968539, 0.48908840550983207, 0.056241633652193895, 0.7058572413225775, 0.99, 0.22738436656807198, 0.4264539124297747, 0.6555414053490566, 0.3074596114630148, 0.13376929346084174, 0.16378483587037002, 0.8175629663777669]
[0.317782977979274, 0.2448241577995792, 0.12845585404045531, 0.6805319085965494, 0.13392280718710575, 0.4800469024968539, 0.48908840550983207, 0.056241633652193895, 0.7058572413225775, 0.99, 0.22738436656807198, 0.4264539124297747, 0.6555414053490566, 0.3074596114630148, 0.13376929346084174, 0.16378483587037002, 0.8175629663777669]
Training loss = 0.011147492329279581
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.76
Training loss = 0.01293046588699023
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.009517133831977845
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.012594184925158819
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.010617436468601226
step = 4, Training Accuracy: 0.8666666666666667
Training loss = 0.010058733349045118
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75625
Training loss = 0.011004376833637556
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.010916842768589655
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.010077220896879832
step = 8, Training Accuracy: 0.88
Training loss = 0.010161816477775573
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.010789305518070856
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.75
Training loss = 0.009258086358507474
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.00954600582520167
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.009526402801275253
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.008189584016799928
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.75125
params:  [0.3690910517445645, 0.1964451524657847, 0.34617339967656646, 0.41012224921382046, 0.1761851492448795, 0.28823345522555477, 0.4333473340442976, 0.43109836434953475, 0.6576434016612298, 0.9013815924765906, 0.12551782820837995, 0.5153275899009441, 0.5033934823882734, 0.30237278148527325, 0.11453721714178194, 0.4022382815985731, 0.5534578977591903]
[0.3690910517445645, 0.1964451524657847, 0.34617339967656646, 0.41012224921382046, 0.1761851492448795, 0.28823345522555477, 0.4333473340442976, 0.43109836434953475, 0.6576434016612298, 0.9013815924765906, 0.12551782820837995, 0.5153275899009441, 0.5033934823882734, 0.30237278148527325, 0.11453721714178194, 0.4022382815985731, 0.5534578977591903]
Training loss = 0.011590401381254197
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.75625
Training loss = 0.010846323668956756
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.011504002436995507
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.011782890409231186
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.011104461749394735
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.01023400214811166
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.73625
Training loss = 0.01062141756216685
step = 6, Training Accuracy: 0.9066666666666666
Training loss = 0.010845436652501424
step = 7, Training Accuracy: 0.91
Training loss = 0.008590137561162312
step = 8, Training Accuracy: 0.9133333333333333
Training loss = 0.009952362428108852
step = 9, Training Accuracy: 0.89
Training loss = 0.009441022922595342
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.75
Training loss = 0.00962502248585224
step = 11, Training Accuracy: 0.89
Training loss = 0.010494470496972401
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.007855178117752075
step = 13, Training Accuracy: 0.91
Training loss = 0.009105374813079834
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.75375
params:  [0.5369712342643362, 0.3480166329828366, 0.07045008422569801, 0.6050984220768953, 0.3750652989606747, 0.3065728810650083, 0.29421139554165365, 0.2946051049430353, 0.6910777195641997, 0.7888622225061757, 0.06291220765073391, 0.4593679685658261, 0.6700301320651225, 0.6113792567936127, 0.46201510262421985, 0.0826590268053499, 0.7117895357138841]
[0.5369712342643362, 0.3480166329828366, 0.07045008422569801, 0.6050984220768953, 0.3750652989606747, 0.3065728810650083, 0.29421139554165365, 0.2946051049430353, 0.6910777195641997, 0.7888622225061757, 0.06291220765073391, 0.4593679685658261, 0.6700301320651225, 0.6113792567936127, 0.46201510262421985, 0.0826590268053499, 0.7117895357138841]
Training loss = 0.019291735192139944
step = 0, Training Accuracy: 0.7233333333333334
Validation Accuracy: 0.75
Training loss = 0.0158427623907725
step = 1, Training Accuracy: 0.76
Training loss = 0.016649644573529562
step = 2, Training Accuracy: 0.7766666666666666
Training loss = 0.015471160411834717
step = 3, Training Accuracy: 0.7966666666666666
Training loss = 0.015374096781015397
step = 4, Training Accuracy: 0.8033333333333333
Training loss = 0.01567379762729009
step = 5, Training Accuracy: 0.7833333333333333
Validation Accuracy: 0.75625
Training loss = 0.014648511260747909
step = 6, Training Accuracy: 0.7833333333333333
Training loss = 0.01533707042535146
step = 7, Training Accuracy: 0.77
Training loss = 0.01575888256231944
step = 8, Training Accuracy: 0.7966666666666666
Training loss = 0.013863557676474254
step = 9, Training Accuracy: 0.78
Training loss = 0.013618458310763042
step = 10, Training Accuracy: 0.82
Validation Accuracy: 0.75125
Training loss = 0.013377488056818643
step = 11, Training Accuracy: 0.82
Training loss = 0.01420408790310224
step = 12, Training Accuracy: 0.84
Training loss = 0.013408405582110088
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.013821018586556117
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.75
params:  [0.3554209189622246, 0.3221811885037244, 0.024355916449811818, 0.4089105705325487, 0.11487828735033795, 0.2346875042767778, 0.5921105407042125, 0.28484589242419095, 0.9112584557200061, 0.99, 0.2939370393717147, 0.34722776974579456, 0.5249489281330446, 0.33414534717898975, 0.31087326127239145, 0.27320701620616694, 0.5653969708880681]
[0.3554209189622246, 0.3221811885037244, 0.024355916449811818, 0.4089105705325487, 0.11487828735033795, 0.2346875042767778, 0.5921105407042125, 0.28484589242419095, 0.9112584557200061, 0.99, 0.2939370393717147, 0.34722776974579456, 0.5249489281330446, 0.33414534717898975, 0.31087326127239145, 0.27320701620616694, 0.5653969708880681]
Training loss = 0.0102839861313502
step = 0, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.75125
Training loss = 0.011545732220013937
step = 1, Training Accuracy: 0.85
Training loss = 0.01035045159359773
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.008859218508005142
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.010848395129044851
step = 4, Training Accuracy: 0.88
Training loss = 0.01167969341079394
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.7375
Training loss = 0.010812898228565852
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.010054545799891153
step = 7, Training Accuracy: 0.8866666666666667
Training loss = 0.010543800070881843
step = 8, Training Accuracy: 0.87
Training loss = 0.010695335119962693
step = 9, Training Accuracy: 0.88
Training loss = 0.010031855727235476
step = 10, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.7375
Training loss = 0.008954634716113408
step = 11, Training Accuracy: 0.8966666666666666
Training loss = 0.008933843225240707
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.009102738350629807
step = 13, Training Accuracy: 0.88
Training loss = 0.008358202005426089
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.745
params:  [0.33321970151674146, 0.29810777093910623, 0.16543970595124294, 0.28202547216558166, 0.0694093334364626, 0.25235956807885696, 0.6078509951864487, 0.13907241617131336, 0.699271218281714, 0.6890330481235538, 0.3697868458093062, 0.40427687902896614, 0.6032873528907646, 0.3035178822709025, 0.13105869376165377, 0.22618423868678203, 0.8052046992754373]
[0.33321970151674146, 0.29810777093910623, 0.16543970595124294, 0.28202547216558166, 0.0694093334364626, 0.25235956807885696, 0.6078509951864487, 0.13907241617131336, 0.699271218281714, 0.6890330481235538, 0.3697868458093062, 0.40427687902896614, 0.6032873528907646, 0.3035178822709025, 0.13105869376165377, 0.22618423868678203, 0.8052046992754373]
Training loss = 0.011922258486350378
step = 0, Training Accuracy: 0.87
Validation Accuracy: 0.74
Training loss = 0.0130502982934316
step = 1, Training Accuracy: 0.83
Training loss = 0.010530711114406587
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.010746275335550308
step = 3, Training Accuracy: 0.8633333333333333
Training loss = 0.01085508073369662
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.011364623109499613
step = 5, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.73875
Training loss = 0.009519580118358136
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.01043969213962555
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.010637631267309188
step = 8, Training Accuracy: 0.88
Training loss = 0.009857074469327926
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.010722659627596538
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.73
Training loss = 0.01134178857008616
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.011727954496939978
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.009640319099028904
step = 13, Training Accuracy: 0.89
Training loss = 0.010638139645258586
step = 14, Training Accuracy: 0.86
Validation Accuracy: 0.74125
params:  [0.43238787660804384, 0.5363444162870643, 0.13049509920584212, 0.5417366872220635, 0.19039071685233483, 0.35783270017465385, 0.5978188038293, 0.18668578366712868, 0.8665599253940413, 0.9276730344902069, 0.01, 0.6194997360709149, 0.8011498213793844, 0.34474562574932993, 0.4147788218441121, 0.07028996588595227, 0.615602148547128]
[0.43238787660804384, 0.5363444162870643, 0.13049509920584212, 0.5417366872220635, 0.19039071685233483, 0.35783270017465385, 0.5978188038293, 0.18668578366712868, 0.8665599253940413, 0.9276730344902069, 0.01, 0.6194997360709149, 0.8011498213793844, 0.34474562574932993, 0.4147788218441121, 0.07028996588595227, 0.615602148547128]
Training loss = 0.010988329549630483
step = 0, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.74375
Training loss = 0.011451829224824905
step = 1, Training Accuracy: 0.83
Training loss = 0.01074291522304217
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.008698472529649734
step = 3, Training Accuracy: 0.88
Training loss = 0.01155331497391065
step = 4, Training Accuracy: 0.83
Training loss = 0.009593302309513092
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.735
Training loss = 0.010007175554831823
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.010825273692607879
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.010096771568059921
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.008828288316726685
step = 9, Training Accuracy: 0.9
Training loss = 0.008846052040656407
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.73875
Training loss = 0.009637299180030822
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.009629130537311237
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.008817564149697622
step = 13, Training Accuracy: 0.89
Training loss = 0.008398184726635616
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.7425
params:  [0.3094074309271528, 0.1642376962417132, 0.17947892305047086, 0.33756012776502997, 0.01, 0.28011424661399636, 0.4121828084211271, 0.1481793324905453, 0.7550305620275563, 0.9437369248993636, 0.04339950732389423, 0.3861392221155062, 0.6455614043753894, 0.36408920654846527, 0.5185494946076605, 0.27756680206551426, 0.962014938595721]
[0.3094074309271528, 0.1642376962417132, 0.17947892305047086, 0.33756012776502997, 0.01, 0.28011424661399636, 0.4121828084211271, 0.1481793324905453, 0.7550305620275563, 0.9437369248993636, 0.04339950732389423, 0.3861392221155062, 0.6455614043753894, 0.36408920654846527, 0.5185494946076605, 0.27756680206551426, 0.962014938595721]
Training loss = 0.010606094847122828
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.7475
Training loss = 0.008445047338803609
step = 1, Training Accuracy: 0.8866666666666667
Training loss = 0.009159287785490354
step = 2, Training Accuracy: 0.88
Training loss = 0.008913539697726567
step = 3, Training Accuracy: 0.9
Training loss = 0.00830356016755104
step = 4, Training Accuracy: 0.89
Training loss = 0.008597783992687862
step = 5, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.745
Training loss = 0.00647584542632103
step = 6, Training Accuracy: 0.9266666666666666
Training loss = 0.007746408954262734
step = 7, Training Accuracy: 0.9033333333333333
Training loss = 0.009018132636944454
step = 8, Training Accuracy: 0.89
Training loss = 0.0071337710072596865
step = 9, Training Accuracy: 0.9266666666666666
Training loss = 0.007598347341020902
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.7525
Training loss = 0.008399848143259685
step = 11, Training Accuracy: 0.9
Training loss = 0.0057307928055524825
step = 12, Training Accuracy: 0.9366666666666666
Training loss = 0.0077939393122990926
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.006787494098146756
step = 14, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.75625
15 	8     	0.750312	0.00678204	0.74125	0.7625 
params:  [0.27877735366035167, 0.3189687592889387, 0.09743823143714442, 0.4083065684850075, 0.1396043601022793, 0.2698284448097542, 0.5375992498920634, 0.31855753947638377, 0.99, 0.7900118014411293, 0.2646767725644407, 0.46610112696622835, 0.3335491037907393, 0.26955680830610845, 0.21369111738792268, 0.3441180179536157, 0.5275178216592998]
[0.27877735366035167, 0.3189687592889387, 0.09743823143714442, 0.4083065684850075, 0.1396043601022793, 0.2698284448097542, 0.5375992498920634, 0.31855753947638377, 0.99, 0.7900118014411293, 0.2646767725644407, 0.46610112696622835, 0.3335491037907393, 0.26955680830610845, 0.21369111738792268, 0.3441180179536157, 0.5275178216592998]
Training loss = 0.01230361541112264
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.7575
Training loss = 0.011251700321833293
step = 1, Training Accuracy: 0.86
Training loss = 0.010387139320373535
step = 2, Training Accuracy: 0.88
Training loss = 0.010210725441575051
step = 3, Training Accuracy: 0.8466666666666667
Training loss = 0.01055359423160553
step = 4, Training Accuracy: 0.8633333333333333
Training loss = 0.01034987449645996
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.735
Training loss = 0.009710721423228581
step = 6, Training Accuracy: 0.9
Training loss = 0.010516992310682932
step = 7, Training Accuracy: 0.8733333333333333
Training loss = 0.009272421623269717
step = 8, Training Accuracy: 0.9
Training loss = 0.009684982026616733
step = 9, Training Accuracy: 0.8633333333333333
Training loss = 0.008416470115383465
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7425
Training loss = 0.00876072292526563
step = 11, Training Accuracy: 0.88
Training loss = 0.007254688665270805
step = 12, Training Accuracy: 0.91
Training loss = 0.009170921593904495
step = 13, Training Accuracy: 0.9066666666666666
Training loss = 0.008085163893798987
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.735
params:  [0.0949363302949848, 0.21905132194574842, 0.339520317289017, 0.5434353887360506, 0.01, 0.28024893853666516, 0.3906653269346176, 0.19140796531088589, 0.8449505069324824, 0.9697917718344964, 0.01, 0.5314086395064324, 0.5065309354492913, 0.4755721363662008, 0.22658305263633333, 0.10595955879184882, 0.7186173269985613]
[0.0949363302949848, 0.21905132194574842, 0.339520317289017, 0.5434353887360506, 0.01, 0.28024893853666516, 0.3906653269346176, 0.19140796531088589, 0.8449505069324824, 0.9697917718344964, 0.01, 0.5314086395064324, 0.5065309354492913, 0.4755721363662008, 0.22658305263633333, 0.10595955879184882, 0.7186173269985613]
Training loss = 0.011899587859710058
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.74125
Training loss = 0.010381320814291635
step = 1, Training Accuracy: 0.87
Training loss = 0.010277060916026433
step = 2, Training Accuracy: 0.87
Training loss = 0.009644230951865514
step = 3, Training Accuracy: 0.89
Training loss = 0.008777544945478439
step = 4, Training Accuracy: 0.89
Training loss = 0.009780667622884114
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.765
Training loss = 0.008872230425477029
step = 6, Training Accuracy: 0.9066666666666666
Training loss = 0.007248970220486323
step = 7, Training Accuracy: 0.9233333333333333
Training loss = 0.00828059121966362
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.008668592323859532
step = 9, Training Accuracy: 0.91
Training loss = 0.0074340058118104934
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7625
Training loss = 0.007151602779825529
step = 11, Training Accuracy: 0.9
Training loss = 0.006883441333969434
step = 12, Training Accuracy: 0.9433333333333334
Training loss = 0.007662610809008281
step = 13, Training Accuracy: 0.91
Training loss = 0.007561231926083565
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.7575
params:  [0.07156281923127264, 0.27828306715624, 0.21718558956891526, 0.3707781304630793, 0.01, 0.6407977628622139, 0.44485106869766106, 0.35479622433802716, 0.8623348893063588, 0.9866151993914117, 0.01, 0.5208714520099741, 0.6751432021601035, 0.39239836965988717, 0.09419261262826129, 0.3637810083298375, 0.8367251205650547]
[0.07156281923127264, 0.27828306715624, 0.21718558956891526, 0.3707781304630793, 0.01, 0.6407977628622139, 0.44485106869766106, 0.35479622433802716, 0.8623348893063588, 0.9866151993914117, 0.01, 0.5208714520099741, 0.6751432021601035, 0.39239836965988717, 0.09419261262826129, 0.3637810083298375, 0.8367251205650547]
Training loss = 0.01046580046415329
step = 0, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.7575
Training loss = 0.009899976005156835
step = 1, Training Accuracy: 0.88
Training loss = 0.009595212688048681
step = 2, Training Accuracy: 0.91
Training loss = 0.009906006753444671
step = 3, Training Accuracy: 0.9
Training loss = 0.009335425893465678
step = 4, Training Accuracy: 0.9066666666666666
Training loss = 0.007003233209252358
step = 5, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.75
Training loss = 0.0075541761641701064
step = 6, Training Accuracy: 0.92
Training loss = 0.008146925469239553
step = 7, Training Accuracy: 0.92
Training loss = 0.008388034875194232
step = 8, Training Accuracy: 0.89
Training loss = 0.007917471925417582
step = 9, Training Accuracy: 0.9066666666666666
Training loss = 0.008198105643192927
step = 10, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.75
Training loss = 0.007055289447307587
step = 11, Training Accuracy: 0.93
Training loss = 0.005574121847748756
step = 12, Training Accuracy: 0.94
Training loss = 0.006535486231247584
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.008473612939318021
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.7525
params:  [0.09106208245237721, 0.12279238376236856, 0.17058997351987157, 0.39101027065877375, 0.11547696201959144, 0.23387833322063395, 0.48932708965344657, 0.23737307740114774, 0.8905265398963473, 0.99, 0.01, 0.4989173732416418, 0.5451206466448708, 0.3229530655412677, 0.23012857922562688, 0.17568961950378562, 0.9066566476512874]
[0.09106208245237721, 0.12279238376236856, 0.17058997351987157, 0.39101027065877375, 0.11547696201959144, 0.23387833322063395, 0.48932708965344657, 0.23737307740114774, 0.8905265398963473, 0.99, 0.01, 0.4989173732416418, 0.5451206466448708, 0.3229530655412677, 0.23012857922562688, 0.17568961950378562, 0.9066566476512874]
Training loss = 0.013254029924670856
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.755
Training loss = 0.01006061961253484
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.008526685362060865
step = 2, Training Accuracy: 0.88
Training loss = 0.010030807107686996
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.01004236360390981
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.010076158866286277
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.75625
Training loss = 0.008356739729642869
step = 6, Training Accuracy: 0.89
Training loss = 0.00889419158299764
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.007554835826158524
step = 8, Training Accuracy: 0.92
Training loss = 0.008376250863075257
step = 9, Training Accuracy: 0.9133333333333333
Training loss = 0.00872288204729557
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75875
Training loss = 0.008421622961759568
step = 11, Training Accuracy: 0.9
Training loss = 0.00748179775973161
step = 12, Training Accuracy: 0.9233333333333333
Training loss = 0.006989549075563748
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.008018833100795746
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.755
params:  [0.4894251384592604, 0.5319470756861093, 0.21706972981514214, 0.5792857451481668, 0.3048778299417365, 0.37058567268861575, 0.36074283025218057, 0.02741161995366212, 0.7624029469701772, 0.7586960268620938, 0.21972961741333996, 0.2844490942805476, 0.4953867522293539, 0.21396716364236018, 0.17083796187011582, 0.2282849134717464, 0.7444252531679967]
[0.4894251384592604, 0.5319470756861093, 0.21706972981514214, 0.5792857451481668, 0.3048778299417365, 0.37058567268861575, 0.36074283025218057, 0.02741161995366212, 0.7624029469701772, 0.7586960268620938, 0.21972961741333996, 0.2844490942805476, 0.4953867522293539, 0.21396716364236018, 0.17083796187011582, 0.2282849134717464, 0.7444252531679967]
Training loss = 0.014911589324474334
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.7525
Training loss = 0.012556534310181935
step = 1, Training Accuracy: 0.82
Training loss = 0.015685291786988576
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.01411803553501765
step = 3, Training Accuracy: 0.8133333333333334
Training loss = 0.01286449392636617
step = 4, Training Accuracy: 0.83
Training loss = 0.014444159865379334
step = 5, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.73875
Training loss = 0.014237123976151149
step = 6, Training Accuracy: 0.8233333333333334
Training loss = 0.011392596860726674
step = 7, Training Accuracy: 0.86
Training loss = 0.013827044119437536
step = 8, Training Accuracy: 0.8333333333333334
Training loss = 0.012023307134707768
step = 9, Training Accuracy: 0.8633333333333333
Training loss = 0.010290874987840653
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.74125
Training loss = 0.01105420097708702
step = 11, Training Accuracy: 0.87
Training loss = 0.010877419660488764
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.013546233077843984
step = 13, Training Accuracy: 0.8366666666666667
Training loss = 0.01199518342812856
step = 14, Training Accuracy: 0.86
Validation Accuracy: 0.745
params:  [0.4201084407728045, 0.5382379346602738, 0.044420748662312604, 0.5253885678854038, 0.01, 0.09811674815197927, 0.22471048862030107, 0.33614771336482413, 0.7848828646209736, 0.9829330301108692, 0.15997629739589037, 0.4554914944859022, 0.8934219950733964, 0.4525827400087656, 0.34263495179676184, 0.28809269586594344, 0.6394836618110235]
[0.4201084407728045, 0.5382379346602738, 0.044420748662312604, 0.5253885678854038, 0.01, 0.09811674815197927, 0.22471048862030107, 0.33614771336482413, 0.7848828646209736, 0.9829330301108692, 0.15997629739589037, 0.4554914944859022, 0.8934219950733964, 0.4525827400087656, 0.34263495179676184, 0.28809269586594344, 0.6394836618110235]
Training loss = 0.010828045308589935
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.74375
Training loss = 0.009525245477755864
step = 1, Training Accuracy: 0.8866666666666667
Training loss = 0.010970924347639084
step = 2, Training Accuracy: 0.8566666666666667
Training loss = 0.010203464676936468
step = 3, Training Accuracy: 0.86
Training loss = 0.00826569702476263
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.009417772814631463
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.73375
Training loss = 0.008296167502800624
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.009897743215163549
step = 7, Training Accuracy: 0.88
Training loss = 0.008979608913262686
step = 8, Training Accuracy: 0.88
Training loss = 0.009001327802737553
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.008829313715298971
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73375
Training loss = 0.007623169124126434
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.00767419325808684
step = 12, Training Accuracy: 0.91
Training loss = 0.009727680732806524
step = 13, Training Accuracy: 0.87
Training loss = 0.008512709538141887
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.73
params:  [0.17419802235426557, 0.20730877421219224, 0.1351157869018887, 0.18575271607722463, 0.3367804110434473, 0.266435761463555, 0.4998951968682885, 0.45433488075690903, 0.6939485972496147, 0.99, 0.07423284332687355, 0.5117516254454966, 0.48784369205572153, 0.22336604537874, 0.3120983269122886, 0.2784261608455066, 0.84797778527019]
[0.17419802235426557, 0.20730877421219224, 0.1351157869018887, 0.18575271607722463, 0.3367804110434473, 0.266435761463555, 0.4998951968682885, 0.45433488075690903, 0.6939485972496147, 0.99, 0.07423284332687355, 0.5117516254454966, 0.48784369205572153, 0.22336604537874, 0.3120983269122886, 0.2784261608455066, 0.84797778527019]
Training loss = 0.011943924675385157
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7375
Training loss = 0.01138477365175883
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.011129293590784073
step = 2, Training Accuracy: 0.86
Training loss = 0.009966159711281458
step = 3, Training Accuracy: 0.8733333333333333
Training loss = 0.009571923563877741
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.010038043608268103
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7425
Training loss = 0.00889561911424001
step = 6, Training Accuracy: 0.9
Training loss = 0.009733491837978362
step = 7, Training Accuracy: 0.88
Training loss = 0.010417241752147675
step = 8, Training Accuracy: 0.8966666666666666
Training loss = 0.009895665148893992
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.009189799080292384
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73625
Training loss = 0.010243333627780278
step = 11, Training Accuracy: 0.8433333333333334
Training loss = 0.009526703208684921
step = 12, Training Accuracy: 0.86
Training loss = 0.010151220212380092
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.009090698063373566
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.74125
params:  [0.45625236725131624, 0.4882590393571279, 0.07411159892660764, 0.47413718087925794, 0.18686908691472226, 0.26733628415486943, 0.6050627254277039, 0.12415683507745237, 0.8527578954000493, 0.8324187506711324, 0.1284921489826621, 0.6900615724082554, 0.44807958672210474, 0.4081123621241574, 0.14265976403285857, 0.44350374013898164, 0.6830512576348226]
[0.45625236725131624, 0.4882590393571279, 0.07411159892660764, 0.47413718087925794, 0.18686908691472226, 0.26733628415486943, 0.6050627254277039, 0.12415683507745237, 0.8527578954000493, 0.8324187506711324, 0.1284921489826621, 0.6900615724082554, 0.44807958672210474, 0.4081123621241574, 0.14265976403285857, 0.44350374013898164, 0.6830512576348226]
Training loss = 0.012269981255133946
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.74
Training loss = 0.011470383902390798
step = 1, Training Accuracy: 0.85
Training loss = 0.012666680961847305
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.009791679282983145
step = 3, Training Accuracy: 0.85
Training loss = 0.011553196509679158
step = 4, Training Accuracy: 0.86
Training loss = 0.012103917797406515
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.75625
Training loss = 0.010484842459360759
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.009777885327736536
step = 7, Training Accuracy: 0.88
Training loss = 0.010098681648572286
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.010218469252189
step = 9, Training Accuracy: 0.87
Training loss = 0.008310867547988892
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.75
Training loss = 0.011013590395450593
step = 11, Training Accuracy: 0.87
Training loss = 0.00879604200522105
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.008902144928773245
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.008696695417165756
step = 14, Training Accuracy: 0.89
Validation Accuracy: 0.7575
16 	8     	0.746719	0.00990063	0.73   	0.7575 
params:  [0.34819309260208287, 0.17114325479066678, 0.3440342876546798, 0.1424091632722882, 0.01, 0.2285857883913832, 0.49290905839920973, 0.2364933330172024, 0.9695715808380581, 0.833721572950847, 0.1745597999815837, 0.4800874423410707, 0.5261556136644975, 0.3881393629355352, 0.0410964175441052, 0.27472009773425116, 0.8878130385158709]
[0.34819309260208287, 0.17114325479066678, 0.3440342876546798, 0.1424091632722882, 0.01, 0.2285857883913832, 0.49290905839920973, 0.2364933330172024, 0.9695715808380581, 0.833721572950847, 0.1745597999815837, 0.4800874423410707, 0.5261556136644975, 0.3881393629355352, 0.0410964175441052, 0.27472009773425116, 0.8878130385158709]
Training loss = 0.012907510896523793
step = 0, Training Accuracy: 0.85
Validation Accuracy: 0.75375
Training loss = 0.012755853434403738
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.010966506799062093
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.011618638435999553
step = 3, Training Accuracy: 0.8633333333333333
Training loss = 0.01054699753721555
step = 4, Training Accuracy: 0.8666666666666667
Training loss = 0.011212399303913117
step = 5, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.74875
Training loss = 0.010636459042628606
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.01146782303849856
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.00988079272210598
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.009531865417957306
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.009636495014031727
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.74625
Training loss = 0.00945930669705073
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.012095693200826645
step = 12, Training Accuracy: 0.8633333333333333
Training loss = 0.008958416928847631
step = 13, Training Accuracy: 0.9
Training loss = 0.009780998701850573
step = 14, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73625
params:  [0.3171131711555061, 0.3313121865662066, 0.09027969056040913, 0.6829426404291963, 0.04613327987391909, 0.3256461293134743, 0.42104888897375725, 0.10626416574730775, 0.8355750743679591, 0.99, 0.10556909947984125, 0.5774247775015546, 0.6219812315828759, 0.358707312778887, 0.16516151910476484, 0.47651408100621834, 0.99]
[0.3171131711555061, 0.3313121865662066, 0.09027969056040913, 0.6829426404291963, 0.04613327987391909, 0.3256461293134743, 0.42104888897375725, 0.10626416574730775, 0.8355750743679591, 0.99, 0.10556909947984125, 0.5774247775015546, 0.6219812315828759, 0.358707312778887, 0.16516151910476484, 0.47651408100621834, 0.99]
Training loss = 0.011054182300964992
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.7425
Training loss = 0.010977688680092494
step = 1, Training Accuracy: 0.8866666666666667
Training loss = 0.010948132475217183
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.010962656711538633
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.011020340323448181
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.011380656361579896
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.74875
Training loss = 0.010599227001269659
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.009903683811426162
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.010497552951176962
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.009138054152329763
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.011500153640906017
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.73875
Training loss = 0.010538692077000936
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.008263597836097081
step = 12, Training Accuracy: 0.89
Training loss = 0.010351512730121613
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.008285073091586432
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.735
params:  [0.5445553371517775, 0.11910485682502003, 0.3280996557745469, 0.5185314719661155, 0.1735286874746914, 0.4126447662512306, 0.541259823512355, 0.18163855007235494, 0.9030754404309835, 0.9760781394251854, 0.030477881087675303, 0.4739214152316471, 0.4817107385753548, 0.4150673033116352, 0.04714524212304019, 0.34185327725436154, 0.38928377613616216]
[0.5445553371517775, 0.11910485682502003, 0.3280996557745469, 0.5185314719661155, 0.1735286874746914, 0.4126447662512306, 0.541259823512355, 0.18163855007235494, 0.9030754404309835, 0.9760781394251854, 0.030477881087675303, 0.4739214152316471, 0.4817107385753548, 0.4150673033116352, 0.04714524212304019, 0.34185327725436154, 0.38928377613616216]
Training loss = 0.01206908235947291
step = 0, Training Accuracy: 0.85
Validation Accuracy: 0.74125
Training loss = 0.01347707599401474
step = 1, Training Accuracy: 0.8333333333333334
Training loss = 0.01088970830043157
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.01296389803290367
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.010884425242741903
step = 4, Training Accuracy: 0.8433333333333334
Training loss = 0.011278035889069239
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.745
Training loss = 0.01338862140973409
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.01169341708223025
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.012046140184005101
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.00846799522638321
step = 9, Training Accuracy: 0.91
Training loss = 0.011149993588527043
step = 10, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.745
Training loss = 0.01386388212442398
step = 11, Training Accuracy: 0.8266666666666667
Training loss = 0.010226929783821106
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.010772996892531713
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.010482958058516184
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.74375
params:  [0.35590772698436995, 0.2228300073961243, 0.3413479122605476, 0.571939174875111, 0.18532846752816268, 0.12502201751182507, 0.3697813130415931, 0.11096922925090263, 0.497847340651405, 0.99, 0.01, 0.515689437910207, 0.5722677811911995, 0.41398747498111677, 0.3208503260879976, 0.29885481232297734, 0.9005546820851388]
[0.35590772698436995, 0.2228300073961243, 0.3413479122605476, 0.571939174875111, 0.18532846752816268, 0.12502201751182507, 0.3697813130415931, 0.11096922925090263, 0.497847340651405, 0.99, 0.01, 0.515689437910207, 0.5722677811911995, 0.41398747498111677, 0.3208503260879976, 0.29885481232297734, 0.9005546820851388]
Training loss = 0.011568198253711064
step = 0, Training Accuracy: 0.8333333333333334
Validation Accuracy: 0.745
Training loss = 0.013090112755695979
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.013027990361054738
step = 2, Training Accuracy: 0.8333333333333334
Training loss = 0.011427947183450063
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.013147240181763968
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.011529781967401504
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.74125
Training loss = 0.01008065755168597
step = 6, Training Accuracy: 0.88
Training loss = 0.012121627628803254
step = 7, Training Accuracy: 0.87
Training loss = 0.01035382553935051
step = 8, Training Accuracy: 0.8766666666666667
Training loss = 0.012170396248499553
step = 9, Training Accuracy: 0.84
Training loss = 0.010313819299141566
step = 10, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.73
Training loss = 0.011324387341737747
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.010919098804394404
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.01034382904569308
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.01079501435160637
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.735
params:  [0.31499415861614205, 0.1294044839372177, 0.17542711727078425, 0.5667813796510958, 0.01, 0.0977006917469054, 0.6115759980506966, 0.19300865316177862, 0.8788259175275704, 0.99, 0.1179141977603147, 0.6172188411694005, 0.4797295769704478, 0.29957220366435183, 0.2510030279744349, 0.23550134222157731, 0.612359898393038]
[0.31499415861614205, 0.1294044839372177, 0.17542711727078425, 0.5667813796510958, 0.01, 0.0977006917469054, 0.6115759980506966, 0.19300865316177862, 0.8788259175275704, 0.99, 0.1179141977603147, 0.6172188411694005, 0.4797295769704478, 0.29957220366435183, 0.2510030279744349, 0.23550134222157731, 0.612359898393038]
Training loss = 0.011470622569322585
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.735
Training loss = 0.010557900220155715
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.009154465446869531
step = 2, Training Accuracy: 0.9033333333333333
Training loss = 0.010428398052851359
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.01237210378050804
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.009559385577837627
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.7275
Training loss = 0.01014299730459849
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.007678493410348892
step = 7, Training Accuracy: 0.9233333333333333
Training loss = 0.009872032552957535
step = 8, Training Accuracy: 0.89
Training loss = 0.009540729522705078
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.009761411199967067
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7375
Training loss = 0.007535415117939313
step = 11, Training Accuracy: 0.9
Training loss = 0.008986319502194723
step = 12, Training Accuracy: 0.9
Training loss = 0.007854747672875723
step = 13, Training Accuracy: 0.9266666666666666
Training loss = 0.009479031562805176
step = 14, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.74
params:  [0.01, 0.3000516454965922, 0.3343112980671492, 0.3585494472694841, 0.109920023707999, 0.47735407450017775, 0.6123924867769779, 0.31483088821609195, 0.8426164534559225, 0.9370155454046287, 0.28051968865571925, 0.6418363093034553, 0.5538528467276923, 0.6720011238418415, 0.08776835074647095, 0.157707718535323, 0.7225308781886866]
[0.01, 0.3000516454965922, 0.3343112980671492, 0.3585494472694841, 0.109920023707999, 0.47735407450017775, 0.6123924867769779, 0.31483088821609195, 0.8426164534559225, 0.9370155454046287, 0.28051968865571925, 0.6418363093034553, 0.5538528467276923, 0.6720011238418415, 0.08776835074647095, 0.157707718535323, 0.7225308781886866]
Training loss = 0.009491428981224697
step = 0, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.74
Training loss = 0.009831434885660807
step = 1, Training Accuracy: 0.8866666666666667
Training loss = 0.01002123678723971
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.008893327265977859
step = 3, Training Accuracy: 0.9033333333333333
Training loss = 0.007968552162249882
step = 4, Training Accuracy: 0.9066666666666666
Training loss = 0.007949805756409963
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.73
Training loss = 0.008539600546161335
step = 6, Training Accuracy: 0.91
Training loss = 0.007407961785793305
step = 7, Training Accuracy: 0.9233333333333333
Training loss = 0.007458213319381078
step = 8, Training Accuracy: 0.9166666666666666
Training loss = 0.00785881241162618
step = 9, Training Accuracy: 0.9
Training loss = 0.007739631533622742
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.735
Training loss = 0.007551046411196391
step = 11, Training Accuracy: 0.93
Training loss = 0.008026772538820903
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.006812422374884288
step = 13, Training Accuracy: 0.94
Training loss = 0.007419715424378713
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.73625
params:  [0.16869149831957858, 0.016817922359258053, 0.192016096793371, 0.3917369145190026, 0.04378355271045318, 0.16254292472479281, 0.41441300469933795, 0.411232916038327, 0.8745720785819332, 0.8669588694352921, 0.01, 0.5812955701532296, 0.3093030246587085, 0.4081898582406611, 0.15211315209058915, 0.3974328202248658, 0.7939398921808267]
[0.16869149831957858, 0.016817922359258053, 0.192016096793371, 0.3917369145190026, 0.04378355271045318, 0.16254292472479281, 0.41441300469933795, 0.411232916038327, 0.8745720785819332, 0.8669588694352921, 0.01, 0.5812955701532296, 0.3093030246587085, 0.4081898582406611, 0.15211315209058915, 0.3974328202248658, 0.7939398921808267]
Training loss = 0.01184648960828781
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.735
Training loss = 0.011939270148674647
step = 1, Training Accuracy: 0.88
Training loss = 0.011807172894477844
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.011366451382637024
step = 3, Training Accuracy: 0.8733333333333333
Training loss = 0.012196465680996577
step = 4, Training Accuracy: 0.87
Training loss = 0.009871516575415929
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.72875
Training loss = 0.010968981881936391
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.010734721769889195
step = 7, Training Accuracy: 0.8633333333333333
Training loss = 0.010976586639881134
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.010077049682537715
step = 9, Training Accuracy: 0.88
Training loss = 0.01087386578321457
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.73375
Training loss = 0.009952965080738068
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.009575999875863394
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.009780965348084768
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.009103719343741734
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.7325
params:  [0.19642975287083317, 0.22788675863752017, 0.24258152983703346, 0.47586431770634147, 0.1138162262173204, 0.2538346287347728, 0.5216537491832051, 0.16657712385788973, 0.917935239724243, 0.8853245923793694, 0.014863885860080785, 0.36009938966688215, 0.3677335466389916, 0.32370434960214733, 0.07637793421040416, 0.14483141674284195, 0.8730649994663953]
[0.19642975287083317, 0.22788675863752017, 0.24258152983703346, 0.47586431770634147, 0.1138162262173204, 0.2538346287347728, 0.5216537491832051, 0.16657712385788973, 0.917935239724243, 0.8853245923793694, 0.014863885860080785, 0.36009938966688215, 0.3677335466389916, 0.32370434960214733, 0.07637793421040416, 0.14483141674284195, 0.8730649994663953]
Training loss = 0.009099806795517603
step = 0, Training Accuracy: 0.87
Validation Accuracy: 0.73625
Training loss = 0.00980441078543663
step = 1, Training Accuracy: 0.8866666666666667
Training loss = 0.009521702180306117
step = 2, Training Accuracy: 0.88
Training loss = 0.009089942425489425
step = 3, Training Accuracy: 0.88
Training loss = 0.010334304173787435
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.009532358994086583
step = 5, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.745
Training loss = 0.008325562477111817
step = 6, Training Accuracy: 0.9166666666666666
Training loss = 0.008051402593652408
step = 7, Training Accuracy: 0.9133333333333333
Training loss = 0.008286278645197551
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.007398355652888616
step = 9, Training Accuracy: 0.9
Training loss = 0.008534896870454152
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7475
Training loss = 0.008692322075366974
step = 11, Training Accuracy: 0.9
Training loss = 0.007789283494154612
step = 12, Training Accuracy: 0.9266666666666666
Training loss = 0.009630408585071564
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.009339738190174102
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.7475
17 	8     	0.738281	0.00475729	0.7325 	0.7475 
params:  [0.22083636208144142, 0.1625374351857896, 0.2490452391415697, 0.49745377984906175, 0.09466364832880123, 0.39234527787606915, 0.4462431302493608, 0.07825455415743339, 0.99, 0.8624473778534717, 0.07311129754707263, 0.5244161818043044, 0.4970552963958233, 0.4192723000605126, 0.0386283575757129, 0.19947400516662384, 0.8187922318654408]
[0.22083636208144142, 0.1625374351857896, 0.2490452391415697, 0.49745377984906175, 0.09466364832880123, 0.39234527787606915, 0.4462431302493608, 0.07825455415743339, 0.99, 0.8624473778534717, 0.07311129754707263, 0.5244161818043044, 0.4970552963958233, 0.4192723000605126, 0.0386283575757129, 0.19947400516662384, 0.8187922318654408]
Training loss = 0.011614794035752614
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.75375
Training loss = 0.011748777081569036
step = 1, Training Accuracy: 0.8333333333333334
Training loss = 0.010552366475264231
step = 2, Training Accuracy: 0.87
Training loss = 0.009181634585062663
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.010888187487920125
step = 4, Training Accuracy: 0.87
Training loss = 0.010023498584826788
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.75125
Training loss = 0.010601493418216706
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.009827177027861277
step = 7, Training Accuracy: 0.88
Training loss = 0.008576799780130387
step = 8, Training Accuracy: 0.9133333333333333
Training loss = 0.009975304702917735
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.011071149831016858
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.75375
Training loss = 0.01179518257578214
step = 11, Training Accuracy: 0.86
Training loss = 0.008597012360890706
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.010381899376710257
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.009567914754152298
step = 14, Training Accuracy: 0.89
Validation Accuracy: 0.75375
params:  [0.23600817672999358, 0.2047536051349316, 0.2345899257299001, 0.3800809870733983, 0.04871487895500405, 0.10216133685770579, 0.48967812840490793, 0.08309915341507033, 0.906193758170284, 0.7851618421784146, 0.01, 0.2877869858361333, 0.5592399844190674, 0.33999798449160884, 0.07207638033025361, 0.08336311467177676, 0.7313934767821277]
[0.23600817672999358, 0.2047536051349316, 0.2345899257299001, 0.3800809870733983, 0.04871487895500405, 0.10216133685770579, 0.48967812840490793, 0.08309915341507033, 0.906193758170284, 0.7851618421784146, 0.01, 0.2877869858361333, 0.5592399844190674, 0.33999798449160884, 0.07207638033025361, 0.08336311467177676, 0.7313934767821277]
Training loss = 0.00803647592663765
step = 0, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.75125
Training loss = 0.007804824908574422
step = 1, Training Accuracy: 0.92
Training loss = 0.008759738802909851
step = 2, Training Accuracy: 0.89
Training loss = 0.0072327595700820285
step = 3, Training Accuracy: 0.9
Training loss = 0.007778824095924695
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.007692699035008748
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.75875
Training loss = 0.007767397835850716
step = 6, Training Accuracy: 0.91
Training loss = 0.007260809540748596
step = 7, Training Accuracy: 0.89
Training loss = 0.007526128739118576
step = 8, Training Accuracy: 0.9
Training loss = 0.007484458088874817
step = 9, Training Accuracy: 0.91
Training loss = 0.007580992380777995
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.7525
Training loss = 0.0074548775454362235
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.007378271967172623
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.007396682798862457
step = 13, Training Accuracy: 0.9
Training loss = 0.006059707552194596
step = 14, Training Accuracy: 0.92
Validation Accuracy: 0.76
params:  [0.13121705913923265, 0.28852132851643686, 0.27081966157969983, 0.5012360622368289, 0.2970928755235688, 0.09679567495251493, 0.529973273221075, 0.19786349908443263, 0.8124600756542872, 0.732651459286482, 0.0903681479121782, 0.7606898478894082, 0.47047071363900006, 0.4315636623842165, 0.10478298654992024, 0.11385030953248164, 0.8495518280245258]
[0.13121705913923265, 0.28852132851643686, 0.27081966157969983, 0.5012360622368289, 0.2970928755235688, 0.09679567495251493, 0.529973273221075, 0.19786349908443263, 0.8124600756542872, 0.732651459286482, 0.0903681479121782, 0.7606898478894082, 0.47047071363900006, 0.4315636623842165, 0.10478298654992024, 0.11385030953248164, 0.8495518280245258]
Training loss = 0.011053212185700734
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.76
Training loss = 0.009679524724682172
step = 1, Training Accuracy: 0.88
Training loss = 0.011511601110299429
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.009709136386712391
step = 3, Training Accuracy: 0.87
Training loss = 0.009404321809609732
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.012580932726462683
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.75
Training loss = 0.01027051438887914
step = 6, Training Accuracy: 0.8633333333333333
Training loss = 0.00916905328631401
step = 7, Training Accuracy: 0.88
Training loss = 0.011592940638462702
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.010237376739581427
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.011543646852175394
step = 10, Training Accuracy: 0.85
Validation Accuracy: 0.75375
Training loss = 0.010627474387486775
step = 11, Training Accuracy: 0.88
Training loss = 0.010112047791481019
step = 12, Training Accuracy: 0.9
Training loss = 0.010402207672595977
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.009179776807626088
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.7425
params:  [0.10694963737216859, 0.294213383796901, 0.1329921909272316, 0.5561450486105163, 0.20981694278704305, 0.2130569249531284, 0.5138527347363921, 0.14201884011466592, 0.9463373968722446, 0.8929510436271272, 0.13294547639718474, 0.4339672630069413, 0.45075413641745027, 0.4338100676840624, 0.07163116216012333, 0.3035490609155485, 0.7409593969996604]
[0.10694963737216859, 0.294213383796901, 0.1329921909272316, 0.5561450486105163, 0.20981694278704305, 0.2130569249531284, 0.5138527347363921, 0.14201884011466592, 0.9463373968722446, 0.8929510436271272, 0.13294547639718474, 0.4339672630069413, 0.45075413641745027, 0.4338100676840624, 0.07163116216012333, 0.3035490609155485, 0.7409593969996604]
Training loss = 0.011325658410787582
step = 0, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.745
Training loss = 0.01057600550353527
step = 1, Training Accuracy: 0.86
Training loss = 0.013258399963378907
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.012143774529298147
step = 3, Training Accuracy: 0.85
Training loss = 0.013169122735659282
step = 4, Training Accuracy: 0.8466666666666667
Training loss = 0.011388714164495469
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.73875
Training loss = 0.011842720707257589
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.011536717116832733
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.01231990560889244
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.009782583440343539
step = 9, Training Accuracy: 0.86
Training loss = 0.010525615413983664
step = 10, Training Accuracy: 0.88
Validation Accuracy: 0.73625
Training loss = 0.011387814730405808
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.01191628391544024
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.012510155985752742
step = 13, Training Accuracy: 0.8666666666666667
Training loss = 0.008578416009744008
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.74125
params:  [0.25308733318111654, 0.1602094890858362, 0.25471537665881366, 0.5415544162681566, 0.24796025557343776, 0.33589464981440137, 0.6608057311665259, 0.1607733335830951, 0.804881854981989, 0.9307169756188813, 0.06176273064783073, 0.31456940249237614, 0.2401091951183811, 0.08409728219296847, 0.01, 0.2800995831278186, 0.565083464620165]
[0.25308733318111654, 0.1602094890858362, 0.25471537665881366, 0.5415544162681566, 0.24796025557343776, 0.33589464981440137, 0.6608057311665259, 0.1607733335830951, 0.804881854981989, 0.9307169756188813, 0.06176273064783073, 0.31456940249237614, 0.2401091951183811, 0.08409728219296847, 0.01, 0.2800995831278186, 0.565083464620165]
Training loss = 0.010778592775265376
step = 0, Training Accuracy: 0.85
Validation Accuracy: 0.7375
Training loss = 0.011506069352229436
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.00947195549805959
step = 2, Training Accuracy: 0.9033333333333333
Training loss = 0.008992516696453094
step = 3, Training Accuracy: 0.88
Training loss = 0.009923520212372144
step = 4, Training Accuracy: 0.88
Training loss = 0.010119052280982336
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.75125
Training loss = 0.010019666602214178
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.00951279158393542
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.0092342838148276
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.009982779125372569
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.01029744898279508
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7625
Training loss = 0.010033226211865743
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.011166036253174146
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.008054028525948524
step = 13, Training Accuracy: 0.91
Training loss = 0.009180069665114084
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.76
params:  [0.24497015261344732, 0.20309710395432973, 0.37854361371597045, 0.6605147861334151, 0.15257538076111668, 0.277148016615262, 0.6370957468651535, 0.15098731841001084, 0.9223568474391747, 0.6991442071842309, 0.0788892884511937, 0.6040378773474877, 0.4837879243664179, 0.5157578583282425, 0.01, 0.17973254360285693, 0.9278470516659316]
[0.24497015261344732, 0.20309710395432973, 0.37854361371597045, 0.6605147861334151, 0.15257538076111668, 0.277148016615262, 0.6370957468651535, 0.15098731841001084, 0.9223568474391747, 0.6991442071842309, 0.0788892884511937, 0.6040378773474877, 0.4837879243664179, 0.5157578583282425, 0.01, 0.17973254360285693, 0.9278470516659316]
Training loss = 0.011581539089481035
step = 0, Training Accuracy: 0.8366666666666667
Validation Accuracy: 0.765
Training loss = 0.01021227459112803
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.010331410666306813
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.01052200014392535
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.011484263887008031
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.010974005063374837
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7425
Training loss = 0.010478603839874267
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.009962736715873083
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.01140580877661705
step = 8, Training Accuracy: 0.8966666666666666
Training loss = 0.009510123034318288
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.011334015677372614
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.73875
Training loss = 0.009572697480519613
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.00895218605796496
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.010238666236400604
step = 13, Training Accuracy: 0.9
Training loss = 0.010153952290614447
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.7425
params:  [0.3107905311986271, 0.10608099534400683, 0.25128679576661456, 0.521133417527555, 0.02715871660358342, 0.1453259053914405, 0.48996232508245113, 0.2246052488357565, 0.7086601016644829, 0.869547235128304, 0.08891890684777147, 0.3213830890441099, 0.582447493813351, 0.4041755423821489, 0.01, 0.4102880016654544, 0.6368614251647097]
[0.3107905311986271, 0.10608099534400683, 0.25128679576661456, 0.521133417527555, 0.02715871660358342, 0.1453259053914405, 0.48996232508245113, 0.2246052488357565, 0.7086601016644829, 0.869547235128304, 0.08891890684777147, 0.3213830890441099, 0.582447493813351, 0.4041755423821489, 0.01, 0.4102880016654544, 0.6368614251647097]
Training loss = 0.008310963561137518
step = 0, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.74125
Training loss = 0.00805389496187369
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.008421459595362345
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.008329900006453197
step = 3, Training Accuracy: 0.9
Training loss = 0.00808787953108549
step = 4, Training Accuracy: 0.89
Training loss = 0.009171677281459173
step = 5, Training Accuracy: 0.9
Validation Accuracy: 0.7575
Training loss = 0.007906739066044489
step = 6, Training Accuracy: 0.91
Training loss = 0.009573345879713695
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.009133804043134054
step = 8, Training Accuracy: 0.9133333333333333
Training loss = 0.01087363988161087
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.008976966639359792
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.7525
Training loss = 0.008218782593806585
step = 11, Training Accuracy: 0.9166666666666666
Training loss = 0.007227154995004336
step = 12, Training Accuracy: 0.92
Training loss = 0.00826546291510264
step = 13, Training Accuracy: 0.91
Training loss = 0.007585580373803774
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.75125
params:  [0.27417650987158226, 0.08251364966624873, 0.19601579352436937, 0.362996815086061, 0.18008288254393118, 0.29631627228381135, 0.4349809823655909, 0.20100703710010706, 0.8659675093644874, 0.8789089070259831, 0.13000753465947237, 0.5127667425930666, 0.6943476237883668, 0.3911439536358189, 0.30642148497231714, 0.23164712284011368, 0.6591201247488144]
[0.27417650987158226, 0.08251364966624873, 0.19601579352436937, 0.362996815086061, 0.18008288254393118, 0.29631627228381135, 0.4349809823655909, 0.20100703710010706, 0.8659675093644874, 0.8789089070259831, 0.13000753465947237, 0.5127667425930666, 0.6943476237883668, 0.3911439536358189, 0.30642148497231714, 0.23164712284011368, 0.6591201247488144]
Training loss = 0.00990546852350235
step = 0, Training Accuracy: 0.86
Validation Accuracy: 0.74125
Training loss = 0.010969813962777455
step = 1, Training Accuracy: 0.88
Training loss = 0.01219340443611145
step = 2, Training Accuracy: 0.8633333333333333
Training loss = 0.01133396642903487
step = 3, Training Accuracy: 0.89
Training loss = 0.010421587278445561
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.010501909504334132
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.7325
Training loss = 0.010172086854775747
step = 6, Training Accuracy: 0.8733333333333333
Training loss = 0.009591861317555109
step = 7, Training Accuracy: 0.89
Training loss = 0.010568945159514745
step = 8, Training Accuracy: 0.8966666666666666
Training loss = 0.009505889515082041
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.010283717761437099
step = 10, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.73875
Training loss = 0.007668599287668864
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.011224747151136399
step = 12, Training Accuracy: 0.8733333333333333
Training loss = 0.01081285133957863
step = 13, Training Accuracy: 0.87
Training loss = 0.009910159856081008
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.73375
18 	8     	0.748125	0.00894864	0.73375	0.76   
params:  [0.175728494720488, 0.23037581176266697, 0.26096642543626847, 0.47187196820505595, 0.20096012966725302, 0.27626685431457865, 0.6117013057856089, 0.01, 0.9474980056405211, 0.920463982632736, 0.011980303274454259, 0.40717130367646126, 0.7648501229127067, 0.3139730321216214, 0.01, 0.026202124395996312, 0.8012317333365276]
[0.175728494720488, 0.23037581176266697, 0.26096642543626847, 0.47187196820505595, 0.20096012966725302, 0.27626685431457865, 0.6117013057856089, 0.01, 0.9474980056405211, 0.920463982632736, 0.011980303274454259, 0.40717130367646126, 0.7648501229127067, 0.3139730321216214, 0.01, 0.026202124395996312, 0.8012317333365276]
Training loss = 0.011810993601878483
step = 0, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.745
Training loss = 0.012602666914463044
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.008756895512342454
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.0091746703038613
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.009507598380247752
step = 4, Training Accuracy: 0.9133333333333333
Training loss = 0.010132473409175873
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.745
Training loss = 0.007373630777001381
step = 6, Training Accuracy: 0.93
Training loss = 0.008972365657488506
step = 7, Training Accuracy: 0.89
Training loss = 0.009252353211243947
step = 8, Training Accuracy: 0.89
Training loss = 0.008847199355562527
step = 9, Training Accuracy: 0.8933333333333333
Training loss = 0.00816234181324641
step = 10, Training Accuracy: 0.91
Validation Accuracy: 0.735
Training loss = 0.008135915795962016
step = 11, Training Accuracy: 0.91
Training loss = 0.0070318178087472915
step = 12, Training Accuracy: 0.9166666666666666
Training loss = 0.007294140209754308
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.008277896990378697
step = 14, Training Accuracy: 0.91
Validation Accuracy: 0.74
params:  [0.2556801386142071, 0.151287173038002, 0.2986155941699151, 0.4428574949474681, 0.08774586537372098, 0.046997763256728525, 0.596019002565074, 0.07539518583626029, 0.7998223717424828, 0.8066397823411261, 0.047500530410671524, 0.4634457255719822, 0.5278817962314775, 0.34604234602731426, 0.16564018675631215, 0.21198933545241722, 0.5974575494968603]
[0.2556801386142071, 0.151287173038002, 0.2986155941699151, 0.4428574949474681, 0.08774586537372098, 0.046997763256728525, 0.596019002565074, 0.07539518583626029, 0.7998223717424828, 0.8066397823411261, 0.047500530410671524, 0.4634457255719822, 0.5278817962314775, 0.34604234602731426, 0.16564018675631215, 0.21198933545241722, 0.5974575494968603]
Training loss = 0.009677311331033707
step = 0, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.73625
Training loss = 0.00958724170923233
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.007441396365563075
step = 2, Training Accuracy: 0.9133333333333333
Training loss = 0.009793556779623031
step = 3, Training Accuracy: 0.89
Training loss = 0.00916097770134608
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.008827808996041615
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.73625
Training loss = 0.008459916214148204
step = 6, Training Accuracy: 0.92
Training loss = 0.007826814701159795
step = 7, Training Accuracy: 0.9066666666666666
Training loss = 0.00910297935207685
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.0077899979303280515
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.007809866120417913
step = 10, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.7375
Training loss = 0.007661094392339389
step = 11, Training Accuracy: 0.91
Training loss = 0.008008689333995183
step = 12, Training Accuracy: 0.9066666666666666
Training loss = 0.007906042908628782
step = 13, Training Accuracy: 0.9133333333333333
Training loss = 0.008038096403082211
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.74
params:  [0.31296430574359124, 0.07156652076412602, 0.2684820452725706, 0.39233955920480995, 0.13083681014766296, 0.14523293608525428, 0.4126933114744483, 0.22215178589241202, 0.8569043107748029, 0.6936961293263453, 0.01, 0.28801167446558007, 0.4283572092778353, 0.3622522981699803, 0.01, 0.21833633146097725, 0.5262052480690483]
[0.31296430574359124, 0.07156652076412602, 0.2684820452725706, 0.39233955920480995, 0.13083681014766296, 0.14523293608525428, 0.4126933114744483, 0.22215178589241202, 0.8569043107748029, 0.6936961293263453, 0.01, 0.28801167446558007, 0.4283572092778353, 0.3622522981699803, 0.01, 0.21833633146097725, 0.5262052480690483]
Training loss = 0.00855046605070432
step = 0, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.73125
Training loss = 0.010129343420267105
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.008935763786236445
step = 2, Training Accuracy: 0.89
Training loss = 0.00864906184375286
step = 3, Training Accuracy: 0.91
Training loss = 0.00872373605767886
step = 4, Training Accuracy: 0.89
Training loss = 0.00818918506304423
step = 5, Training Accuracy: 0.9233333333333333
Validation Accuracy: 0.73125
Training loss = 0.010590003232161204
step = 6, Training Accuracy: 0.89
Training loss = 0.008927534719308218
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.008349497690796853
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.008448884884516398
step = 9, Training Accuracy: 0.9
Training loss = 0.008922428141037624
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.73875
Training loss = 0.008335203578074773
step = 11, Training Accuracy: 0.91
Training loss = 0.0088988379885753
step = 12, Training Accuracy: 0.9
Training loss = 0.00897176777323087
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.00775857537984848
step = 14, Training Accuracy: 0.92
Validation Accuracy: 0.74
params:  [0.271729555796886, 0.21392450989473502, 0.20840862393657772, 0.49427813909261414, 0.1813044347691269, 0.3432640903167141, 0.5141759795150703, 0.22294828178622467, 0.8882609714646681, 0.9336881306686923, 0.075289580677563, 0.025994959508035542, 0.37669886385713364, 0.54865072050746, 0.2443259191807475, 0.2090580065778953, 0.6725037869253238]
[0.271729555796886, 0.21392450989473502, 0.20840862393657772, 0.49427813909261414, 0.1813044347691269, 0.3432640903167141, 0.5141759795150703, 0.22294828178622467, 0.8882609714646681, 0.9336881306686923, 0.075289580677563, 0.025994959508035542, 0.37669886385713364, 0.54865072050746, 0.2443259191807475, 0.2090580065778953, 0.6725037869253238]
Training loss = 0.013217590550581615
step = 0, Training Accuracy: 0.83
Validation Accuracy: 0.74125
Training loss = 0.012333358079195023
step = 1, Training Accuracy: 0.88
Training loss = 0.010852786153554917
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.011339735388755798
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.011504546304543813
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.01192605088154475
step = 5, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74
Training loss = 0.01115129272143046
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.011379087815682093
step = 7, Training Accuracy: 0.84
Training loss = 0.010536478012800216
step = 8, Training Accuracy: 0.89
Training loss = 0.010189365992943445
step = 9, Training Accuracy: 0.8533333333333334
Training loss = 0.011399437834819158
step = 10, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7425
Training loss = 0.009575108587741852
step = 11, Training Accuracy: 0.89
Training loss = 0.011433401753505072
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.010710153927405675
step = 13, Training Accuracy: 0.87
Training loss = 0.010408836901187896
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.74375
params:  [0.40501828258884753, 0.2520581485918988, 0.23159065829071473, 0.2897940628486451, 0.01, 0.17365982662870716, 0.49703743019391977, 0.15859084802133644, 0.9041276144274872, 0.8197090222368366, 0.01, 0.40962286135060155, 0.592183521438927, 0.30846042352335523, 0.10057111351107748, 0.13637908758366696, 0.6582106839113802]
[0.40501828258884753, 0.2520581485918988, 0.23159065829071473, 0.2897940628486451, 0.01, 0.17365982662870716, 0.49703743019391977, 0.15859084802133644, 0.9041276144274872, 0.8197090222368366, 0.01, 0.40962286135060155, 0.592183521438927, 0.30846042352335523, 0.10057111351107748, 0.13637908758366696, 0.6582106839113802]
Training loss = 0.008661236154536405
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.74875
Training loss = 0.008668311461806298
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.009718904544909795
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.007760779981811842
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.008806071132421494
step = 4, Training Accuracy: 0.9033333333333333
Training loss = 0.009736213982105255
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.74375
Training loss = 0.008107674097021421
step = 6, Training Accuracy: 0.9
Training loss = 0.008306172589461008
step = 7, Training Accuracy: 0.91
Training loss = 0.007162152330080668
step = 8, Training Accuracy: 0.9266666666666666
Training loss = 0.0075121062497297926
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.007982490782936414
step = 10, Training Accuracy: 0.9333333333333333
Validation Accuracy: 0.74125
Training loss = 0.007947630236546198
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.007269826928774516
step = 12, Training Accuracy: 0.9133333333333333
Training loss = 0.00842217003305753
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.008552888731161752
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.735
params:  [0.19328422394162437, 0.1378483017305535, 0.24922999334835283, 0.378390626340704, 0.06875722522957017, 0.0765710670244833, 0.48192308799816724, 0.03810422463141913, 0.9160445224913564, 0.7396282303749783, 0.10176745784760034, 0.32762647251171295, 0.4878423131313679, 0.2182045436479459, 0.18403578437373977, 0.04031690385228048, 0.7021973783179737]
[0.19328422394162437, 0.1378483017305535, 0.24922999334835283, 0.378390626340704, 0.06875722522957017, 0.0765710670244833, 0.48192308799816724, 0.03810422463141913, 0.9160445224913564, 0.7396282303749783, 0.10176745784760034, 0.32762647251171295, 0.4878423131313679, 0.2182045436479459, 0.18403578437373977, 0.04031690385228048, 0.7021973783179737]
Training loss = 0.008962738638122877
step = 0, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.735
Training loss = 0.00980670156578223
step = 1, Training Accuracy: 0.88
Training loss = 0.008485729917883872
step = 2, Training Accuracy: 0.88
Training loss = 0.009743859966595967
step = 3, Training Accuracy: 0.8733333333333333
Training loss = 0.009046660463015239
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.007805747811992963
step = 5, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.7375
Training loss = 0.008918806811173756
step = 6, Training Accuracy: 0.9166666666666666
Training loss = 0.008608833700418473
step = 7, Training Accuracy: 0.9066666666666666
Training loss = 0.007981513589620591
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.009455177336931228
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.007701679915189743
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.74375
Training loss = 0.008174046849211056
step = 11, Training Accuracy: 0.8933333333333333
Training loss = 0.0077923660973707835
step = 12, Training Accuracy: 0.9033333333333333
Training loss = 0.007073598926266035
step = 13, Training Accuracy: 0.9233333333333333
Training loss = 0.00913362205028534
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.74125
params:  [0.2386821537755333, 0.13519386851390952, 0.3285626173914685, 0.40890058963372905, 0.24399594143533593, 0.2486094197408716, 0.5979627566815193, 0.07769280177531737, 0.7155766019428065, 0.9049287704126558, 0.01, 0.26749611716157395, 0.37781255758377497, 0.19313973409960372, 0.01, 0.10041608222082665, 0.7900034252791294]
[0.2386821537755333, 0.13519386851390952, 0.3285626173914685, 0.40890058963372905, 0.24399594143533593, 0.2486094197408716, 0.5979627566815193, 0.07769280177531737, 0.7155766019428065, 0.9049287704126558, 0.01, 0.26749611716157395, 0.37781255758377497, 0.19313973409960372, 0.01, 0.10041608222082665, 0.7900034252791294]
Training loss = 0.00933833325902621
step = 0, Training Accuracy: 0.9
Validation Accuracy: 0.74
Training loss = 0.0093416162331899
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.01097970262169838
step = 2, Training Accuracy: 0.87
Training loss = 0.008439048727353415
step = 3, Training Accuracy: 0.9066666666666666
Training loss = 0.008713236227631569
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.010790383170048396
step = 5, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.73625
Training loss = 0.010515715678532917
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.009682350183526674
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.007801734283566475
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.007847437721987566
step = 9, Training Accuracy: 0.8966666666666666
Training loss = 0.007736292108893395
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.73875
Training loss = 0.00859391987323761
step = 11, Training Accuracy: 0.8766666666666667
Training loss = 0.009401969065268835
step = 12, Training Accuracy: 0.8966666666666666
Training loss = 0.007654268344243368
step = 13, Training Accuracy: 0.9033333333333333
Training loss = 0.008649963488181433
step = 14, Training Accuracy: 0.89
Validation Accuracy: 0.73875
params:  [0.37627472470782974, 0.19316930473837318, 0.10595015049880441, 0.4967698537092911, 0.14846893456667773, 0.2685471610074941, 0.5197883635993994, 0.14335415944770738, 0.9665974241697579, 0.799428051759533, 0.27138190639603454, 0.3854650836842108, 0.60216791871997, 0.24335139665389766, 0.06619269882872537, 0.01, 0.8320665498918686]
[0.37627472470782974, 0.19316930473837318, 0.10595015049880441, 0.4967698537092911, 0.14846893456667773, 0.2685471610074941, 0.5197883635993994, 0.14335415944770738, 0.9665974241697579, 0.799428051759533, 0.27138190639603454, 0.3854650836842108, 0.60216791871997, 0.24335139665389766, 0.06619269882872537, 0.01, 0.8320665498918686]
Training loss = 0.011927696913480758
step = 0, Training Accuracy: 0.88
Validation Accuracy: 0.74125
Training loss = 0.011292701462904613
step = 1, Training Accuracy: 0.88
Training loss = 0.010421573892235756
step = 2, Training Accuracy: 0.8866666666666667
Training loss = 0.009892092396815618
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.008544655293226242
step = 4, Training Accuracy: 0.9266666666666666
Training loss = 0.008432719061772028
step = 5, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.73625
Training loss = 0.009672175645828247
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.008772179608543713
step = 7, Training Accuracy: 0.89
Training loss = 0.009366726378599803
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.010724308093388875
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.009818955361843108
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.73
Training loss = 0.008906991332769393
step = 11, Training Accuracy: 0.89
Training loss = 0.00965316116809845
step = 12, Training Accuracy: 0.9
Training loss = 0.008964122782150905
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.009032514691352845
step = 14, Training Accuracy: 0.89
Validation Accuracy: 0.7325
19 	8     	0.738906	0.00333293	0.7325 	0.74375
params:  [0.334984819980319, 0.04852004324819176, 0.3349895147026704, 0.5872973835731887, 0.10392090704843536, 0.21722828897809413, 0.5976610658635219, 0.04194349828420038, 0.9247203633444617, 0.8776036144115417, 0.14508711440309668, 0.01, 0.32907827108927623, 0.39321191728831417, 0.12983424791038123, 0.01, 0.9558801698065329]
[0.334984819980319, 0.04852004324819176, 0.3349895147026704, 0.5872973835731887, 0.10392090704843536, 0.21722828897809413, 0.5976610658635219, 0.04194349828420038, 0.9247203633444617, 0.8776036144115417, 0.14508711440309668, 0.01, 0.32907827108927623, 0.39321191728831417, 0.12983424791038123, 0.01, 0.9558801698065329]
Training loss = 0.011468152552843093
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.73875
Training loss = 0.012207540149490038
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.012876088718573252
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.012631950477759044
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.01087148353457451
step = 4, Training Accuracy: 0.85
Training loss = 0.011005683143933614
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.73625
Training loss = 0.012075304687023163
step = 6, Training Accuracy: 0.8466666666666667
Training loss = 0.011753751089175542
step = 7, Training Accuracy: 0.85
Training loss = 0.010065489908059439
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.010985803504784902
step = 9, Training Accuracy: 0.88
Training loss = 0.01040754069884618
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.74375
Training loss = 0.009751685162385304
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.011937594364086786
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.011005279694994291
step = 13, Training Accuracy: 0.8833333333333333
Training loss = 0.010596516678730647
step = 14, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.745
params:  [0.19011869134839524, 0.01, 0.3800048579607773, 0.45414834089209266, 0.12747560534947092, 0.254383755385474, 0.5895354671598224, 0.22332794479742327, 0.99, 0.6956108781138622, 0.048165959319577824, 0.18530837788489987, 0.37728227867518926, 0.12348784458003498, 0.11475886203944533, 0.09676219062998347, 0.5557567347341604]
[0.19011869134839524, 0.01, 0.3800048579607773, 0.45414834089209266, 0.12747560534947092, 0.254383755385474, 0.5895354671598224, 0.22332794479742327, 0.99, 0.6956108781138622, 0.048165959319577824, 0.18530837788489987, 0.37728227867518926, 0.12348784458003498, 0.11475886203944533, 0.09676219062998347, 0.5557567347341604]
Training loss = 0.007094478458166123
step = 0, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.74625
Training loss = 0.008047170639038086
step = 1, Training Accuracy: 0.9233333333333333
Training loss = 0.009644928574562072
step = 2, Training Accuracy: 0.9133333333333333
Training loss = 0.006895907744765281
step = 3, Training Accuracy: 0.92
Training loss = 0.007134223928054174
step = 4, Training Accuracy: 0.9233333333333333
Training loss = 0.00653911034266154
step = 5, Training Accuracy: 0.92
Validation Accuracy: 0.74125
Training loss = 0.006410345857342084
step = 6, Training Accuracy: 0.93
Training loss = 0.007450045794248581
step = 7, Training Accuracy: 0.9366666666666666
Training loss = 0.008980714827775954
step = 8, Training Accuracy: 0.9066666666666666
Training loss = 0.007684099997083346
step = 9, Training Accuracy: 0.9033333333333333
Training loss = 0.007219691127538681
step = 10, Training Accuracy: 0.92
Validation Accuracy: 0.745
Training loss = 0.005456834261616071
step = 11, Training Accuracy: 0.9366666666666666
Training loss = 0.007553294450044632
step = 12, Training Accuracy: 0.92
Training loss = 0.006330684547622999
step = 13, Training Accuracy: 0.9366666666666666
Training loss = 0.00648008959988753
step = 14, Training Accuracy: 0.93
Validation Accuracy: 0.75
params:  [0.1531914744613451, 0.08217703972248823, 0.35856521523226825, 0.3954012973279637, 0.46424002285560395, 0.0923859000020835, 0.5276016314919892, 0.09688627595418621, 0.99, 0.8793808281872705, 0.16018701320150974, 0.12062571341518491, 0.4993442007178133, 0.34331584583265307, 0.325672565011825, 0.11396616561512471, 0.315599346126415]
[0.1531914744613451, 0.08217703972248823, 0.35856521523226825, 0.3954012973279637, 0.46424002285560395, 0.0923859000020835, 0.5276016314919892, 0.09688627595418621, 0.99, 0.8793808281872705, 0.16018701320150974, 0.12062571341518491, 0.4993442007178133, 0.34331584583265307, 0.325672565011825, 0.11396616561512471, 0.315599346126415]
Training loss = 0.016125112970670066
step = 0, Training Accuracy: 0.8133333333333334
Validation Accuracy: 0.75875
Training loss = 0.014873934785525005
step = 1, Training Accuracy: 0.8533333333333334
Training loss = 0.01246301551659902
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.013681592245896657
step = 3, Training Accuracy: 0.8266666666666667
Training loss = 0.014650691548983257
step = 4, Training Accuracy: 0.8166666666666667
Training loss = 0.012340963383515676
step = 5, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.76375
Training loss = 0.012180349578460058
step = 6, Training Accuracy: 0.8533333333333334
Training loss = 0.013601515491803487
step = 7, Training Accuracy: 0.8133333333333334
Training loss = 0.01206214318672816
step = 8, Training Accuracy: 0.82
Training loss = 0.013924560397863389
step = 9, Training Accuracy: 0.8433333333333334
Training loss = 0.012378156234820683
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7625
Training loss = 0.01183020072678725
step = 11, Training Accuracy: 0.85
Training loss = 0.011746951142946879
step = 12, Training Accuracy: 0.8266666666666667
Training loss = 0.010455292165279389
step = 13, Training Accuracy: 0.8633333333333333
Training loss = 0.011212186614672343
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.77125
params:  [0.2429320381703192, 0.18432100204060578, 0.3098679682524704, 0.5223416610656383, 0.2086537457000695, 0.24292870026530852, 0.4030495450001699, 0.1352260924582151, 0.8254191850939929, 0.8059083239066054, 0.01, 0.16600021110454033, 0.2657389189233001, 0.5705586563410769, 0.17932364028019082, 0.19422585088964242, 0.7170526390200417]
[0.2429320381703192, 0.18432100204060578, 0.3098679682524704, 0.5223416610656383, 0.2086537457000695, 0.24292870026530852, 0.4030495450001699, 0.1352260924582151, 0.8254191850939929, 0.8059083239066054, 0.01, 0.16600021110454033, 0.2657389189233001, 0.5705586563410769, 0.17932364028019082, 0.19422585088964242, 0.7170526390200417]
Training loss = 0.012773497303326924
step = 0, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.75875
Training loss = 0.011434518446524938
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.011149636954069137
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.010621216744184495
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.010831063290437063
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.011549620603521665
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.74625
Training loss = 0.010049499422311783
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.010727408081293107
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.010956269601980845
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.011560907860596974
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.011022407710552216
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.75
Training loss = 0.01100541094938914
step = 11, Training Accuracy: 0.87
Training loss = 0.010370273739099503
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.011040222843488058
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.009684619257847468
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.74875
params:  [0.23512844266175492, 0.18059807072158593, 0.2659214446930869, 0.3244167194680605, 0.2789098714607226, 0.1216111175503759, 0.3766911179399413, 0.08555776189876688, 0.7625552567474094, 0.9472596193893935, 0.18073429077877629, 0.38362953177001724, 0.5381607949246306, 0.251545399490697, 0.15659485043210258, 0.3174270157050618, 0.6336197376302661]
[0.23512844266175492, 0.18059807072158593, 0.2659214446930869, 0.3244167194680605, 0.2789098714607226, 0.1216111175503759, 0.3766911179399413, 0.08555776189876688, 0.7625552567474094, 0.9472596193893935, 0.18073429077877629, 0.38362953177001724, 0.5381607949246306, 0.251545399490697, 0.15659485043210258, 0.3174270157050618, 0.6336197376302661]
Training loss = 0.011872570266326269
step = 0, Training Accuracy: 0.88
Validation Accuracy: 0.75
Training loss = 0.013336826960245768
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.012391614019870758
step = 2, Training Accuracy: 0.8566666666666667
Training loss = 0.010066035389900207
step = 3, Training Accuracy: 0.9
Training loss = 0.00957967941959699
step = 4, Training Accuracy: 0.8866666666666667
Training loss = 0.009954596857229868
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.745
Training loss = 0.012342626278599103
step = 6, Training Accuracy: 0.8766666666666667
Training loss = 0.011124662558237712
step = 7, Training Accuracy: 0.87
Training loss = 0.01008955458799998
step = 8, Training Accuracy: 0.89
Training loss = 0.011717856427033742
step = 9, Training Accuracy: 0.87
Training loss = 0.010619882295529048
step = 10, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.74
Training loss = 0.012134631872177124
step = 11, Training Accuracy: 0.87
Training loss = 0.008815215627352397
step = 12, Training Accuracy: 0.89
Training loss = 0.010268399069706599
step = 13, Training Accuracy: 0.89
Training loss = 0.011284955392281214
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.73375
params:  [0.2357309439026081, 0.12803834182693874, 0.21471884101980196, 0.4782981803306513, 0.10578559624031234, 0.17176133468410065, 0.42843603386500295, 0.13029023756332456, 0.7677686732431833, 0.9186830042926534, 0.24206887085299195, 0.12685229252218092, 0.48327397231452807, 0.667925459860504, 0.15391040504716944, 0.4265145775187636, 0.693890731229317]
[0.2357309439026081, 0.12803834182693874, 0.21471884101980196, 0.4782981803306513, 0.10578559624031234, 0.17176133468410065, 0.42843603386500295, 0.13029023756332456, 0.7677686732431833, 0.9186830042926534, 0.24206887085299195, 0.12685229252218092, 0.48327397231452807, 0.667925459860504, 0.15391040504716944, 0.4265145775187636, 0.693890731229317]
Training loss = 0.010399108231067657
step = 0, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.73375
Training loss = 0.009480478763580323
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.010770338873068492
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.009711280713478725
step = 3, Training Accuracy: 0.89
Training loss = 0.00996181699136893
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.009990844900409381
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7325
Training loss = 0.00830230322976907
step = 6, Training Accuracy: 0.9
Training loss = 0.00942730113863945
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.010253279929359755
step = 8, Training Accuracy: 0.8733333333333333
Training loss = 0.010411740144093832
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.011410492236415545
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.7375
Training loss = 0.011344463800390562
step = 11, Training Accuracy: 0.85
Training loss = 0.009060992598533631
step = 12, Training Accuracy: 0.91
Training loss = 0.011180986762046813
step = 13, Training Accuracy: 0.88
Training loss = 0.008456185907125473
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.72875
params:  [0.14918320741310684, 0.21295195105058431, 0.18385567769445094, 0.489922762309126, 0.21744127381214046, 0.28095430700316193, 0.40493555277911925, 0.07367464379994193, 0.8495682360534311, 0.99, 0.09387345167195249, 0.4304558634987048, 0.5350722668294997, 0.5905474164217981, 0.2674697551192321, 0.23701653433750736, 0.7735718896371172]
[0.14918320741310684, 0.21295195105058431, 0.18385567769445094, 0.489922762309126, 0.21744127381214046, 0.28095430700316193, 0.40493555277911925, 0.07367464379994193, 0.8495682360534311, 0.99, 0.09387345167195249, 0.4304558634987048, 0.5350722668294997, 0.5905474164217981, 0.2674697551192321, 0.23701653433750736, 0.7735718896371172]
Training loss = 0.013032783816258112
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7375
Training loss = 0.010842271745204926
step = 1, Training Accuracy: 0.88
Training loss = 0.01166144440571467
step = 2, Training Accuracy: 0.8433333333333334
Training loss = 0.009667663723230363
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.011747098863124847
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.010315944204727808
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74625
Training loss = 0.010090266962846121
step = 6, Training Accuracy: 0.88
Training loss = 0.011855950877070427
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.012177217155694961
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.009394247978925706
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.01154735565185547
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74375
Training loss = 0.01056291068593661
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.011029985497395197
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.009263415535291036
step = 13, Training Accuracy: 0.9066666666666666
Training loss = 0.0112152332564195
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.74875
params:  [0.20729501915023202, 0.01, 0.23104947764677025, 0.5140803011266397, 0.3855531257625324, 0.11717508313111338, 0.551868293034846, 0.11459333979033175, 0.8422360150541703, 0.799032375408085, 0.09251391685165654, 0.23415076606995705, 0.39795741286673797, 0.33351584570160286, 0.2029288798811647, 0.09784664693913482, 0.7060575215767404]
[0.20729501915023202, 0.01, 0.23104947764677025, 0.5140803011266397, 0.3855531257625324, 0.11717508313111338, 0.551868293034846, 0.11459333979033175, 0.8422360150541703, 0.799032375408085, 0.09251391685165654, 0.23415076606995705, 0.39795741286673797, 0.33351584570160286, 0.2029288798811647, 0.09784664693913482, 0.7060575215767404]
Training loss = 0.014746469557285308
step = 0, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.73875
Training loss = 0.012763637403647105
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.010955301051338514
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.011742923855781556
step = 3, Training Accuracy: 0.84
Training loss = 0.011537803560495377
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.011092267135779063
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.7475
Training loss = 0.012807953904072444
step = 6, Training Accuracy: 0.85
Training loss = 0.01089148129026095
step = 7, Training Accuracy: 0.88
Training loss = 0.011251719544331233
step = 8, Training Accuracy: 0.8533333333333334
Training loss = 0.011355511645476024
step = 9, Training Accuracy: 0.8566666666666667
Training loss = 0.012455796251694361
step = 10, Training Accuracy: 0.85
Validation Accuracy: 0.745
Training loss = 0.010421835333108902
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.012350326602657636
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.010344161490599314
step = 13, Training Accuracy: 0.86
Training loss = 0.010550459573666255
step = 14, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.75125
20 	8     	0.747188	0.0119037 	0.72875	0.77125
params:  [0.19321979732274758, 0.035835068679416715, 0.2919056763415539, 0.4890581125236051, 0.29748151055653954, 0.438634270582288, 0.5517971071491041, 0.017554301387842586, 0.9238466452712997, 0.7975884414903154, 0.01, 0.06549048865153495, 0.20042499207958908, 0.158980647114602, 0.14562335860627837, 0.23569842787065665, 0.3619222864113047]
[0.19321979732274758, 0.035835068679416715, 0.2919056763415539, 0.4890581125236051, 0.29748151055653954, 0.438634270582288, 0.5517971071491041, 0.017554301387842586, 0.9238466452712997, 0.7975884414903154, 0.01, 0.06549048865153495, 0.20042499207958908, 0.158980647114602, 0.14562335860627837, 0.23569842787065665, 0.3619222864113047]
Training loss = 0.010746736750006676
step = 0, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.74875
Training loss = 0.01077356276412805
step = 1, Training Accuracy: 0.8733333333333333
Training loss = 0.01019833783308665
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.010387801229953766
step = 3, Training Accuracy: 0.87
Training loss = 0.009990041404962539
step = 4, Training Accuracy: 0.8766666666666667
Training loss = 0.010071713129679362
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.73625
Training loss = 0.010733479162057242
step = 6, Training Accuracy: 0.88
Training loss = 0.008899417370557786
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.010747756014267603
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.009122670243183771
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.011016087780396144
step = 10, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.74125
Training loss = 0.009185222089290618
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.007796762386957804
step = 12, Training Accuracy: 0.9
Training loss = 0.010366580138603846
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.008759808018803597
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.73
params:  [0.10243706519373989, 0.046633858057206656, 0.16066828473922104, 0.4264388751952002, 0.3891144774420761, 0.19769120674758106, 0.41887355079418837, 0.2103710028182672, 0.99, 0.8606455771638192, 0.1322380639185945, 0.13515633798913695, 0.3616044868788355, 0.41764236693037116, 0.26708611831982915, 0.0385311729114501, 0.3067594269287771]
[0.10243706519373989, 0.046633858057206656, 0.16066828473922104, 0.4264388751952002, 0.3891144774420761, 0.19769120674758106, 0.41887355079418837, 0.2103710028182672, 0.99, 0.8606455771638192, 0.1322380639185945, 0.13515633798913695, 0.3616044868788355, 0.41764236693037116, 0.26708611831982915, 0.0385311729114501, 0.3067594269287771]
Training loss = 0.008497811704874039
step = 0, Training Accuracy: 0.9
Validation Accuracy: 0.73875
Training loss = 0.00935042550166448
step = 1, Training Accuracy: 0.8966666666666666
Training loss = 0.008736438875397046
step = 2, Training Accuracy: 0.9
Training loss = 0.009168702289462089
step = 3, Training Accuracy: 0.8966666666666666
Training loss = 0.00851208932697773
step = 4, Training Accuracy: 0.9166666666666666
Training loss = 0.009411864231030147
step = 5, Training Accuracy: 0.9
Validation Accuracy: 0.74875
Training loss = 0.009205314839879672
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.011114074438810349
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.010974415540695191
step = 8, Training Accuracy: 0.89
Training loss = 0.007816015581289927
step = 9, Training Accuracy: 0.9
Training loss = 0.009103123446305593
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.7475
Training loss = 0.006990074788530668
step = 11, Training Accuracy: 0.9033333333333333
Training loss = 0.009447529415289561
step = 12, Training Accuracy: 0.89
Training loss = 0.009005410422881444
step = 13, Training Accuracy: 0.91
Training loss = 0.008704179426034292
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75375
params:  [0.2789862437290883, 0.039044762975793776, 0.22350217673004974, 0.7404998802361993, 0.204874275991564, 0.23695028099814136, 0.44721909671990195, 0.04706418073938433, 0.9470367225365656, 0.7483691521281572, 0.01, 0.02535907050003075, 0.44942322466886947, 0.26328282837077965, 0.41225567565990273, 0.0821296679254728, 0.4543542376578212]
[0.2789862437290883, 0.039044762975793776, 0.22350217673004974, 0.7404998802361993, 0.204874275991564, 0.23695028099814136, 0.44721909671990195, 0.04706418073938433, 0.9470367225365656, 0.7483691521281572, 0.01, 0.02535907050003075, 0.44942322466886947, 0.26328282837077965, 0.41225567565990273, 0.0821296679254728, 0.4543542376578212]
Training loss = 0.00965013242016236
step = 0, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.75
Training loss = 0.011634719123442968
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.010643322666486104
step = 2, Training Accuracy: 0.8566666666666667
Training loss = 0.011118086030085881
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.008594840330382188
step = 4, Training Accuracy: 0.8933333333333333
Training loss = 0.01050671085715294
step = 5, Training Accuracy: 0.89
Validation Accuracy: 0.7475
Training loss = 0.009595254187782605
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.011191774730881055
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.009939903020858764
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.010800506174564361
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.008949869324763616
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.75
Training loss = 0.011427417248487472
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.011483990326523781
step = 12, Training Accuracy: 0.88
Training loss = 0.009222024430831274
step = 13, Training Accuracy: 0.89
Training loss = 0.00843284954627355
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.76125
params:  [0.1235210353147288, 0.01, 0.2447166264357865, 0.4368215744801985, 0.5888242195738267, 0.11984778451413147, 0.658157839236405, 0.22317355203780964, 0.7671201050205249, 0.9761018289626282, 0.01, 0.3894428131463826, 0.45009836116867025, 0.3743112945756811, 0.1343674330086454, 0.13409225445789608, 0.38754420858107186]
[0.1235210353147288, 0.01, 0.2447166264357865, 0.4368215744801985, 0.5888242195738267, 0.11984778451413147, 0.658157839236405, 0.22317355203780964, 0.7671201050205249, 0.9761018289626282, 0.01, 0.3894428131463826, 0.45009836116867025, 0.3743112945756811, 0.1343674330086454, 0.13409225445789608, 0.38754420858107186]
Training loss = 0.009646241416533788
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.755
Training loss = 0.01031578227877617
step = 1, Training Accuracy: 0.8833333333333333
Training loss = 0.01010423019528389
step = 2, Training Accuracy: 0.89
Training loss = 0.011090774734814962
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.008618525515000025
step = 4, Training Accuracy: 0.9033333333333333
Training loss = 0.011553678810596466
step = 5, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.74375
Training loss = 0.00968314528465271
step = 6, Training Accuracy: 0.88
Training loss = 0.010817482868830363
step = 7, Training Accuracy: 0.8833333333333333
Training loss = 0.008995765621463458
step = 8, Training Accuracy: 0.9033333333333333
Training loss = 0.010404678185780843
step = 9, Training Accuracy: 0.8833333333333333
Training loss = 0.008961242685715357
step = 10, Training Accuracy: 0.89
Validation Accuracy: 0.7575
Training loss = 0.009023550500472387
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.008770063320795695
step = 12, Training Accuracy: 0.8866666666666667
Training loss = 0.009417434086402257
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.009466101129849752
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.7425
params:  [0.23399256460883042, 0.01, 0.3204963169869879, 0.5411401665977709, 0.3524209739341755, 0.049211291540545515, 0.41652040858691297, 0.1910987681053316, 0.99, 0.8330963744634149, 0.16894187509981015, 0.04461770696792042, 0.4092584622382934, 0.3524431915286983, 0.3374872358189144, 0.01, 0.49846388336738484]
[0.23399256460883042, 0.01, 0.3204963169869879, 0.5411401665977709, 0.3524209739341755, 0.049211291540545515, 0.41652040858691297, 0.1910987681053316, 0.99, 0.8330963744634149, 0.16894187509981015, 0.04461770696792042, 0.4092584622382934, 0.3524431915286983, 0.3374872358189144, 0.01, 0.49846388336738484]
Training loss = 0.011397223969300587
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.75
Training loss = 0.011330929597218832
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.011006774107615153
step = 2, Training Accuracy: 0.8733333333333333
Training loss = 0.012841320435206095
step = 3, Training Accuracy: 0.84
Training loss = 0.010241749982039134
step = 4, Training Accuracy: 0.8833333333333333
Training loss = 0.010892834067344665
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.755
Training loss = 0.011707543780406317
step = 6, Training Accuracy: 0.86
Training loss = 0.01199458286166191
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.00874572673191627
step = 8, Training Accuracy: 0.8833333333333333
Training loss = 0.00988456313808759
step = 9, Training Accuracy: 0.89
Training loss = 0.010318836371103923
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7525
Training loss = 0.009769039899110795
step = 11, Training Accuracy: 0.8866666666666667
Training loss = 0.009369215120871861
step = 12, Training Accuracy: 0.8766666666666667
Training loss = 0.009612196584542593
step = 13, Training Accuracy: 0.8866666666666667
Training loss = 0.009021237790584564
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.755
params:  [0.12204777371300642, 0.01, 0.4487769315249039, 0.2852263030456266, 0.43253681508646347, 0.16237495916650285, 0.4887571446109042, 0.14650436923814933, 0.9352842785797069, 0.9422611667455857, 0.01, 0.2660093459824796, 0.5742789824771555, 0.3699582549157457, 0.3076891260045921, 0.1373648600605079, 0.48460417234145187]
[0.12204777371300642, 0.01, 0.4487769315249039, 0.2852263030456266, 0.43253681508646347, 0.16237495916650285, 0.4887571446109042, 0.14650436923814933, 0.9352842785797069, 0.9422611667455857, 0.01, 0.2660093459824796, 0.5742789824771555, 0.3699582549157457, 0.3076891260045921, 0.1373648600605079, 0.48460417234145187]
Training loss = 0.008430056795477868
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.7575
Training loss = 0.009390588055054347
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.010463840911785762
step = 2, Training Accuracy: 0.8566666666666667
Training loss = 0.010513267119725545
step = 3, Training Accuracy: 0.88
Training loss = 0.010562199801206588
step = 4, Training Accuracy: 0.9066666666666666
Training loss = 0.010420132825771967
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.75125
Training loss = 0.009342279781897863
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.009711426148811975
step = 7, Training Accuracy: 0.87
Training loss = 0.009276431500911713
step = 8, Training Accuracy: 0.88
Training loss = 0.009507853190104167
step = 9, Training Accuracy: 0.87
Training loss = 0.008366955121358237
step = 10, Training Accuracy: 0.9
Validation Accuracy: 0.75
Training loss = 0.008198409875233969
step = 11, Training Accuracy: 0.8966666666666666
Training loss = 0.009244177291790645
step = 12, Training Accuracy: 0.8566666666666667
Training loss = 0.008829033325115839
step = 13, Training Accuracy: 0.89
Training loss = 0.010290456290046374
step = 14, Training Accuracy: 0.87
Validation Accuracy: 0.745
params:  [0.261083174722958, 0.01, 0.43577469675086744, 0.5052587205220178, 0.09747584049580138, 0.24832242356921183, 0.6449056932222692, 0.20307992705944228, 0.99, 0.8439245785728062, 0.18927844877130168, 0.3407752109723994, 0.4995635038332666, 0.336623595727965, 0.194703009356103, 0.2230430629815399, 0.45694079325719433]
[0.261083174722958, 0.01, 0.43577469675086744, 0.5052587205220178, 0.09747584049580138, 0.24832242356921183, 0.6449056932222692, 0.20307992705944228, 0.99, 0.8439245785728062, 0.18927844877130168, 0.3407752109723994, 0.4995635038332666, 0.336623595727965, 0.194703009356103, 0.2230430629815399, 0.45694079325719433]
Training loss = 0.00938832864165306
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.7425
Training loss = 0.010475324243307113
step = 1, Training Accuracy: 0.87
Training loss = 0.009137100925048193
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.008074183737238248
step = 3, Training Accuracy: 0.9066666666666666
Training loss = 0.00907981589436531
step = 4, Training Accuracy: 0.9133333333333333
Training loss = 0.008646045277516046
step = 5, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.73625
Training loss = 0.00891912726064523
step = 6, Training Accuracy: 0.9033333333333333
Training loss = 0.010019180203477541
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.007917508979638417
step = 8, Training Accuracy: 0.91
Training loss = 0.009530682762463888
step = 9, Training Accuracy: 0.88
Training loss = 0.00820426846543948
step = 10, Training Accuracy: 0.9133333333333333
Validation Accuracy: 0.73
Training loss = 0.009097145001093546
step = 11, Training Accuracy: 0.9233333333333333
Training loss = 0.00881381387511889
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.008891450762748719
step = 13, Training Accuracy: 0.8966666666666666
Training loss = 0.007772881103058656
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.73125
params:  [0.2163400830118799, 0.01, 0.24737280705761278, 0.3198860933171239, 0.24995828082530439, 0.07265108309470153, 0.37721960692271267, 0.35571499474085255, 0.99, 0.7674252579364996, 0.2796616305728717, 0.18881209758725953, 0.386686826100799, 0.18402369379348574, 0.35309640403503817, 0.01, 0.4312635853091657]
[0.2163400830118799, 0.01, 0.24737280705761278, 0.3198860933171239, 0.24995828082530439, 0.07265108309470153, 0.37721960692271267, 0.35571499474085255, 0.99, 0.7674252579364996, 0.2796616305728717, 0.18881209758725953, 0.386686826100799, 0.18402369379348574, 0.35309640403503817, 0.01, 0.4312635853091657]
Training loss = 0.008995381444692611
step = 0, Training Accuracy: 0.88
Validation Accuracy: 0.73375
Training loss = 0.009819839348395665
step = 1, Training Accuracy: 0.8933333333333333
Training loss = 0.0085106726984183
step = 2, Training Accuracy: 0.9066666666666666
Training loss = 0.009408846249183018
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.008999668955802918
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.010731756339470546
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7425
Training loss = 0.010898324251174927
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.008736280674735706
step = 7, Training Accuracy: 0.92
Training loss = 0.007943058585127194
step = 8, Training Accuracy: 0.92
Training loss = 0.008646352589130402
step = 9, Training Accuracy: 0.89
Training loss = 0.007924636378884315
step = 10, Training Accuracy: 0.9166666666666666
Validation Accuracy: 0.7325
Training loss = 0.0069728917131821316
step = 11, Training Accuracy: 0.9133333333333333
Training loss = 0.008101683706045151
step = 12, Training Accuracy: 0.92
Training loss = 0.009371374795834223
step = 13, Training Accuracy: 0.9
Training loss = 0.008957340766986212
step = 14, Training Accuracy: 0.9066666666666666
Validation Accuracy: 0.73875
21 	8     	0.744687	0.010602  	0.73   	0.76125
params:  [0.3478098711647015, 0.3035019234740824, 0.021583893599550485, 0.5019080091285512, 0.6175486000430848, 0.12305894941133136, 0.22607565946653133, 0.3201600844961329, 0.99, 0.8140508782274736, 0.010163488955957839, 0.031190197321855736, 0.39989225486718516, 0.2816509132962399, 0.3418462822700373, 0.12497296331218243, 0.5117955319765408]
[0.3478098711647015, 0.3035019234740824, 0.021583893599550485, 0.5019080091285512, 0.6175486000430848, 0.12305894941133136, 0.22607565946653133, 0.3201600844961329, 0.99, 0.8140508782274736, 0.010163488955957839, 0.031190197321855736, 0.39989225486718516, 0.2816509132962399, 0.3418462822700373, 0.12497296331218243, 0.5117955319765408]
Training loss = 0.012247522821029028
step = 0, Training Accuracy: 0.82
Validation Accuracy: 0.75125
Training loss = 0.011847920914491017
step = 1, Training Accuracy: 0.8466666666666667
Training loss = 0.01056273765861988
step = 2, Training Accuracy: 0.87
Training loss = 0.014268452127774556
step = 3, Training Accuracy: 0.84
Training loss = 0.013616259694099426
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.012182074338197709
step = 5, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.755
Training loss = 0.010108793824911118
step = 6, Training Accuracy: 0.87
Training loss = 0.011041626085837681
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.009807424247264862
step = 8, Training Accuracy: 0.8666666666666667
Training loss = 0.011404701371987661
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.01174044981598854
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.755
Training loss = 0.013150158027807871
step = 11, Training Accuracy: 0.8233333333333334
Training loss = 0.01107420469323794
step = 12, Training Accuracy: 0.85
Training loss = 0.011690161824226379
step = 13, Training Accuracy: 0.8333333333333334
Training loss = 0.012878173689047496
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.75375
params:  [0.20497801253719997, 0.02855038561103578, 0.3663620741510415, 0.4195401707513625, 0.46932530382658744, 0.01, 0.5690701956741397, 0.17544725925679105, 0.99, 0.8935069760274745, 0.22354556210437934, 0.13648681089456938, 0.5330963238783881, 0.32435101045479525, 0.39022462295062416, 0.015044379263236915, 0.23792377188503017]
[0.20497801253719997, 0.02855038561103578, 0.3663620741510415, 0.4195401707513625, 0.46932530382658744, 0.01, 0.5690701956741397, 0.17544725925679105, 0.99, 0.8935069760274745, 0.22354556210437934, 0.13648681089456938, 0.5330963238783881, 0.32435101045479525, 0.39022462295062416, 0.015044379263236915, 0.23792377188503017]
Training loss = 0.00928877184788386
step = 0, Training Accuracy: 0.89
Validation Accuracy: 0.75375
Training loss = 0.011045472671588261
step = 1, Training Accuracy: 0.8433333333333334
Training loss = 0.011586497823397318
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.00964850587149461
step = 3, Training Accuracy: 0.87
Training loss = 0.011646889199813206
step = 4, Training Accuracy: 0.8666666666666667
Training loss = 0.010290736878911654
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.7525
Training loss = 0.011653656462828317
step = 6, Training Accuracy: 0.8566666666666667
Training loss = 0.009796009163061777
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.010143090734879175
step = 8, Training Accuracy: 0.88
Training loss = 0.009741207510232925
step = 9, Training Accuracy: 0.89
Training loss = 0.012641295393308005
step = 10, Training Accuracy: 0.82
Validation Accuracy: 0.75125
Training loss = 0.010235893055796624
step = 11, Training Accuracy: 0.8633333333333333
Training loss = 0.011352354337771734
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.010852543065945307
step = 13, Training Accuracy: 0.8466666666666667
Training loss = 0.008929667522509893
step = 14, Training Accuracy: 0.9
Validation Accuracy: 0.74875
params:  [0.32927173925761055, 0.10684367508235887, 0.28526188226340327, 0.5923168788353692, 0.3590371871613899, 0.01, 0.37085092768514244, 0.045592768177341564, 0.8724611940059726, 0.8171348426939921, 0.09044993258955977, 0.01, 0.5268183014684161, 0.4155914192242315, 0.2538788790876466, 0.07436412790007985, 0.595639574438131]
[0.32927173925761055, 0.10684367508235887, 0.28526188226340327, 0.5923168788353692, 0.3590371871613899, 0.01, 0.37085092768514244, 0.045592768177341564, 0.8724611940059726, 0.8171348426939921, 0.09044993258955977, 0.01, 0.5268183014684161, 0.4155914192242315, 0.2538788790876466, 0.07436412790007985, 0.595639574438131]
Training loss = 0.01079663872718811
step = 0, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.74875
Training loss = 0.010756462067365646
step = 1, Training Accuracy: 0.8666666666666667
Training loss = 0.011019512017567953
step = 2, Training Accuracy: 0.8766666666666667
Training loss = 0.011800762688120206
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.01110692525903384
step = 4, Training Accuracy: 0.88
Training loss = 0.010480829725662867
step = 5, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.7525
Training loss = 0.008723995933930079
step = 6, Training Accuracy: 0.91
Training loss = 0.009104282756646475
step = 7, Training Accuracy: 0.9
Training loss = 0.009286844879388809
step = 8, Training Accuracy: 0.8933333333333333
Training loss = 0.01125461682677269
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.011288444772362708
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.755
Training loss = 0.01086027830839157
step = 11, Training Accuracy: 0.88
Training loss = 0.009810273547967275
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.010418317516644796
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.013050893545150757
step = 14, Training Accuracy: 0.84
Validation Accuracy: 0.75
params:  [0.16705012248462753, 0.01, 0.162919120488419, 0.6129839763694434, 0.3498589323902395, 0.26947192061613934, 0.42687818992078386, 0.01, 0.99, 0.5890747759913582, 0.0418116225864036, 0.01, 0.3601848270039158, 0.41448638188972853, 0.4617890184254586, 0.01, 0.2937496776304048]
[0.16705012248462753, 0.01, 0.162919120488419, 0.6129839763694434, 0.3498589323902395, 0.26947192061613934, 0.42687818992078386, 0.01, 0.99, 0.5890747759913582, 0.0418116225864036, 0.01, 0.3601848270039158, 0.41448638188972853, 0.4617890184254586, 0.01, 0.2937496776304048]
Training loss = 0.010920124153296153
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.74875
Training loss = 0.009761353631814322
step = 1, Training Accuracy: 0.8766666666666667
Training loss = 0.009859737108151119
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.010397341996431352
step = 3, Training Accuracy: 0.8766666666666667
Training loss = 0.010340539266665777
step = 4, Training Accuracy: 0.86
Training loss = 0.010881371150414149
step = 5, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.75375
Training loss = 0.009677270352840424
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.010796062052249908
step = 7, Training Accuracy: 0.8533333333333334
Training loss = 0.009058121020595233
step = 8, Training Accuracy: 0.88
Training loss = 0.009481203059355419
step = 9, Training Accuracy: 0.8733333333333333
Training loss = 0.010455538084109624
step = 10, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.75
Training loss = 0.011477734446525573
step = 11, Training Accuracy: 0.8566666666666667
Training loss = 0.011097007940212886
step = 12, Training Accuracy: 0.8466666666666667
Training loss = 0.009462871054808298
step = 13, Training Accuracy: 0.87
Training loss = 0.01025622859597206
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.75375
params:  [0.10709420889259338, 0.01, 0.475031204890614, 0.5676211813513033, 0.32677556858154033, 0.016223186926940025, 0.2652141179010532, 0.01, 0.7962250031214788, 0.754432052066135, 0.06535316679495204, 0.01, 0.3750564704348292, 0.40273790952153826, 0.31642376741604716, 0.01, 0.4980662151479203]
[0.10709420889259338, 0.01, 0.475031204890614, 0.5676211813513033, 0.32677556858154033, 0.016223186926940025, 0.2652141179010532, 0.01, 0.7962250031214788, 0.754432052066135, 0.06535316679495204, 0.01, 0.3750564704348292, 0.40273790952153826, 0.31642376741604716, 0.01, 0.4980662151479203]
Training loss = 0.011740850905577341
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.7475
Training loss = 0.013535037090380987
step = 1, Training Accuracy: 0.84
Training loss = 0.011914790719747543
step = 2, Training Accuracy: 0.85
Training loss = 0.012448353370030722
step = 3, Training Accuracy: 0.8533333333333334
Training loss = 0.011466076597571373
step = 4, Training Accuracy: 0.87
Training loss = 0.013018759886423746
step = 5, Training Accuracy: 0.86
Validation Accuracy: 0.74625
Training loss = 0.012434116452932359
step = 6, Training Accuracy: 0.84
Training loss = 0.012412179112434387
step = 7, Training Accuracy: 0.8433333333333334
Training loss = 0.01137409970164299
step = 8, Training Accuracy: 0.85
Training loss = 0.010309569587310155
step = 9, Training Accuracy: 0.85
Training loss = 0.012778752247492472
step = 10, Training Accuracy: 0.87
Validation Accuracy: 0.74125
Training loss = 0.010259704689184825
step = 11, Training Accuracy: 0.8833333333333333
Training loss = 0.012028142958879471
step = 12, Training Accuracy: 0.8333333333333334
Training loss = 0.00967581828435262
step = 13, Training Accuracy: 0.88
Training loss = 0.010738423814376195
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.7525
params:  [0.04088519380796701, 0.01, 0.25740322300203294, 0.5367259848554665, 0.40291729351155586, 0.3574415876316192, 0.47520821959593673, 0.01, 0.99, 0.712408317795949, 0.05644789079397644, 0.15672709697556708, 0.42647398070361114, 0.033173500114651866, 0.4501419481719102, 0.01, 0.4383137257077488]
[0.04088519380796701, 0.01, 0.25740322300203294, 0.5367259848554665, 0.40291729351155586, 0.3574415876316192, 0.47520821959593673, 0.01, 0.99, 0.712408317795949, 0.05644789079397644, 0.15672709697556708, 0.42647398070361114, 0.033173500114651866, 0.4501419481719102, 0.01, 0.4383137257077488]
Training loss = 0.010220062235991161
step = 0, Training Accuracy: 0.88
Validation Accuracy: 0.7425
Training loss = 0.00809464544057846
step = 1, Training Accuracy: 0.89
Training loss = 0.008168147206306457
step = 2, Training Accuracy: 0.8833333333333333
Training loss = 0.008139725426832835
step = 3, Training Accuracy: 0.9066666666666666
Training loss = 0.007799300526579221
step = 4, Training Accuracy: 0.9166666666666666
Training loss = 0.00795066292087237
step = 5, Training Accuracy: 0.93
Validation Accuracy: 0.72875
Training loss = 0.009361629734436671
step = 6, Training Accuracy: 0.8866666666666667
Training loss = 0.0077257785697778065
step = 7, Training Accuracy: 0.8966666666666666
Training loss = 0.007203263367215792
step = 8, Training Accuracy: 0.93
Training loss = 0.0083078304429849
step = 9, Training Accuracy: 0.92
Training loss = 0.007499942630529404
step = 10, Training Accuracy: 0.8966666666666666
Validation Accuracy: 0.72125
Training loss = 0.00833693727850914
step = 11, Training Accuracy: 0.9066666666666666
Training loss = 0.0083515352755785
step = 12, Training Accuracy: 0.9
Training loss = 0.00751148521900177
step = 13, Training Accuracy: 0.91
Training loss = 0.008158653825521469
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.72875
params:  [0.28190979472876104, 0.053971960106442514, 0.32897008837449965, 0.4237173218460874, 0.32626901060197055, 0.19519926538434476, 0.4836641199753472, 0.09903078860507143, 0.7177494198820547, 0.7752411933858562, 0.2203118060595982, 0.2148062150614799, 0.4181910020137253, 0.3033658594418675, 0.37790627176723807, 0.15506429561633905, 0.4275498739381738]
[0.28190979472876104, 0.053971960106442514, 0.32897008837449965, 0.4237173218460874, 0.32626901060197055, 0.19519926538434476, 0.4836641199753472, 0.09903078860507143, 0.7177494198820547, 0.7752411933858562, 0.2203118060595982, 0.2148062150614799, 0.4181910020137253, 0.3033658594418675, 0.37790627176723807, 0.15506429561633905, 0.4275498739381738]
Training loss = 0.01112390287220478
step = 0, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.73625
Training loss = 0.01140771875778834
step = 1, Training Accuracy: 0.84
Training loss = 0.012253299901882807
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.010928731809059778
step = 3, Training Accuracy: 0.8566666666666667
Training loss = 0.010962360799312591
step = 4, Training Accuracy: 0.8566666666666667
Training loss = 0.010989095071951548
step = 5, Training Accuracy: 0.8633333333333333
Validation Accuracy: 0.75
Training loss = 0.01145560160279274
step = 6, Training Accuracy: 0.8833333333333333
Training loss = 0.011710679680109025
step = 7, Training Accuracy: 0.8366666666666667
Training loss = 0.009854783291618029
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.010318008760611216
step = 9, Training Accuracy: 0.8666666666666667
Training loss = 0.009601219768325488
step = 10, Training Accuracy: 0.8933333333333333
Validation Accuracy: 0.75
Training loss = 0.011425792425870895
step = 11, Training Accuracy: 0.87
Training loss = 0.010371619512637457
step = 12, Training Accuracy: 0.8833333333333333
Training loss = 0.011641640563805898
step = 13, Training Accuracy: 0.8566666666666667
Training loss = 0.01049501617749532
step = 14, Training Accuracy: 0.88
Validation Accuracy: 0.74875
params:  [0.13108918744412978, 0.1443246265283307, 0.17841699788228615, 0.678658293152951, 0.2818543210943473, 0.2019206262226561, 0.403803249304929, 0.01, 0.8208664615641454, 0.9504395594228569, 0.07467470073429307, 0.28292942407326, 0.48960816852277644, 0.3711685787414864, 0.3964843927238912, 0.08238815956492204, 0.5328770308502958]
[0.13108918744412978, 0.1443246265283307, 0.17841699788228615, 0.678658293152951, 0.2818543210943473, 0.2019206262226561, 0.403803249304929, 0.01, 0.8208664615641454, 0.9504395594228569, 0.07467470073429307, 0.28292942407326, 0.48960816852277644, 0.3711685787414864, 0.3964843927238912, 0.08238815956492204, 0.5328770308502958]
Training loss = 0.011576239665349325
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.745
Training loss = 0.009591508706410725
step = 1, Training Accuracy: 0.89
Training loss = 0.011979060123364131
step = 2, Training Accuracy: 0.8366666666666667
Training loss = 0.00959736945728461
step = 3, Training Accuracy: 0.8866666666666667
Training loss = 0.01074734536310037
step = 4, Training Accuracy: 0.8733333333333333
Training loss = 0.011031469106674194
step = 5, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.74125
Training loss = 0.012404892891645432
step = 6, Training Accuracy: 0.8666666666666667
Training loss = 0.01140873317917188
step = 7, Training Accuracy: 0.8466666666666667
Training loss = 0.011371295750141144
step = 8, Training Accuracy: 0.86
Training loss = 0.010120472386479378
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.011423279444376627
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7375
Training loss = 0.01151302660504977
step = 11, Training Accuracy: 0.87
Training loss = 0.011811309357484182
step = 12, Training Accuracy: 0.85
Training loss = 0.009785197824239731
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.009494132349888483
step = 14, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.74375
22 	8     	0.7475  	0.00773082	0.72875	0.75375
params:  [0.41144076368147603, 0.24319993859164332, 0.0960074592159976, 0.736705021420652, 0.43976970467015786, 0.25376312623832764, 0.3648191635555468, 0.223484381750145, 0.8279565436142869, 0.888190233677808, 0.01, 0.01, 0.39915348575117965, 0.28019485321918247, 0.375993466124416, 0.01, 0.5702557481122424]
[0.41144076368147603, 0.24319993859164332, 0.0960074592159976, 0.736705021420652, 0.43976970467015786, 0.25376312623832764, 0.3648191635555468, 0.223484381750145, 0.8279565436142869, 0.888190233677808, 0.01, 0.01, 0.39915348575117965, 0.28019485321918247, 0.375993466124416, 0.01, 0.5702557481122424]
Training loss = 0.011422244757413864
step = 0, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.75125
Training loss = 0.01065991019209226
step = 1, Training Accuracy: 0.85
Training loss = 0.010974235534667968
step = 2, Training Accuracy: 0.8666666666666667
Training loss = 0.010339949031670887
step = 3, Training Accuracy: 0.87
Training loss = 0.01128909354408582
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.010731966644525527
step = 5, Training Accuracy: 0.87
Validation Accuracy: 0.7475
Training loss = 0.01114381136993567
step = 6, Training Accuracy: 0.86
Training loss = 0.010430174221595128
step = 7, Training Accuracy: 0.8566666666666667
Training loss = 0.011753470798333486
step = 8, Training Accuracy: 0.8566666666666667
Training loss = 0.010609197467565536
step = 9, Training Accuracy: 0.85
Training loss = 0.01236534963051478
step = 10, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.74375
Training loss = 0.012013643284638723
step = 11, Training Accuracy: 0.84
Training loss = 0.01051685854792595
step = 12, Training Accuracy: 0.8666666666666667
Training loss = 0.011175842185815176
step = 13, Training Accuracy: 0.8733333333333333
Training loss = 0.01024015004436175
step = 14, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.74625
params:  [0.08451259702449077, 0.2801605329294792, 0.23601186220187795, 0.502705428879585, 0.4634535817920116, 0.10679857613470266, 0.38899122982565415, 0.049812402347754414, 0.8296740382446341, 0.849251054743953, 0.01, 0.0567742117485939, 0.48239925204053713, 0.48092199676148684, 0.31354721422645304, 0.04005900402094406, 0.4011778795667026]
[0.08451259702449077, 0.2801605329294792, 0.23601186220187795, 0.502705428879585, 0.4634535817920116, 0.10679857613470266, 0.38899122982565415, 0.049812402347754414, 0.8296740382446341, 0.849251054743953, 0.01, 0.0567742117485939, 0.48239925204053713, 0.48092199676148684, 0.31354721422645304, 0.04005900402094406, 0.4011778795667026]
Training loss = 0.010319445878267289
step = 0, Training Accuracy: 0.8533333333333334
Validation Accuracy: 0.7375
Training loss = 0.010142756998538971
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.010940897067387899
step = 2, Training Accuracy: 0.86
Training loss = 0.009885279436906178
step = 3, Training Accuracy: 0.88
Training loss = 0.009990536918242773
step = 4, Training Accuracy: 0.89
Training loss = 0.010085180004437765
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.7475
Training loss = 0.011471405227979025
step = 6, Training Accuracy: 0.82
Training loss = 0.008943993051846823
step = 7, Training Accuracy: 0.8933333333333333
Training loss = 0.009545234690109889
step = 8, Training Accuracy: 0.87
Training loss = 0.00944599876801173
step = 9, Training Accuracy: 0.8766666666666667
Training loss = 0.00967484936118126
step = 10, Training Accuracy: 0.8833333333333333
Validation Accuracy: 0.74625
Training loss = 0.01000735469162464
step = 11, Training Accuracy: 0.8633333333333333
Training loss = 0.008033531854550044
step = 12, Training Accuracy: 0.9
Training loss = 0.007997435033321381
step = 13, Training Accuracy: 0.8933333333333333
Training loss = 0.009954040696223576
step = 14, Training Accuracy: 0.9033333333333333
Validation Accuracy: 0.74875
params:  [0.06151058678477678, 0.05634955528118235, 0.18267173063106115, 0.5601041042599345, 0.5923116326897316, 0.01, 0.34360545854731106, 0.057467600624343335, 0.99, 0.7706013883373777, 0.08862601650738218, 0.01, 0.6663018463751224, 0.41351559117792286, 0.36930388377829837, 0.015414487254506844, 0.3153589351698145]
[0.06151058678477678, 0.05634955528118235, 0.18267173063106115, 0.5601041042599345, 0.5923116326897316, 0.01, 0.34360545854731106, 0.057467600624343335, 0.99, 0.7706013883373777, 0.08862601650738218, 0.01, 0.6663018463751224, 0.41351559117792286, 0.36930388377829837, 0.015414487254506844, 0.3153589351698145]
Training loss = 0.013014606883128483
step = 0, Training Accuracy: 0.8033333333333333
Validation Accuracy: 0.75
Training loss = 0.01288397858540217
step = 1, Training Accuracy: 0.8233333333333334
Training loss = 0.012743211736281713
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.013394017865260443
step = 3, Training Accuracy: 0.84
Training loss = 0.011774426400661469
step = 4, Training Accuracy: 0.84
Training loss = 0.014885266572237014
step = 5, Training Accuracy: 0.82
Validation Accuracy: 0.75125
Training loss = 0.013069896350304285
step = 6, Training Accuracy: 0.84
Training loss = 0.01230790620048841
step = 7, Training Accuracy: 0.84
Training loss = 0.015311795522769293
step = 8, Training Accuracy: 0.8
Training loss = 0.012888965209325155
step = 9, Training Accuracy: 0.82
Training loss = 0.010183655818303427
step = 10, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.7525
Training loss = 0.010601634681224823
step = 11, Training Accuracy: 0.8666666666666667
Training loss = 0.014253291189670564
step = 12, Training Accuracy: 0.8366666666666667
Training loss = 0.012810304462909698
step = 13, Training Accuracy: 0.8433333333333334
Training loss = 0.013385104537010193
step = 14, Training Accuracy: 0.82
Validation Accuracy: 0.74875
params:  [0.25584792337295603, 0.19908190239499862, 0.07862492474521768, 0.49115235833629056, 0.34033457394615385, 0.014455059384116759, 0.1885507002899932, 0.26362518223529674, 0.99, 0.7415792223845429, 0.04316777796752877, 0.12381473154096567, 0.2842782640755267, 0.2777944464871608, 0.31327429505752374, 0.16259546683414644, 0.5538850987587909]
[0.25584792337295603, 0.19908190239499862, 0.07862492474521768, 0.49115235833629056, 0.34033457394615385, 0.014455059384116759, 0.1885507002899932, 0.26362518223529674, 0.99, 0.7415792223845429, 0.04316777796752877, 0.12381473154096567, 0.2842782640755267, 0.2777944464871608, 0.31327429505752374, 0.16259546683414644, 0.5538850987587909]
Training loss = 0.009896570046742758
step = 0, Training Accuracy: 0.8766666666666667
Validation Accuracy: 0.75
Training loss = 0.0117433234055837
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.013096184035142263
step = 2, Training Accuracy: 0.8233333333333334
Training loss = 0.00976023440559705
step = 3, Training Accuracy: 0.8833333333333333
Training loss = 0.010126940061648687
step = 4, Training Accuracy: 0.87
Training loss = 0.009724302018682162
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.7425
Training loss = 0.01122480238477389
step = 6, Training Accuracy: 0.8366666666666667
Training loss = 0.01104581207036972
step = 7, Training Accuracy: 0.8866666666666667
Training loss = 0.01183065190911293
step = 8, Training Accuracy: 0.85
Training loss = 0.010474876264731089
step = 9, Training Accuracy: 0.87
Training loss = 0.010425633937120437
step = 10, Training Accuracy: 0.8733333333333333
Validation Accuracy: 0.7475
Training loss = 0.010133087585369746
step = 11, Training Accuracy: 0.89
Training loss = 0.009780954023202261
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.009812335918347041
step = 13, Training Accuracy: 0.89
Training loss = 0.009797904839118321
step = 14, Training Accuracy: 0.8666666666666667
Validation Accuracy: 0.75125
params:  [0.2443644130790555, 0.18744029065686615, 0.025888584874719706, 0.6781057844559595, 0.3524857461499247, 0.12266702744194155, 0.33244500444808167, 0.11899574734317486, 0.9020512336150899, 0.9351521417512954, 0.015461860177064643, 0.01, 0.44354311723258966, 0.23430803462593947, 0.4033718596649679, 0.01, 0.4201792282162888]
[0.2443644130790555, 0.18744029065686615, 0.025888584874719706, 0.6781057844559595, 0.3524857461499247, 0.12266702744194155, 0.33244500444808167, 0.11899574734317486, 0.9020512336150899, 0.9351521417512954, 0.015461860177064643, 0.01, 0.44354311723258966, 0.23430803462593947, 0.4033718596649679, 0.01, 0.4201792282162888]
Training loss = 0.010916808247566223
step = 0, Training Accuracy: 0.88
Validation Accuracy: 0.75625
Training loss = 0.008896192163228988
step = 1, Training Accuracy: 0.8633333333333333
Training loss = 0.009521530071894328
step = 2, Training Accuracy: 0.89
Training loss = 0.009150209824244181
step = 3, Training Accuracy: 0.8666666666666667
Training loss = 0.00864125907421112
step = 4, Training Accuracy: 0.8966666666666666
Training loss = 0.00923369305829207
step = 5, Training Accuracy: 0.88
Validation Accuracy: 0.74875
Training loss = 0.010204111486673354
step = 6, Training Accuracy: 0.8933333333333333
Training loss = 0.012778617292642593
step = 7, Training Accuracy: 0.85
Training loss = 0.009750270197788874
step = 8, Training Accuracy: 0.8866666666666667
Training loss = 0.00880342662334442
step = 9, Training Accuracy: 0.8866666666666667
Training loss = 0.008867550790309906
step = 10, Training Accuracy: 0.88
Validation Accuracy: 0.74125
Training loss = 0.008613167603810628
step = 11, Training Accuracy: 0.8733333333333333
Training loss = 0.00850996772448222
step = 12, Training Accuracy: 0.8933333333333333
Training loss = 0.008759922583897909
step = 13, Training Accuracy: 0.88
Training loss = 0.008739536056915919
step = 14, Training Accuracy: 0.8866666666666667
Validation Accuracy: 0.74125
params:  [0.3023544524199452, 0.33093372459903864, 0.01, 0.7285689866086784, 0.4602945959945048, 0.26583098152628026, 0.07812746165078865, 0.24772985576254397, 0.99, 0.42563391455284827, 0.10410333344421385, 0.15381796212259763, 0.4131859019372654, 0.22090334258845348, 0.4053253396731522, 0.02887698981322115, 0.45969417802226875]
[0.3023544524199452, 0.33093372459903864, 0.01, 0.7285689866086784, 0.4602945959945048, 0.26583098152628026, 0.07812746165078865, 0.24772985576254397, 0.99, 0.42563391455284827, 0.10410333344421385, 0.15381796212259763, 0.4131859019372654, 0.22090334258845348, 0.4053253396731522, 0.02887698981322115, 0.45969417802226875]
Training loss = 0.011939217448234557
step = 0, Training Accuracy: 0.85
Validation Accuracy: 0.75125
Training loss = 0.011619233240683874
step = 1, Training Accuracy: 0.86
Training loss = 0.011363570392131806
step = 2, Training Accuracy: 0.8533333333333334
Training loss = 0.010373203853766123
step = 3, Training Accuracy: 0.8433333333333334
Training loss = 0.011176418711741765
step = 4, Training Accuracy: 0.8533333333333334
Training loss = 0.011616293142239253
step = 5, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7375
Training loss = 0.012103420595328013
step = 6, Training Accuracy: 0.84
Training loss = 0.009878883709510167
step = 7, Training Accuracy: 0.8766666666666667
Training loss = 0.011170530021190643
step = 8, Training Accuracy: 0.8466666666666667
Training loss = 0.011659621546665827
step = 9, Training Accuracy: 0.8466666666666667
Training loss = 0.010764326776067416
step = 10, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74
Training loss = 0.011566713005304336
step = 11, Training Accuracy: 0.8466666666666667
Training loss = 0.011240702668825785
step = 12, Training Accuracy: 0.8533333333333334
Training loss = 0.012397761593262354
step = 13, Training Accuracy: 0.8766666666666667
Training loss = 0.010514029810825984
step = 14, Training Accuracy: 0.8466666666666667
Validation Accuracy: 0.74125
params:  [0.12433545520532979, 0.03821509630201128, 0.2551028161716668, 0.537282630801466, 0.6469159408214269, 0.03533878722049581, 0.472678415380396, 0.34536022039404685, 0.99, 0.6860254116053663, 0.01, 0.01, 0.47886443812845536, 0.3037188270334394, 0.4587209231313714, 0.3121513973785959, 0.479719105591029]
[0.12433545520532979, 0.03821509630201128, 0.2551028161716668, 0.537282630801466, 0.6469159408214269, 0.03533878722049581, 0.472678415380396, 0.34536022039404685, 0.99, 0.6860254116053663, 0.01, 0.01, 0.47886443812845536, 0.3037188270334394, 0.4587209231313714, 0.3121513973785959, 0.479719105591029]
Training loss = 0.013632077872753143
step = 0, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7475
Training loss = 0.015487828999757767
step = 1, Training Accuracy: 0.8066666666666666
Training loss = 0.01749602794647217
step = 2, Training Accuracy: 0.7533333333333333
Training loss = 0.013353381901979447
step = 3, Training Accuracy: 0.84
Training loss = 0.013179537951946258
step = 4, Training Accuracy: 0.8233333333333334
Training loss = 0.016113527168830237
step = 5, Training Accuracy: 0.7933333333333333
Validation Accuracy: 0.755
Training loss = 0.012887976964314778
step = 6, Training Accuracy: 0.8266666666666667
Training loss = 0.014799304654200872
step = 7, Training Accuracy: 0.8
Training loss = 0.014501979649066925
step = 8, Training Accuracy: 0.8166666666666667
Training loss = 0.013310882349809011
step = 9, Training Accuracy: 0.8333333333333334
Training loss = 0.01419661005338033
step = 10, Training Accuracy: 0.7966666666666666
Validation Accuracy: 0.7525
Training loss = 0.015680826902389526
step = 11, Training Accuracy: 0.8033333333333333
Training loss = 0.012880648523569106
step = 12, Training Accuracy: 0.7966666666666666
Training loss = 0.014383263091246287
step = 13, Training Accuracy: 0.83
Training loss = 0.014221322139104207
step = 14, Training Accuracy: 0.8233333333333334
Validation Accuracy: 0.75
params:  [0.2534993530772065, 0.307914041004697, 0.035414311652124664, 0.5014762728058649, 0.49052229006087467, 0.4065250595832214, 0.397743249911151, 0.08777992665106424, 0.8757566896289037, 0.6965927623844631, 0.1940074592745632, 0.15847066337047475, 0.529020349862599, 0.30223386618851605, 0.3144470710348147, 0.2548779024051863, 0.6184787671897656]
[0.2534993530772065, 0.307914041004697, 0.035414311652124664, 0.5014762728058649, 0.49052229006087467, 0.4065250595832214, 0.397743249911151, 0.08777992665106424, 0.8757566896289037, 0.6965927623844631, 0.1940074592745632, 0.15847066337047475, 0.529020349862599, 0.30223386618851605, 0.3144470710348147, 0.2548779024051863, 0.6184787671897656]
Training loss = 0.013348734726508459
step = 0, Training Accuracy: 0.84
Validation Accuracy: 0.74875
Training loss = 0.011533080587784449
step = 1, Training Accuracy: 0.8566666666666667
Training loss = 0.011793502618869146
step = 2, Training Accuracy: 0.83
Training loss = 0.012792096982399623
step = 3, Training Accuracy: 0.86
Training loss = 0.011958328758676846
step = 4, Training Accuracy: 0.83
Training loss = 0.011810662647088369
step = 5, Training Accuracy: 0.85
Validation Accuracy: 0.74
Training loss = 0.011579606632391612
step = 6, Training Accuracy: 0.83
Training loss = 0.011732504814863204
step = 7, Training Accuracy: 0.8666666666666667
Training loss = 0.011812434097131093
step = 8, Training Accuracy: 0.8633333333333333
Training loss = 0.013115133692820868
step = 9, Training Accuracy: 0.84
Training loss = 0.011385294298330942
step = 10, Training Accuracy: 0.8266666666666667
Validation Accuracy: 0.7525
Training loss = 0.012142989834149678
step = 11, Training Accuracy: 0.86
Training loss = 0.013165512283643087
step = 12, Training Accuracy: 0.8166666666666667
Training loss = 0.011633496085802714
step = 13, Training Accuracy: 0.85
Training loss = 0.013797013461589814
step = 14, Training Accuracy: 0.8566666666666667
Validation Accuracy: 0.74625
23 	8     	0.746719	0.00353208	0.74125	0.75125
params:  [0.087442891787359, 0.12778197851001147, 0.21432342428588327, 0.6195667675971332, 0.6611412901990854, 0.13068638750502592, 0.2823558768695793, 0.1268315479233985, 0.99, 0.6599519481522679, 0.2355977216060391, 0.30503569601688474, 0.32294150958702744, 0.3602086250339006, 0.3017726144195014, 0.01, 0.28213446087796934]
[0.087442891787359, 0.12778197851001147, 0.21432342428588327, 0.6195667675971332, 0.6611412901990854, 0.13068638750502592, 0.2823558768695793, 0.1268315479233985, 0.99, 0.6599519481522679, 0.2355977216060391, 0.30503569601688474, 0.32294150958702744, 0.3602086250339006, 0.3017726144195014, 0.01, 0.28213446087796934]
Training loss = 0.01250897337992986
step = 0, Training Accuracy: 0.8433333333333334
Validation Accuracy: 0.74375
Training loss = 0.01563701331615448
step = 1, Training Accuracy: 0.8166666666666667
Training loss = 0.01376248965660731
step = 2, Training Accuracy: 0.8166666666666667
Training loss = 0.013347343504428864
step = 3, Training Accuracy: 0.8066666666666666
Training loss = 0.012255294173955917
step = 4, Training Accuracy: 0.8666666666666667
