parameter = [0.47615826222934565, 0.6302190543188851, 0.40396096956847294, 0.9200240126763269, 0.01685162040226995, 0.9876105507362524]
Training loss = 0.03517182207533291
step = 0, Training Accuracy: 0.39589285714285716
Validation Accuracy: 0.48125
Training loss = 0.031569738632866315
step = 1, Training Accuracy: 0.47767857142857145
Training loss = 0.030656879427177566
step = 2, Training Accuracy: 0.49660714285714286
Training loss = 0.03047045756663595
step = 3, Training Accuracy: 0.49339285714285713
Training loss = 0.030163526439241
step = 4, Training Accuracy: 0.5044642857142857
Training loss = 0.029960333013108797
step = 5, Training Accuracy: 0.5371428571428571
Training loss = 0.029455066568085126
step = 6, Training Accuracy: 0.5516071428571429
Training loss = 0.028870198226400783
step = 7, Training Accuracy: 0.5594642857142857
Training loss = 0.02840345229421343
step = 8, Training Accuracy: 0.57375
Training loss = 0.02787833971636636
step = 9, Training Accuracy: 0.5871428571428572
Training loss = 0.02686052491622312
step = 10, Training Accuracy: 0.6085714285714285
Training loss = 0.02656225419470242
step = 11, Training Accuracy: 0.6101785714285715
Training loss = 0.026015627895082747
step = 12, Training Accuracy: 0.6151785714285715
Training loss = 0.025732969120144843
step = 13, Training Accuracy: 0.6282142857142857
Training loss = 0.024698136927826065
step = 14, Training Accuracy: 0.6466071428571428
Training loss = 0.024429397519145694
step = 15, Training Accuracy: 0.6414285714285715
Training loss = 0.024033455795475413
step = 16, Training Accuracy: 0.6569642857142857
Training loss = 0.02384901076555252
step = 17, Training Accuracy: 0.6546428571428572
Training loss = 0.023386854864656925
step = 18, Training Accuracy: 0.6689285714285714
Training loss = 0.022977728018803257
step = 19, Training Accuracy: 0.67
Validation Accuracy: 0.70875
parameter = [0.1474309569583241, 0.8613314138109517, 0.44003630577221464, 0.7212885992572478, 0.08366591038506865, 0.679464779137179]
Training loss = 0.023616540602275304
step = 0, Training Accuracy: 0.66375
Validation Accuracy: 0.73875
Training loss = 0.022804224879613946
step = 1, Training Accuracy: 0.6691071428571429
Training loss = 0.022358619485582622
step = 2, Training Accuracy: 0.6792857142857143
Training loss = 0.02201742427157504
step = 3, Training Accuracy: 0.6821428571428572
Training loss = 0.021875038838812283
step = 4, Training Accuracy: 0.6908928571428572
Training loss = 0.021976669398801666
step = 5, Training Accuracy: 0.6889285714285714
Training loss = 0.021103280234549725
step = 6, Training Accuracy: 0.7016071428571429
Training loss = 0.02118290952805962
step = 7, Training Accuracy: 0.7
Training loss = 0.021137687430850097
step = 8, Training Accuracy: 0.7023214285714285
Training loss = 0.02083008647497211
step = 9, Training Accuracy: 0.7157142857142857
Training loss = 0.020859963776809827
step = 10, Training Accuracy: 0.7073214285714285
Training loss = 0.020650280474552085
step = 11, Training Accuracy: 0.7044642857142858
Training loss = 0.02086332241339343
step = 12, Training Accuracy: 0.7048214285714286
Training loss = 0.020598632533635412
step = 13, Training Accuracy: 0.7091071428571428
Training loss = 0.020229408123663493
step = 14, Training Accuracy: 0.7058928571428571
Training loss = 0.020359064313982215
step = 15, Training Accuracy: 0.7132142857142857
Training loss = 0.020160631308598177
step = 16, Training Accuracy: 0.7128571428571429
Training loss = 0.020612975460078033
step = 17, Training Accuracy: 0.7101785714285714
Training loss = 0.01975278079509735
step = 18, Training Accuracy: 0.7201785714285714
Training loss = 0.01982305818902595
step = 19, Training Accuracy: 0.7230357142857143
Validation Accuracy: 0.7675
parameter = [0.19673746145383764, 0.8149524482305617, 0.7696797658382238, 0.39701291838018926, 0.2137573979486891, 0.7423923323846316]
Training loss = 0.020757886517260755
step = 0, Training Accuracy: 0.7041071428571428
Validation Accuracy: 0.77125
Training loss = 0.02059162964246103
step = 1, Training Accuracy: 0.7066071428571429
Training loss = 0.02018537366496665
step = 2, Training Accuracy: 0.7117857142857142
Training loss = 0.019882440253027846
step = 3, Training Accuracy: 0.7183928571428572
Training loss = 0.019962849100785596
step = 4, Training Accuracy: 0.7123214285714285
Training loss = 0.019928467369505338
step = 5, Training Accuracy: 0.7141071428571428
Training loss = 0.019868016791130816
step = 6, Training Accuracy: 0.7251785714285715
Training loss = 0.01955419280167137
step = 7, Training Accuracy: 0.7282142857142857
Training loss = 0.01966175915407283
step = 8, Training Accuracy: 0.7158928571428571
Training loss = 0.019695299118757247
step = 9, Training Accuracy: 0.72
Training loss = 0.019499010220170022
step = 10, Training Accuracy: 0.7251785714285715
Training loss = 0.019249579070934227
step = 11, Training Accuracy: 0.7330357142857142
Training loss = 0.019710690597338335
step = 12, Training Accuracy: 0.7221428571428572
Training loss = 0.01949655035244567
step = 13, Training Accuracy: 0.72625
Training loss = 0.01947453077350344
step = 14, Training Accuracy: 0.725
Training loss = 0.019218207320996692
step = 15, Training Accuracy: 0.725
Training loss = 0.019257585810763495
step = 16, Training Accuracy: 0.7233928571428572
Training loss = 0.019240650587848254
step = 17, Training Accuracy: 0.7275
Training loss = 0.019110817414309297
step = 18, Training Accuracy: 0.7258928571428571
Training loss = 0.018652148587363106
step = 19, Training Accuracy: 0.7394642857142857
Validation Accuracy: 0.77
parameter = [0.0016410517036646866, 0.4654349622200482, 0.21349210455755507, 0.05598080636124514, 0.2807600588893786, 0.6727775726121318]
Training loss = 0.02172086203204734
step = 0, Training Accuracy: 0.6810714285714285
Validation Accuracy: 0.73875
Training loss = 0.020829801160309996
step = 1, Training Accuracy: 0.7078571428571429
Training loss = 0.020200244784355163
step = 2, Training Accuracy: 0.7085714285714285
Training loss = 0.01999131773199354
step = 3, Training Accuracy: 0.7191071428571428
Training loss = 0.02008538099804095
step = 4, Training Accuracy: 0.7116071428571429
Training loss = 0.020042099149099418
step = 5, Training Accuracy: 0.7155357142857143
Training loss = 0.019609976916440897
step = 6, Training Accuracy: 0.72125
Training loss = 0.019705029662166324
step = 7, Training Accuracy: 0.7189285714285715
Training loss = 0.019797117710113527
step = 8, Training Accuracy: 0.7208928571428571
Training loss = 0.019395997039973734
step = 9, Training Accuracy: 0.7283928571428572
Training loss = 0.019676682044352804
step = 10, Training Accuracy: 0.7194642857142857
Training loss = 0.019344414893005576
step = 11, Training Accuracy: 0.7175
Training loss = 0.01947324504277536
step = 12, Training Accuracy: 0.7192857142857143
Training loss = 0.01913247449057443
step = 13, Training Accuracy: 0.7269642857142857
Training loss = 0.019571898074022362
step = 14, Training Accuracy: 0.7248214285714286
Training loss = 0.01915556372276374
step = 15, Training Accuracy: 0.7278571428571429
Training loss = 0.019285145572253637
step = 16, Training Accuracy: 0.7282142857142857
Training loss = 0.01921235653970923
step = 17, Training Accuracy: 0.7225
Training loss = 0.019058727446411336
step = 18, Training Accuracy: 0.7255357142857143
Training loss = 0.019311022290161677
step = 19, Training Accuracy: 0.7257142857142858
Validation Accuracy: 0.7825
gen	nevals	avg     	std      	min    	max   
0  	4     	0.757187	0.0285369	0.70875	0.7825
parameter = [0.15687593637536246, 0.8316686516823, 0.5831338908498166, 0.687370651320018, 0.09867448919758273, 0.6771803377593855]
Training loss = 0.020609823486634662
step = 0, Training Accuracy: 0.7041071428571428
Validation Accuracy: 0.76125
Training loss = 0.01968047239950725
step = 1, Training Accuracy: 0.7191071428571428
Training loss = 0.01918399564921856
step = 2, Training Accuracy: 0.7282142857142857
Training loss = 0.018518167824617453
step = 3, Training Accuracy: 0.7421428571428571
Training loss = 0.018810714866433823
step = 4, Training Accuracy: 0.7314285714285714
Training loss = 0.018905503952077456
step = 5, Training Accuracy: 0.7332142857142857
Training loss = 0.019110785579042774
step = 6, Training Accuracy: 0.72875
Training loss = 0.018528563028999738
step = 7, Training Accuracy: 0.7378571428571429
Training loss = 0.01848927503185613
step = 8, Training Accuracy: 0.7414285714285714
Training loss = 0.018825245771024908
step = 9, Training Accuracy: 0.7373214285714286
Training loss = 0.018615842307252543
step = 10, Training Accuracy: 0.7373214285714286
Training loss = 0.018249973064022405
step = 11, Training Accuracy: 0.7466071428571428
Training loss = 0.018558615329010146
step = 12, Training Accuracy: 0.7403571428571428
Training loss = 0.018270737411720413
step = 13, Training Accuracy: 0.7342857142857143
Training loss = 0.018591234354036194
step = 14, Training Accuracy: 0.7358928571428571
Training loss = 0.01816456218649234
step = 15, Training Accuracy: 0.7410714285714286
Training loss = 0.018284708112478256
step = 16, Training Accuracy: 0.7407142857142858
Training loss = 0.018417169504931996
step = 17, Training Accuracy: 0.7371428571428571
Training loss = 0.01799715200173003
step = 18, Training Accuracy: 0.7421428571428571
Training loss = 0.018358885895993028
step = 19, Training Accuracy: 0.7376785714285714
Validation Accuracy: 0.78125
parameter = [0.9937351930880042, 0.6999774952670302, 0.4506417252015659, 0.23635548700778367, 0.07663879046228922, 0.6776228457941125]
Training loss = 0.01889266552137477
step = 0, Training Accuracy: 0.7321428571428571
Validation Accuracy: 0.7825
Training loss = 0.018624135367572306
step = 1, Training Accuracy: 0.7398214285714285
Training loss = 0.018732854511056628
step = 2, Training Accuracy: 0.73375
Training loss = 0.018426475461040225
step = 3, Training Accuracy: 0.7433928571428572
Training loss = 0.018378860918538912
step = 4, Training Accuracy: 0.74
Training loss = 0.018509897286338464
step = 5, Training Accuracy: 0.7398214285714285
Training loss = 0.01823004479386977
step = 6, Training Accuracy: 0.7410714285714286
Training loss = 0.01809158895164728
step = 7, Training Accuracy: 0.7505357142857143
Training loss = 0.018408193891601903
step = 8, Training Accuracy: 0.7353571428571428
Training loss = 0.018220074054385936
step = 9, Training Accuracy: 0.7491071428571429
Training loss = 0.01847253206585135
step = 10, Training Accuracy: 0.7428571428571429
Training loss = 0.01810007306081908
step = 11, Training Accuracy: 0.7460714285714286
Training loss = 0.01821607927658728
step = 12, Training Accuracy: 0.7423214285714286
Training loss = 0.018321816681751184
step = 13, Training Accuracy: 0.7401785714285715
Training loss = 0.017535613498517445
step = 14, Training Accuracy: 0.7516071428571428
Training loss = 0.017963037746293205
step = 15, Training Accuracy: 0.7398214285714285
Training loss = 0.018419358512120586
step = 16, Training Accuracy: 0.7433928571428572
Training loss = 0.01813328200152942
step = 17, Training Accuracy: 0.74125
Training loss = 0.018216544522770814
step = 18, Training Accuracy: 0.7491071428571429
Training loss = 0.01825870012066194
step = 19, Training Accuracy: 0.7403571428571428
Validation Accuracy: 0.78625
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.019226615561970643
step = 0, Training Accuracy: 0.72625
Validation Accuracy: 0.77875
Training loss = 0.018752268366515636
step = 1, Training Accuracy: 0.7351785714285715
Training loss = 0.019022565494690623
step = 2, Training Accuracy: 0.7298214285714286
Training loss = 0.018606238375817027
step = 3, Training Accuracy: 0.7394642857142857
Training loss = 0.01837003743009908
step = 4, Training Accuracy: 0.7464285714285714
Training loss = 0.018349705854696887
step = 5, Training Accuracy: 0.7442857142857143
Training loss = 0.01857761091951813
step = 6, Training Accuracy: 0.7382142857142857
Training loss = 0.01811241256871394
step = 7, Training Accuracy: 0.7442857142857143
Training loss = 0.01826163664460182
step = 8, Training Accuracy: 0.7446428571428572
Training loss = 0.01842247238116605
step = 9, Training Accuracy: 0.7423214285714286
Training loss = 0.018066557060394967
step = 10, Training Accuracy: 0.7435714285714285
Training loss = 0.017801261597446032
step = 11, Training Accuracy: 0.7503571428571428
Training loss = 0.018164501381771905
step = 12, Training Accuracy: 0.7448214285714285
Training loss = 0.018534598888031075
step = 13, Training Accuracy: 0.7492857142857143
Training loss = 0.018197565749287607
step = 14, Training Accuracy: 0.7432142857142857
Training loss = 0.018103256151080133
step = 15, Training Accuracy: 0.7448214285714285
Training loss = 0.018264352705861842
step = 16, Training Accuracy: 0.7455357142857143
Training loss = 0.017833459877542087
step = 17, Training Accuracy: 0.7533928571428572
Training loss = 0.01803816381841898
step = 18, Training Accuracy: 0.7521428571428571
Training loss = 0.01801276371415172
step = 19, Training Accuracy: 0.7510714285714286
Validation Accuracy: 0.7825
parameter = [0.13453700546169453, 0.5329841124023544, 0.36514687429424186, 0.09777823707646693, 0.1930550135372103, 0.6778001918730293]
Training loss = 0.019848412041153226
step = 0, Training Accuracy: 0.7160714285714286
Validation Accuracy: 0.765
Training loss = 0.01932369134255818
step = 1, Training Accuracy: 0.7246428571428571
Training loss = 0.018624245817107813
step = 2, Training Accuracy: 0.7326785714285714
Training loss = 0.01862472331949643
step = 3, Training Accuracy: 0.7360714285714286
Training loss = 0.018786952825529234
step = 4, Training Accuracy: 0.7321428571428571
Training loss = 0.01852085112461022
step = 5, Training Accuracy: 0.7373214285714286
Training loss = 0.018860006220638752
step = 6, Training Accuracy: 0.7407142857142858
Training loss = 0.01845117735011237
step = 7, Training Accuracy: 0.7408928571428571
Training loss = 0.018741189784237316
step = 8, Training Accuracy: 0.7369642857142857
Training loss = 0.018577974140644073
step = 9, Training Accuracy: 0.7357142857142858
Training loss = 0.018550440626485008
step = 10, Training Accuracy: 0.7380357142857142
Training loss = 0.018904708426977907
step = 11, Training Accuracy: 0.7283928571428572
Training loss = 0.018300632626882623
step = 12, Training Accuracy: 0.7414285714285714
Training loss = 0.01850454507661717
step = 13, Training Accuracy: 0.7326785714285714
Training loss = 0.01851083269076688
step = 14, Training Accuracy: 0.7351785714285715
Training loss = 0.018505417608789036
step = 15, Training Accuracy: 0.73875
Training loss = 0.018497365792947158
step = 16, Training Accuracy: 0.7344642857142857
Training loss = 0.018340315770890032
step = 17, Training Accuracy: 0.7391071428571429
Training loss = 0.01825510687593903
step = 18, Training Accuracy: 0.7405357142857143
Training loss = 0.01833163771246161
step = 19, Training Accuracy: 0.7342857142857143
Validation Accuracy: 0.7775
parameter = [0.47615826222934565, 0.6302190543188851, 0.40396096956847294, 0.9200240126763269, 0.01685162040226995, 0.9876105507362524]
Training loss = 0.02984066835471562
step = 0, Training Accuracy: 0.5728571428571428
Validation Accuracy: 0.64875
Training loss = 0.026186413605298314
step = 1, Training Accuracy: 0.6155357142857143
Training loss = 0.024983677651200974
step = 2, Training Accuracy: 0.6317857142857143
Training loss = 0.024456429045115198
step = 3, Training Accuracy: 0.6391071428571429
Training loss = 0.024206946289965083
step = 4, Training Accuracy: 0.6433928571428571
Training loss = 0.023709911735994474
step = 5, Training Accuracy: 0.6578571428571428
Training loss = 0.023138472102582453
step = 6, Training Accuracy: 0.6644642857142857
Training loss = 0.023282145405454296
step = 7, Training Accuracy: 0.6657142857142857
Training loss = 0.0229875318014196
step = 8, Training Accuracy: 0.6685714285714286
Training loss = 0.02258032510855368
step = 9, Training Accuracy: 0.66875
Training loss = 0.022746828725295407
step = 10, Training Accuracy: 0.6744642857142857
Training loss = 0.02275053790637425
step = 11, Training Accuracy: 0.6676785714285715
Training loss = 0.022591480149754456
step = 12, Training Accuracy: 0.6644642857142857
Training loss = 0.02226698468306235
step = 13, Training Accuracy: 0.6828571428571428
Training loss = 0.022212984817368643
step = 14, Training Accuracy: 0.6825
Training loss = 0.022214487455785276
step = 15, Training Accuracy: 0.6823214285714285
Training loss = 0.022274733877607755
step = 16, Training Accuracy: 0.6819642857142857
Training loss = 0.022338904089161327
step = 17, Training Accuracy: 0.6776785714285715
Training loss = 0.021547329436455456
step = 18, Training Accuracy: 0.6857142857142857
Training loss = 0.021951758803001473
step = 19, Training Accuracy: 0.69
Validation Accuracy: 0.72
parameter = [0.19673746145383764, 0.8149524482305617, 0.7696797658382238, 0.6972661645183106, 0.2137573979486891, 0.7423923323846316]
Training loss = 0.019921919820564134
step = 0, Training Accuracy: 0.7223214285714286
Validation Accuracy: 0.75875
Training loss = 0.019555737035615105
step = 1, Training Accuracy: 0.72625
Training loss = 0.019297848556722914
step = 2, Training Accuracy: 0.7273214285714286
Training loss = 0.01930204874702862
step = 3, Training Accuracy: 0.72875
Training loss = 0.019272959817733082
step = 4, Training Accuracy: 0.7305357142857143
Training loss = 0.019077495453613143
step = 5, Training Accuracy: 0.7314285714285714
Training loss = 0.01928910702999149
step = 6, Training Accuracy: 0.7298214285714286
Training loss = 0.019350480394704002
step = 7, Training Accuracy: 0.7266071428571429
Training loss = 0.019155990912445953
step = 8, Training Accuracy: 0.7314285714285714
Training loss = 0.019131796876234668
step = 9, Training Accuracy: 0.7308928571428571
Training loss = 0.018607137809906686
step = 10, Training Accuracy: 0.735
Training loss = 0.018631061352789403
step = 11, Training Accuracy: 0.7410714285714286
Training loss = 0.01885083899966308
step = 12, Training Accuracy: 0.7346428571428572
Training loss = 0.018664108397705215
step = 13, Training Accuracy: 0.735
Training loss = 0.019044747203588487
step = 14, Training Accuracy: 0.7310714285714286
Training loss = 0.01871237835181611
step = 15, Training Accuracy: 0.7410714285714286
Training loss = 0.018673220234257833
step = 16, Training Accuracy: 0.7360714285714286
Training loss = 0.01859097710145371
step = 17, Training Accuracy: 0.7383928571428572
Training loss = 0.01861174323196922
step = 18, Training Accuracy: 0.7355357142857143
Training loss = 0.018528043856578212
step = 19, Training Accuracy: 0.7433928571428572
Validation Accuracy: 0.77875
parameter = [0.47465039059345376, 0.5106320358925561, 0.3021175557983387, 0.5021652319960087, 0.023487723016207065, 0.8136031524665036]
Training loss = 0.02084188965814454
step = 0, Training Accuracy: 0.7026785714285714
Validation Accuracy: 0.74625
Training loss = 0.02093098615429231
step = 1, Training Accuracy: 0.7008928571428571
Training loss = 0.020297873892954416
step = 2, Training Accuracy: 0.7148214285714286
Training loss = 0.02040816440113953
step = 3, Training Accuracy: 0.7094642857142858
Training loss = 0.02042154972042356
step = 4, Training Accuracy: 0.7098214285714286
Training loss = 0.020511919648519585
step = 5, Training Accuracy: 0.7014285714285714
Training loss = 0.020129681793706757
step = 6, Training Accuracy: 0.7189285714285715
Training loss = 0.02025703045938696
step = 7, Training Accuracy: 0.71125
Training loss = 0.02004029219704015
step = 8, Training Accuracy: 0.7144642857142857
Training loss = 0.0202660542513643
step = 9, Training Accuracy: 0.7123214285714285
Training loss = 0.020020145348140173
step = 10, Training Accuracy: 0.7178571428571429
Training loss = 0.020109907963446208
step = 11, Training Accuracy: 0.7164285714285714
Training loss = 0.02023449067558561
step = 12, Training Accuracy: 0.7180357142857143
Training loss = 0.02034753150173596
step = 13, Training Accuracy: 0.7064285714285714
Training loss = 0.019992099798151426
step = 14, Training Accuracy: 0.7180357142857143
Training loss = 0.020456303469836713
step = 15, Training Accuracy: 0.7133928571428572
Training loss = 0.019804754608443807
step = 16, Training Accuracy: 0.7189285714285715
Training loss = 0.019728949394609247
step = 17, Training Accuracy: 0.7226785714285714
Training loss = 0.019850660344319683
step = 18, Training Accuracy: 0.7166071428571429
Training loss = 0.02010388981550932
step = 19, Training Accuracy: 0.7157142857142857
Validation Accuracy: 0.75375
1  	7     	0.78125 	0.00216506	0.7775 	0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.21686121715678333, 0.8314766151043855]
Training loss = 0.018739532567560672
step = 0, Training Accuracy: 0.7364285714285714
Validation Accuracy: 0.77125
Training loss = 0.018611871669335023
step = 1, Training Accuracy: 0.73875
Training loss = 0.018613615839609077
step = 2, Training Accuracy: 0.7403571428571428
Training loss = 0.018636452539690904
step = 3, Training Accuracy: 0.7321428571428571
Training loss = 0.018430216583822455
step = 4, Training Accuracy: 0.7466071428571428
Training loss = 0.018333423659205438
step = 5, Training Accuracy: 0.7428571428571429
Training loss = 0.0186379313947899
step = 6, Training Accuracy: 0.7317857142857143
Training loss = 0.01871310809361083
step = 7, Training Accuracy: 0.73625
Training loss = 0.01873362593884979
step = 8, Training Accuracy: 0.7358928571428571
Training loss = 0.018531334586441517
step = 9, Training Accuracy: 0.7398214285714285
Training loss = 0.018108106835612228
step = 10, Training Accuracy: 0.7417857142857143
Training loss = 0.018922716001314777
step = 11, Training Accuracy: 0.7348214285714286
Training loss = 0.01864807647253786
step = 12, Training Accuracy: 0.7405357142857143
Training loss = 0.01897688991789307
step = 13, Training Accuracy: 0.7280357142857142
Training loss = 0.01852168553641864
step = 14, Training Accuracy: 0.7421428571428571
Training loss = 0.018439585656992028
step = 15, Training Accuracy: 0.7401785714285715
Training loss = 0.018713437744549342
step = 16, Training Accuracy: 0.7419642857142857
Training loss = 0.018359639564795152
step = 17, Training Accuracy: 0.7426785714285714
Training loss = 0.018515391323183264
step = 18, Training Accuracy: 0.7383928571428572
Training loss = 0.01866629914513656
step = 19, Training Accuracy: 0.7371428571428571
Validation Accuracy: 0.77625
parameter = [0.3940613559015592, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018184155795191014
step = 0, Training Accuracy: 0.7501785714285715
Validation Accuracy: 0.76875
Training loss = 0.018585739407156195
step = 1, Training Accuracy: 0.7373214285714286
Training loss = 0.018585759385355883
step = 2, Training Accuracy: 0.73875
Training loss = 0.01861134918672698
step = 3, Training Accuracy: 0.7405357142857143
Training loss = 0.018543588130601814
step = 4, Training Accuracy: 0.7353571428571428
Training loss = 0.01877817673874753
step = 5, Training Accuracy: 0.7348214285714286
Training loss = 0.018518177574234348
step = 6, Training Accuracy: 0.7394642857142857
Training loss = 0.018412289725882665
step = 7, Training Accuracy: 0.73875
Training loss = 0.01845500840672425
step = 8, Training Accuracy: 0.7460714285714286
Training loss = 0.018510490490921905
step = 9, Training Accuracy: 0.7360714285714286
Training loss = 0.018646388458354132
step = 10, Training Accuracy: 0.7378571428571429
Training loss = 0.018698797119515283
step = 11, Training Accuracy: 0.7391071428571429
Training loss = 0.01816917476377317
step = 12, Training Accuracy: 0.7428571428571429
Training loss = 0.018024533252630915
step = 13, Training Accuracy: 0.7489285714285714
Training loss = 0.01876262338033744
step = 14, Training Accuracy: 0.7373214285714286
Training loss = 0.018491004641566958
step = 15, Training Accuracy: 0.7448214285714285
Training loss = 0.01844672796449491
step = 16, Training Accuracy: 0.7448214285714285
Training loss = 0.018399453360055173
step = 17, Training Accuracy: 0.745
Training loss = 0.018060503756361347
step = 18, Training Accuracy: 0.7444642857142857
Training loss = 0.018373239019087384
step = 19, Training Accuracy: 0.7360714285714286
Validation Accuracy: 0.77625
parameter = [0.2987913421365269, 0.5838591518646845, 0.5118824862683863, 0.7536608706414711, 0.2124426943125732, 0.7102179025067893]
Training loss = 0.018591102068977697
step = 0, Training Accuracy: 0.7375
Validation Accuracy: 0.77
Training loss = 0.018770602691386428
step = 1, Training Accuracy: 0.7367857142857143
Training loss = 0.018715237299246446
step = 2, Training Accuracy: 0.7355357142857143
Training loss = 0.018838446039174284
step = 3, Training Accuracy: 0.7307142857142858
Training loss = 0.019007142058440618
step = 4, Training Accuracy: 0.7357142857142858
Training loss = 0.018809758488621032
step = 5, Training Accuracy: 0.7366071428571429
Training loss = 0.01887130566473518
step = 6, Training Accuracy: 0.7383928571428572
Training loss = 0.018498358109167643
step = 7, Training Accuracy: 0.7401785714285715
Training loss = 0.018884540194911616
step = 8, Training Accuracy: 0.7346428571428572
Training loss = 0.018873735889792444
step = 9, Training Accuracy: 0.7330357142857142
Training loss = 0.01897344747292144
step = 10, Training Accuracy: 0.7342857142857143
Training loss = 0.01891023636396442
step = 11, Training Accuracy: 0.7355357142857143
Training loss = 0.018504441183592592
step = 12, Training Accuracy: 0.7383928571428572
Training loss = 0.018555635517196994
step = 13, Training Accuracy: 0.7416071428571429
Training loss = 0.01860020841870989
step = 14, Training Accuracy: 0.735
Training loss = 0.018717291599937846
step = 15, Training Accuracy: 0.73
Training loss = 0.01880750343735729
step = 16, Training Accuracy: 0.7410714285714286
Training loss = 0.018939425184258393
step = 17, Training Accuracy: 0.7317857142857143
Training loss = 0.01879918687045574
step = 18, Training Accuracy: 0.7328571428571429
Training loss = 0.018921094802873475
step = 19, Training Accuracy: 0.7323214285714286
Validation Accuracy: 0.77125
parameter = [0.3964468608620138, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.018545027327324663
step = 0, Training Accuracy: 0.7466071428571428
Validation Accuracy: 0.77125
Training loss = 0.018405705280601977
step = 1, Training Accuracy: 0.74375
Training loss = 0.01881182695073741
step = 2, Training Accuracy: 0.7301785714285715
Training loss = 0.018905498741992884
step = 3, Training Accuracy: 0.7294642857142857
Training loss = 0.01848381315491029
step = 4, Training Accuracy: 0.7421428571428571
Training loss = 0.01890362987028701
step = 5, Training Accuracy: 0.7376785714285714
Training loss = 0.018540802007274967
step = 6, Training Accuracy: 0.7360714285714286
Training loss = 0.01858878879142659
step = 7, Training Accuracy: 0.7358928571428571
Training loss = 0.018772914329809803
step = 8, Training Accuracy: 0.7405357142857143
Training loss = 0.018731383942067623
step = 9, Training Accuracy: 0.74
Training loss = 0.01875660143260445
step = 10, Training Accuracy: 0.7351785714285715
Training loss = 0.018548227245254175
step = 11, Training Accuracy: 0.7376785714285714
Training loss = 0.018832852531756674
step = 12, Training Accuracy: 0.7319642857142857
Training loss = 0.01884303366499288
step = 13, Training Accuracy: 0.7378571428571429
Training loss = 0.018495995471520085
step = 14, Training Accuracy: 0.73375
Training loss = 0.018772010531808648
step = 15, Training Accuracy: 0.7323214285714286
Training loss = 0.018812449255159923
step = 16, Training Accuracy: 0.7398214285714285
Training loss = 0.018716542625001498
step = 17, Training Accuracy: 0.7333928571428572
Training loss = 0.01893187896481582
step = 18, Training Accuracy: 0.7355357142857143
Training loss = 0.018986670630318777
step = 19, Training Accuracy: 0.7358928571428571
Validation Accuracy: 0.7725
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018101488734994616
step = 0, Training Accuracy: 0.7466071428571428
Validation Accuracy: 0.775
Training loss = 0.0183599059975573
step = 1, Training Accuracy: 0.7396428571428572
Training loss = 0.018464594978306974
step = 2, Training Accuracy: 0.735
Training loss = 0.01816823051976306
step = 3, Training Accuracy: 0.7423214285714286
Training loss = 0.018469717907054085
step = 4, Training Accuracy: 0.7376785714285714
Training loss = 0.018237887823155947
step = 5, Training Accuracy: 0.7421428571428571
Training loss = 0.018846748167915005
step = 6, Training Accuracy: 0.73375
Training loss = 0.018506609669753482
step = 7, Training Accuracy: 0.7426785714285714
Training loss = 0.018417931300188813
step = 8, Training Accuracy: 0.7416071428571429
Training loss = 0.018576619369643076
step = 9, Training Accuracy: 0.7385714285714285
Training loss = 0.018433320708572863
step = 10, Training Accuracy: 0.7398214285714285
Training loss = 0.018586986181991443
step = 11, Training Accuracy: 0.7366071428571429
Training loss = 0.01861102501728705
step = 12, Training Accuracy: 0.7382142857142857
Training loss = 0.01840749819896051
step = 13, Training Accuracy: 0.7446428571428572
Training loss = 0.018479217163154055
step = 14, Training Accuracy: 0.7391071428571429
Training loss = 0.018124651962092946
step = 15, Training Accuracy: 0.7441071428571429
Training loss = 0.018412851190992764
step = 16, Training Accuracy: 0.7451785714285715
Training loss = 0.01860845910651343
step = 17, Training Accuracy: 0.7376785714285714
Training loss = 0.018294311203062533
step = 18, Training Accuracy: 0.7508928571428571
Training loss = 0.018097354482327188
step = 19, Training Accuracy: 0.74125
Validation Accuracy: 0.7775
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227104, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018135286532342434
step = 0, Training Accuracy: 0.7498214285714285
Validation Accuracy: 0.7775
Training loss = 0.018436913288065366
step = 1, Training Accuracy: 0.7403571428571428
Training loss = 0.01833695715027196
step = 2, Training Accuracy: 0.7391071428571429
Training loss = 0.018255134751754148
step = 3, Training Accuracy: 0.7403571428571428
Training loss = 0.018655745035835676
step = 4, Training Accuracy: 0.7358928571428571
Training loss = 0.018396146957363402
step = 5, Training Accuracy: 0.7444642857142857
Training loss = 0.018080741864229952
step = 6, Training Accuracy: 0.7466071428571428
Training loss = 0.018117760930742535
step = 7, Training Accuracy: 0.745
Training loss = 0.018366381217326436
step = 8, Training Accuracy: 0.74875
Training loss = 0.018185021526047163
step = 9, Training Accuracy: 0.7457142857142857
Training loss = 0.018425079860857556
step = 10, Training Accuracy: 0.74125
Training loss = 0.018493374197610785
step = 11, Training Accuracy: 0.7430357142857142
Training loss = 0.018250385304646833
step = 12, Training Accuracy: 0.7453571428571428
Training loss = 0.018712753978158747
step = 13, Training Accuracy: 0.7398214285714285
Training loss = 0.018756262809038162
step = 14, Training Accuracy: 0.7308928571428571
Training loss = 0.018627803474664688
step = 15, Training Accuracy: 0.7383928571428572
Training loss = 0.018439625570816652
step = 16, Training Accuracy: 0.7380357142857142
Training loss = 0.018187389565365655
step = 17, Training Accuracy: 0.7426785714285714
Training loss = 0.018403742185660772
step = 18, Training Accuracy: 0.7383928571428572
Training loss = 0.018293552749923298
step = 19, Training Accuracy: 0.7421428571428571
Validation Accuracy: 0.77875
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018726029571677957
step = 0, Training Accuracy: 0.7314285714285714
Validation Accuracy: 0.78
Training loss = 0.01877944491271462
step = 1, Training Accuracy: 0.74
Training loss = 0.018500000573694705
step = 2, Training Accuracy: 0.7385714285714285
Training loss = 0.018509782664477826
step = 3, Training Accuracy: 0.74375
Training loss = 0.01826381165534258
step = 4, Training Accuracy: 0.7478571428571429
Training loss = 0.01785021895808833
step = 5, Training Accuracy: 0.7471428571428571
Training loss = 0.018618056747530188
step = 6, Training Accuracy: 0.7425
Training loss = 0.01808775837932314
step = 7, Training Accuracy: 0.7423214285714286
Training loss = 0.01870894107967615
step = 8, Training Accuracy: 0.7344642857142857
Training loss = 0.018490484026925904
step = 9, Training Accuracy: 0.7410714285714286
Training loss = 0.0183643244898745
step = 10, Training Accuracy: 0.7401785714285715
Training loss = 0.018122168273798058
step = 11, Training Accuracy: 0.7430357142857142
Training loss = 0.018533055372536182
step = 12, Training Accuracy: 0.74125
Training loss = 0.018610002377203533
step = 13, Training Accuracy: 0.7373214285714286
Training loss = 0.01830732660634177
step = 14, Training Accuracy: 0.7419642857142857
Training loss = 0.01856639632156917
step = 15, Training Accuracy: 0.7367857142857143
Training loss = 0.018374506373490605
step = 16, Training Accuracy: 0.7519642857142858
Training loss = 0.018275037168392114
step = 17, Training Accuracy: 0.7485714285714286
Training loss = 0.018530824620808874
step = 18, Training Accuracy: 0.7373214285714286
Training loss = 0.01838736612881933
step = 19, Training Accuracy: 0.7385714285714285
Validation Accuracy: 0.78
2  	7     	0.780312	0.00255792	0.77625	0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227106, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018162046069545405
step = 0, Training Accuracy: 0.7482142857142857
Validation Accuracy: 0.77625
Training loss = 0.018062492076839718
step = 1, Training Accuracy: 0.74
Training loss = 0.018307789734431677
step = 2, Training Accuracy: 0.7467857142857143
Training loss = 0.01824373989765133
step = 3, Training Accuracy: 0.7473214285714286
Training loss = 0.018137314724070687
step = 4, Training Accuracy: 0.7489285714285714
Training loss = 0.018123355875057832
step = 5, Training Accuracy: 0.7444642857142857
Training loss = 0.018308365345001222
step = 6, Training Accuracy: 0.7444642857142857
Training loss = 0.01860490756375449
step = 7, Training Accuracy: 0.7383928571428572
Training loss = 0.018639061067785536
step = 8, Training Accuracy: 0.73625
Training loss = 0.018437881230243616
step = 9, Training Accuracy: 0.7391071428571429
Training loss = 0.018194230709757123
step = 10, Training Accuracy: 0.7483928571428572
Training loss = 0.018290452914578575
step = 11, Training Accuracy: 0.74375
Training loss = 0.018440824821591376
step = 12, Training Accuracy: 0.7416071428571429
Training loss = 0.018369614561753615
step = 13, Training Accuracy: 0.7405357142857143
Training loss = 0.018350475180361953
step = 14, Training Accuracy: 0.7407142857142858
Training loss = 0.018450427864279067
step = 15, Training Accuracy: 0.7455357142857143
Training loss = 0.018558986357280188
step = 16, Training Accuracy: 0.7378571428571429
Training loss = 0.018516787024480955
step = 17, Training Accuracy: 0.74
Training loss = 0.018355346002749035
step = 18, Training Accuracy: 0.7378571428571429
Training loss = 0.018315993350531374
step = 19, Training Accuracy: 0.7405357142857143
Validation Accuracy: 0.78125
parameter = [0.3940613559015592, 0.640145984339351, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018422552794218065
step = 0, Training Accuracy: 0.7385714285714285
Validation Accuracy: 0.77625
Training loss = 0.018231029505176202
step = 1, Training Accuracy: 0.745
Training loss = 0.01853682838912521
step = 2, Training Accuracy: 0.7396428571428572
Training loss = 0.018448238346193517
step = 3, Training Accuracy: 0.7494642857142857
Training loss = 0.01842153111738818
step = 4, Training Accuracy: 0.7455357142857143
Training loss = 0.018485318353133544
step = 5, Training Accuracy: 0.7421428571428571
Training loss = 0.01846601118466684
step = 6, Training Accuracy: 0.7407142857142858
Training loss = 0.01841632573732308
step = 7, Training Accuracy: 0.7389285714285714
Training loss = 0.018324620420379297
step = 8, Training Accuracy: 0.7394642857142857
Training loss = 0.018281723356672695
step = 9, Training Accuracy: 0.7401785714285715
Training loss = 0.01838948133800711
step = 10, Training Accuracy: 0.7491071428571429
Training loss = 0.018576978205570152
step = 11, Training Accuracy: 0.74625
Training loss = 0.018473514185420103
step = 12, Training Accuracy: 0.74
Training loss = 0.018299219991479602
step = 13, Training Accuracy: 0.7430357142857142
Training loss = 0.018650021835097244
step = 14, Training Accuracy: 0.7455357142857143
Training loss = 0.018308033448244845
step = 15, Training Accuracy: 0.7458928571428571
Training loss = 0.018257128915616445
step = 16, Training Accuracy: 0.74875
Training loss = 0.01870415458721774
step = 17, Training Accuracy: 0.7273214285714286
Training loss = 0.01841031699308327
step = 18, Training Accuracy: 0.7417857142857143
Training loss = 0.018571143890065808
step = 19, Training Accuracy: 0.7405357142857143
Validation Accuracy: 0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018276641187923295
step = 0, Training Accuracy: 0.7491071428571429
Validation Accuracy: 0.7725
Training loss = 0.018144375969256672
step = 1, Training Accuracy: 0.7467857142857143
Training loss = 0.018444957930062497
step = 2, Training Accuracy: 0.7405357142857143
Training loss = 0.01845209636326347
step = 3, Training Accuracy: 0.7448214285714285
Training loss = 0.018359488365905625
step = 4, Training Accuracy: 0.74125
Training loss = 0.018219185966466156
step = 5, Training Accuracy: 0.7444642857142857
Training loss = 0.018167870167110647
step = 6, Training Accuracy: 0.7473214285714286
Training loss = 0.018284998993788448
step = 7, Training Accuracy: 0.7439285714285714
Training loss = 0.01867653553507158
step = 8, Training Accuracy: 0.7346428571428572
Training loss = 0.018414128752691404
step = 9, Training Accuracy: 0.7444642857142857
Training loss = 0.018335390484758785
step = 10, Training Accuracy: 0.7451785714285715
Training loss = 0.0182675178827984
step = 11, Training Accuracy: 0.7414285714285714
Training loss = 0.018157703642334256
step = 12, Training Accuracy: 0.7475
Training loss = 0.018458798942821368
step = 13, Training Accuracy: 0.74125
Training loss = 0.0180751169739025
step = 14, Training Accuracy: 0.7441071428571429
Training loss = 0.01832946767764432
step = 15, Training Accuracy: 0.7464285714285714
Training loss = 0.018207364763532366
step = 16, Training Accuracy: 0.7483928571428572
Training loss = 0.018251354343124797
step = 17, Training Accuracy: 0.7446428571428572
Training loss = 0.01807918361255101
step = 18, Training Accuracy: 0.735
Training loss = 0.018103434565876212
step = 19, Training Accuracy: 0.745
Validation Accuracy: 0.77875
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018142951918499812
step = 0, Training Accuracy: 0.7469642857142857
Validation Accuracy: 0.78
Training loss = 0.0184775601540293
step = 1, Training Accuracy: 0.7483928571428572
Training loss = 0.017954501483057227
step = 2, Training Accuracy: 0.75
Training loss = 0.018765732028654645
step = 3, Training Accuracy: 0.7339285714285714
Training loss = 0.018503590891403812
step = 4, Training Accuracy: 0.735
Training loss = 0.01832948262138026
step = 5, Training Accuracy: 0.7428571428571429
Training loss = 0.018363504404468194
step = 6, Training Accuracy: 0.7410714285714286
Training loss = 0.018154549886073384
step = 7, Training Accuracy: 0.7457142857142857
Training loss = 0.018460094348660538
step = 8, Training Accuracy: 0.7453571428571428
Training loss = 0.018369399101606437
step = 9, Training Accuracy: 0.7398214285714285
Training loss = 0.018611557105822223
step = 10, Training Accuracy: 0.7367857142857143
Training loss = 0.018341970534196922
step = 11, Training Accuracy: 0.74
Training loss = 0.018801378032990864
step = 12, Training Accuracy: 0.7367857142857143
Training loss = 0.018451231036867414
step = 13, Training Accuracy: 0.7383928571428572
Training loss = 0.018249119126370974
step = 14, Training Accuracy: 0.7405357142857143
Training loss = 0.01830290073794978
step = 15, Training Accuracy: 0.7392857142857143
Training loss = 0.018302250371447632
step = 16, Training Accuracy: 0.7421428571428571
Training loss = 0.018417826124600002
step = 17, Training Accuracy: 0.74375
Training loss = 0.018343942404857705
step = 18, Training Accuracy: 0.7405357142857143
Training loss = 0.018457428328692915
step = 19, Training Accuracy: 0.7419642857142857
Validation Accuracy: 0.77875
parameter = [0.3940613559015592, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.01830493401203837
step = 0, Training Accuracy: 0.7455357142857143
Validation Accuracy: 0.7725
Training loss = 0.018580651198114667
step = 1, Training Accuracy: 0.7392857142857143
Training loss = 0.018495916915791374
step = 2, Training Accuracy: 0.7407142857142858
Training loss = 0.01830928148967879
step = 3, Training Accuracy: 0.74125
Training loss = 0.018248352855443954
step = 4, Training Accuracy: 0.7355357142857143
Training loss = 0.01845874343599592
step = 5, Training Accuracy: 0.7353571428571428
Training loss = 0.018089396384145532
step = 6, Training Accuracy: 0.7398214285714285
Training loss = 0.01876113007111209
step = 7, Training Accuracy: 0.7371428571428571
Training loss = 0.018386424322213445
step = 8, Training Accuracy: 0.7405357142857143
Training loss = 0.018375400224966663
step = 9, Training Accuracy: 0.7380357142857142
Training loss = 0.018415909776730195
step = 10, Training Accuracy: 0.7482142857142857
Training loss = 0.018193988368979522
step = 11, Training Accuracy: 0.7457142857142857
Training loss = 0.017677354913737092
step = 12, Training Accuracy: 0.7532142857142857
Training loss = 0.018099131472408773
step = 13, Training Accuracy: 0.7494642857142857
Training loss = 0.018148641139268874
step = 14, Training Accuracy: 0.7439285714285714
Training loss = 0.018706231974065304
step = 15, Training Accuracy: 0.7310714285714286
Training loss = 0.018258862122893335
step = 16, Training Accuracy: 0.7410714285714286
Training loss = 0.018255864504192556
step = 17, Training Accuracy: 0.7426785714285714
Training loss = 0.01851280512554305
step = 18, Training Accuracy: 0.7346428571428572
Training loss = 0.018631410806306772
step = 19, Training Accuracy: 0.7373214285714286
Validation Accuracy: 0.7775
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.30434477330124576, 0.8314766151043855]
Training loss = 0.01901044598115342
step = 0, Training Accuracy: 0.7344642857142857
Validation Accuracy: 0.77625
Training loss = 0.019235275739005634
step = 1, Training Accuracy: 0.7351785714285715
Training loss = 0.019289754629135132
step = 2, Training Accuracy: 0.7260714285714286
Training loss = 0.01929279250758035
step = 3, Training Accuracy: 0.7205357142857143
Training loss = 0.019182947160942213
step = 4, Training Accuracy: 0.7314285714285714
Training loss = 0.019218004643917083
step = 5, Training Accuracy: 0.7323214285714286
Training loss = 0.01919191070433174
step = 6, Training Accuracy: 0.7330357142857142
Training loss = 0.018887112167264734
step = 7, Training Accuracy: 0.73
Training loss = 0.018927757080112184
step = 8, Training Accuracy: 0.7373214285714286
Training loss = 0.0193560698309115
step = 9, Training Accuracy: 0.7235714285714285
Training loss = 0.019178177831428393
step = 10, Training Accuracy: 0.7326785714285714
Training loss = 0.019075489097407885
step = 11, Training Accuracy: 0.7348214285714286
Training loss = 0.01911838345229626
step = 12, Training Accuracy: 0.7335714285714285
Training loss = 0.01926356910594872
step = 13, Training Accuracy: 0.7257142857142858
Training loss = 0.019004005140491893
step = 14, Training Accuracy: 0.7342857142857143
Training loss = 0.01904894494052444
step = 15, Training Accuracy: 0.73125
Training loss = 0.01920338289546115
step = 16, Training Accuracy: 0.7317857142857143
Training loss = 0.019154243330870357
step = 17, Training Accuracy: 0.7258928571428571
Training loss = 0.018898523045437675
step = 18, Training Accuracy: 0.7280357142857142
Training loss = 0.01912668379289763
step = 19, Training Accuracy: 0.7353571428571428
Validation Accuracy: 0.7675
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.01847257870116404
step = 0, Training Accuracy: 0.7464285714285714
Validation Accuracy: 0.7775
Training loss = 0.018491890600749423
step = 1, Training Accuracy: 0.7466071428571428
Training loss = 0.018516904477562223
step = 2, Training Accuracy: 0.7391071428571429
Training loss = 0.018481955251523425
step = 3, Training Accuracy: 0.7385714285714285
Training loss = 0.01820718143135309
step = 4, Training Accuracy: 0.7433928571428572
Training loss = 0.018555285462311336
step = 5, Training Accuracy: 0.7364285714285714
Training loss = 0.018767119252255986
step = 6, Training Accuracy: 0.7319642857142857
Training loss = 0.018497230554265637
step = 7, Training Accuracy: 0.7408928571428571
Training loss = 0.018601449366126743
step = 8, Training Accuracy: 0.7398214285714285
Training loss = 0.018506215608545713
step = 9, Training Accuracy: 0.7380357142857142
Training loss = 0.018417169159012182
step = 10, Training Accuracy: 0.7385714285714285
Training loss = 0.018325462458389147
step = 11, Training Accuracy: 0.7423214285714286
Training loss = 0.018319071770778725
step = 12, Training Accuracy: 0.7426785714285714
Training loss = 0.018230456337332727
step = 13, Training Accuracy: 0.7457142857142857
Training loss = 0.018104646876454352
step = 14, Training Accuracy: 0.7492857142857143
Training loss = 0.018408551870712213
step = 15, Training Accuracy: 0.7430357142857142
Training loss = 0.01841819921242339
step = 16, Training Accuracy: 0.7394642857142857
Training loss = 0.0183535489280309
step = 17, Training Accuracy: 0.7410714285714286
Training loss = 0.018524007318275314
step = 18, Training Accuracy: 0.74125
Training loss = 0.018472436506833348
step = 19, Training Accuracy: 0.7423214285714286
Validation Accuracy: 0.77875
3  	7     	0.781875	0.000625  	0.78125	0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227106, 0.2168612171567833, 0.8314766151043855]
