parameter = [0.47615826222934565, 0.6302190543188851, 0.40396096956847294, 0.9200240126763269, 0.01685162040226995, 0.9876105507362524]
Training loss = 0.03577395876603467
step = 0, Training Accuracy: 0.3798214285714286
Validation Accuracy: 0.485
Training loss = 0.03187399634293148
step = 1, Training Accuracy: 0.4451785714285714
Training loss = 0.03091489193694932
step = 2, Training Accuracy: 0.4785714285714286
Training loss = 0.030531039014458656
step = 3, Training Accuracy: 0.5016071428571428
Training loss = 0.03040032616683415
step = 4, Training Accuracy: 0.5078571428571429
Training loss = 0.030037844085267613
step = 5, Training Accuracy: 0.5242857142857142
Training loss = 0.029575513707739965
step = 6, Training Accuracy: 0.5505357142857142
Training loss = 0.0290765329343932
step = 7, Training Accuracy: 0.5542857142857143
Training loss = 0.028511027470231057
step = 8, Training Accuracy: 0.5633928571428571
Training loss = 0.027748271141733442
step = 9, Training Accuracy: 0.5842857142857143
Training loss = 0.02732903159090451
step = 10, Training Accuracy: 0.5908928571428571
Training loss = 0.026940250556383813
step = 11, Training Accuracy: 0.5939285714285715
Training loss = 0.0260649943032435
step = 12, Training Accuracy: 0.6160714285714286
Training loss = 0.026034435938511576
step = 13, Training Accuracy: 0.6125
Training loss = 0.025412408177341732
step = 14, Training Accuracy: 0.6342857142857142
Training loss = 0.02534712186881474
step = 15, Training Accuracy: 0.625
Training loss = 0.025041462459734508
step = 16, Training Accuracy: 0.6346428571428572
Training loss = 0.024153728666050093
step = 17, Training Accuracy: 0.6507142857142857
Training loss = 0.024122056194714137
step = 18, Training Accuracy: 0.6508928571428572
Training loss = 0.023273909044052875
step = 19, Training Accuracy: 0.6660714285714285
Validation Accuracy: 0.70125
parameter = [0.1474309569583241, 0.8613314138109517, 0.44003630577221464, 0.7212885992572478, 0.08366591038506865, 0.679464779137179]
Training loss = 0.023701483250728676
step = 0, Training Accuracy: 0.6580357142857143
Validation Accuracy: 0.735
Training loss = 0.023123211025127342
step = 1, Training Accuracy: 0.6678571428571428
Training loss = 0.02231725672526019
step = 2, Training Accuracy: 0.6789285714285714
Training loss = 0.022326561207217828
step = 3, Training Accuracy: 0.68125
Training loss = 0.021904938242265155
step = 4, Training Accuracy: 0.6891071428571428
Training loss = 0.02168444229023797
step = 5, Training Accuracy: 0.6951785714285714
Training loss = 0.021512636435883387
step = 6, Training Accuracy: 0.6871428571428572
Training loss = 0.021535848703767572
step = 7, Training Accuracy: 0.6946428571428571
Training loss = 0.02136834393654551
step = 8, Training Accuracy: 0.6991071428571428
Training loss = 0.021231297004435743
step = 9, Training Accuracy: 0.6973214285714285
Training loss = 0.021204253594790185
step = 10, Training Accuracy: 0.7101785714285714
Training loss = 0.02048534922301769
step = 11, Training Accuracy: 0.7064285714285714
Training loss = 0.020601738946778435
step = 12, Training Accuracy: 0.7053571428571429
Training loss = 0.020530931928328104
step = 13, Training Accuracy: 0.7107142857142857
Training loss = 0.02038817276379892
step = 14, Training Accuracy: 0.7023214285714285
Training loss = 0.020265759990683623
step = 15, Training Accuracy: 0.71625
Training loss = 0.020024855216698986
step = 16, Training Accuracy: 0.7092857142857143
Training loss = 0.02038721672126225
step = 17, Training Accuracy: 0.7151785714285714
Training loss = 0.020161204577556678
step = 18, Training Accuracy: 0.7069642857142857
Training loss = 0.020329941612269198
step = 19, Training Accuracy: 0.7103571428571429
Validation Accuracy: 0.76875
parameter = [0.19673746145383764, 0.8149524482305617, 0.7696797658382238, 0.39701291838018926, 0.2137573979486891, 0.7423923323846316]
Training loss = 0.020989908458931106
step = 0, Training Accuracy: 0.7021428571428572
Validation Accuracy: 0.755
Training loss = 0.020300401507743766
step = 1, Training Accuracy: 0.7105357142857143
Training loss = 0.02040506774293525
step = 2, Training Accuracy: 0.7169642857142857
Training loss = 0.02045281486319644
step = 3, Training Accuracy: 0.7139285714285715
Training loss = 0.01998790885720934
step = 4, Training Accuracy: 0.7246428571428571
Training loss = 0.02012512565191303
step = 5, Training Accuracy: 0.7164285714285714
Training loss = 0.02003009056938546
step = 6, Training Accuracy: 0.71
Training loss = 0.019978973902761935
step = 7, Training Accuracy: 0.7239285714285715
Training loss = 0.01939729402640036
step = 8, Training Accuracy: 0.7257142857142858
Training loss = 0.019794992527791433
step = 9, Training Accuracy: 0.7228571428571429
Training loss = 0.019275995732418127
step = 10, Training Accuracy: 0.7271428571428571
Training loss = 0.019251402797443525
step = 11, Training Accuracy: 0.7269642857142857
Training loss = 0.019732206458491938
step = 12, Training Accuracy: 0.7158928571428571
Training loss = 0.019268929825297425
step = 13, Training Accuracy: 0.7266071428571429
Training loss = 0.01929865367178406
step = 14, Training Accuracy: 0.7235714285714285
Training loss = 0.019423404479665417
step = 15, Training Accuracy: 0.72875
Training loss = 0.01940895729831287
step = 16, Training Accuracy: 0.7217857142857143
Training loss = 0.01893795240138258
step = 17, Training Accuracy: 0.72875
Training loss = 0.019095090494624205
step = 18, Training Accuracy: 0.7314285714285714
Training loss = 0.019417529170002255
step = 19, Training Accuracy: 0.7226785714285714
Validation Accuracy: 0.77625
parameter = [0.0016410517036646866, 0.4654349622200482, 0.21349210455755507, 0.05598080636124514, 0.2807600588893786, 0.6727775726121318]
Training loss = 0.021362918721778052
step = 0, Training Accuracy: 0.6935714285714286
Validation Accuracy: 0.77
Training loss = 0.02057274790214641
step = 1, Training Accuracy: 0.71125
Training loss = 0.020052644040967738
step = 2, Training Accuracy: 0.7146428571428571
Training loss = 0.020498773232102394
step = 3, Training Accuracy: 0.7132142857142857
Training loss = 0.02010136519691774
step = 4, Training Accuracy: 0.7157142857142857
Training loss = 0.019693746875439372
step = 5, Training Accuracy: 0.71875
Training loss = 0.019696479072528226
step = 6, Training Accuracy: 0.7203571428571428
Training loss = 0.019987457123185906
step = 7, Training Accuracy: 0.71875
Training loss = 0.019568699550415788
step = 8, Training Accuracy: 0.7226785714285714
Training loss = 0.01972980027220079
step = 9, Training Accuracy: 0.7276785714285714
Training loss = 0.019539683663419315
step = 10, Training Accuracy: 0.7271428571428571
Training loss = 0.019344886371067592
step = 11, Training Accuracy: 0.72875
Training loss = 0.019384495174246175
step = 12, Training Accuracy: 0.7255357142857143
Training loss = 0.019253276432199137
step = 13, Training Accuracy: 0.7323214285714286
Training loss = 0.019353905761880535
step = 14, Training Accuracy: 0.7258928571428571
Training loss = 0.019262469240597317
step = 15, Training Accuracy: 0.7257142857142858
Training loss = 0.019465523117354937
step = 16, Training Accuracy: 0.7271428571428571
Training loss = 0.01885611859283277
step = 17, Training Accuracy: 0.7323214285714286
Training loss = 0.018789822166519504
step = 18, Training Accuracy: 0.7335714285714285
Training loss = 0.018899003214069777
step = 19, Training Accuracy: 0.735
Validation Accuracy: 0.785
gen	nevals	avg     	std      	min    	max  
0  	4     	0.757813	0.0331589	0.70125	0.785
parameter = [0.15687593637536246, 0.8316686516823, 0.5831338908498166, 0.687370651320018, 0.09867448919758273, 0.6771803377593855]
Training loss = 0.020388890297285148
step = 0, Training Accuracy: 0.7096428571428571
Validation Accuracy: 0.765
Training loss = 0.019486889115401678
step = 1, Training Accuracy: 0.7360714285714286
Training loss = 0.019426991407360348
step = 2, Training Accuracy: 0.7271428571428571
Training loss = 0.019030274354985784
step = 3, Training Accuracy: 0.7296428571428571
Training loss = 0.018841877510505062
step = 4, Training Accuracy: 0.7385714285714285
Training loss = 0.018829305608357702
step = 5, Training Accuracy: 0.7301785714285715
Training loss = 0.018754796087741853
step = 6, Training Accuracy: 0.745
Training loss = 0.01874703035290752
step = 7, Training Accuracy: 0.7364285714285714
Training loss = 0.018266684003174306
step = 8, Training Accuracy: 0.745
Training loss = 0.018501772454806737
step = 9, Training Accuracy: 0.7392857142857143
Training loss = 0.01859643414616585
step = 10, Training Accuracy: 0.7403571428571428
Training loss = 0.018757261303918703
step = 11, Training Accuracy: 0.7392857142857143
Training loss = 0.018625085156943118
step = 12, Training Accuracy: 0.7385714285714285
Training loss = 0.018628966329353198
step = 13, Training Accuracy: 0.7360714285714286
Training loss = 0.018165579363703728
step = 14, Training Accuracy: 0.745
Training loss = 0.018444006900702203
step = 15, Training Accuracy: 0.7396428571428572
Training loss = 0.018450241913752895
step = 16, Training Accuracy: 0.7398214285714285
Training loss = 0.018354830209697994
step = 17, Training Accuracy: 0.7398214285714285
Training loss = 0.018279815901603016
step = 18, Training Accuracy: 0.7435714285714285
Training loss = 0.01835987865392651
step = 19, Training Accuracy: 0.7441071428571429
Validation Accuracy: 0.7775
parameter = [0.9937351930880042, 0.6999774952670302, 0.4506417252015659, 0.23635548700778367, 0.07663879046228922, 0.6776228457941125]
Training loss = 0.018783474750816823
step = 0, Training Accuracy: 0.7283928571428572
Validation Accuracy: 0.78125
Training loss = 0.01855730472398656
step = 1, Training Accuracy: 0.7417857142857143
Training loss = 0.018545054239886147
step = 2, Training Accuracy: 0.7419642857142857
Training loss = 0.01849070431398494
step = 3, Training Accuracy: 0.7373214285714286
Training loss = 0.018332013674080373
step = 4, Training Accuracy: 0.7408928571428571
Training loss = 0.018226896998073373
step = 5, Training Accuracy: 0.7451785714285715
Training loss = 0.01825226475085531
step = 6, Training Accuracy: 0.7403571428571428
Training loss = 0.018335097735481604
step = 7, Training Accuracy: 0.7414285714285714
Training loss = 0.018035079235477106
step = 8, Training Accuracy: 0.7466071428571428
Training loss = 0.018241281610514436
step = 9, Training Accuracy: 0.7441071428571429
Training loss = 0.01817354577460459
step = 10, Training Accuracy: 0.7441071428571429
Training loss = 0.018186514984284127
step = 11, Training Accuracy: 0.7467857142857143
Training loss = 0.01799731861267771
step = 12, Training Accuracy: 0.7503571428571428
Training loss = 0.018124359510838985
step = 13, Training Accuracy: 0.7467857142857143
Training loss = 0.01819138111812728
step = 14, Training Accuracy: 0.7391071428571429
Training loss = 0.017816114319222313
step = 15, Training Accuracy: 0.7425
Training loss = 0.017883196973374912
step = 16, Training Accuracy: 0.7510714285714286
Training loss = 0.018133524246513842
step = 17, Training Accuracy: 0.74125
Training loss = 0.018055097114826953
step = 18, Training Accuracy: 0.7469642857142857
Training loss = 0.01774717176599162
step = 19, Training Accuracy: 0.7510714285714286
Validation Accuracy: 0.785
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.01856860055987324
step = 0, Training Accuracy: 0.7344642857142857
Validation Accuracy: 0.76875
Training loss = 0.019022921440856797
step = 1, Training Accuracy: 0.7351785714285715
Training loss = 0.01845159957983664
step = 2, Training Accuracy: 0.7339285714285714
Training loss = 0.018333417725350177
step = 3, Training Accuracy: 0.7455357142857143
Training loss = 0.01820030443370342
step = 4, Training Accuracy: 0.7423214285714286
Training loss = 0.01819535842431443
step = 5, Training Accuracy: 0.7410714285714286
Training loss = 0.018291081791477544
step = 6, Training Accuracy: 0.7401785714285715
Training loss = 0.018161673790642194
step = 7, Training Accuracy: 0.7466071428571428
Training loss = 0.01801505041441747
step = 8, Training Accuracy: 0.7453571428571428
Training loss = 0.018185853484485832
step = 9, Training Accuracy: 0.7426785714285714
Training loss = 0.018001169918903284
step = 10, Training Accuracy: 0.7523214285714286
Training loss = 0.018307664399700507
step = 11, Training Accuracy: 0.7425
Training loss = 0.018261623057935918
step = 12, Training Accuracy: 0.7426785714285714
Training loss = 0.017958788839834077
step = 13, Training Accuracy: 0.7391071428571429
Training loss = 0.018092616562332426
step = 14, Training Accuracy: 0.7476785714285714
Training loss = 0.01821114581078291
step = 15, Training Accuracy: 0.7442857142857143
Training loss = 0.018130394353398256
step = 16, Training Accuracy: 0.7441071428571429
Training loss = 0.018249482954187053
step = 17, Training Accuracy: 0.7525
Training loss = 0.017868335763258594
step = 18, Training Accuracy: 0.7464285714285714
Training loss = 0.017752041981688567
step = 19, Training Accuracy: 0.7489285714285714
Validation Accuracy: 0.7825
parameter = [0.13453700546169453, 0.5329841124023544, 0.36514687429424186, 0.09777823707646693, 0.1930550135372103, 0.6778001918730293]
Training loss = 0.019400985682649273
step = 0, Training Accuracy: 0.7239285714285715
Validation Accuracy: 0.785
Training loss = 0.018946571515074798
step = 1, Training Accuracy: 0.7303571428571428
Training loss = 0.018846087764416423
step = 2, Training Accuracy: 0.735
Training loss = 0.018691588890339648
step = 3, Training Accuracy: 0.7333928571428572
Training loss = 0.018512841910123826
step = 4, Training Accuracy: 0.73875
Training loss = 0.01877330611859049
step = 5, Training Accuracy: 0.7321428571428571
Training loss = 0.018813550248742105
step = 6, Training Accuracy: 0.7326785714285714
Training loss = 0.01824045717184033
step = 7, Training Accuracy: 0.7398214285714285
Training loss = 0.01878864947706461
step = 8, Training Accuracy: 0.725
Training loss = 0.01844879255763122
step = 9, Training Accuracy: 0.7342857142857143
Training loss = 0.01861249032829489
step = 10, Training Accuracy: 0.735
Training loss = 0.018622653409838676
step = 11, Training Accuracy: 0.7319642857142857
Training loss = 0.018674566032631058
step = 12, Training Accuracy: 0.7380357142857142
Training loss = 0.018391240084809916
step = 13, Training Accuracy: 0.7430357142857142
Training loss = 0.018217015106763157
step = 14, Training Accuracy: 0.7426785714285714
Training loss = 0.018590175275291717
step = 15, Training Accuracy: 0.7371428571428571
Training loss = 0.018510740010866097
step = 16, Training Accuracy: 0.7416071428571429
Training loss = 0.018534636848739214
step = 17, Training Accuracy: 0.7403571428571428
Training loss = 0.01847515414335898
step = 18, Training Accuracy: 0.73375
Training loss = 0.018624205759593417
step = 19, Training Accuracy: 0.7375
Validation Accuracy: 0.77625
parameter = [0.47615826222934565, 0.6302190543188851, 0.40396096956847294, 0.9200240126763269, 0.01685162040226995, 0.9876105507362524]
Training loss = 0.028936099080102785
step = 0, Training Accuracy: 0.5746428571428571
Validation Accuracy: 0.61875
Training loss = 0.02654876764331545
step = 1, Training Accuracy: 0.5985714285714285
Training loss = 0.025998816809483936
step = 2, Training Accuracy: 0.6221428571428571
Training loss = 0.025616020703954356
step = 3, Training Accuracy: 0.6264285714285714
Training loss = 0.025010347845298903
step = 4, Training Accuracy: 0.6325
Training loss = 0.024867915830441883
step = 5, Training Accuracy: 0.6358928571428571
Training loss = 0.02421607059559652
step = 6, Training Accuracy: 0.6446428571428572
Training loss = 0.024430517836340836
step = 7, Training Accuracy: 0.6360714285714286
Training loss = 0.024309407909001622
step = 8, Training Accuracy: 0.6460714285714285
Training loss = 0.023896692874176163
step = 9, Training Accuracy: 0.6560714285714285
Training loss = 0.023738424809915677
step = 10, Training Accuracy: 0.6575
Training loss = 0.02384454978896039
step = 11, Training Accuracy: 0.6464285714285715
Training loss = 0.023809018156358174
step = 12, Training Accuracy: 0.6496428571428572
Training loss = 0.023710848752941403
step = 13, Training Accuracy: 0.655
Training loss = 0.023803576289543084
step = 14, Training Accuracy: 0.6514285714285715
Training loss = 0.023514826106173652
step = 15, Training Accuracy: 0.6558928571428572
Training loss = 0.023244439427341734
step = 16, Training Accuracy: 0.6648214285714286
Training loss = 0.02312119109822171
step = 17, Training Accuracy: 0.6621428571428571
Training loss = 0.022874658927321434
step = 18, Training Accuracy: 0.6708928571428572
Training loss = 0.022769017113106593
step = 19, Training Accuracy: 0.67875
Validation Accuracy: 0.69875
parameter = [0.19673746145383764, 0.8149524482305617, 0.7696797658382238, 0.6436370358083565, 0.2137573979486891, 0.7423923323846316]
Training loss = 0.018708193940775737
step = 0, Training Accuracy: 0.7392857142857143
Validation Accuracy: 0.7525
Training loss = 0.019197932407259942
step = 1, Training Accuracy: 0.7275
Training loss = 0.018816678736891065
step = 2, Training Accuracy: 0.7325
Training loss = 0.01883089323128973
step = 3, Training Accuracy: 0.72875
Training loss = 0.01869258437305689
step = 4, Training Accuracy: 0.7342857142857143
Training loss = 0.01891459744955812
step = 5, Training Accuracy: 0.7351785714285715
Training loss = 0.018988116660288403
step = 6, Training Accuracy: 0.7289285714285715
Training loss = 0.018456931380288942
step = 7, Training Accuracy: 0.7410714285714286
Training loss = 0.01850671453135354
step = 8, Training Accuracy: 0.7376785714285714
Training loss = 0.018422877309577805
step = 9, Training Accuracy: 0.7396428571428572
Training loss = 0.01861751041774239
step = 10, Training Accuracy: 0.7383928571428572
Training loss = 0.018525236254291876
step = 11, Training Accuracy: 0.7419642857142857
Training loss = 0.018538281550364834
step = 12, Training Accuracy: 0.7341071428571428
Training loss = 0.01853525642837797
step = 13, Training Accuracy: 0.7351785714285715
Training loss = 0.018578609590019498
step = 14, Training Accuracy: 0.7394642857142857
Training loss = 0.018599225607301508
step = 15, Training Accuracy: 0.7366071428571429
Training loss = 0.018278114231569425
step = 16, Training Accuracy: 0.7451785714285715
Training loss = 0.0184015757430877
step = 17, Training Accuracy: 0.7448214285714285
Training loss = 0.018508530498615332
step = 18, Training Accuracy: 0.7425
Training loss = 0.018504546518836703
step = 19, Training Accuracy: 0.7421428571428571
Validation Accuracy: 0.77625
parameter = [0.47465039059345376, 0.5106320358925561, 0.3021175557983387, 0.5021652319960087, 0.023487723016207065, 0.8136031524665036]
Training loss = 0.02103041617998055
step = 0, Training Accuracy: 0.6935714285714286
Validation Accuracy: 0.75125
Training loss = 0.021125353670545986
step = 1, Training Accuracy: 0.6976785714285715
Training loss = 0.021011429562100344
step = 2, Training Accuracy: 0.7010714285714286
Training loss = 0.020711121894419194
step = 3, Training Accuracy: 0.7025
Training loss = 0.020635372325778008
step = 4, Training Accuracy: 0.7066071428571429
Training loss = 0.02063186533216919
step = 5, Training Accuracy: 0.7058928571428571
Training loss = 0.02093187287982021
step = 6, Training Accuracy: 0.7023214285714285
Training loss = 0.02052190205880574
step = 7, Training Accuracy: 0.7119642857142857
Training loss = 0.020175771127854073
step = 8, Training Accuracy: 0.7096428571428571
Training loss = 0.020373877670083727
step = 9, Training Accuracy: 0.7142857142857143
Training loss = 0.020398077182471754
step = 10, Training Accuracy: 0.71625
Training loss = 0.02037509409976857
step = 11, Training Accuracy: 0.71
Training loss = 0.020301007415567125
step = 12, Training Accuracy: 0.72
Training loss = 0.02032426370041711
step = 13, Training Accuracy: 0.7142857142857143
Training loss = 0.020462263870452132
step = 14, Training Accuracy: 0.7119642857142857
Training loss = 0.02075228502707822
step = 15, Training Accuracy: 0.7069642857142857
Training loss = 0.02043213438774858
step = 16, Training Accuracy: 0.71125
Training loss = 0.02036558329526867
step = 17, Training Accuracy: 0.705
Training loss = 0.020401525066367217
step = 18, Training Accuracy: 0.7135714285714285
Training loss = 0.02035620021500758
step = 19, Training Accuracy: 0.7114285714285714
Validation Accuracy: 0.75
1  	7     	0.780937	0.00270633	0.77625	0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.21686121715678333, 0.8314766151043855]
Training loss = 0.018500792314963682
step = 0, Training Accuracy: 0.7310714285714286
Validation Accuracy: 0.77375
Training loss = 0.018647341297141143
step = 1, Training Accuracy: 0.7385714285714285
Training loss = 0.01808696602604219
step = 2, Training Accuracy: 0.7480357142857142
Training loss = 0.01859495987317392
step = 3, Training Accuracy: 0.7428571428571429
Training loss = 0.01834581784371819
step = 4, Training Accuracy: 0.7432142857142857
Training loss = 0.018175181347344603
step = 5, Training Accuracy: 0.7442857142857143
Training loss = 0.018259221773062433
step = 6, Training Accuracy: 0.7460714285714286
Training loss = 0.01845096061804465
step = 7, Training Accuracy: 0.7369642857142857
Training loss = 0.01865728680576597
step = 8, Training Accuracy: 0.7394642857142857
Training loss = 0.018071626696203436
step = 9, Training Accuracy: 0.7464285714285714
Training loss = 0.018517942508416516
step = 10, Training Accuracy: 0.7428571428571429
Training loss = 0.018373899443873336
step = 11, Training Accuracy: 0.7416071428571429
Training loss = 0.018273751022560257
step = 12, Training Accuracy: 0.7455357142857143
Training loss = 0.018489447475544044
step = 13, Training Accuracy: 0.7430357142857142
Training loss = 0.01857436515390873
step = 14, Training Accuracy: 0.7394642857142857
Training loss = 0.018145674653351308
step = 15, Training Accuracy: 0.7403571428571428
Training loss = 0.0182168139357652
step = 16, Training Accuracy: 0.7455357142857143
Training loss = 0.018373839695538793
step = 17, Training Accuracy: 0.7478571428571429
Training loss = 0.01833256086600678
step = 18, Training Accuracy: 0.7358928571428571
Training loss = 0.018508941105433874
step = 19, Training Accuracy: 0.7398214285714285
Validation Accuracy: 0.7775
parameter = [0.3940613559015592, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018277765475213526
step = 0, Training Accuracy: 0.7491071428571429
Validation Accuracy: 0.78
Training loss = 0.01847030609846115
step = 1, Training Accuracy: 0.7360714285714286
Training loss = 0.018576931320130826
step = 2, Training Accuracy: 0.7405357142857143
Training loss = 0.01823025946638414
step = 3, Training Accuracy: 0.7433928571428572
Training loss = 0.018133402466773987
step = 4, Training Accuracy: 0.7492857142857143
Training loss = 0.018201148206634182
step = 5, Training Accuracy: 0.7394642857142857
Training loss = 0.018377067643616882
step = 6, Training Accuracy: 0.7410714285714286
Training loss = 0.018450272077960628
step = 7, Training Accuracy: 0.7405357142857143
Training loss = 0.018346797339618207
step = 8, Training Accuracy: 0.7442857142857143
Training loss = 0.018239115865102837
step = 9, Training Accuracy: 0.7423214285714286
Training loss = 0.01834234235009977
step = 10, Training Accuracy: 0.7408928571428571
Training loss = 0.018351087884179183
step = 11, Training Accuracy: 0.7382142857142857
Training loss = 0.0180875504602279
step = 12, Training Accuracy: 0.7428571428571429
Training loss = 0.018433496536953108
step = 13, Training Accuracy: 0.7405357142857143
Training loss = 0.01856780490172761
step = 14, Training Accuracy: 0.7394642857142857
Training loss = 0.018074657054884095
step = 15, Training Accuracy: 0.7475
Training loss = 0.018283930766795364
step = 16, Training Accuracy: 0.75125
Training loss = 0.018582923550690925
step = 17, Training Accuracy: 0.7357142857142858
Training loss = 0.01826322971710137
step = 18, Training Accuracy: 0.7396428571428572
Training loss = 0.018263375253549645
step = 19, Training Accuracy: 0.7435714285714285
Validation Accuracy: 0.77375
parameter = [0.2987913421365269, 0.5838591518646845, 0.5118824862683863, 0.7536608706414711, 0.2124426943125732, 0.7102179025067893]
Training loss = 0.018315890197243008
step = 0, Training Accuracy: 0.7401785714285715
Validation Accuracy: 0.77625
Training loss = 0.0181652430711048
step = 1, Training Accuracy: 0.7410714285714286
Training loss = 0.018840953361775194
step = 2, Training Accuracy: 0.7375
Training loss = 0.018611682850335324
step = 3, Training Accuracy: 0.7392857142857143
Training loss = 0.018143137826451234
step = 4, Training Accuracy: 0.7478571428571429
Training loss = 0.018388857575399535
step = 5, Training Accuracy: 0.7428571428571429
Training loss = 0.01830611084720918
step = 6, Training Accuracy: 0.7426785714285714
Training loss = 0.01818600362965039
step = 7, Training Accuracy: 0.7471428571428571
Training loss = 0.018682621062866278
step = 8, Training Accuracy: 0.7358928571428571
Training loss = 0.01855447226337024
step = 9, Training Accuracy: 0.7423214285714286
Training loss = 0.018359826819172927
step = 10, Training Accuracy: 0.7416071428571429
Training loss = 0.01868571271321603
step = 11, Training Accuracy: 0.7341071428571428
Training loss = 0.01845626601683242
step = 12, Training Accuracy: 0.745
Training loss = 0.01838034491986036
step = 13, Training Accuracy: 0.7426785714285714
Training loss = 0.01841039008860077
step = 14, Training Accuracy: 0.73125
Training loss = 0.018847855616893085
step = 15, Training Accuracy: 0.7357142857142858
Training loss = 0.01842894339667899
step = 16, Training Accuracy: 0.7410714285714286
Training loss = 0.01835513278309788
step = 17, Training Accuracy: 0.7414285714285714
Training loss = 0.01838069398488317
step = 18, Training Accuracy: 0.7364285714285714
Training loss = 0.018724235243030955
step = 19, Training Accuracy: 0.7373214285714286
Validation Accuracy: 0.77
parameter = [0.3964468608620138, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.018124792325709547
step = 0, Training Accuracy: 0.7441071428571429
Validation Accuracy: 0.78
Training loss = 0.018509749094290394
step = 1, Training Accuracy: 0.7360714285714286
Training loss = 0.018426844477653505
step = 2, Training Accuracy: 0.7348214285714286
Training loss = 0.01836045352475984
step = 3, Training Accuracy: 0.73875
Training loss = 0.018530390289212977
step = 4, Training Accuracy: 0.7405357142857143
Training loss = 0.018492013530007432
step = 5, Training Accuracy: 0.7389285714285714
Training loss = 0.018671205517436777
step = 6, Training Accuracy: 0.7419642857142857
Training loss = 0.018356469720602037
step = 7, Training Accuracy: 0.74125
Training loss = 0.018333978801965713
step = 8, Training Accuracy: 0.74125
Training loss = 0.018473547042480536
step = 9, Training Accuracy: 0.7382142857142857
Training loss = 0.01883885454918657
step = 10, Training Accuracy: 0.735
Training loss = 0.018637597427836487
step = 11, Training Accuracy: 0.7375
Training loss = 0.018217804160501276
step = 12, Training Accuracy: 0.7416071428571429
Training loss = 0.018492811412683555
step = 13, Training Accuracy: 0.7392857142857143
Training loss = 0.018517670897500855
step = 14, Training Accuracy: 0.7383928571428572
Training loss = 0.018441001001213278
step = 15, Training Accuracy: 0.7435714285714285
Training loss = 0.0183999554174287
step = 16, Training Accuracy: 0.7416071428571429
Training loss = 0.01846501282283238
step = 17, Training Accuracy: 0.7389285714285714
Training loss = 0.01803092788372721
step = 18, Training Accuracy: 0.7405357142857143
Training loss = 0.018416410349309444
step = 19, Training Accuracy: 0.7423214285714286
Validation Accuracy: 0.78375
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.01805947625211307
step = 0, Training Accuracy: 0.7451785714285715
Validation Accuracy: 0.78125
Training loss = 0.018213585343744072
step = 1, Training Accuracy: 0.7410714285714286
Training loss = 0.018247196301817894
step = 2, Training Accuracy: 0.7444642857142857
Training loss = 0.017952568914209092
step = 3, Training Accuracy: 0.7528571428571429
Training loss = 0.018098629345851287
step = 4, Training Accuracy: 0.7435714285714285
Training loss = 0.01845817602106503
step = 5, Training Accuracy: 0.7401785714285715
Training loss = 0.01804685398936272
step = 6, Training Accuracy: 0.7451785714285715
Training loss = 0.01823487023689917
step = 7, Training Accuracy: 0.7530357142857143
Training loss = 0.01798622226608651
step = 8, Training Accuracy: 0.7464285714285714
Training loss = 0.01785730129373925
step = 9, Training Accuracy: 0.75
Training loss = 0.017754421292671137
step = 10, Training Accuracy: 0.7557142857142857
Training loss = 0.018396348426384584
step = 11, Training Accuracy: 0.74375
Training loss = 0.018604705179376263
step = 12, Training Accuracy: 0.7382142857142857
Training loss = 0.018338435531726906
step = 13, Training Accuracy: 0.7426785714285714
Training loss = 0.018213899247348307
step = 14, Training Accuracy: 0.74625
Training loss = 0.018256644159555434
step = 15, Training Accuracy: 0.7417857142857143
Training loss = 0.0179414981550404
step = 16, Training Accuracy: 0.7557142857142857
Training loss = 0.018113905005156992
step = 17, Training Accuracy: 0.7389285714285714
Training loss = 0.018243359529546328
step = 18, Training Accuracy: 0.7444642857142857
Training loss = 0.018218425890164716
step = 19, Training Accuracy: 0.7442857142857143
Validation Accuracy: 0.78
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227104, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018092652011130538
step = 0, Training Accuracy: 0.7475
Validation Accuracy: 0.77875
Training loss = 0.018199707817818437
step = 1, Training Accuracy: 0.7439285714285714
Training loss = 0.018165520397680147
step = 2, Training Accuracy: 0.7453571428571428
Training loss = 0.018207171373069285
step = 3, Training Accuracy: 0.7480357142857142
Training loss = 0.01831896891019174
step = 4, Training Accuracy: 0.7391071428571429
Training loss = 0.018173477431493147
step = 5, Training Accuracy: 0.74625
Training loss = 0.018576939680746623
step = 6, Training Accuracy: 0.7342857142857143
Training loss = 0.01828835238303457
step = 7, Training Accuracy: 0.7485714285714286
Training loss = 0.01840745822127376
step = 8, Training Accuracy: 0.7433928571428572
Training loss = 0.01815685084355729
step = 9, Training Accuracy: 0.7369642857142857
Training loss = 0.018572015799582006
step = 10, Training Accuracy: 0.7373214285714286
Training loss = 0.018299869831119264
step = 11, Training Accuracy: 0.7391071428571429
Training loss = 0.01815421737730503
step = 12, Training Accuracy: 0.7528571428571429
Training loss = 0.018293649181723593
step = 13, Training Accuracy: 0.7391071428571429
Training loss = 0.018342179538948195
step = 14, Training Accuracy: 0.74375
Training loss = 0.017829281485506467
step = 15, Training Accuracy: 0.7482142857142857
Training loss = 0.018077341839671135
step = 16, Training Accuracy: 0.7469642857142857
Training loss = 0.018451823794416018
step = 17, Training Accuracy: 0.7373214285714286
Training loss = 0.01823400308511087
step = 18, Training Accuracy: 0.7410714285714286
Training loss = 0.017855412401258944
step = 19, Training Accuracy: 0.7451785714285715
Validation Accuracy: 0.7825
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018224641185786043
step = 0, Training Accuracy: 0.7475
Validation Accuracy: 0.77875
Training loss = 0.018399145863950254
step = 1, Training Accuracy: 0.7448214285714285
Training loss = 0.018107791149190495
step = 2, Training Accuracy: 0.74
Training loss = 0.018137086874672345
step = 3, Training Accuracy: 0.7526785714285714
Training loss = 0.01790509400623185
step = 4, Training Accuracy: 0.7466071428571428
Training loss = 0.018204318157264164
step = 5, Training Accuracy: 0.7496428571428572
Training loss = 0.018417778579252107
step = 6, Training Accuracy: 0.7421428571428571
Training loss = 0.018038048142833368
step = 7, Training Accuracy: 0.7503571428571428
Training loss = 0.018300563474850994
step = 8, Training Accuracy: 0.7453571428571428
Training loss = 0.018097686735647066
step = 9, Training Accuracy: 0.74625
Training loss = 0.018633515244083746
step = 10, Training Accuracy: 0.7351785714285715
Training loss = 0.0181921681069902
step = 11, Training Accuracy: 0.7483928571428572
Training loss = 0.018303701058030128
step = 12, Training Accuracy: 0.7408928571428571
Training loss = 0.018395076963518348
step = 13, Training Accuracy: 0.745
Training loss = 0.018026266821793147
step = 14, Training Accuracy: 0.7508928571428571
Training loss = 0.018258979810135707
step = 15, Training Accuracy: 0.7448214285714285
Training loss = 0.01831610057502985
step = 16, Training Accuracy: 0.74375
Training loss = 0.018238072826394013
step = 17, Training Accuracy: 0.74125
Training loss = 0.018374943653387682
step = 18, Training Accuracy: 0.74125
Training loss = 0.018243793861142228
step = 19, Training Accuracy: 0.7460714285714286
Validation Accuracy: 0.775
2  	7     	0.780625	0.00400195	0.77375	0.78375
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227106, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018288870911513055
step = 0, Training Accuracy: 0.7405357142857143
Validation Accuracy: 0.7775
Training loss = 0.01798607796962772
step = 1, Training Accuracy: 0.7523214285714286
Training loss = 0.018262145785348755
step = 2, Training Accuracy: 0.7408928571428571
Training loss = 0.018665602127356188
step = 3, Training Accuracy: 0.7430357142857142
Training loss = 0.017994359843432905
step = 4, Training Accuracy: 0.7482142857142857
Training loss = 0.01805638447936092
step = 5, Training Accuracy: 0.7435714285714285
Training loss = 0.018288239826049123
step = 6, Training Accuracy: 0.7439285714285714
Training loss = 0.018405025074524538
step = 7, Training Accuracy: 0.7403571428571428
Training loss = 0.0185097101437194
step = 8, Training Accuracy: 0.7364285714285714
Training loss = 0.018080313487776688
step = 9, Training Accuracy: 0.7471428571428571
Training loss = 0.018112843829606262
step = 10, Training Accuracy: 0.7442857142857143
Training loss = 0.01852456733584404
step = 11, Training Accuracy: 0.7385714285714285
Training loss = 0.01801655815115997
step = 12, Training Accuracy: 0.74125
Training loss = 0.01855740545583623
step = 13, Training Accuracy: 0.73625
Training loss = 0.018017723895609378
step = 14, Training Accuracy: 0.74375
Training loss = 0.018239661465798106
step = 15, Training Accuracy: 0.74125
Training loss = 0.01848809849470854
step = 16, Training Accuracy: 0.7439285714285714
Training loss = 0.018454345826591764
step = 17, Training Accuracy: 0.7455357142857143
Training loss = 0.018285353194390026
step = 18, Training Accuracy: 0.7469642857142857
Training loss = 0.0184907293426139
step = 19, Training Accuracy: 0.7396428571428572
Validation Accuracy: 0.78
parameter = [0.3953264816225227, 0.5493500325792158, 0.4916695131837777, 0.7063849315538604, 0.21766403855865166, 0.751058193267523]
Training loss = 0.018120249690754073
step = 0, Training Accuracy: 0.74875
Validation Accuracy: 0.78375
Training loss = 0.01838579520583153
step = 1, Training Accuracy: 0.7414285714285714
Training loss = 0.01828580622694322
step = 2, Training Accuracy: 0.7441071428571429
Training loss = 0.01795089508805956
step = 3, Training Accuracy: 0.7464285714285714
Training loss = 0.018266319052449295
step = 4, Training Accuracy: 0.7444642857142857
Training loss = 0.018498119243553705
step = 5, Training Accuracy: 0.7453571428571428
Training loss = 0.01848338187805244
step = 6, Training Accuracy: 0.74
Training loss = 0.017991907601909978
step = 7, Training Accuracy: 0.7405357142857143
Training loss = 0.018817870308245933
step = 8, Training Accuracy: 0.7353571428571428
Training loss = 0.017942947447299958
step = 9, Training Accuracy: 0.7426785714285714
Training loss = 0.01867596364979233
step = 10, Training Accuracy: 0.7326785714285714
Training loss = 0.018581037766167097
step = 11, Training Accuracy: 0.7466071428571428
Training loss = 0.018380335925945214
step = 12, Training Accuracy: 0.7376785714285714
Training loss = 0.018659126231712956
step = 13, Training Accuracy: 0.7417857142857143
Training loss = 0.01842540530753987
step = 14, Training Accuracy: 0.7448214285714285
Training loss = 0.018085498501147542
step = 15, Training Accuracy: 0.7442857142857143
Training loss = 0.018353893613176686
step = 16, Training Accuracy: 0.7458928571428571
Training loss = 0.018313161475317818
step = 17, Training Accuracy: 0.7466071428571428
Training loss = 0.01859887845814228
step = 18, Training Accuracy: 0.7396428571428572
Training loss = 0.018357839265039987
step = 19, Training Accuracy: 0.7408928571428571
Validation Accuracy: 0.77875
parameter = [0.3964468608620138, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.01844232710876635
step = 0, Training Accuracy: 0.74625
Validation Accuracy: 0.775
Training loss = 0.018586278802582196
step = 1, Training Accuracy: 0.7369642857142857
Training loss = 0.01816752294876746
step = 2, Training Accuracy: 0.7453571428571428
Training loss = 0.018627108489828452
step = 3, Training Accuracy: 0.7346428571428572
Training loss = 0.0186287484690547
step = 4, Training Accuracy: 0.7382142857142857
Training loss = 0.018365637127842224
step = 5, Training Accuracy: 0.74125
Training loss = 0.018529424060668265
step = 6, Training Accuracy: 0.7444642857142857
Training loss = 0.018191675187221597
step = 7, Training Accuracy: 0.7441071428571429
Training loss = 0.01828804432281426
step = 8, Training Accuracy: 0.7433928571428572
Training loss = 0.018460483183818203
step = 9, Training Accuracy: 0.7344642857142857
Training loss = 0.018377083875238894
step = 10, Training Accuracy: 0.7396428571428572
Training loss = 0.018400261465992246
step = 11, Training Accuracy: 0.74375
Training loss = 0.018303649058299405
step = 12, Training Accuracy: 0.7405357142857143
Training loss = 0.01853496007089104
step = 13, Training Accuracy: 0.7407142857142858
Training loss = 0.01856849976416145
step = 14, Training Accuracy: 0.7392857142857143
Training loss = 0.018505178679312978
step = 15, Training Accuracy: 0.74375
Training loss = 0.018171376048454216
step = 16, Training Accuracy: 0.7423214285714286
Training loss = 0.01837483427886452
step = 17, Training Accuracy: 0.7369642857142857
Training loss = 0.01851358752165522
step = 18, Training Accuracy: 0.7396428571428572
Training loss = 0.01844007593180452
step = 19, Training Accuracy: 0.7432142857142857
Validation Accuracy: 0.78125
parameter = [0.3964468608620138, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.01829848263412714
step = 0, Training Accuracy: 0.7423214285714286
Validation Accuracy: 0.78
Training loss = 0.01873482978769711
step = 1, Training Accuracy: 0.7373214285714286
Training loss = 0.018704620307045325
step = 2, Training Accuracy: 0.7335714285714285
Training loss = 0.018663718434316773
step = 3, Training Accuracy: 0.73875
Training loss = 0.01865402587290321
step = 4, Training Accuracy: 0.7419642857142857
Training loss = 0.018538312810872284
step = 5, Training Accuracy: 0.7446428571428572
Training loss = 0.018590907848307064
step = 6, Training Accuracy: 0.7383928571428572
Training loss = 0.018265304969889776
step = 7, Training Accuracy: 0.7416071428571429
Training loss = 0.018469391044761453
step = 8, Training Accuracy: 0.7416071428571429
Training loss = 0.018222147875598498
step = 9, Training Accuracy: 0.7391071428571429
Training loss = 0.01828612591006926
step = 10, Training Accuracy: 0.745
Training loss = 0.01869717320693391
step = 11, Training Accuracy: 0.7357142857142858
Training loss = 0.01819726801876511
step = 12, Training Accuracy: 0.7425
Training loss = 0.018265002524214133
step = 13, Training Accuracy: 0.7446428571428572
Training loss = 0.01845582171742405
step = 14, Training Accuracy: 0.7426785714285714
Training loss = 0.01840237925627402
step = 15, Training Accuracy: 0.7425
Training loss = 0.018505953630166393
step = 16, Training Accuracy: 0.74
Training loss = 0.01852278514632157
step = 17, Training Accuracy: 0.7408928571428571
Training loss = 0.01818741179470505
step = 18, Training Accuracy: 0.7448214285714285
Training loss = 0.01815357234328985
step = 19, Training Accuracy: 0.7544642857142857
Validation Accuracy: 0.78375
parameter = [0.3940613559015592, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018502045231206077
step = 0, Training Accuracy: 0.7455357142857143
Validation Accuracy: 0.78
Training loss = 0.01820930411240884
step = 1, Training Accuracy: 0.7408928571428571
Training loss = 0.01809307627379894
step = 2, Training Accuracy: 0.75125
Training loss = 0.018233114326638836
step = 3, Training Accuracy: 0.7430357142857142
Training loss = 0.018420448851372513
step = 4, Training Accuracy: 0.74625
Training loss = 0.018016784680741173
step = 5, Training Accuracy: 0.7478571428571429
Training loss = 0.018011013072516235
step = 6, Training Accuracy: 0.7460714285714286
Training loss = 0.018215685055724212
step = 7, Training Accuracy: 0.7433928571428572
Training loss = 0.01830255673932178
step = 8, Training Accuracy: 0.7403571428571428
Training loss = 0.018381753468087743
step = 9, Training Accuracy: 0.7417857142857143
Training loss = 0.01816525310277939
step = 10, Training Accuracy: 0.7446428571428572
Training loss = 0.018280433368469986
step = 11, Training Accuracy: 0.7442857142857143
Training loss = 0.01828049191406795
step = 12, Training Accuracy: 0.7491071428571429
Training loss = 0.01813089654381786
step = 13, Training Accuracy: 0.7473214285714286
Training loss = 0.01825703364397798
step = 14, Training Accuracy: 0.7514285714285714
Training loss = 0.018391123030866895
step = 15, Training Accuracy: 0.7414285714285714
Training loss = 0.018311956971883772
step = 16, Training Accuracy: 0.7414285714285714
Training loss = 0.018355804958513806
step = 17, Training Accuracy: 0.7419642857142857
Training loss = 0.018011875578335352
step = 18, Training Accuracy: 0.7457142857142857
Training loss = 0.01824118043162993
step = 19, Training Accuracy: 0.745
Validation Accuracy: 0.77625
parameter = [0.3964468608620138, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.18052656455081662, 0.6930859859934353]
Training loss = 0.01805223373430116
step = 0, Training Accuracy: 0.7473214285714286
Validation Accuracy: 0.7775
Training loss = 0.018317473222102438
step = 1, Training Accuracy: 0.7378571428571429
Training loss = 0.01830245815217495
step = 2, Training Accuracy: 0.7426785714285714
Training loss = 0.01845213441976479
step = 3, Training Accuracy: 0.7401785714285715
Training loss = 0.018084918457482543
step = 4, Training Accuracy: 0.74375
Training loss = 0.018100929010127272
step = 5, Training Accuracy: 0.7455357142857143
Training loss = 0.01844062514603138
step = 6, Training Accuracy: 0.7392857142857143
Training loss = 0.01845334348401853
step = 7, Training Accuracy: 0.7408928571428571
Training loss = 0.018311671039887838
step = 8, Training Accuracy: 0.7455357142857143
Training loss = 0.0183675825755511
step = 9, Training Accuracy: 0.7453571428571428
Training loss = 0.01836218138890607
step = 10, Training Accuracy: 0.7435714285714285
Training loss = 0.018318161661071437
step = 11, Training Accuracy: 0.7405357142857143
Training loss = 0.01850893476179668
step = 12, Training Accuracy: 0.7367857142857143
Training loss = 0.01877208456929241
step = 13, Training Accuracy: 0.7371428571428571
Training loss = 0.018310292586684228
step = 14, Training Accuracy: 0.7389285714285714
Training loss = 0.018129286164683955
step = 15, Training Accuracy: 0.7496428571428572
Training loss = 0.01841429824807814
step = 16, Training Accuracy: 0.7442857142857143
Training loss = 0.018574012209262167
step = 17, Training Accuracy: 0.7433928571428572
Training loss = 0.01839012156107596
step = 18, Training Accuracy: 0.7435714285714285
Training loss = 0.0182085591341768
step = 19, Training Accuracy: 0.7446428571428572
Validation Accuracy: 0.77875
parameter = [0.3940613559015593, 0.6401459843393509, 0.5372383179830428, 0.7102268361227105, 0.2168612171567833, 0.8314766151043855]
Training loss = 0.018333182106060643
step = 0, Training Accuracy: 0.7575
Validation Accuracy: 0.7825
Training loss = 0.018676935035203183
step = 1, Training Accuracy: 0.7403571428571428
Training loss = 0.01825501412153244
step = 2, Training Accuracy: 0.7442857142857143
Training loss = 0.01859857843390533
step = 3, Training Accuracy: 0.7360714285714286
Training loss = 0.018084440774151257
step = 4, Training Accuracy: 0.7423214285714286
Training loss = 0.018367654335285935
step = 5, Training Accuracy: 0.7408928571428571
Training loss = 0.018011550376457828
step = 6, Training Accuracy: 0.7467857142857143
Training loss = 0.018271993952138084
step = 7, Training Accuracy: 0.7467857142857143
Training loss = 0.01844793599098921
step = 8, Training Accuracy: 0.7444642857142857
Training loss = 0.0185202241476093
step = 9, Training Accuracy: 0.7469642857142857
Training loss = 0.018347972211028848
step = 10, Training Accuracy: 0.7367857142857143
Training loss = 0.018462642420615467
step = 11, Training Accuracy: 0.7414285714285714
Training loss = 0.018301215916872025
step = 12, Training Accuracy: 0.7385714285714285
Training loss = 0.017973279085542474
step = 13, Training Accuracy: 0.7433928571428572
Training loss = 0.01811938577996833
step = 14, Training Accuracy: 0.7385714285714285
Training loss = 0.018093452395073005
step = 15, Training Accuracy: 0.7475
Training loss = 0.018357362728565932
step = 16, Training Accuracy: 0.7448214285714285
Training loss = 0.01839326782418149
step = 17, Training Accuracy: 0.7430357142857142
Training loss = 0.018233019055000372
step = 18, Training Accuracy: 0.745
Training loss = 0.0179079106343644
step = 19, Training Accuracy: 0.7491071428571429
Validation Accuracy: 0.7775
3  	7     	0.78125 	0.000883883	0.78   	0.7825 
parameter = [0.3941762614150692, 0.6105548879162251, 0.5070878453358544, 0.7069226425694695, 0.2088629183873411, 0.7939983339538585]
Training loss = 0.018330494607133523
step = 0, Training Accuracy: 0.7475
Validation Accuracy: 0.78
Training loss = 0.0187591950116413
step = 1, Training Accuracy: 0.7317857142857143
Training loss = 0.01787152944930962
step = 2, Training Accuracy: 0.7546428571428572
Training loss = 0.01855066067938294
step = 3, Training Accuracy: 0.7375
Training loss = 0.01833653621907745
step = 4, Training Accuracy: 0.7458928571428571
Training loss = 0.018291032665542193
step = 5, Training Accuracy: 0.7410714285714286
Training loss = 0.018534457513264248
step = 6, Training Accuracy: 0.7328571428571429
Training loss = 0.01854973595057215
step = 7, Training Accuracy: 0.7401785714285715
Training loss = 0.018230414880173545
step = 8, Training Accuracy: 0.7394642857142857
Training loss = 0.01827645433800561
step = 9, Training Accuracy: 0.7396428571428572
Training loss = 0.018168716760618348
step = 10, Training Accuracy: 0.7464285714285714
Training loss = 0.017993287806000027
step = 11, Training Accuracy: 0.7498214285714285
Training loss = 0.0181490937939712
step = 12, Training Accuracy: 0.74375
Training loss = 0.01823972082563809
step = 13, Training Accuracy: 0.7439285714285714
Training loss = 0.018098269903234075
step = 14, Training Accuracy: 0.7496428571428572
Training loss = 0.017908119798770973
step = 15, Training Accuracy: 0.7475
Training loss = 0.01841847164822476
step = 16, Training Accuracy: 0.7401785714285715
Training loss = 0.018456857449242046
step = 17, Training Accuracy: 0.7414285714285714
Training loss = 0.01822901286716972
step = 18, Training Accuracy: 0.7369642857142857
Training loss = 0.0181863219078098
step = 19, Training Accuracy: 0.7428571428571429
Validation Accuracy: 0.77875
parameter = [0.39644686086201386, 0.5522006008192137, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.01837885568184512
step = 0, Training Accuracy: 0.7375
Validation Accuracy: 0.77625
Training loss = 0.0185306780146701
step = 1, Training Accuracy: 0.7435714285714285
Training loss = 0.018092716815216202
step = 2, Training Accuracy: 0.74875
Training loss = 0.018478577280683178
step = 3, Training Accuracy: 0.7460714285714286
Training loss = 0.018389728372650488
step = 4, Training Accuracy: 0.7464285714285714
Training loss = 0.01835992447499718
step = 5, Training Accuracy: 0.7339285714285714
Training loss = 0.018583249524235724
step = 6, Training Accuracy: 0.7346428571428572
Training loss = 0.01841014468244144
step = 7, Training Accuracy: 0.73875
Training loss = 0.01851608337568385
step = 8, Training Accuracy: 0.7376785714285714
Training loss = 0.018563190496393614
step = 9, Training Accuracy: 0.7432142857142857
Training loss = 0.01866688221693039
step = 10, Training Accuracy: 0.7394642857142857
Training loss = 0.01859063760510513
step = 11, Training Accuracy: 0.7455357142857143
Training loss = 0.018801984403814587
step = 12, Training Accuracy: 0.7319642857142857
Training loss = 0.018412098842007774
step = 13, Training Accuracy: 0.7444642857142857
Training loss = 0.018383205266935484
step = 14, Training Accuracy: 0.7375
Training loss = 0.01836019446275064
step = 15, Training Accuracy: 0.7444642857142857
Training loss = 0.018356121060039317
step = 16, Training Accuracy: 0.7398214285714285
Training loss = 0.0185485192867262
step = 17, Training Accuracy: 0.7441071428571429
Training loss = 0.01807758747466973
step = 18, Training Accuracy: 0.7466071428571428
Training loss = 0.01854613056672471
step = 19, Training Accuracy: 0.7410714285714286
Validation Accuracy: 0.775
parameter = [0.3964468608620138, 0.5522006008192136, 0.3986942650626178, 0.7027611237698669, 0.20534906299921862, 0.6930859859934353]
Training loss = 0.018350339079541818
step = 0, Training Accuracy: 0.7367857142857143
Validation Accuracy: 0.78
Training loss = 0.018274861920092787
step = 1, Training Accuracy: 0.7430357142857142
Training loss = 0.01878898249672992
step = 2, Training Accuracy: 0.7391071428571429
Training loss = 0.018454667191420283
step = 3, Training Accuracy: 0.7423214285714286
Training loss = 0.018484334711517607
step = 4, Training Accuracy: 0.7441071428571429
Training loss = 0.018224461616149972
step = 5, Training Accuracy: 0.7530357142857143
Training loss = 0.018457380405494144
step = 6, Training Accuracy: 0.7380357142857142
Training loss = 0.018357000601078782
step = 7, Training Accuracy: 0.7496428571428572
Training loss = 0.018580656019704683
step = 8, Training Accuracy: 0.7385714285714285
Training loss = 0.01838399544890438
step = 9, Training Accuracy: 0.7355357142857143
Training loss = 0.018497122238789285
step = 10, Training Accuracy: 0.7408928571428571
Training loss = 0.01794618661914553
step = 11, Training Accuracy: 0.7507142857142857
Training loss = 0.01834978909896953
step = 12, Training Accuracy: 0.7417857142857143
Training loss = 0.01829097113439015
step = 13, Training Accuracy: 0.7389285714285714
Training loss = 0.01835612161883286
step = 14, Training Accuracy: 0.7430357142857142
Training loss = 0.018630242688315254
step = 15, Training Accuracy: 0.7398214285714285
Training loss = 0.01838521286313023
step = 16, Training Accuracy: 0.7407142857142858
Training loss = 0.01834188625216484
step = 17, Training Accuracy: 0.7433928571428572
Training loss = 0.018289989813097887
step = 18, Training Accuracy: 0.7444642857142857
Training loss = 0.01824494249586548
step = 19, Training Accuracy: 0.7371428571428571
Validation Accuracy: 0.7775
parameter = [0.39665657996095016, 0.5484267174266787, 0.5457811124296035, 0.7105371646535841, 0.2172313341805274, 0.7999335719895005]
Training loss = 0.018311647357685226
step = 0, Training Accuracy: 0.7410714285714286
Validation Accuracy: 0.7775
Training loss = 0.018596203540052685
step = 1, Training Accuracy: 0.73875
Training loss = 0.018443739882537297
step = 2, Training Accuracy: 0.7398214285714285
Training loss = 0.01843912163483245
step = 3, Training Accuracy: 0.7455357142857143
Training loss = 0.018309695412005698
step = 4, Training Accuracy: 0.7401785714285715
Training loss = 0.01786494717001915
step = 5, Training Accuracy: 0.7485714285714286
Training loss = 0.018221121105764595
step = 6, Training Accuracy: 0.7423214285714286
Training loss = 0.017999954409897326
step = 7, Training Accuracy: 0.7482142857142857
Training loss = 0.01830446911177465
step = 8, Training Accuracy: 0.7460714285714286
Training loss = 0.018038972419287477
step = 9, Training Accuracy: 0.7451785714285715
Training loss = 0.018360383601060935
step = 10, Training Accuracy: 0.7430357142857142
Training loss = 0.018255713507533074
step = 11, Training Accuracy: 0.7423214285714286
Training loss = 0.01842222107840436
step = 12, Training Accuracy: 0.7466071428571428
Training loss = 0.018101005618061338
step = 13, Training Accuracy: 0.7444642857142857
Training loss = 0.018188945379640376
step = 14, Training Accuracy: 0.7417857142857143
Training loss = 0.01841183715100799
step = 15, Training Accuracy: 0.7407142857142858
Training loss = 0.01843493783048221
step = 16, Training Accuracy: 0.7435714285714285
Training loss = 0.0182406394343291
step = 17, Training Accuracy: 0.7405357142857143
Training loss = 0.018352961385888714
step = 18, Training Accuracy: 0.7457142857142857
Training loss = 0.018236521988042764
step = 19, Training Accuracy: 0.7444642857142857
Validation Accuracy: 0.7775
parameter = [0.39650501603466887, 0.6399730712775206, 0.4894231695856869, 0.7053804268629202, 0.2126379856783794, 0.831794619791595]
Training loss = 0.018119547888636588
step = 0, Training Accuracy: 0.7446428571428572
Validation Accuracy: 0.77875
Training loss = 0.018289861317191805
step = 1, Training Accuracy: 0.73875
Training loss = 0.01830808172268527
step = 2, Training Accuracy: 0.7398214285714285
Training loss = 0.01866221786609718
step = 3, Training Accuracy: 0.7398214285714285
Training loss = 0.018023557796009948
step = 4, Training Accuracy: 0.7471428571428571
Training loss = 0.018213399169700488
step = 5, Training Accuracy: 0.7508928571428571
Training loss = 0.01846412727875369
step = 6, Training Accuracy: 0.7446428571428572
Training loss = 0.018254874502973896
step = 7, Training Accuracy: 0.7514285714285714
Training loss = 0.017935261742344926
step = 8, Training Accuracy: 0.7525
Training loss = 0.018320966703551156
step = 9, Training Accuracy: 0.7507142857142857
Training loss = 0.018340999127498696
step = 10, Training Accuracy: 0.7405357142857143
Training loss = 0.018288437612354755
step = 11, Training Accuracy: 0.7455357142857143
Training loss = 0.01835569715393441
step = 12, Training Accuracy: 0.7494642857142857
Training loss = 0.018346197961696557
step = 13, Training Accuracy: 0.7435714285714285
Training loss = 0.018312183836741106
step = 14, Training Accuracy: 0.7410714285714286
Training loss = 0.018471544885209627
step = 15, Training Accuracy: 0.7414285714285714
Training loss = 0.018675739743879864
step = 16, Training Accuracy: 0.7333928571428572
Training loss = 0.018269978713776384
step = 17, Training Accuracy: 0.7448214285714285
Training loss = 0.01831931478210858
step = 18, Training Accuracy: 0.7435714285714285
Training loss = 0.018075245719935212
step = 19, Training Accuracy: 0.7464285714285714
Validation Accuracy: 0.78125
parameter = [0.394152731579476, 0.6422470170673572, 0.46075341403056547, 0.7069904202313984, 0.2120910030060858, 0.8127911740120353]
Training loss = 0.018370036828730788
step = 0, Training Accuracy: 0.7371428571428571
Validation Accuracy: 0.7825
Training loss = 0.018121973089873792
step = 1, Training Accuracy: 0.7433928571428572
Training loss = 0.018290645687707833
step = 2, Training Accuracy: 0.7401785714285715
Training loss = 0.018513031149549143
step = 3, Training Accuracy: 0.74125
Training loss = 0.01824831176549196
step = 4, Training Accuracy: 0.7428571428571429
Training loss = 0.018422000067574638
step = 5, Training Accuracy: 0.7455357142857143
Training loss = 0.018364327837313923
step = 6, Training Accuracy: 0.7389285714285714
Training loss = 0.018512065352073736
step = 7, Training Accuracy: 0.7355357142857143
Training loss = 0.01830747631511518
step = 8, Training Accuracy: 0.7392857142857143
Training loss = 0.01823533837816545
step = 9, Training Accuracy: 0.745
Training loss = 0.01831264686371599
step = 10, Training Accuracy: 0.7421428571428571
Training loss = 0.01810837005398103
step = 11, Training Accuracy: 0.74
Training loss = 0.01844925940568958
step = 12, Training Accuracy: 0.7414285714285714
Training loss = 0.018530824519693852
step = 13, Training Accuracy: 0.7383928571428572
Training loss = 0.01840258339686053
step = 14, Training Accuracy: 0.7408928571428571
Training loss = 0.0185083972503032
step = 15, Training Accuracy: 0.7426785714285714
Training loss = 0.018115819575531142
step = 16, Training Accuracy: 0.7401785714285715
Training loss = 0.018098034864025458
step = 17, Training Accuracy: 0.7460714285714286
Training loss = 0.018557046726346015
step = 18, Training Accuracy: 0.7398214285714285
Training loss = 0.01807614090187209
step = 19, Training Accuracy: 0.7478571428571429
Validation Accuracy: 0.775
parameter = [0.39423933099140906, 0.5809672966316035, 0.41382740776451055, 0.7023954378509423, 0.21001361693387766, 0.700500044857376]
Training loss = 0.01830050939427955
step = 0, Training Accuracy: 0.7448214285714285
Validation Accuracy: 0.78
Training loss = 0.018434213580829757
step = 1, Training Accuracy: 0.7403571428571428
Training loss = 0.01859365474964891
step = 2, Training Accuracy: 0.7360714285714286
Training loss = 0.01838014829903841
step = 3, Training Accuracy: 0.7414285714285714
Training loss = 0.01857607909079109
step = 4, Training Accuracy: 0.7364285714285714
Training loss = 0.018008261285722256
step = 5, Training Accuracy: 0.7439285714285714
Training loss = 0.018638328739574977
step = 6, Training Accuracy: 0.7342857142857143
Training loss = 0.018258137197366783
step = 7, Training Accuracy: 0.7428571428571429
Training loss = 0.01841102857674871
step = 8, Training Accuracy: 0.7369642857142857
Training loss = 0.01840081603931529
step = 9, Training Accuracy: 0.7433928571428572
Training loss = 0.01857977382838726
step = 10, Training Accuracy: 0.7351785714285715
Training loss = 0.018249016622347493
step = 11, Training Accuracy: 0.7407142857142858
Training loss = 0.01819965701018061
step = 12, Training Accuracy: 0.7433928571428572
Training loss = 0.018222828496779715
step = 13, Training Accuracy: 0.7457142857142857
Training loss = 0.01829704072858606
step = 14, Training Accuracy: 0.7408928571428571
Training loss = 0.018508361939873014
step = 15, Training Accuracy: 0.7475
Training loss = 0.018349243828228542
step = 16, Training Accuracy: 0.7442857142857143
Training loss = 0.0184165536825146
step = 17, Training Accuracy: 0.7392857142857143
Training loss = 0.018472063397722584
step = 18, Training Accuracy: 0.7317857142857143
Training loss = 0.018470688121659414
step = 19, Training Accuracy: 0.7471428571428571
Validation Accuracy: 0.77875
4  	7     	0.778125	0.000625   	0.7775 	0.77875
parameter = [0.39665657996095016, 0.5484267174266787, 0.5457811124296035, 0.7105371646535841, 0.2172313341805274, 0.7999335719895005]
Training loss = 0.018293026419622556
step = 0, Training Accuracy: 0.74375
Validation Accuracy: 0.78
Training loss = 0.017991654532296317
step = 1, Training Accuracy: 0.7469642857142857
Training loss = 0.018443310766347816
step = 2, Training Accuracy: 0.7432142857142857
Training loss = 0.018486907013825007
step = 3, Training Accuracy: 0.7451785714285715
Training loss = 0.018315876019852503
step = 4, Training Accuracy: 0.7471428571428571
Training loss = 0.01844392413539546
step = 5, Training Accuracy: 0.7391071428571429
Training loss = 0.018231403694621156
step = 6, Training Accuracy: 0.7364285714285714
Training loss = 0.018307198509573935
step = 7, Training Accuracy: 0.7398214285714285
Training loss = 0.01835751459534679
step = 8, Training Accuracy: 0.7391071428571429
Training loss = 0.017958584342684065
step = 9, Training Accuracy: 0.7457142857142857
Training loss = 0.01822066629571574
step = 10, Training Accuracy: 0.7416071428571429
Training loss = 0.018031556100717612
step = 11, Training Accuracy: 0.7458928571428571
Training loss = 0.018240370569484573
step = 12, Training Accuracy: 0.7376785714285714
Training loss = 0.018329164423048497
step = 13, Training Accuracy: 0.7441071428571429
Training loss = 0.017861355654895307
step = 14, Training Accuracy: 0.7439285714285714
Training loss = 0.018105124362877437
step = 15, Training Accuracy: 0.7442857142857143
Training loss = 0.01812706554574626
step = 16, Training Accuracy: 0.7407142857142858
Training loss = 0.01850492720093046
step = 17, Training Accuracy: 0.7382142857142857
Training loss = 0.018120895773172378
step = 18, Training Accuracy: 0.7507142857142857
Training loss = 0.018528659239943537
step = 19, Training Accuracy: 0.7408928571428571
Validation Accuracy: 0.78
parameter = [0.37560031080260325, 0.5484267174266787, 0.5457811124296035, 0.7105371646535841, 0.2172313341805274, 0.9348581102494267]
Training loss = 0.01810787174850702
step = 0, Training Accuracy: 0.7498214285714285
Validation Accuracy: 0.77875
Training loss = 0.018122422519539085
step = 1, Training Accuracy: 0.7391071428571429
Training loss = 0.0182258965075016
step = 2, Training Accuracy: 0.7435714285714285
Training loss = 0.018190852600548948
step = 3, Training Accuracy: 0.7441071428571429
Training loss = 0.018005818602229866
step = 4, Training Accuracy: 0.7496428571428572
Training loss = 0.018311170100101404
step = 5, Training Accuracy: 0.7423214285714286
Training loss = 0.018273122773638795
step = 6, Training Accuracy: 0.7471428571428571
Training loss = 0.018060638925858907
step = 7, Training Accuracy: 0.7466071428571428
Training loss = 0.017917001944567475
step = 8, Training Accuracy: 0.7519642857142858
Training loss = 0.0178480448360954
step = 9, Training Accuracy: 0.7467857142857143
Training loss = 0.018333846548838274
step = 10, Training Accuracy: 0.7460714285714286
Training loss = 0.01810356717556715
step = 11, Training Accuracy: 0.7457142857142857
Training loss = 0.01808615580201149
step = 12, Training Accuracy: 0.7446428571428572
Training loss = 0.01843875913215535
step = 13, Training Accuracy: 0.7432142857142857
Training loss = 0.018088641656296593
step = 14, Training Accuracy: 0.7416071428571429
Training loss = 0.01819323569536209
step = 15, Training Accuracy: 0.7389285714285714
Training loss = 0.018046523822205406
step = 16, Training Accuracy: 0.7430357142857142
Training loss = 0.018122200423053333
step = 17, Training Accuracy: 0.7446428571428572
Training loss = 0.01801037376480443
step = 18, Training Accuracy: 0.7516071428571428
Training loss = 0.01807052643703563
step = 19, Training Accuracy: 0.74625
Validation Accuracy: 0.78
parameter = [0.3964896022613883, 0.5720378645744564, 0.4061320159122036, 0.710191283605287, 0.21266799373156708, 0.6998816344630756]
Training loss = 0.018383095961596284
step = 0, Training Accuracy: 0.7458928571428571
Validation Accuracy: 0.78
Training loss = 0.017959929385355542
step = 1, Training Accuracy: 0.7473214285714286
Training loss = 0.01827351968203272
step = 2, Training Accuracy: 0.7444642857142857
Training loss = 0.018692334166594913
step = 3, Training Accuracy: 0.7396428571428572
Training loss = 0.018124751581677367
step = 4, Training Accuracy: 0.7446428571428572
Training loss = 0.018126283745680538
step = 5, Training Accuracy: 0.7491071428571429
Training loss = 0.018663799815944262
step = 6, Training Accuracy: 0.7473214285714286
Training loss = 0.018555776152227605
step = 7, Training Accuracy: 0.7408928571428571
Training loss = 0.018367964449737754
step = 8, Training Accuracy: 0.7382142857142857
Training loss = 0.01831327516053404
step = 9, Training Accuracy: 0.7403571428571428
Training loss = 0.01813393246914659
step = 10, Training Accuracy: 0.7417857142857143
Training loss = 0.018433769260134014
step = 11, Training Accuracy: 0.74125
Training loss = 0.018274036587349007
step = 12, Training Accuracy: 0.7425
Training loss = 0.01828049577240433
step = 13, Training Accuracy: 0.7485714285714286
Training loss = 0.01843743448810918
step = 14, Training Accuracy: 0.7441071428571429
Training loss = 0.018593359706657275
step = 15, Training Accuracy: 0.7373214285714286
Training loss = 0.018043193061436924
step = 16, Training Accuracy: 0.7469642857142857
Training loss = 0.01831470842340163
step = 17, Training Accuracy: 0.7407142857142858
Training loss = 0.018813327476382254
step = 18, Training Accuracy: 0.7355357142857143
Training loss = 0.018631871268153192
step = 19, Training Accuracy: 0.7316071428571429
Validation Accuracy: 0.78
parameter = [0.39395419182457875, 0.5541251478326045, 0.39670835779940133, 0.7047767039852965, 0.2079783751333723, 0.7418494593815447]
Training loss = 0.018237796862210547
step = 0, Training Accuracy: 0.7446428571428572
Validation Accuracy: 0.77375
Training loss = 0.0180925094549145
step = 1, Training Accuracy: 0.74625
Training loss = 0.018497263469866344
step = 2, Training Accuracy: 0.7391071428571429
Training loss = 0.018385944510144848
step = 3, Training Accuracy: 0.7455357142857143
Training loss = 0.018234166588102067
step = 4, Training Accuracy: 0.7494642857142857
Training loss = 0.01835973537926163
step = 5, Training Accuracy: 0.7417857142857143
Training loss = 0.018299775602562088
step = 6, Training Accuracy: 0.7378571428571429
Training loss = 0.018308976462909153
step = 7, Training Accuracy: 0.7439285714285714
Training loss = 0.01829010625502893
step = 8, Training Accuracy: 0.7453571428571428
Training loss = 0.018769792722804206
step = 9, Training Accuracy: 0.7344642857142857
Training loss = 0.018288012370467188
step = 10, Training Accuracy: 0.7425
Training loss = 0.0183360573968717
step = 11, Training Accuracy: 0.7396428571428572
Training loss = 0.018395370138542994
step = 12, Training Accuracy: 0.7435714285714285
Training loss = 0.018488280725266253
step = 13, Training Accuracy: 0.7360714285714286
Training loss = 0.018326325890208994
step = 14, Training Accuracy: 0.7373214285714286
Training loss = 0.018295390329190662
step = 15, Training Accuracy: 0.74125
Training loss = 0.018523789894367967
step = 16, Training Accuracy: 0.7394642857142857
Training loss = 0.0183370030777795
step = 17, Training Accuracy: 0.7457142857142857
Training loss = 0.01835628916110311
step = 18, Training Accuracy: 0.7364285714285714
Training loss = 0.01855492494468178
step = 19, Training Accuracy: 0.74
Validation Accuracy: 0.78125
parameter = [0.39512806994100824, 0.6101508451563941, 0.5333908212445951, 0.7084369287151008, 0.2155214208726134, 0.7991767554820821]
Training loss = 0.018209502340427466
step = 0, Training Accuracy: 0.74875
Validation Accuracy: 0.77625
Training loss = 0.018294545374810695
step = 1, Training Accuracy: 0.7405357142857143
Training loss = 0.018069849557110242
step = 2, Training Accuracy: 0.7426785714285714
Training loss = 0.018436824386673316
step = 3, Training Accuracy: 0.7392857142857143
Training loss = 0.01848182084304946
step = 4, Training Accuracy: 0.7430357142857142
Training loss = 0.01824868858924934
step = 5, Training Accuracy: 0.7441071428571429
Training loss = 0.018245135119983127
step = 6, Training Accuracy: 0.7392857142857143
Training loss = 0.0182959569511669
step = 7, Training Accuracy: 0.7441071428571429
Training loss = 0.01844732342021806
step = 8, Training Accuracy: 0.7405357142857143
Training loss = 0.01843902910394328
step = 9, Training Accuracy: 0.7426785714285714
Training loss = 0.018392791556460515
step = 10, Training Accuracy: 0.7378571428571429
Training loss = 0.018182368773434845
step = 11, Training Accuracy: 0.7455357142857143
Training loss = 0.01802740504699094
step = 12, Training Accuracy: 0.7505357142857143
Training loss = 0.018259823242468495
step = 13, Training Accuracy: 0.7448214285714285
Training loss = 0.018174711066697324
step = 14, Training Accuracy: 0.74875
Training loss = 0.018335984774998256
step = 15, Training Accuracy: 0.7442857142857143
Training loss = 0.01830496588455779
step = 16, Training Accuracy: 0.7467857142857143
Training loss = 0.018099554372685296
step = 17, Training Accuracy: 0.7480357142857142
Training loss = 0.01837847217917442
step = 18, Training Accuracy: 0.7401785714285715
Training loss = 0.018149380987243995
step = 19, Training Accuracy: 0.7455357142857143
Validation Accuracy: 0.77625
5  	5     	0.78    	0.00125    	0.77875	0.78125
parameter = [0.393959508884828, 0.6134668943920061, 0.5099761713316784, 0.7057471261942272, 0.20799416112119892, 0.7813611045752379]
Training loss = 0.018403271649565015
step = 0, Training Accuracy: 0.7371428571428571
Validation Accuracy: 0.7775
Training loss = 0.018573087533669812
step = 1, Training Accuracy: 0.7385714285714285
Training loss = 0.018429563508502073
step = 2, Training Accuracy: 0.7380357142857142
Training loss = 0.01822595580880131
step = 3, Training Accuracy: 0.7389285714285714
Training loss = 0.018054896742105486
step = 4, Training Accuracy: 0.7523214285714286
Training loss = 0.018296212192092622
step = 5, Training Accuracy: 0.7430357142857142
Training loss = 0.01856761101101126
step = 6, Training Accuracy: 0.7369642857142857
Training loss = 0.01837791976651975
step = 7, Training Accuracy: 0.7435714285714285
Training loss = 0.01822157599031925
step = 8, Training Accuracy: 0.7458928571428571
Training loss = 0.01824758571705648
step = 9, Training Accuracy: 0.7478571428571429
Training loss = 0.0185614491839494
step = 10, Training Accuracy: 0.73875
Training loss = 0.018101582718747004
step = 11, Training Accuracy: 0.74625
Training loss = 0.017831182543720518
step = 12, Training Accuracy: 0.7501785714285715
Training loss = 0.018349651002458164
step = 13, Training Accuracy: 0.7428571428571429
Training loss = 0.018310280011168547
step = 14, Training Accuracy: 0.7441071428571429
Training loss = 0.01851027757461582
step = 15, Training Accuracy: 0.735
Training loss = 0.01859004459742989
step = 16, Training Accuracy: 0.7342857142857143
Training loss = 0.018344085434717793
step = 17, Training Accuracy: 0.7416071428571429
Training loss = 0.018248260361807687
step = 18, Training Accuracy: 0.7401785714285715
Training loss = 0.01805411998182535
step = 19, Training Accuracy: 0.7473214285714286
Validation Accuracy: 0.77875
parameter = [0.39413141102456545, 0.5664029558898821, 0.4121145125136081, 0.7028891595692875, 0.20836933902161006, 0.7426358982516591]
Training loss = 0.01772892800824983
step = 0, Training Accuracy: 0.7526785714285714
Validation Accuracy: 0.7775
Training loss = 0.018624331669083664
step = 1, Training Accuracy: 0.7389285714285714
Training loss = 0.018572363911994866
step = 2, Training Accuracy: 0.7353571428571428
Training loss = 0.01834994601351874
step = 3, Training Accuracy: 0.7451785714285715
Training loss = 0.018099631390401296
step = 4, Training Accuracy: 0.7491071428571429
Training loss = 0.01854983108916453
step = 5, Training Accuracy: 0.7398214285714285
Training loss = 0.018628790415823458
step = 6, Training Accuracy: 0.7383928571428572
Training loss = 0.01817423381975719
step = 7, Training Accuracy: 0.7433928571428572
Training loss = 0.01825572380529983
step = 8, Training Accuracy: 0.7344642857142857
Training loss = 0.018431636786886623
step = 9, Training Accuracy: 0.7357142857142858
Training loss = 0.01811143715998956
step = 10, Training Accuracy: 0.7451785714285715
Training loss = 0.018155313448182175
step = 11, Training Accuracy: 0.7451785714285715
Training loss = 0.018285891179527555
step = 12, Training Accuracy: 0.7396428571428572
Training loss = 0.018195791877806188
step = 13, Training Accuracy: 0.74625
Training loss = 0.018369506746530533
step = 14, Training Accuracy: 0.7492857142857143
Training loss = 0.01804784971688475
step = 15, Training Accuracy: 0.7428571428571429
Training loss = 0.01824665865195649
step = 16, Training Accuracy: 0.7369642857142857
Training loss = 0.018442739179091793
step = 17, Training Accuracy: 0.7398214285714285
Training loss = 0.01805472908275468
step = 18, Training Accuracy: 0.7496428571428572
Training loss = 0.018235447758010455
step = 19, Training Accuracy: 0.7466071428571428
Validation Accuracy: 0.77625
parameter = [0.3941806177578237, 0.6032574607410279, 0.4781812114776004, 0.7056201936025053, 0.20844711456035292, 0.7750737322806256]
Training loss = 0.018341377824544907
step = 0, Training Accuracy: 0.7455357142857143
Validation Accuracy: 0.7725
Training loss = 0.018220210916229655
step = 1, Training Accuracy: 0.7401785714285715
Training loss = 0.018338791845100268
step = 2, Training Accuracy: 0.7410714285714286
Training loss = 0.018259816728532316
step = 3, Training Accuracy: 0.7473214285714286
Training loss = 0.018156035452016762
step = 4, Training Accuracy: 0.74
Training loss = 0.018385613188147545
step = 5, Training Accuracy: 0.7455357142857143
Training loss = 0.017900085763207504
step = 6, Training Accuracy: 0.7483928571428572
Training loss = 0.01814986232135977
step = 7, Training Accuracy: 0.7467857142857143
Training loss = 0.01827241773051875
step = 8, Training Accuracy: 0.7358928571428571
Training loss = 0.018206005468964576
step = 9, Training Accuracy: 0.7483928571428572
Training loss = 0.018306512859250817
step = 10, Training Accuracy: 0.7433928571428572
Training loss = 0.018273905270865986
step = 11, Training Accuracy: 0.7457142857142857
Training loss = 0.017987653150090148
step = 12, Training Accuracy: 0.74875
Training loss = 0.018414676929158824
step = 13, Training Accuracy: 0.7417857142857143
Training loss = 0.01831886919481414
step = 14, Training Accuracy: 0.7351785714285715
Training loss = 0.01846122872084379
step = 15, Training Accuracy: 0.7376785714285714
Training loss = 0.018236990624240466
step = 16, Training Accuracy: 0.7467857142857143
Training loss = 0.018222488851419517
step = 17, Training Accuracy: 0.7398214285714285
Training loss = 0.01822099661188466
step = 18, Training Accuracy: 0.7425
Training loss = 0.018333387071532863
step = 19, Training Accuracy: 0.7444642857142857
Validation Accuracy: 0.78375
parameter = [0.39417728673410324, 0.5721733802306438, 0.4110770728929469, 0.7036567312181252, 0.21016669992726614, 0.74411729892817]
Training loss = 0.018607271095471722
step = 0, Training Accuracy: 0.7423214285714286
Validation Accuracy: 0.77625
Training loss = 0.018160453034298762
step = 1, Training Accuracy: 0.7498214285714285
Training loss = 0.018379549442657402
step = 2, Training Accuracy: 0.7435714285714285
Training loss = 0.01845128049807889
step = 3, Training Accuracy: 0.7328571428571429
Training loss = 0.01807696889021567
step = 4, Training Accuracy: 0.7503571428571428
Training loss = 0.018376362781439507
step = 5, Training Accuracy: 0.7394642857142857
Training loss = 0.018059828653931617
step = 6, Training Accuracy: 0.7421428571428571
Training loss = 0.018411384198282445
step = 7, Training Accuracy: 0.7439285714285714
Training loss = 0.018373511007853918
step = 8, Training Accuracy: 0.7401785714285715
Training loss = 0.018464423487229008
step = 9, Training Accuracy: 0.7448214285714285
Training loss = 0.01851400243916682
step = 10, Training Accuracy: 0.74125
Training loss = 0.018290435038506983
step = 11, Training Accuracy: 0.7430357142857142
Training loss = 0.01849480965839965
step = 12, Training Accuracy: 0.7410714285714286
Training loss = 0.01819762921226876
step = 13, Training Accuracy: 0.7460714285714286
Training loss = 0.018471448123455046
step = 14, Training Accuracy: 0.7417857142857143
Training loss = 0.0182602833211422
step = 15, Training Accuracy: 0.7439285714285714
Training loss = 0.018326144676123347
step = 16, Training Accuracy: 0.7371428571428571
Training loss = 0.018260460756719114
step = 17, Training Accuracy: 0.7389285714285714
Training loss = 0.018267586917749472
step = 18, Training Accuracy: 0.7430357142857142
Training loss = 0.018572187375809464
step = 19, Training Accuracy: 0.7439285714285714
Validation Accuracy: 0.775
parameter = [0.39419133458728045, 0.6081087767819736, 0.4063747572852278, 0.7048587456433839, 0.20996912190738223, 0.7782700468457893]
Training loss = 0.01831236011747803
step = 0, Training Accuracy: 0.7475
Validation Accuracy: 0.7775
Training loss = 0.018320868499577047
step = 1, Training Accuracy: 0.7433928571428572
Training loss = 0.018167822935751508
step = 2, Training Accuracy: 0.7542857142857143
Training loss = 0.018068051348839487
step = 3, Training Accuracy: 0.745
Training loss = 0.018124246006565436
step = 4, Training Accuracy: 0.745
Training loss = 0.018263009626950537
step = 5, Training Accuracy: 0.7423214285714286
Training loss = 0.01821041218404259
step = 6, Training Accuracy: 0.7471428571428571
Training loss = 0.018401660296533787
step = 7, Training Accuracy: 0.74
Training loss = 0.01807543824825968
step = 8, Training Accuracy: 0.7475
Training loss = 0.018388820593910556
step = 9, Training Accuracy: 0.74
Training loss = 0.018346344072903906
step = 10, Training Accuracy: 0.7382142857142857
Training loss = 0.01862065671810082
step = 11, Training Accuracy: 0.7407142857142858
Training loss = 0.01823108159005642
step = 12, Training Accuracy: 0.7533928571428572
Training loss = 0.01823674273278032
step = 13, Training Accuracy: 0.7371428571428571
Training loss = 0.018130197237644877
step = 14, Training Accuracy: 0.7469642857142857
Training loss = 0.018247156244303498
step = 15, Training Accuracy: 0.7426785714285714
Training loss = 0.01800997944282634
step = 16, Training Accuracy: 0.7423214285714286
Training loss = 0.01871449662638562
step = 17, Training Accuracy: 0.7342857142857143
Training loss = 0.01829404054475682
step = 18, Training Accuracy: 0.7391071428571429
Training loss = 0.0184842713283641
step = 19, Training Accuracy: 0.7351785714285715
Validation Accuracy: 0.7775
parameter = [0.3940385632025366, 0.6059926668487424, 0.4523006897250339, 0.7063177613902655, 0.20881477334901963, 0.7474727572754575]
Training loss = 0.01858010532068355
step = 0, Training Accuracy: 0.7323214285714286
Validation Accuracy: 0.7775
Training loss = 0.018393020310572217
step = 1, Training Accuracy: 0.7407142857142858
Training loss = 0.018315167895385197
step = 2, Training Accuracy: 0.745
Training loss = 0.018184152256165233
step = 3, Training Accuracy: 0.7423214285714286
Training loss = 0.018463099391332696
step = 4, Training Accuracy: 0.73875
Training loss = 0.018135121230568204
step = 5, Training Accuracy: 0.7421428571428571
Training loss = 0.018543833760278565
step = 6, Training Accuracy: 0.7403571428571428
Training loss = 0.01833677425980568
step = 7, Training Accuracy: 0.7430357142857142
Training loss = 0.018317566114876952
step = 8, Training Accuracy: 0.74375
Training loss = 0.01849502667784691
step = 9, Training Accuracy: 0.73625
Training loss = 0.01862658378268991
step = 10, Training Accuracy: 0.7367857142857143
Training loss = 0.0187591144282903
step = 11, Training Accuracy: 0.7408928571428571
Training loss = 0.018215636595019274
step = 12, Training Accuracy: 0.74625
Training loss = 0.018107338712683746
step = 13, Training Accuracy: 0.7446428571428572
Training loss = 0.018478843734732697
step = 14, Training Accuracy: 0.7416071428571429
Training loss = 0.018302542152149336
step = 15, Training Accuracy: 0.7442857142857143
Training loss = 0.01848793055330004
step = 16, Training Accuracy: 0.735
Training loss = 0.01822980074478047
step = 17, Training Accuracy: 0.7466071428571428
Training loss = 0.01843462260706084
step = 18, Training Accuracy: 0.7432142857142857
Training loss = 0.018288234610642707
step = 19, Training Accuracy: 0.7419642857142857
Validation Accuracy: 0.77375
parameter = [0.39395419182457875, 0.5541251478326045, 0.39670835779940133, 0.7047767039852965, 0.2079783751333723, 0.7418494593815447]
Training loss = 0.01863631663577897
step = 0, Training Accuracy: 0.7339285714285714
Validation Accuracy: 0.785
Training loss = 0.01816829891609294
step = 1, Training Accuracy: 0.7494642857142857
Training loss = 0.018599778892738478
step = 2, Training Accuracy: 0.7341071428571428
Training loss = 0.018324952380997794
step = 3, Training Accuracy: 0.7435714285714285
Training loss = 0.018255088685878686
step = 4, Training Accuracy: 0.7389285714285714
Training loss = 0.018007005756454808
step = 5, Training Accuracy: 0.7373214285714286
Training loss = 0.018109579932476794
step = 6, Training Accuracy: 0.7444642857142857
Training loss = 0.018473471722432546
step = 7, Training Accuracy: 0.7392857142857143
Training loss = 0.018505645985049862
step = 8, Training Accuracy: 0.73875
Training loss = 0.018301802607519287
step = 9, Training Accuracy: 0.7403571428571428
Training loss = 0.018624490004565033
step = 10, Training Accuracy: 0.73125
Training loss = 0.018484303041228226
step = 11, Training Accuracy: 0.7426785714285714
Training loss = 0.018055163153580255
step = 12, Training Accuracy: 0.7426785714285714
Training loss = 0.018706158282501356
step = 13, Training Accuracy: 0.7339285714285714
Training loss = 0.018057039300245898
step = 14, Training Accuracy: 0.7475
Training loss = 0.018297130454863822
step = 15, Training Accuracy: 0.7414285714285714
Training loss = 0.018392606686268534
step = 16, Training Accuracy: 0.7408928571428571
Training loss = 0.0181396867494498
step = 17, Training Accuracy: 0.7432142857142857
Training loss = 0.018332650501813208
step = 18, Training Accuracy: 0.7389285714285714
Training loss = 0.018351126520761422
step = 19, Training Accuracy: 0.7464285714285714
Validation Accuracy: 0.7775
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018443296530417035
step = 0, Training Accuracy: 0.7407142857142858
Validation Accuracy: 0.78125
Training loss = 0.01830787220703704
step = 1, Training Accuracy: 0.7457142857142857
Training loss = 0.018362466879189014
step = 2, Training Accuracy: 0.7410714285714286
Training loss = 0.018481152440820423
step = 3, Training Accuracy: 0.7341071428571428
Training loss = 0.01846486838268382
step = 4, Training Accuracy: 0.7392857142857143
Training loss = 0.01811800708728177
step = 5, Training Accuracy: 0.7478571428571429
Training loss = 0.018474252644394125
step = 6, Training Accuracy: 0.7373214285714286
Training loss = 0.018356685196714743
step = 7, Training Accuracy: 0.7432142857142857
Training loss = 0.0183584150192993
step = 8, Training Accuracy: 0.7403571428571428
Training loss = 0.018264623023569584
step = 9, Training Accuracy: 0.7476785714285714
Training loss = 0.018283277797911848
step = 10, Training Accuracy: 0.7435714285714285
Training loss = 0.018521806874445507
step = 11, Training Accuracy: 0.7444642857142857
Training loss = 0.018440139900360787
step = 12, Training Accuracy: 0.7344642857142857
Training loss = 0.01841373763446297
step = 13, Training Accuracy: 0.74875
Training loss = 0.01855901273765734
step = 14, Training Accuracy: 0.7341071428571428
Training loss = 0.018594493445541177
step = 15, Training Accuracy: 0.7385714285714285
Training loss = 0.018445806529905115
step = 16, Training Accuracy: 0.7373214285714286
Training loss = 0.018688336083931584
step = 17, Training Accuracy: 0.7369642857142857
Training loss = 0.01845081831727709
step = 18, Training Accuracy: 0.74125
Training loss = 0.018196327180734703
step = 19, Training Accuracy: 0.7396428571428572
Validation Accuracy: 0.78
6  	8     	0.780625	0.001875   	0.77875	0.78375
parameter = [0.39411355242350776, 0.5851829677323547, 0.4833110095988248, 0.7026522757465448, 0.20887306156826546, 0.7192906429662449]
Training loss = 0.018244989801730428
step = 0, Training Accuracy: 0.7433928571428572
Validation Accuracy: 0.77875
Training loss = 0.018081247386123454
step = 1, Training Accuracy: 0.7475
Training loss = 0.018439859258277076
step = 2, Training Accuracy: 0.7383928571428572
Training loss = 0.018217409274407795
step = 3, Training Accuracy: 0.7432142857142857
Training loss = 0.018390914318817002
step = 4, Training Accuracy: 0.7392857142857143
Training loss = 0.018340583172227656
step = 5, Training Accuracy: 0.7398214285714285
Training loss = 0.01825737169810704
step = 6, Training Accuracy: 0.7414285714285714
Training loss = 0.0183632777897375
step = 7, Training Accuracy: 0.7428571428571429
Training loss = 0.01831635096775634
step = 8, Training Accuracy: 0.7417857142857143
Training loss = 0.018363114606056895
step = 9, Training Accuracy: 0.7419642857142857
Training loss = 0.018662508028958524
step = 10, Training Accuracy: 0.7414285714285714
Training loss = 0.018371849320828915
step = 11, Training Accuracy: 0.7405357142857143
Training loss = 0.018461747536701816
step = 12, Training Accuracy: 0.7378571428571429
Training loss = 0.01830349194684199
step = 13, Training Accuracy: 0.7371428571428571
Training loss = 0.018165717790169376
step = 14, Training Accuracy: 0.7482142857142857
Training loss = 0.018245784837220395
step = 15, Training Accuracy: 0.74
Training loss = 0.018465031651513916
step = 16, Training Accuracy: 0.7423214285714286
Training loss = 0.01862737475229161
step = 17, Training Accuracy: 0.7410714285714286
Training loss = 0.01841962584427425
step = 18, Training Accuracy: 0.7419642857142857
Training loss = 0.018004077236567224
step = 19, Training Accuracy: 0.7451785714285715
Validation Accuracy: 0.78125
parameter = [0.39402584461477663, 0.6052004976825085, 0.5039207908721575, 0.7056953081038582, 0.20822558293059926, 0.780826295441843]
Training loss = 0.018316127216177328
step = 0, Training Accuracy: 0.7407142857142858
Validation Accuracy: 0.77
Training loss = 0.01825350565037557
step = 1, Training Accuracy: 0.7517857142857143
Training loss = 0.018307238966226578
step = 2, Training Accuracy: 0.7475
Training loss = 0.01855040776410273
step = 3, Training Accuracy: 0.7417857142857143
Training loss = 0.018287826457193918
step = 4, Training Accuracy: 0.7416071428571429
Training loss = 0.018473479806312493
step = 5, Training Accuracy: 0.7394642857142857
Training loss = 0.01869997248585735
step = 6, Training Accuracy: 0.7464285714285714
Training loss = 0.017964151884828295
step = 7, Training Accuracy: 0.75125
Training loss = 0.018582145571708678
step = 8, Training Accuracy: 0.7325
Training loss = 0.018377295051302228
step = 9, Training Accuracy: 0.7439285714285714
Training loss = 0.018418752902320455
step = 10, Training Accuracy: 0.7403571428571428
Training loss = 0.01835015564092568
step = 11, Training Accuracy: 0.7403571428571428
Training loss = 0.01827638605343444
step = 12, Training Accuracy: 0.7448214285714285
Training loss = 0.018483153102653367
step = 13, Training Accuracy: 0.7380357142857142
Training loss = 0.01834243640303612
step = 14, Training Accuracy: 0.7366071428571429
Training loss = 0.01831577479839325
step = 15, Training Accuracy: 0.7410714285714286
Training loss = 0.01804335469113929
step = 16, Training Accuracy: 0.7457142857142857
Training loss = 0.018228918388485907
step = 17, Training Accuracy: 0.7416071428571429
Training loss = 0.01817558470581259
step = 18, Training Accuracy: 0.7478571428571429
Training loss = 0.018181321062147617
step = 19, Training Accuracy: 0.7483928571428572
Validation Accuracy: 0.78375
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018385841979512145
step = 0, Training Accuracy: 0.7455357142857143
Validation Accuracy: 0.78
Training loss = 0.01869952403541122
step = 1, Training Accuracy: 0.74
Training loss = 0.01855438613465854
step = 2, Training Accuracy: 0.7376785714285714
Training loss = 0.018405300921627454
step = 3, Training Accuracy: 0.7419642857142857
Training loss = 0.018396775397871223
step = 4, Training Accuracy: 0.7432142857142857
Training loss = 0.01824155123106071
step = 5, Training Accuracy: 0.7444642857142857
Training loss = 0.01817149759403297
step = 6, Training Accuracy: 0.7419642857142857
Training loss = 0.018445024570184095
step = 7, Training Accuracy: 0.7369642857142857
Training loss = 0.018474080706281322
step = 8, Training Accuracy: 0.7435714285714285
Training loss = 0.018630278882171426
step = 9, Training Accuracy: 0.7396428571428572
Training loss = 0.018374355132026333
step = 10, Training Accuracy: 0.7428571428571429
Training loss = 0.01789322009044034
step = 11, Training Accuracy: 0.7510714285714286
Training loss = 0.018764082682984216
step = 12, Training Accuracy: 0.7376785714285714
Training loss = 0.018328735705997263
step = 13, Training Accuracy: 0.7376785714285714
Training loss = 0.018654615160609993
step = 14, Training Accuracy: 0.7369642857142857
Training loss = 0.01821547804666417
step = 15, Training Accuracy: 0.7455357142857143
Training loss = 0.018150862359574863
step = 16, Training Accuracy: 0.7448214285714285
Training loss = 0.01848566451242992
step = 17, Training Accuracy: 0.745
Training loss = 0.018332900548619885
step = 18, Training Accuracy: 0.7476785714285714
Training loss = 0.018315944921757494
step = 19, Training Accuracy: 0.7389285714285714
Validation Accuracy: 0.77875
parameter = [0.3939929848243343, 0.6155070443065995, 0.4939523193371955, 0.7032232677028927, 0.20915786640254608, 0.7614465083324296]
Training loss = 0.0182462939247489
step = 0, Training Accuracy: 0.7428571428571429
Validation Accuracy: 0.77875
Training loss = 0.01833907034780298
step = 1, Training Accuracy: 0.7433928571428572
Training loss = 0.01879916801516499
step = 2, Training Accuracy: 0.7323214285714286
Training loss = 0.017869240886398723
step = 3, Training Accuracy: 0.7546428571428572
Training loss = 0.018367997759154864
step = 4, Training Accuracy: 0.74625
Training loss = 0.018289289490452835
step = 5, Training Accuracy: 0.7385714285714285
Training loss = 0.018030061769698347
step = 6, Training Accuracy: 0.7473214285714286
Training loss = 0.018424530204917702
step = 7, Training Accuracy: 0.7432142857142857
Training loss = 0.01851371503302029
step = 8, Training Accuracy: 0.7341071428571428
Training loss = 0.01801162088023765
step = 9, Training Accuracy: 0.7498214285714285
Training loss = 0.01828106370887586
step = 10, Training Accuracy: 0.7467857142857143
Training loss = 0.018408664507525308
step = 11, Training Accuracy: 0.74125
Training loss = 0.01814414177622114
step = 12, Training Accuracy: 0.74625
Training loss = 0.018518603896456105
step = 13, Training Accuracy: 0.7392857142857143
Training loss = 0.01809814967215061
step = 14, Training Accuracy: 0.7369642857142857
Training loss = 0.018321443886629173
step = 15, Training Accuracy: 0.7444642857142857
Training loss = 0.018413581640592643
step = 16, Training Accuracy: 0.7385714285714285
Training loss = 0.017991388786051953
step = 17, Training Accuracy: 0.7516071428571428
Training loss = 0.018238040591989246
step = 18, Training Accuracy: 0.7426785714285714
Training loss = 0.018450573374118123
step = 19, Training Accuracy: 0.7367857142857143
Validation Accuracy: 0.775
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018697879213307584
step = 0, Training Accuracy: 0.7389285714285714
Validation Accuracy: 0.7775
Training loss = 0.01859043224049466
step = 1, Training Accuracy: 0.7376785714285714
Training loss = 0.018482835750494683
step = 2, Training Accuracy: 0.7417857142857143
Training loss = 0.01811949171658073
step = 3, Training Accuracy: 0.7491071428571429
Training loss = 0.018051425202616624
step = 4, Training Accuracy: 0.7489285714285714
Training loss = 0.018374571832162994
step = 5, Training Accuracy: 0.7433928571428572
Training loss = 0.01844602509800877
step = 6, Training Accuracy: 0.7392857142857143
Training loss = 0.018356164395809175
step = 7, Training Accuracy: 0.7401785714285715
Training loss = 0.018059553424162524
step = 8, Training Accuracy: 0.7442857142857143
Training loss = 0.01851490011172635
step = 9, Training Accuracy: 0.7448214285714285
Training loss = 0.018158528842031955
step = 10, Training Accuracy: 0.7475
Training loss = 0.018134150707295963
step = 11, Training Accuracy: 0.7510714285714286
Training loss = 0.018488616671945367
step = 12, Training Accuracy: 0.7408928571428571
Training loss = 0.0181981186196208
step = 13, Training Accuracy: 0.7482142857142857
Training loss = 0.01837099999721561
step = 14, Training Accuracy: 0.7448214285714285
Training loss = 0.01847264691655125
step = 15, Training Accuracy: 0.7403571428571428
Training loss = 0.018299857936799526
step = 16, Training Accuracy: 0.7428571428571429
Training loss = 0.018303049568619046
step = 17, Training Accuracy: 0.7480357142857142
Training loss = 0.01828194602259568
step = 18, Training Accuracy: 0.74625
Training loss = 0.018457592113741806
step = 19, Training Accuracy: 0.7414285714285714
Validation Accuracy: 0.77625
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.01819471427904708
step = 0, Training Accuracy: 0.745
Validation Accuracy: 0.7825
Training loss = 0.018222048820129462
step = 1, Training Accuracy: 0.7457142857142857
Training loss = 0.0183931797051004
step = 2, Training Accuracy: 0.7369642857142857
Training loss = 0.018629407393080846
step = 3, Training Accuracy: 0.7396428571428572
Training loss = 0.018108962699770928
step = 4, Training Accuracy: 0.7467857142857143
Training loss = 0.0184783678821155
step = 5, Training Accuracy: 0.7339285714285714
Training loss = 0.018310851023665496
step = 6, Training Accuracy: 0.74
Training loss = 0.01814119341117995
step = 7, Training Accuracy: 0.7426785714285714
Training loss = 0.018108605375247343
step = 8, Training Accuracy: 0.7441071428571429
Training loss = 0.018274037103567804
step = 9, Training Accuracy: 0.7451785714285715
Training loss = 0.018183324778718608
step = 10, Training Accuracy: 0.7425
Training loss = 0.018517110954437938
step = 11, Training Accuracy: 0.7376785714285714
Training loss = 0.01831308662891388
step = 12, Training Accuracy: 0.74
Training loss = 0.01844712466533695
step = 13, Training Accuracy: 0.7455357142857143
Training loss = 0.01853303337203605
step = 14, Training Accuracy: 0.7401785714285715
Training loss = 0.01820834425411054
step = 15, Training Accuracy: 0.7446428571428572
Training loss = 0.01859976090490818
step = 16, Training Accuracy: 0.7373214285714286
Training loss = 0.01858140466468675
step = 17, Training Accuracy: 0.7369642857142857
Training loss = 0.01789177810507161
step = 18, Training Accuracy: 0.7492857142857143
Training loss = 0.018629119444106307
step = 19, Training Accuracy: 0.7369642857142857
Validation Accuracy: 0.78
parameter = [0.3941212624452153, 0.5626277184129718, 0.39359376249633454, 0.7034106746861309, 0.2098264695331431, 0.7357998175423562]
Training loss = 0.018155368843248913
step = 0, Training Accuracy: 0.7458928571428571
Validation Accuracy: 0.78125
Training loss = 0.018407233926866735
step = 1, Training Accuracy: 0.7414285714285714
Training loss = 0.01827068061700889
step = 2, Training Accuracy: 0.7482142857142857
Training loss = 0.018254396143768514
step = 3, Training Accuracy: 0.7378571428571429
Training loss = 0.017980742433241437
step = 4, Training Accuracy: 0.7482142857142857
Training loss = 0.018574572296014853
step = 5, Training Accuracy: 0.7392857142857143
Training loss = 0.018317460774310998
step = 6, Training Accuracy: 0.7423214285714286
Training loss = 0.01880987619182893
step = 7, Training Accuracy: 0.7385714285714285
Training loss = 0.01806179222783872
step = 8, Training Accuracy: 0.7391071428571429
Training loss = 0.018307157132242406
step = 9, Training Accuracy: 0.7392857142857143
Training loss = 0.018145274615713528
step = 10, Training Accuracy: 0.7441071428571429
Training loss = 0.01813281190714666
step = 11, Training Accuracy: 0.7385714285714285
Training loss = 0.01827232141047716
step = 12, Training Accuracy: 0.7433928571428572
Training loss = 0.018285643346607684
step = 13, Training Accuracy: 0.7432142857142857
Training loss = 0.018220855609646865
step = 14, Training Accuracy: 0.7419642857142857
Training loss = 0.018430051627968038
step = 15, Training Accuracy: 0.7428571428571429
Training loss = 0.018241430526333195
step = 16, Training Accuracy: 0.7453571428571428
Training loss = 0.018268774332744736
step = 17, Training Accuracy: 0.7417857142857143
Training loss = 0.01802930738776922
step = 18, Training Accuracy: 0.7410714285714286
Training loss = 0.018462079215262616
step = 19, Training Accuracy: 0.74
Validation Accuracy: 0.78125
parameter = [0.3939946979972284, 0.5818259616865258, 0.46663369663458243, 0.7030707238040167, 0.20792033080260017, 0.7741788779116495]
Training loss = 0.017668071085853235
step = 0, Training Accuracy: 0.7494642857142857
Validation Accuracy: 0.775
Training loss = 0.01836857794118779
step = 1, Training Accuracy: 0.74
Training loss = 0.01821444378367492
step = 2, Training Accuracy: 0.7478571428571429
Training loss = 0.018733927336122308
step = 3, Training Accuracy: 0.7383928571428572
Training loss = 0.01828741348747696
step = 4, Training Accuracy: 0.74125
Training loss = 0.01844476339008127
step = 5, Training Accuracy: 0.7373214285714286
Training loss = 0.018717770422143596
step = 6, Training Accuracy: 0.7332142857142857
Training loss = 0.018090178540774753
step = 7, Training Accuracy: 0.7485714285714286
Training loss = 0.018795878158083985
step = 8, Training Accuracy: 0.7310714285714286
Training loss = 0.018223717106240135
step = 9, Training Accuracy: 0.7469642857142857
Training loss = 0.018455703487353665
step = 10, Training Accuracy: 0.7432142857142857
Training loss = 0.01871107422879764
step = 11, Training Accuracy: 0.7414285714285714
Training loss = 0.018618062085339
step = 12, Training Accuracy: 0.74
Training loss = 0.018014507389494352
step = 13, Training Accuracy: 0.75
Training loss = 0.018407405109277793
step = 14, Training Accuracy: 0.7423214285714286
Training loss = 0.01854625195264816
step = 15, Training Accuracy: 0.7358928571428571
Training loss = 0.0179404081510646
step = 16, Training Accuracy: 0.7460714285714286
Training loss = 0.018151864505239895
step = 17, Training Accuracy: 0.7480357142857142
Training loss = 0.018410946503281594
step = 18, Training Accuracy: 0.7485714285714286
Training loss = 0.018413580554936613
step = 19, Training Accuracy: 0.7326785714285714
Validation Accuracy: 0.77625
7  	8     	0.781562	0.00136216 	0.78   	0.78375
parameter = [0.3941212624452153, 0.5626277184129718, 0.39359376249633454, 0.7034106746861309, 0.2098264695331431, 0.7357998175423562]
Training loss = 0.018839313675250324
step = 0, Training Accuracy: 0.7325
Validation Accuracy: 0.775
Training loss = 0.018370174595287867
step = 1, Training Accuracy: 0.7394642857142857
Training loss = 0.018132026488227503
step = 2, Training Accuracy: 0.7425
Training loss = 0.01815213210348572
step = 3, Training Accuracy: 0.7453571428571428
Training loss = 0.018373683621840817
step = 4, Training Accuracy: 0.7426785714285714
Training loss = 0.018550935643059867
step = 5, Training Accuracy: 0.7417857142857143
Training loss = 0.018280191655669895
step = 6, Training Accuracy: 0.7408928571428571
Training loss = 0.018498246829424584
step = 7, Training Accuracy: 0.7394642857142857
Training loss = 0.018354209501828467
step = 8, Training Accuracy: 0.7421428571428571
Training loss = 0.018412222707910198
step = 9, Training Accuracy: 0.7385714285714285
Training loss = 0.018229946047067642
step = 10, Training Accuracy: 0.7482142857142857
Training loss = 0.018151899091899393
step = 11, Training Accuracy: 0.7503571428571428
Training loss = 0.018416543374104158
step = 12, Training Accuracy: 0.7396428571428572
Training loss = 0.018117488622665406
step = 13, Training Accuracy: 0.7476785714285714
Training loss = 0.018274401564683233
step = 14, Training Accuracy: 0.7396428571428572
Training loss = 0.01863438977726868
step = 15, Training Accuracy: 0.7369642857142857
Training loss = 0.01849093502121312
step = 16, Training Accuracy: 0.735
Training loss = 0.01838270070829562
step = 17, Training Accuracy: 0.7475
Training loss = 0.01836288728884288
step = 18, Training Accuracy: 0.7423214285714286
Training loss = 0.01851144079118967
step = 19, Training Accuracy: 0.74375
Validation Accuracy: 0.77625
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018527408907456057
step = 0, Training Accuracy: 0.7417857142857143
Validation Accuracy: 0.7775
Training loss = 0.01801237994006702
step = 1, Training Accuracy: 0.7460714285714286
Training loss = 0.01830869322908776
step = 2, Training Accuracy: 0.7473214285714286
Training loss = 0.018301312199660708
step = 3, Training Accuracy: 0.7398214285714285
Training loss = 0.018379771379487854
step = 4, Training Accuracy: 0.74125
Training loss = 0.018001119829714297
step = 5, Training Accuracy: 0.7444642857142857
Training loss = 0.018169478875185763
step = 6, Training Accuracy: 0.7442857142857143
Training loss = 0.018305116673665387
step = 7, Training Accuracy: 0.7491071428571429
Training loss = 0.018224047337259564
step = 8, Training Accuracy: 0.7376785714285714
Training loss = 0.01870942741100277
step = 9, Training Accuracy: 0.7364285714285714
Training loss = 0.01828142334307943
step = 10, Training Accuracy: 0.7467857142857143
Training loss = 0.01820961518479245
step = 11, Training Accuracy: 0.7475
Training loss = 0.01842441748827696
step = 12, Training Accuracy: 0.7366071428571429
Training loss = 0.018370537560965332
step = 13, Training Accuracy: 0.7414285714285714
Training loss = 0.01830360496150596
step = 14, Training Accuracy: 0.7455357142857143
Training loss = 0.01835647011441844
step = 15, Training Accuracy: 0.7451785714285715
Training loss = 0.018603731840848924
step = 16, Training Accuracy: 0.7405357142857143
Training loss = 0.018299783335200378
step = 17, Training Accuracy: 0.7410714285714286
Training loss = 0.018292958236166407
step = 18, Training Accuracy: 0.7476785714285714
Training loss = 0.0183404109201261
step = 19, Training Accuracy: 0.7396428571428572
Validation Accuracy: 0.78125
parameter = [0.39407406352477575, 0.6018847679570261, 0.48585165122113094, 0.7044671986660127, 0.20841533381226246, 0.7803028044095217]
Training loss = 0.01830700522022588
step = 0, Training Accuracy: 0.7335714285714285
Validation Accuracy: 0.77875
Training loss = 0.01831450148352555
step = 1, Training Accuracy: 0.7453571428571428
Training loss = 0.018003673776984214
step = 2, Training Accuracy: 0.7489285714285714
Training loss = 0.018437574271644865
step = 3, Training Accuracy: 0.7403571428571428
Training loss = 0.018538378908165863
step = 4, Training Accuracy: 0.7351785714285715
Training loss = 0.018154700504881997
step = 5, Training Accuracy: 0.7460714285714286
Training loss = 0.01828251060630594
step = 6, Training Accuracy: 0.74875
Training loss = 0.01820587945835931
step = 7, Training Accuracy: 0.7460714285714286
Training loss = 0.018133186304143498
step = 8, Training Accuracy: 0.7451785714285715
Training loss = 0.01844403582492045
step = 9, Training Accuracy: 0.735
Training loss = 0.018510419901992592
step = 10, Training Accuracy: 0.7408928571428571
Training loss = 0.018101455457508565
step = 11, Training Accuracy: 0.7475
Training loss = 0.018155215180345945
step = 12, Training Accuracy: 0.7425
Training loss = 0.017981026640960147
step = 13, Training Accuracy: 0.7448214285714285
Training loss = 0.01802057095404182
step = 14, Training Accuracy: 0.7460714285714286
Training loss = 0.018501519161675656
step = 15, Training Accuracy: 0.7435714285714285
Training loss = 0.018201954407351357
step = 16, Training Accuracy: 0.7410714285714286
Training loss = 0.018447795105831964
step = 17, Training Accuracy: 0.7392857142857143
Training loss = 0.018683917580970696
step = 18, Training Accuracy: 0.7394642857142857
Training loss = 0.01807844252990825
step = 19, Training Accuracy: 0.7430357142857142
Validation Accuracy: 0.77875
parameter = [0.3941212624452153, 0.5626277184129718, 0.39359376249633454, 0.7034106746861309, 0.2098264695331431, 0.7357998175423562]
Training loss = 0.01848655020552022
step = 0, Training Accuracy: 0.7330357142857142
Validation Accuracy: 0.78125
Training loss = 0.01840695308255298
step = 1, Training Accuracy: 0.73375
Training loss = 0.018567321321793964
step = 2, Training Accuracy: 0.7425
Training loss = 0.01818094989550965
step = 3, Training Accuracy: 0.7416071428571429
Training loss = 0.01805227753307138
step = 4, Training Accuracy: 0.7442857142857143
Training loss = 0.01843311286930527
step = 5, Training Accuracy: 0.7396428571428572
Training loss = 0.018313617695655142
step = 6, Training Accuracy: 0.7494642857142857
Training loss = 0.01833375407648938
step = 7, Training Accuracy: 0.7457142857142857
Training loss = 0.018446383279349124
step = 8, Training Accuracy: 0.73875
Training loss = 0.01850242292774575
step = 9, Training Accuracy: 0.7403571428571428
Training loss = 0.018390873899417265
step = 10, Training Accuracy: 0.7442857142857143
Training loss = 0.018368884862533637
step = 11, Training Accuracy: 0.7383928571428572
Training loss = 0.01865622506609985
step = 12, Training Accuracy: 0.74
Training loss = 0.018506968723876135
step = 13, Training Accuracy: 0.7467857142857143
Training loss = 0.018353821842798164
step = 14, Training Accuracy: 0.7441071428571429
Training loss = 0.01830016896660839
step = 15, Training Accuracy: 0.7491071428571429
Training loss = 0.018109247993145672
step = 16, Training Accuracy: 0.7501785714285715
Training loss = 0.018240157105028628
step = 17, Training Accuracy: 0.7423214285714286
Training loss = 0.018219414007450852
step = 18, Training Accuracy: 0.7439285714285714
Training loss = 0.018300801095153606
step = 19, Training Accuracy: 0.7403571428571428
Validation Accuracy: 0.775
parameter = [0.39402584461477663, 0.6052004976825085, 0.5039207908721575, 0.7056953081038582, 0.20822558293059926, 0.780826295441843]
Training loss = 0.018359214977494306
step = 0, Training Accuracy: 0.7407142857142858
Validation Accuracy: 0.77875
Training loss = 0.018055719823709557
step = 1, Training Accuracy: 0.7451785714285715
Training loss = 0.018336889067930833
step = 2, Training Accuracy: 0.7473214285714286
Training loss = 0.01837715698140008
step = 3, Training Accuracy: 0.7448214285714285
Training loss = 0.018180411936981337
step = 4, Training Accuracy: 0.7433928571428572
Training loss = 0.01810044123658112
step = 5, Training Accuracy: 0.7476785714285714
Training loss = 0.01831688118832452
step = 6, Training Accuracy: 0.75
Training loss = 0.018265908238078866
step = 7, Training Accuracy: 0.7419642857142857
Training loss = 0.018339919655450752
step = 8, Training Accuracy: 0.7401785714285715
Training loss = 0.01843602141631501
step = 9, Training Accuracy: 0.7398214285714285
Training loss = 0.018352958469518593
step = 10, Training Accuracy: 0.7423214285714286
Training loss = 0.01823382233402559
step = 11, Training Accuracy: 0.7417857142857143
Training loss = 0.01808273617710386
step = 12, Training Accuracy: 0.7469642857142857
Training loss = 0.018155441768467426
step = 13, Training Accuracy: 0.7460714285714286
Training loss = 0.018445585535040925
step = 14, Training Accuracy: 0.7426785714285714
Training loss = 0.018322872583355222
step = 15, Training Accuracy: 0.7460714285714286
Training loss = 0.01815563463206802
step = 16, Training Accuracy: 0.7444642857142857
Training loss = 0.018206610046327114
step = 17, Training Accuracy: 0.74625
Training loss = 0.018185055724212102
step = 18, Training Accuracy: 0.7375
Training loss = 0.01845819940524442
step = 19, Training Accuracy: 0.7369642857142857
Validation Accuracy: 0.77625
parameter = [0.6907277014682273, 0.5626277184129718, 0.39359376249633454, 0.7034106746861309, 0.2098264695331431, 0.7357998175423562]
Training loss = 0.018364068099430628
step = 0, Training Accuracy: 0.7430357142857142
Validation Accuracy: 0.77875
Training loss = 0.01847334289657218
step = 1, Training Accuracy: 0.74125
Training loss = 0.018331246125910965
step = 2, Training Accuracy: 0.7432142857142857
Training loss = 0.018158766201564244
step = 3, Training Accuracy: 0.7446428571428572
Training loss = 0.018364376963249274
step = 4, Training Accuracy: 0.7344642857142857
Training loss = 0.018498834669589997
step = 5, Training Accuracy: 0.73875
Training loss = 0.018441482910088132
step = 6, Training Accuracy: 0.74125
Training loss = 0.018368906368102345
step = 7, Training Accuracy: 0.7407142857142858
Training loss = 0.01859161415802581
step = 8, Training Accuracy: 0.7407142857142858
Training loss = 0.018297420867851803
step = 9, Training Accuracy: 0.7421428571428571
Training loss = 0.01798868503421545
step = 10, Training Accuracy: 0.7451785714285715
Training loss = 0.018270569177610534
step = 11, Training Accuracy: 0.74375
Training loss = 0.01823631240321057
step = 12, Training Accuracy: 0.745
Training loss = 0.018011734331292766
step = 13, Training Accuracy: 0.7507142857142857
Training loss = 0.01839065021170037
step = 14, Training Accuracy: 0.7389285714285714
Training loss = 0.01830774870301996
step = 15, Training Accuracy: 0.7321428571428571
Training loss = 0.01875539728041206
step = 16, Training Accuracy: 0.7298214285714286
Training loss = 0.018572747292263166
step = 17, Training Accuracy: 0.735
Training loss = 0.018295376960720335
step = 18, Training Accuracy: 0.7460714285714286
Training loss = 0.018299280389079027
step = 19, Training Accuracy: 0.7371428571428571
Validation Accuracy: 0.77625
8  	6     	0.78125 	0.00176777 	0.77875	0.78375
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.01835482747959239
step = 0, Training Accuracy: 0.7485714285714286
Validation Accuracy: 0.77875
Training loss = 0.018034906296857767
step = 1, Training Accuracy: 0.7457142857142857
Training loss = 0.018422886691987515
step = 2, Training Accuracy: 0.7416071428571429
Training loss = 0.017931061152900968
step = 3, Training Accuracy: 0.7485714285714286
Training loss = 0.01828195793820279
step = 4, Training Accuracy: 0.7428571428571429
Training loss = 0.01840144977505718
step = 5, Training Accuracy: 0.7441071428571429
Training loss = 0.018133122362196444
step = 6, Training Accuracy: 0.7467857142857143
Training loss = 0.018865169579429287
step = 7, Training Accuracy: 0.7364285714285714
Training loss = 0.018580140136182307
step = 8, Training Accuracy: 0.7369642857142857
Training loss = 0.018615690443132604
step = 9, Training Accuracy: 0.74
Training loss = 0.018393190284924848
step = 10, Training Accuracy: 0.7441071428571429
Training loss = 0.018413623672510895
step = 11, Training Accuracy: 0.7430357142857142
Training loss = 0.018554641513952187
step = 12, Training Accuracy: 0.7385714285714285
Training loss = 0.018268663324415683
step = 13, Training Accuracy: 0.7425
Training loss = 0.018351006406758512
step = 14, Training Accuracy: 0.7442857142857143
Training loss = 0.018576573133468628
step = 15, Training Accuracy: 0.7407142857142858
Training loss = 0.018575526018227848
step = 16, Training Accuracy: 0.7405357142857143
Training loss = 0.01817975787711995
step = 17, Training Accuracy: 0.7416071428571429
Training loss = 0.018348542340099812
step = 18, Training Accuracy: 0.7455357142857143
Training loss = 0.018198127528386458
step = 19, Training Accuracy: 0.7435714285714285
Validation Accuracy: 0.78125
parameter = [0.39405726956491294, 0.6038573746519024, 0.43094420297376973, 0.7045794731355634, 0.2090537782906391, 0.7408476985560637]
Training loss = 0.018523575386830738
step = 0, Training Accuracy: 0.7410714285714286
Validation Accuracy: 0.78125
Training loss = 0.01852854052292449
step = 1, Training Accuracy: 0.7442857142857143
Training loss = 0.018446209143315042
step = 2, Training Accuracy: 0.7428571428571429
Training loss = 0.018377879613212175
step = 3, Training Accuracy: 0.7435714285714285
Training loss = 0.018386763371527195
step = 4, Training Accuracy: 0.7414285714285714
Training loss = 0.018843619424317563
step = 5, Training Accuracy: 0.7333928571428572
Training loss = 0.01804633451891797
step = 6, Training Accuracy: 0.7453571428571428
Training loss = 0.018449899431850228
step = 7, Training Accuracy: 0.7430357142857142
Training loss = 0.018428259103425913
step = 8, Training Accuracy: 0.7353571428571428
Training loss = 0.018554121990289007
step = 9, Training Accuracy: 0.7394642857142857
Training loss = 0.01847399272556816
step = 10, Training Accuracy: 0.7460714285714286
Training loss = 0.01847315700990813
step = 11, Training Accuracy: 0.7389285714285714
Training loss = 0.018049717939325742
step = 12, Training Accuracy: 0.7453571428571428
Training loss = 0.01824725016951561
step = 13, Training Accuracy: 0.7364285714285714
Training loss = 0.01836271899619273
step = 14, Training Accuracy: 0.735
Training loss = 0.01843077869287559
step = 15, Training Accuracy: 0.7414285714285714
Training loss = 0.01812045675303255
step = 16, Training Accuracy: 0.7389285714285714
Training loss = 0.018323254138231278
step = 17, Training Accuracy: 0.7419642857142857
Training loss = 0.018133451246789525
step = 18, Training Accuracy: 0.7471428571428571
Training loss = 0.018075113945773668
step = 19, Training Accuracy: 0.7430357142857142
Validation Accuracy: 0.77625
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.017992047050169536
step = 0, Training Accuracy: 0.7482142857142857
Validation Accuracy: 0.78125
Training loss = 0.018170036636292936
step = 1, Training Accuracy: 0.7423214285714286
Training loss = 0.01841114274093083
step = 2, Training Accuracy: 0.7401785714285715
Training loss = 0.01830597556063107
step = 3, Training Accuracy: 0.73875
Training loss = 0.018227197011666637
step = 4, Training Accuracy: 0.7478571428571429
Training loss = 0.018482005542942454
step = 5, Training Accuracy: 0.7426785714285714
Training loss = 0.0186452559701034
step = 6, Training Accuracy: 0.7401785714285715
Training loss = 0.01857828132276024
step = 7, Training Accuracy: 0.7394642857142857
Training loss = 0.018335209733673506
step = 8, Training Accuracy: 0.7425
Training loss = 0.018407342449894972
step = 9, Training Accuracy: 0.7382142857142857
Training loss = 0.0185240875557065
step = 10, Training Accuracy: 0.7417857142857143
Training loss = 0.018448341031159672
step = 11, Training Accuracy: 0.7376785714285714
Training loss = 0.01842123231185334
step = 12, Training Accuracy: 0.7364285714285714
Training loss = 0.018440719033990586
step = 13, Training Accuracy: 0.7453571428571428
Training loss = 0.018273399232753687
step = 14, Training Accuracy: 0.7394642857142857
Training loss = 0.01861581765115261
step = 15, Training Accuracy: 0.74375
Training loss = 0.01843611851866756
step = 16, Training Accuracy: 0.7394642857142857
Training loss = 0.018589970815394607
step = 17, Training Accuracy: 0.7358928571428571
Training loss = 0.018250117413699626
step = 18, Training Accuracy: 0.7403571428571428
Training loss = 0.01840514921184097
step = 19, Training Accuracy: 0.7421428571428571
Validation Accuracy: 0.77875
parameter = [0.394058483637227, 0.6029756389135509, 0.49682301361133474, 0.7048993221394064, 0.20836190636061863, 0.78033383401399]
Training loss = 0.018209058833973748
step = 0, Training Accuracy: 0.7416071428571429
Validation Accuracy: 0.78
Training loss = 0.018396963791123457
step = 1, Training Accuracy: 0.7373214285714286
Training loss = 0.018173033478004592
step = 2, Training Accuracy: 0.7426785714285714
Training loss = 0.018623056417065006
step = 3, Training Accuracy: 0.7460714285714286
Training loss = 0.01806843275470393
step = 4, Training Accuracy: 0.7407142857142858
Training loss = 0.018391692606466156
step = 5, Training Accuracy: 0.7458928571428571
Training loss = 0.018161588822092328
step = 6, Training Accuracy: 0.7453571428571428
Training loss = 0.018304055774850506
step = 7, Training Accuracy: 0.7453571428571428
Training loss = 0.018360866648810252
step = 8, Training Accuracy: 0.74125
Training loss = 0.0185994808322617
step = 9, Training Accuracy: 0.7373214285714286
Training loss = 0.018394474019961696
step = 10, Training Accuracy: 0.7419642857142857
Training loss = 0.018505301496812277
step = 11, Training Accuracy: 0.7403571428571428
Training loss = 0.018591764782156264
step = 12, Training Accuracy: 0.7382142857142857
Training loss = 0.01828113892780883
step = 13, Training Accuracy: 0.7453571428571428
Training loss = 0.01832304221178804
step = 14, Training Accuracy: 0.7432142857142857
Training loss = 0.018289493950349945
step = 15, Training Accuracy: 0.7416071428571429
Training loss = 0.018354396665734903
step = 16, Training Accuracy: 0.7442857142857143
Training loss = 0.018446621240249702
step = 17, Training Accuracy: 0.7408928571428571
Training loss = 0.018029658389942988
step = 18, Training Accuracy: 0.7444642857142857
Training loss = 0.018338104018143245
step = 19, Training Accuracy: 0.7469642857142857
Validation Accuracy: 0.77625
parameter = [0.39402584461477663, 0.6052004976825085, 0.5039207908721575, 0.7056953081038582, 0.20822558293059926, 0.780826295441843]
Training loss = 0.018642446611608778
step = 0, Training Accuracy: 0.7401785714285715
Validation Accuracy: 0.7775
Training loss = 0.018353154025971888
step = 1, Training Accuracy: 0.7446428571428572
Training loss = 0.01835873468113797
step = 2, Training Accuracy: 0.7453571428571428
Training loss = 0.018311715200543404
step = 3, Training Accuracy: 0.7428571428571429
Training loss = 0.01832447286695242
step = 4, Training Accuracy: 0.7376785714285714
Training loss = 0.018604326881468295
step = 5, Training Accuracy: 0.7389285714285714
Training loss = 0.018302561465118612
step = 6, Training Accuracy: 0.7391071428571429
Training loss = 0.01844200690409967
step = 7, Training Accuracy: 0.7430357142857142
Training loss = 0.018450530474739414
step = 8, Training Accuracy: 0.7348214285714286
Training loss = 0.018793418114738804
step = 9, Training Accuracy: 0.7367857142857143
Training loss = 0.01846407269260713
step = 10, Training Accuracy: 0.7425
Training loss = 0.018126582274479525
step = 11, Training Accuracy: 0.7442857142857143
Training loss = 0.01839158679225615
step = 12, Training Accuracy: 0.7389285714285714
Training loss = 0.018366631732455323
step = 13, Training Accuracy: 0.7421428571428571
Training loss = 0.01839557613645281
step = 14, Training Accuracy: 0.7396428571428572
Training loss = 0.01830825420894793
step = 15, Training Accuracy: 0.73875
Training loss = 0.018249920276658876
step = 16, Training Accuracy: 0.7448214285714285
Training loss = 0.018244077722941125
step = 17, Training Accuracy: 0.7398214285714285
Training loss = 0.018048731849661895
step = 18, Training Accuracy: 0.74375
Training loss = 0.018561907171138696
step = 19, Training Accuracy: 0.7375
Validation Accuracy: 0.78125
parameter = [0.394064151967233, 0.5738026668567864, 0.46132812910932214, 0.7026380182426927, 0.20925797261423046, 0.732157897702409]
Training loss = 0.018647194137530666
step = 0, Training Accuracy: 0.735
Validation Accuracy: 0.78125
Training loss = 0.018354870080947876
step = 1, Training Accuracy: 0.7441071428571429
Training loss = 0.018425391795379775
step = 2, Training Accuracy: 0.7416071428571429
Training loss = 0.01837269845285586
step = 3, Training Accuracy: 0.7432142857142857
Training loss = 0.018652825956898076
step = 4, Training Accuracy: 0.7392857142857143
Training loss = 0.018048819330121788
step = 5, Training Accuracy: 0.7432142857142857
Training loss = 0.01851929164890732
step = 6, Training Accuracy: 0.7394642857142857
Training loss = 0.01873600760740893
step = 7, Training Accuracy: 0.73625
Training loss = 0.018266485413270337
step = 8, Training Accuracy: 0.7425
Training loss = 0.018463646886604172
step = 9, Training Accuracy: 0.7383928571428572
Training loss = 0.018311281230832848
step = 10, Training Accuracy: 0.7475
Training loss = 0.01861439199852092
step = 11, Training Accuracy: 0.7360714285714286
Training loss = 0.018563261633472782
step = 12, Training Accuracy: 0.7373214285714286
Training loss = 0.018502382918127946
step = 13, Training Accuracy: 0.7373214285714286
Training loss = 0.01815953653305769
step = 14, Training Accuracy: 0.7421428571428571
Training loss = 0.01842359551361629
step = 15, Training Accuracy: 0.73875
Training loss = 0.017871359517531736
step = 16, Training Accuracy: 0.7564285714285715
Training loss = 0.018583029625671252
step = 17, Training Accuracy: 0.7355357142857143
Training loss = 0.018539214804768563
step = 18, Training Accuracy: 0.74
Training loss = 0.01851838519530637
step = 19, Training Accuracy: 0.7357142857142858
Validation Accuracy: 0.77875
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018163213740502084
step = 0, Training Accuracy: 0.7385714285714285
Validation Accuracy: 0.77625
Training loss = 0.01812634702239718
step = 1, Training Accuracy: 0.7410714285714286
Training loss = 0.018652554393879004
step = 2, Training Accuracy: 0.73875
Training loss = 0.018002371176012926
step = 3, Training Accuracy: 0.7458928571428571
Training loss = 0.01848628302237817
step = 4, Training Accuracy: 0.7408928571428571
Training loss = 0.01829090965645654
step = 5, Training Accuracy: 0.7421428571428571
Training loss = 0.018241449897842748
step = 6, Training Accuracy: 0.7367857142857143
Training loss = 0.0181642276261534
step = 7, Training Accuracy: 0.7417857142857143
Training loss = 0.018206684280719077
step = 8, Training Accuracy: 0.7417857142857143
Training loss = 0.018424442910722323
step = 9, Training Accuracy: 0.7408928571428571
Training loss = 0.018407940241907325
step = 10, Training Accuracy: 0.7428571428571429
Training loss = 0.01827888106128999
step = 11, Training Accuracy: 0.7430357142857142
Training loss = 0.018645569517144134
step = 12, Training Accuracy: 0.7369642857142857
Training loss = 0.018266260996460913
step = 13, Training Accuracy: 0.7471428571428571
Training loss = 0.01861827883337225
step = 14, Training Accuracy: 0.7351785714285715
Training loss = 0.01836120887526444
step = 15, Training Accuracy: 0.7453571428571428
Training loss = 0.018415312224200793
step = 16, Training Accuracy: 0.74
Training loss = 0.018775873663169997
step = 17, Training Accuracy: 0.73625
Training loss = 0.018493502214550973
step = 18, Training Accuracy: 0.7430357142857142
Training loss = 0.01833716588893107
step = 19, Training Accuracy: 0.7467857142857143
Validation Accuracy: 0.78
9  	7     	0.780625	0.000625   	0.78   	0.78125
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018045022029961857
step = 0, Training Accuracy: 0.7442857142857143
Validation Accuracy: 0.78125
Training loss = 0.018431908754365785
step = 1, Training Accuracy: 0.7458928571428571
Training loss = 0.018270891101232596
step = 2, Training Accuracy: 0.7405357142857143
Training loss = 0.018322684104953493
step = 3, Training Accuracy: 0.7451785714285715
Training loss = 0.01875526580427374
step = 4, Training Accuracy: 0.7253571428571428
Training loss = 0.018238870107701848
step = 5, Training Accuracy: 0.7426785714285714
Training loss = 0.01841623700090817
step = 6, Training Accuracy: 0.7396428571428572
Training loss = 0.018170882585857594
step = 7, Training Accuracy: 0.7435714285714285
Training loss = 0.01833166300186089
step = 8, Training Accuracy: 0.7421428571428571
Training loss = 0.0186676804668137
step = 9, Training Accuracy: 0.7364285714285714
Training loss = 0.018407172738973583
step = 10, Training Accuracy: 0.7405357142857143
Training loss = 0.01848203124212367
step = 11, Training Accuracy: 0.7407142857142858
Training loss = 0.01800422006951911
step = 12, Training Accuracy: 0.7410714285714286
Training loss = 0.018002853984279293
step = 13, Training Accuracy: 0.7471428571428571
Training loss = 0.018487542984741076
step = 14, Training Accuracy: 0.7373214285714286
Training loss = 0.01863610431019749
step = 15, Training Accuracy: 0.7275
Training loss = 0.01791748855795179
step = 16, Training Accuracy: 0.7507142857142857
Training loss = 0.018461118143584046
step = 17, Training Accuracy: 0.7421428571428571
Training loss = 0.018423263366733278
step = 18, Training Accuracy: 0.7408928571428571
Training loss = 0.018602876908012798
step = 19, Training Accuracy: 0.7344642857142857
Validation Accuracy: 0.77375
parameter = [0.39401600534806513, 0.5582041949061584, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.01841246940727745
step = 0, Training Accuracy: 0.7385714285714285
Validation Accuracy: 0.78375
Training loss = 0.018262776626007896
step = 1, Training Accuracy: 0.7428571428571429
Training loss = 0.01846299218279975
step = 2, Training Accuracy: 0.7401785714285715
Training loss = 0.01846720684851919
step = 3, Training Accuracy: 0.7371428571428571
Training loss = 0.018498232620103017
step = 4, Training Accuracy: 0.7364285714285714
Training loss = 0.018588430461074626
step = 5, Training Accuracy: 0.7428571428571429
Training loss = 0.01822540476386036
step = 6, Training Accuracy: 0.7419642857142857
Training loss = 0.01844641177249806
step = 7, Training Accuracy: 0.7430357142857142
Training loss = 0.0181765195195164
step = 8, Training Accuracy: 0.7471428571428571
Training loss = 0.018367865053670746
step = 9, Training Accuracy: 0.7419642857142857
Training loss = 0.01833229587546417
step = 10, Training Accuracy: 0.7460714285714286
Training loss = 0.018409580918295042
step = 11, Training Accuracy: 0.7392857142857143
Training loss = 0.018575639964214394
step = 12, Training Accuracy: 0.73875
Training loss = 0.018288336204630988
step = 13, Training Accuracy: 0.7392857142857143
Training loss = 0.01854905018849032
step = 14, Training Accuracy: 0.7491071428571429
Training loss = 0.018086393805486815
step = 15, Training Accuracy: 0.7435714285714285
Training loss = 0.018213158574487482
step = 16, Training Accuracy: 0.7494642857142857
Training loss = 0.018291552678814955
step = 17, Training Accuracy: 0.74125
Training loss = 0.018370866951133522
step = 18, Training Accuracy: 0.7457142857142857
Training loss = 0.018160984250051634
step = 19, Training Accuracy: 0.7475
Validation Accuracy: 0.77375
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018124643761132445
step = 0, Training Accuracy: 0.7541071428571429
Validation Accuracy: 0.77875
Training loss = 0.01811701785773039
step = 1, Training Accuracy: 0.7485714285714286
Training loss = 0.018635257674115044
step = 2, Training Accuracy: 0.7364285714285714
Training loss = 0.018505881460649628
step = 3, Training Accuracy: 0.73625
Training loss = 0.018275826520153454
step = 4, Training Accuracy: 0.7426785714285714
Training loss = 0.01826537630387715
step = 5, Training Accuracy: 0.7410714285714286
Training loss = 0.01837680334491389
step = 6, Training Accuracy: 0.7346428571428572
Training loss = 0.01830096564122609
step = 7, Training Accuracy: 0.7391071428571429
Training loss = 0.01837723915598222
step = 8, Training Accuracy: 0.745
Training loss = 0.018525881698088987
step = 9, Training Accuracy: 0.7353571428571428
Training loss = 0.01836983653583697
step = 10, Training Accuracy: 0.7416071428571429
Training loss = 0.018300802558660508
step = 11, Training Accuracy: 0.7419642857142857
Training loss = 0.018454061060079508
step = 12, Training Accuracy: 0.7482142857142857
Training loss = 0.018697599050189768
step = 13, Training Accuracy: 0.73375
Training loss = 0.018622792544109482
step = 14, Training Accuracy: 0.7375
Training loss = 0.018609626553952693
step = 15, Training Accuracy: 0.7421428571428571
Training loss = 0.01829768811485597
step = 16, Training Accuracy: 0.7433928571428572
Training loss = 0.01825311956661088
step = 17, Training Accuracy: 0.7442857142857143
Training loss = 0.018476645020501954
step = 18, Training Accuracy: 0.7421428571428571
Training loss = 0.018869243993290832
step = 19, Training Accuracy: 0.7364285714285714
Validation Accuracy: 0.77375
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.01831675578973123
step = 0, Training Accuracy: 0.7403571428571428
Validation Accuracy: 0.78125
Training loss = 0.01838538273636784
step = 1, Training Accuracy: 0.7439285714285714
Training loss = 0.018402879796922206
step = 2, Training Accuracy: 0.7451785714285715
Training loss = 0.018310347039784704
step = 3, Training Accuracy: 0.7448214285714285
Training loss = 0.018172781994300228
step = 4, Training Accuracy: 0.74875
Training loss = 0.0180406118663294
step = 5, Training Accuracy: 0.7442857142857143
Training loss = 0.01835664642176458
step = 6, Training Accuracy: 0.7410714285714286
Training loss = 0.018540214060672692
step = 7, Training Accuracy: 0.7367857142857143
Training loss = 0.017983014099299907
step = 8, Training Accuracy: 0.7508928571428571
Training loss = 0.018642402392412934
step = 9, Training Accuracy: 0.7380357142857142
Training loss = 0.018190554561359543
step = 10, Training Accuracy: 0.7417857142857143
Training loss = 0.018243476705891746
step = 11, Training Accuracy: 0.7410714285714286
Training loss = 0.01822147527443511
step = 12, Training Accuracy: 0.7448214285714285
Training loss = 0.018372494711407592
step = 13, Training Accuracy: 0.7419642857142857
Training loss = 0.018254337310791017
step = 14, Training Accuracy: 0.7444642857142857
Training loss = 0.018386028424969742
step = 15, Training Accuracy: 0.7426785714285714
Training loss = 0.01837843740625041
step = 16, Training Accuracy: 0.7410714285714286
Training loss = 0.018407759410994394
step = 17, Training Accuracy: 0.7417857142857143
Training loss = 0.018421444557607174
step = 18, Training Accuracy: 0.7351785714285715
Training loss = 0.018482752027256147
step = 19, Training Accuracy: 0.7373214285714286
Validation Accuracy: 0.77625
parameter = [0.39401600534806513, 0.5582041949061585, 0.40092002713537545, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018545880860516003
step = 0, Training Accuracy: 0.7375
Validation Accuracy: 0.78125
Training loss = 0.018288260347076826
step = 1, Training Accuracy: 0.74625
Training loss = 0.018332617692649364
step = 2, Training Accuracy: 0.7375
Training loss = 0.018576748913952282
step = 3, Training Accuracy: 0.7323214285714286
Training loss = 0.018519215003720352
step = 4, Training Accuracy: 0.7444642857142857
Training loss = 0.018152875905590397
step = 5, Training Accuracy: 0.7448214285714285
Training loss = 0.018342483703579222
step = 6, Training Accuracy: 0.745
Training loss = 0.018254089302250316
step = 7, Training Accuracy: 0.7446428571428572
Training loss = 0.01832804705415453
step = 8, Training Accuracy: 0.7398214285714285
Training loss = 0.01808355847639697
step = 9, Training Accuracy: 0.7473214285714286
Training loss = 0.018322805959199155
step = 10, Training Accuracy: 0.7394642857142857
Training loss = 0.018362191393971442
step = 11, Training Accuracy: 0.7457142857142857
Training loss = 0.01823893410286733
step = 12, Training Accuracy: 0.7425
Training loss = 0.018200424957488266
step = 13, Training Accuracy: 0.7428571428571429
Training loss = 0.018693039901554586
step = 14, Training Accuracy: 0.7380357142857142
Training loss = 0.018039279228874616
step = 15, Training Accuracy: 0.7448214285714285
Training loss = 0.018286868147552014
step = 16, Training Accuracy: 0.7435714285714285
Training loss = 0.018491021346833024
step = 17, Training Accuracy: 0.7344642857142857
Training loss = 0.018371660400714192
step = 18, Training Accuracy: 0.7442857142857143
Training loss = 0.018537141611533504
step = 19, Training Accuracy: 0.7410714285714286
Validation Accuracy: 0.7775
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.7022745522326239, 0.20972711586092246, 0.7225184884616984]
Training loss = 0.018551893484379564
step = 0, Training Accuracy: 0.7430357142857142
Validation Accuracy: 0.78125
Training loss = 0.01834946527012757
step = 1, Training Accuracy: 0.7442857142857143
Training loss = 0.018557318943951812
step = 2, Training Accuracy: 0.7464285714285714
Training loss = 0.018414856445576465
step = 3, Training Accuracy: 0.7453571428571428
Training loss = 0.01845940189169986
step = 4, Training Accuracy: 0.7376785714285714
Training loss = 0.018277824372053147
step = 5, Training Accuracy: 0.7366071428571429
Training loss = 0.01828982787472861
step = 6, Training Accuracy: 0.74625
Training loss = 0.018456531284110886
step = 7, Training Accuracy: 0.7339285714285714
Training loss = 0.018419957336570536
step = 8, Training Accuracy: 0.7321428571428571
Training loss = 0.01842792154422828
step = 9, Training Accuracy: 0.7403571428571428
Training loss = 0.018541433811187745
step = 10, Training Accuracy: 0.7405357142857143
Training loss = 0.018159399522202354
step = 11, Training Accuracy: 0.7410714285714286
Training loss = 0.018191218940275054
step = 12, Training Accuracy: 0.7501785714285715
Training loss = 0.01863353282213211
step = 13, Training Accuracy: 0.7367857142857143
Training loss = 0.018641057732914176
step = 14, Training Accuracy: 0.7383928571428572
Training loss = 0.01822223533477102
step = 15, Training Accuracy: 0.7485714285714286
Training loss = 0.018185812351959093
step = 16, Training Accuracy: 0.7458928571428571
Training loss = 0.01832919164161597
step = 17, Training Accuracy: 0.7335714285714285
Training loss = 0.018533270454832485
step = 18, Training Accuracy: 0.74125
Training loss = 0.018691144766552106
step = 19, Training Accuracy: 0.7342857142857143
Validation Accuracy: 0.77875
parameter = [0.3940160053480651, 0.5582041949061585, 0.40092002713537533, 0.702274552232624, 0.2097271158609224, 0.7225184884616985]
Training loss = 0.01853652880660125
step = 0, Training Accuracy: 0.7357142857142858
Validation Accuracy: 0.77625
Training loss = 0.01819228055753878
step = 1, Training Accuracy: 0.7389285714285714
Training loss = 0.018371844701468944
step = 2, Training Accuracy: 0.7421428571428571
Training loss = 0.01854441145701068
step = 3, Training Accuracy: 0.7428571428571429
Training loss = 0.018744667023420335
step = 4, Training Accuracy: 0.7276785714285714
Training loss = 0.01833595743668931
step = 5, Training Accuracy: 0.7398214285714285
Training loss = 0.01805664215236902
step = 6, Training Accuracy: 0.7455357142857143
Training loss = 0.018500762225261758
step = 7, Training Accuracy: 0.7451785714285715
Training loss = 0.018878210628671305
step = 8, Training Accuracy: 0.73875
Training loss = 0.01853357887693814
step = 9, Training Accuracy: 0.7408928571428571
Training loss = 0.018345768047230586
step = 10, Training Accuracy: 0.7433928571428572
Training loss = 0.01841556158981153
step = 11, Training Accuracy: 0.7442857142857143
Training loss = 0.01867360655750547
step = 12, Training Accuracy: 0.7383928571428572
Training loss = 0.018658013216086795
step = 13, Training Accuracy: 0.7396428571428572
Training loss = 0.01841950769403151
step = 14, Training Accuracy: 0.73625
Training loss = 0.01874998293284859
step = 15, Training Accuracy: 0.7357142857142858
Training loss = 0.018090601457016808
step = 16, Training Accuracy: 0.7491071428571429
Training loss = 0.018416417416717325
step = 17, Training Accuracy: 0.7446428571428572
Training loss = 0.01863787642014878
step = 18, Training Accuracy: 0.7394642857142857
Training loss = 0.01845940479742629
step = 19, Training Accuracy: 0.7408928571428571
Validation Accuracy: 0.7825
10 	7     	0.78    	0.00364434 	0.77375	0.7825 
parameter = [0.39401600534806513, 0.5582041949061585, 0.40092002713537533, 0.7022745522326241, 0.20972711586092246, 0.7225184884616984]
Training loss = 0.018655162890042577
step = 0, Training Accuracy: 0.7360714285714286
Validation Accuracy: 0.7775
Training loss = 0.017841127243425164
step = 1, Training Accuracy: 0.7553571428571428
Training loss = 0.018107096579458033
step = 2, Training Accuracy: 0.7466071428571428
Training loss = 0.018240757919847965
step = 3, Training Accuracy: 0.7466071428571428
Training loss = 0.0181230403855443
step = 4, Training Accuracy: 0.7389285714285714
Training loss = 0.018393934156213487
step = 5, Training Accuracy: 0.7378571428571429
Training loss = 0.018241771278636795
step = 6, Training Accuracy: 0.7471428571428571
Training loss = 0.01871432738112552
step = 7, Training Accuracy: 0.7323214285714286
Training loss = 0.018533404134213925
step = 8, Training Accuracy: 0.7391071428571429
Training loss = 0.01800801454378026
step = 9, Training Accuracy: 0.7464285714285714
Training loss = 0.018447835189955575
step = 10, Training Accuracy: 0.7367857142857143
Training loss = 0.018419747389853
step = 11, Training Accuracy: 0.7458928571428571
Training loss = 0.018302461552832806
step = 12, Training Accuracy: 0.7391071428571429
Training loss = 0.018342735448053905
step = 13, Training Accuracy: 0.7430357142857142
Training loss = 0.01824768114835024
step = 14, Training Accuracy: 0.7469642857142857
Training loss = 0.018591496364346573
step = 15, Training Accuracy: 0.7403571428571428
Training loss = 0.01842497892677784
step = 16, Training Accuracy: 0.7453571428571428
Training loss = 0.018106322724904332
step = 17, Training Accuracy: 0.7519642857142858
Training loss = 0.018566931182784692
step = 18, Training Accuracy: 0.7366071428571429
Training loss = 0.01873043006552117
step = 19, Training Accuracy: 0.7394642857142857
Validation Accuracy: 0.785
parameter = [0.39401600534806513, 0.5582041949061585, 0.4009200271353754, 0.702274552232624, 0.20972711586092244, 0.7225184884616984]
Training loss = 0.018271883938993726
step = 0, Training Accuracy: 0.74
Validation Accuracy: 0.78375
Training loss = 0.018150165719645363
step = 1, Training Accuracy: 0.7433928571428572
Training loss = 0.018164197935589724
step = 2, Training Accuracy: 0.745
Training loss = 0.01838128280426775
step = 3, Training Accuracy: 0.7433928571428572
Training loss = 0.018544222563505172
step = 4, Training Accuracy: 0.7314285714285714
Training loss = 0.018429202208561556
step = 5, Training Accuracy: 0.7408928571428571
Training loss = 0.018358758059995515
step = 6, Training Accuracy: 0.7430357142857142
Training loss = 0.018328474493963378
step = 7, Training Accuracy: 0.7408928571428571
Training loss = 0.01833411555737257
step = 8, Training Accuracy: 0.7401785714285715
Training loss = 0.01825734774981226
step = 9, Training Accuracy: 0.7476785714285714
Training loss = 0.01824631851698671
step = 10, Training Accuracy: 0.7476785714285714
Training loss = 0.018546702452003957
step = 11, Training Accuracy: 0.7401785714285715
Training loss = 0.01849250895636422
step = 12, Training Accuracy: 0.7417857142857143
Training loss = 0.01858950188649552
step = 13, Training Accuracy: 0.7371428571428571
Training loss = 0.018519014535205706
step = 14, Training Accuracy: 0.7398214285714285
Training loss = 0.018326529903071268
step = 15, Training Accuracy: 0.7430357142857142
Training loss = 0.018412192107311317
step = 16, Training Accuracy: 0.7348214285714286
Training loss = 0.018595425039529802
step = 17, Training Accuracy: 0.7421428571428571
Training loss = 0.01823056120957647
step = 18, Training Accuracy: 0.7433928571428572
Training loss = 0.01848717282393149
step = 19, Training Accuracy: 0.7383928571428572
Validation Accuracy: 0.7725
parameter = [0.39401600534806513, 0.5582041949061585, 0.40092002713537533, 0.7022745522326241, 0.2097271158609224, 0.7225184884616985]
Training loss = 0.018387336432933808
step = 0, Training Accuracy: 0.7414285714285714
Validation Accuracy: 0.7825
Training loss = 0.018189766077058654
step = 1, Training Accuracy: 0.7408928571428571
Training loss = 0.01845609930476972
step = 2, Training Accuracy: 0.7385714285714285
Training loss = 0.018446174428931303
step = 3, Training Accuracy: 0.7376785714285714
Training loss = 0.01848885125347546
step = 4, Training Accuracy: 0.7378571428571429
Training loss = 0.018469197175332477
step = 5, Training Accuracy: 0.7483928571428572
Training loss = 0.01862694044730493
step = 6, Training Accuracy: 0.7403571428571428
Training loss = 0.01844773400042738
step = 7, Training Accuracy: 0.7419642857142857
Training loss = 0.018510112134473664
step = 8, Training Accuracy: 0.7394642857142857
Training loss = 0.018240554912814073
step = 9, Training Accuracy: 0.7444642857142857
Training loss = 0.018193425605339663
step = 10, Training Accuracy: 0.7392857142857143
Training loss = 0.01843432290745633
step = 11, Training Accuracy: 0.7401785714285715
Training loss = 0.018393705412745476
step = 12, Training Accuracy: 0.7391071428571429
Training loss = 0.018578065857291222
step = 13, Training Accuracy: 0.7380357142857142
Training loss = 0.01856010243296623
step = 14, Training Accuracy: 0.7357142857142858
Training loss = 0.01832867406308651
step = 15, Training Accuracy: 0.74
Training loss = 0.018387595308678492
step = 16, Training Accuracy: 0.7357142857142858
Training loss = 0.018448913496519836
step = 17, Training Accuracy: 0.7426785714285714
Training loss = 0.0180160605268819
step = 18, Training Accuracy: 0.75375
Training loss = 0.018482300442244325
step = 19, Training Accuracy: 0.7448214285714285
Validation Accuracy: 0.7775
parameter = [0.39401600534806513, 0.5582041949061585, 0.40092002713537533, 0.702274552232624, 0.2097271158609224, 0.7225184884616984]
Training loss = 0.01845422814999308
step = 0, Training Accuracy: 0.7416071428571429
Validation Accuracy: 0.78
Training loss = 0.017997498570808342
step = 1, Training Accuracy: 0.7469642857142857
Training loss = 0.018611616492271423
step = 2, Training Accuracy: 0.7405357142857143
Training loss = 0.01846554161714656
step = 3, Training Accuracy: 0.7407142857142858
Training loss = 0.018463638584528652
step = 4, Training Accuracy: 0.7439285714285714
Training loss = 0.018504074322325842
step = 5, Training Accuracy: 0.7398214285714285
Training loss = 0.018361860290169715
step = 6, Training Accuracy: 0.7425
Training loss = 0.018256800999598845
step = 7, Training Accuracy: 0.7416071428571429
Training loss = 0.018545985599713665
step = 8, Training Accuracy: 0.74
Training loss = 0.01841755997389555
step = 9, Training Accuracy: 0.73875
Training loss = 0.018295568696090152
step = 10, Training Accuracy: 0.7389285714285714
Training loss = 0.01818823702633381
step = 11, Training Accuracy: 0.7446428571428572
Training loss = 0.01845446881971189
step = 12, Training Accuracy: 0.74375
Training loss = 0.018234652440462795
step = 13, Training Accuracy: 0.74125
Training loss = 0.01844192823661225
step = 14, Training Accuracy: 0.7375
Training loss = 0.01811216779053211
step = 15, Training Accuracy: 0.7464285714285714
Training loss = 0.018268901045833317
step = 16, Training Accuracy: 0.7430357142857142
Training loss = 0.018164339954299585
step = 17, Training Accuracy: 0.74625
Training loss = 0.018308778112488135
step = 18, Training Accuracy: 0.7451785714285715
Training loss = 0.018486516305378505
step = 19, Training Accuracy: 0.7416071428571429
Validation Accuracy: 0.78625
parameter = [0.3940160053480651, 0.5582041949061585, 0.40092002713537533, 0.702274552232624, 0.2097271158609224, 0.7225184884616984]
Training loss = 0.01812586748706443
step = 0, Training Accuracy: 0.7428571428571429
Validation Accuracy: 0.7775
Training loss = 0.018031540587544442
step = 1, Training Accuracy: 0.74875
Training loss = 0.018304926747722286
step = 2, Training Accuracy: 0.7507142857142857
Training loss = 0.018133633184645857
step = 3, Training Accuracy: 0.7433928571428572
Training loss = 0.018330752940050193
step = 4, Training Accuracy: 0.7464285714285714
Training loss = 0.018334901992763792
step = 5, Training Accuracy: 0.7426785714285714
Training loss = 0.01863534394119467
step = 6, Training Accuracy: 0.7339285714285714
Training loss = 0.01849217670836619
step = 7, Training Accuracy: 0.7430357142857142
Training loss = 0.01809820689793144
step = 8, Training Accuracy: 0.7416071428571429
Training loss = 0.01852785817214421
step = 9, Training Accuracy: 0.7403571428571428
Training loss = 0.018371624015271662
step = 10, Training Accuracy: 0.7380357142857142
Training loss = 0.018310380413063935
step = 11, Training Accuracy: 0.7482142857142857
Training loss = 0.01801807649433613
step = 12, Training Accuracy: 0.7519642857142858
Training loss = 0.01830693554665361
step = 13, Training Accuracy: 0.7410714285714286
Training loss = 0.018221460761768477
step = 14, Training Accuracy: 0.7433928571428572
Training loss = 0.018381763558302608
step = 15, Training Accuracy: 0.7383928571428572
Training loss = 0.0184383715902056
step = 16, Training Accuracy: 0.7408928571428571
Training loss = 0.018043053288544927
step = 17, Training Accuracy: 0.7480357142857142
Training loss = 0.018319644241460733
step = 18, Training Accuracy: 0.7398214285714285
Training loss = 0.01861728038106646
step = 19, Training Accuracy: 0.7380357142857142
Validation Accuracy: 0.77625
parameter = [0.39401600534806513, 0.5582041949061584, 0.40092002713537533, 0.7022745522326239, 0.20972711586092244, 0.7225184884616984]
